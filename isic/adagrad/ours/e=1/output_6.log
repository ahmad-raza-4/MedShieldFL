nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.4 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/12/2025 06:48:17:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/12/2025 06:48:17:DEBUG:ChannelConnectivity.IDLE
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739371697.349498 2074723 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
02/12/2025 06:48:17:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
02/12/2025 06:57:44:INFO:
[92mINFO [0m:      Received: train message e3c4d652-cb1d-49ae-8249-853200c2dac5
02/12/2025 06:57:44:INFO:Received: train message e3c4d652-cb1d-49ae-8249-853200c2dac5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 07:01:12:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 07:33:57:INFO:
[92mINFO [0m:      Received: evaluate message 85d9545b-bf64-4235-81e5-aa18a13d2250
02/12/2025 07:33:57:INFO:Received: evaluate message 85d9545b-bf64-4235-81e5-aa18a13d2250
[92mINFO [0m:      Sent reply
02/12/2025 07:37:59:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 07:39:09:INFO:
[92mINFO [0m:      Received: train message 3c0823e8-5a41-4e43-8920-b6ace2b9fe46
02/12/2025 07:39:09:INFO:Received: train message 3c0823e8-5a41-4e43-8920-b6ace2b9fe46
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 07:41:50:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:14:29:INFO:
[92mINFO [0m:      Received: evaluate message 547b6e2f-47b2-494b-94fb-23a2e2e2a431
02/12/2025 08:14:29:INFO:Received: evaluate message 547b6e2f-47b2-494b-94fb-23a2e2e2a431
[92mINFO [0m:      Sent reply
02/12/2025 08:18:50:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:19:20:INFO:
[92mINFO [0m:      Received: train message 830f104a-b422-4bc7-8c28-e44fb5e65128
02/12/2025 08:19:20:INFO:Received: train message 830f104a-b422-4bc7-8c28-e44fb5e65128
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 08:22:30:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:54:29:INFO:
[92mINFO [0m:      Received: evaluate message 7bf7c37c-c268-4cf8-9051-21aec2a9ab07
02/12/2025 08:54:29:INFO:Received: evaluate message 7bf7c37c-c268-4cf8-9051-21aec2a9ab07
[92mINFO [0m:      Sent reply
02/12/2025 08:58:56:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 08:59:31:INFO:
[92mINFO [0m:      Received: train message d0d3d285-a08b-4fd7-9f26-5e8ebc4ec242
02/12/2025 08:59:31:INFO:Received: train message d0d3d285-a08b-4fd7-9f26-5e8ebc4ec242
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 09:02:51:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 09:34:09:INFO:
[92mINFO [0m:      Received: evaluate message 9b29efcd-ebb6-4646-a0d7-8343c95518b1
02/12/2025 09:34:09:INFO:Received: evaluate message 9b29efcd-ebb6-4646-a0d7-8343c95518b1
[92mINFO [0m:      Sent reply
02/12/2025 09:37:58:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 09:39:13:INFO:
[92mINFO [0m:      Received: train message 3c8d8c4f-f678-4bf4-acd5-6f41c3bc575d
02/12/2025 09:39:13:INFO:Received: train message 3c8d8c4f-f678-4bf4-acd5-6f41c3bc575d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 09:42:20:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:13:57:INFO:
[92mINFO [0m:      Received: evaluate message 197ddac0-93b0-4838-b22f-bfe6139d875b
02/12/2025 10:13:57:INFO:Received: evaluate message 197ddac0-93b0-4838-b22f-bfe6139d875b
[92mINFO [0m:      Sent reply
02/12/2025 10:17:53:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:18:50:INFO:
[92mINFO [0m:      Received: train message 84b8e30a-32d8-42af-88c3-b5477f0a0138
02/12/2025 10:18:50:INFO:Received: train message 84b8e30a-32d8-42af-88c3-b5477f0a0138
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 10:21:24:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:54:13:INFO:
[92mINFO [0m:      Received: evaluate message f5f68b6e-7b53-4bf4-932c-d1d53970b3f9
02/12/2025 10:54:13:INFO:Received: evaluate message f5f68b6e-7b53-4bf4-932c-d1d53970b3f9
[92mINFO [0m:      Sent reply
02/12/2025 10:58:31:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 10:58:57:INFO:
[92mINFO [0m:      Received: train message 35d47986-da37-4be0-b6b7-712acd15b13c
02/12/2025 10:58:57:INFO:Received: train message 35d47986-da37-4be0-b6b7-712acd15b13c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 11:01:39:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 11:34:13:INFO:
[92mINFO [0m:      Received: evaluate message 3daf7710-633f-4cd8-8c7e-28a7b7753d27
02/12/2025 11:34:13:INFO:Received: evaluate message 3daf7710-633f-4cd8-8c7e-28a7b7753d27
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=1']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124], 'accuracy': [0.23076923076923078], 'auc': [0.47906768884808504], 'precision': [0.22101253138913413], 'recall': [0.23076923076923078], 'f1': [0.19973020403381153]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868], 'accuracy': [0.23076923076923078, 0.2730567861457914], 'auc': [0.47906768884808504, 0.501405220341778], 'precision': [0.22101253138913413, 0.24545349552426635], 'recall': [0.23076923076923078, 0.2730567861457914], 'f1': [0.19973020403381153, 0.21604162997075227]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/12/2025 11:38:30:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 11:38:48:INFO:
[92mINFO [0m:      Received: train message 59cdc699-e5f1-4887-9cc5-90d6f4b8efd7
02/12/2025 11:38:48:INFO:Received: train message 59cdc699-e5f1-4887-9cc5-90d6f4b8efd7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 11:41:27:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:13:33:INFO:
[92mINFO [0m:      Received: evaluate message c7a58142-47eb-498f-8b92-62f9a667dd8d
02/12/2025 12:13:33:INFO:Received: evaluate message c7a58142-47eb-498f-8b92-62f9a667dd8d
[92mINFO [0m:      Sent reply
02/12/2025 12:17:54:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:18:28:INFO:
[92mINFO [0m:      Received: train message c93d5673-d092-42e5-b441-cabc923cf1bd
02/12/2025 12:18:28:INFO:Received: train message c93d5673-d092-42e5-b441-cabc923cf1bd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 12:21:45:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:52:46:INFO:
[92mINFO [0m:      Received: evaluate message e74a428f-9204-46a9-a7d3-d948c74f4029
02/12/2025 12:52:46:INFO:Received: evaluate message e74a428f-9204-46a9-a7d3-d948c74f4029
[92mINFO [0m:      Sent reply
02/12/2025 12:57:02:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 12:57:46:INFO:
[92mINFO [0m:      Received: train message 61aedabf-12e0-45bd-84d9-18a6da406abb
02/12/2025 12:57:46:INFO:Received: train message 61aedabf-12e0-45bd-84d9-18a6da406abb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 13:01:05:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 13:32:23:INFO:
[92mINFO [0m:      Received: evaluate message 83433e40-610a-4c26-985a-c6a5986ee5d2
02/12/2025 13:32:23:INFO:Received: evaluate message 83433e40-610a-4c26-985a-c6a5986ee5d2
[92mINFO [0m:      Sent reply
02/12/2025 13:36:40:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 13:36:55:INFO:
[92mINFO [0m:      Received: train message 8835aae4-56f5-4b62-952c-532bbfa86805
02/12/2025 13:36:55:INFO:Received: train message 8835aae4-56f5-4b62-952c-532bbfa86805
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 13:39:23:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:11:05:INFO:
[92mINFO [0m:      Received: evaluate message c28b1fa9-dd3b-4ef7-931d-1eceb9e93745
02/12/2025 14:11:05:INFO:Received: evaluate message c28b1fa9-dd3b-4ef7-931d-1eceb9e93745

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/12/2025 14:14:48:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:16:01:INFO:
[92mINFO [0m:      Received: train message d5d0178e-8a76-4737-ac5f-2eeffd2af3ee
02/12/2025 14:16:01:INFO:Received: train message d5d0178e-8a76-4737-ac5f-2eeffd2af3ee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 14:19:15:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:49:58:INFO:
[92mINFO [0m:      Received: evaluate message bead6c69-dee5-412e-81af-ddc314331b2b
02/12/2025 14:49:58:INFO:Received: evaluate message bead6c69-dee5-412e-81af-ddc314331b2b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 14:54:11:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 14:55:21:INFO:
[92mINFO [0m:      Received: train message 6f802634-92ad-4297-94ee-617112e161a4
02/12/2025 14:55:21:INFO:Received: train message 6f802634-92ad-4297-94ee-617112e161a4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 14:58:35:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 15:29:12:INFO:
[92mINFO [0m:      Received: evaluate message 7dc81c61-9438-4333-b8a6-f37928e6738f
02/12/2025 15:29:12:INFO:Received: evaluate message 7dc81c61-9438-4333-b8a6-f37928e6738f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 15:33:35:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 15:34:06:INFO:
[92mINFO [0m:      Received: train message 4f0891ef-2532-42e6-86a9-d4f5f3488397
02/12/2025 15:34:06:INFO:Received: train message 4f0891ef-2532-42e6-86a9-d4f5f3488397
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 15:37:40:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:08:30:INFO:
[92mINFO [0m:      Received: evaluate message b4364b31-02ab-462b-b341-06d6400f8f71
02/12/2025 16:08:30:INFO:Received: evaluate message b4364b31-02ab-462b-b341-06d6400f8f71

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 16:12:57:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:13:21:INFO:
[92mINFO [0m:      Received: train message c8e59e20-50d0-4163-86d8-beade49c61fd
02/12/2025 16:13:21:INFO:Received: train message c8e59e20-50d0-4163-86d8-beade49c61fd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 16:16:44:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:47:50:INFO:
[92mINFO [0m:      Received: evaluate message f36441e0-509e-4002-a9c8-e315f60d2527
02/12/2025 16:47:50:INFO:Received: evaluate message f36441e0-509e-4002-a9c8-e315f60d2527
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 16:52:20:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 16:53:02:INFO:
[92mINFO [0m:      Received: train message cdefc216-6529-4bf0-b995-4f8d0fd1d81a
02/12/2025 16:53:02:INFO:Received: train message cdefc216-6529-4bf0-b995-4f8d0fd1d81a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 16:56:25:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 17:26:31:INFO:
[92mINFO [0m:      Received: evaluate message d88f2ba6-3853-4ece-89c9-852a2244cd35
02/12/2025 17:26:31:INFO:Received: evaluate message d88f2ba6-3853-4ece-89c9-852a2244cd35
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 17:30:59:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 17:31:37:INFO:
[92mINFO [0m:      Received: train message ca5b6c51-6867-4ff1-b482-fbc7e3857c94
02/12/2025 17:31:37:INFO:Received: train message ca5b6c51-6867-4ff1-b482-fbc7e3857c94
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 17:35:00:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:05:17:INFO:
[92mINFO [0m:      Received: evaluate message a944458b-242b-4d1b-a419-c4c575d56c02
02/12/2025 18:05:17:INFO:Received: evaluate message a944458b-242b-4d1b-a419-c4c575d56c02

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 18:10:00:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:10:19:INFO:
[92mINFO [0m:      Received: train message 643c1a65-e415-4761-a037-9bdc0c634b45
02/12/2025 18:10:19:INFO:Received: train message 643c1a65-e415-4761-a037-9bdc0c634b45
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 18:13:20:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:43:34:INFO:
[92mINFO [0m:      Received: evaluate message eb4a4fed-341a-4442-9641-7ae20139c462
02/12/2025 18:43:34:INFO:Received: evaluate message eb4a4fed-341a-4442-9641-7ae20139c462
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 18:47:39:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 18:48:40:INFO:
[92mINFO [0m:      Received: train message 06cc84b9-feee-407f-b6d9-10232073b27d
02/12/2025 18:48:40:INFO:Received: train message 06cc84b9-feee-407f-b6d9-10232073b27d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 18:51:50:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 19:22:15:INFO:
[92mINFO [0m:      Received: evaluate message 48d31309-0ad5-4d00-9486-1cf7a1ae11c1
02/12/2025 19:22:15:INFO:Received: evaluate message 48d31309-0ad5-4d00-9486-1cf7a1ae11c1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 19:26:32:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 19:27:23:INFO:
[92mINFO [0m:      Received: train message 8d1bf6e3-0ed2-4fd3-82e7-00adfa7d49e5
02/12/2025 19:27:23:INFO:Received: train message 8d1bf6e3-0ed2-4fd3-82e7-00adfa7d49e5

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 19:30:45:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:01:12:INFO:
[92mINFO [0m:      Received: evaluate message 8423397e-dfd5-4577-8824-c74063fcba5a
02/12/2025 20:01:12:INFO:Received: evaluate message 8423397e-dfd5-4577-8824-c74063fcba5a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 20:05:32:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:06:19:INFO:
[92mINFO [0m:      Received: train message bfd5bed8-b8a6-48ae-8eec-89010926aa4b
02/12/2025 20:06:19:INFO:Received: train message bfd5bed8-b8a6-48ae-8eec-89010926aa4b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 20:09:34:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:40:11:INFO:
[92mINFO [0m:      Received: evaluate message facd6e6b-134e-4c88-a293-c2039094d871
02/12/2025 20:40:11:INFO:Received: evaluate message facd6e6b-134e-4c88-a293-c2039094d871
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 20:44:28:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 20:44:51:INFO:
[92mINFO [0m:      Received: train message 4657ac30-584e-46dd-a1e2-70d5721ce04c
02/12/2025 20:44:51:INFO:Received: train message 4657ac30-584e-46dd-a1e2-70d5721ce04c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 20:47:59:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 21:19:39:INFO:
[92mINFO [0m:      Received: evaluate message 2b542f63-233a-4c10-ace5-96b8919bba87
02/12/2025 21:19:39:INFO:Received: evaluate message 2b542f63-233a-4c10-ace5-96b8919bba87
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 21:23:58:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 21:24:36:INFO:
[92mINFO [0m:      Received: train message c0b2567f-1750-4873-90db-2b53c69dbb9d
02/12/2025 21:24:36:INFO:Received: train message c0b2567f-1750-4873-90db-2b53c69dbb9d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 21:27:44:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:01:58:INFO:
[92mINFO [0m:      Received: evaluate message 55b4de0e-bf30-4dc6-96ed-731260f1f68a
02/12/2025 22:01:58:INFO:Received: evaluate message 55b4de0e-bf30-4dc6-96ed-731260f1f68a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 22:06:42:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:07:03:INFO:
[92mINFO [0m:      Received: train message aedc0ce1-941e-4d92-9670-7ada9fd32057
02/12/2025 22:07:03:INFO:Received: train message aedc0ce1-941e-4d92-9670-7ada9fd32057
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 22:10:14:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:44:21:INFO:
[92mINFO [0m:      Received: evaluate message 021c7760-4d95-417c-a29b-5fcfd30e7e27
02/12/2025 22:44:21:INFO:Received: evaluate message 021c7760-4d95-417c-a29b-5fcfd30e7e27

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 22:48:51:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 22:49:12:INFO:
[92mINFO [0m:      Received: train message 54bc88c6-2167-4766-bcac-ad9df38e12be
02/12/2025 22:49:12:INFO:Received: train message 54bc88c6-2167-4766-bcac-ad9df38e12be
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 22:52:21:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 23:24:24:INFO:
[92mINFO [0m:      Received: evaluate message a9c51a7f-0524-4d99-a519-772d635bad63
02/12/2025 23:24:24:INFO:Received: evaluate message a9c51a7f-0524-4d99-a519-772d635bad63
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 23:28:42:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 23:29:19:INFO:
[92mINFO [0m:      Received: train message 4aebf8b2-0df0-42e1-8db1-1ab87212c5e4
02/12/2025 23:29:19:INFO:Received: train message 4aebf8b2-0df0-42e1-8db1-1ab87212c5e4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 23:32:30:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:04:46:INFO:
[92mINFO [0m:      Received: evaluate message 0f5618f9-d760-43f5-95d6-261e4b812e9b
02/13/2025 00:04:46:INFO:Received: evaluate message 0f5618f9-d760-43f5-95d6-261e4b812e9b

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 00:09:00:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:09:21:INFO:
[92mINFO [0m:      Received: train message 5779123f-1c76-4c04-88c9-869c3ee311c1
02/13/2025 00:09:21:INFO:Received: train message 5779123f-1c76-4c04-88c9-869c3ee311c1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 00:11:45:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:44:29:INFO:
[92mINFO [0m:      Received: evaluate message 3c1c619e-2e95-42e2-9653-7b3f12e1bc75
02/13/2025 00:44:29:INFO:Received: evaluate message 3c1c619e-2e95-42e2-9653-7b3f12e1bc75
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 00:48:39:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 00:49:14:INFO:
[92mINFO [0m:      Received: train message b40c8ecf-0fbf-49c5-ae32-2a9768e2f19b
02/13/2025 00:49:14:INFO:Received: train message b40c8ecf-0fbf-49c5-ae32-2a9768e2f19b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 00:52:24:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 01:24:44:INFO:
[92mINFO [0m:      Received: evaluate message 872d4398-fb49-460d-8f37-e3c2f5e7c1fe
02/13/2025 01:24:44:INFO:Received: evaluate message 872d4398-fb49-460d-8f37-e3c2f5e7c1fe

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 01:28:52:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 01:29:29:INFO:
[92mINFO [0m:      Received: train message 87a57356-7f23-4571-98f7-3ddb0b31d9de
02/13/2025 01:29:29:INFO:Received: train message 87a57356-7f23-4571-98f7-3ddb0b31d9de
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 01:32:43:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:01:08:INFO:
[92mINFO [0m:      Received: evaluate message 33b5322d-14dd-45d9-bc06-d22d6a60728f
02/13/2025 02:01:08:INFO:Received: evaluate message 33b5322d-14dd-45d9-bc06-d22d6a60728f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 02:05:21:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:05:42:INFO:
[92mINFO [0m:      Received: train message 5acc8baa-f9f1-4e54-bc34-b21af28c96ac
02/13/2025 02:05:42:INFO:Received: train message 5acc8baa-f9f1-4e54-bc34-b21af28c96ac

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566, 1.586201695919613], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137, 0.6918262726394175], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595, 0.44779088340065876], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447, 0.32103453513458136]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/13/2025 02:08:47:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:35:57:INFO:
[92mINFO [0m:      Received: evaluate message b8586420-3ff7-45bc-95e0-551924864570
02/13/2025 02:35:57:INFO:Received: evaluate message b8586420-3ff7-45bc-95e0-551924864570
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/13/2025 02:39:23:INFO:Sent reply
[92mINFO [0m:      
02/13/2025 02:40:22:INFO:
[92mINFO [0m:      Received: reconnect message 19b86af1-a160-489c-9357-21bac60cb5ad
02/13/2025 02:40:22:INFO:Received: reconnect message 19b86af1-a160-489c-9357-21bac60cb5ad
02/13/2025 02:40:22:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/13/2025 02:40:22:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566, 1.586201695919613, 1.577787735411298], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532, 0.4111961337092227], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137, 0.6918262726394175, 0.6979438649202241], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595, 0.44779088340065876, 0.448109658694975], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532, 0.4111961337092227], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447, 0.32103453513458136, 0.32303410546437183]}



Final client history:
{'loss': [2.0173472301450124, 1.9692220573121868, 1.9331589776375964, 1.9018368994666355, 1.8732899690509754, 1.8499746561722559, 1.8275922128763402, 1.8075958928224591, 1.7909520976830526, 1.7757831192573519, 1.7605823942407983, 1.7468264180712292, 1.7348096517608187, 1.7222676387103968, 1.7105655573665544, 1.6999053474976273, 1.690211929575935, 1.6804169813471843, 1.6707135195027327, 1.6608813995802254, 1.6522977231636768, 1.6439504018964857, 1.63568381642892, 1.6273481504023195, 1.6192431298177952, 1.610962968798256, 1.60289161065102, 1.594999874641566, 1.586201695919613, 1.577787735411298], 'accuracy': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532, 0.4111961337092227], 'auc': [0.47906768884808504, 0.501405220341778, 0.5177397217311858, 0.5330781139385186, 0.5470287954932171, 0.5584185044941312, 0.5692275422011274, 0.5782793989051829, 0.586225312277157, 0.5931915861091579, 0.5996846073463692, 0.6062443990187549, 0.6119252815275078, 0.6177653275258115, 0.6230818469993102, 0.6288185825158552, 0.6334282249597593, 0.6385435914202361, 0.6430719162524935, 0.6480502897894215, 0.6527522730403362, 0.6571732332791853, 0.6618339438132055, 0.6665791998315672, 0.6713983389190645, 0.6765262141689584, 0.6810600259300666, 0.6861095616406137, 0.6918262726394175, 0.6979438649202241], 'precision': [0.22101253138913413, 0.24545349552426635, 0.2631498125486612, 0.27743548331483986, 0.28287036139210436, 0.28935976464171664, 0.31504053514526503, 0.33064385906597277, 0.33775301305831534, 0.35694695142630395, 0.3589369071996388, 0.36438727861154696, 0.3734681410987087, 0.37411085006299855, 0.3751744889433332, 0.3756022779893182, 0.3791765574336262, 0.3862127161087948, 0.3903895334206116, 0.39666414217120216, 0.40373219701899044, 0.409866061809784, 0.4137377306156404, 0.4171311748403305, 0.42537838776104797, 0.4365305248961834, 0.43831746857881404, 0.4450257202276595, 0.44779088340065876, 0.448109658694975], 'recall': [0.23076923076923078, 0.2730567861457914, 0.29440193314538865, 0.3101087394281112, 0.3205799436165928, 0.3270237615787354, 0.33668948852194924, 0.34313330648409185, 0.34756343133306483, 0.35400724929520744, 0.35521546516310915, 0.3576318968989126, 0.3612565445026178, 0.36286749899315346, 0.36367297623842126, 0.36568666935159083, 0.3685058397100282, 0.3709222714458317, 0.3729359645590012, 0.3769633507853403, 0.3805879983890455, 0.38461538461538464, 0.38703181635118805, 0.3886427708417237, 0.39347563431333066, 0.3991139750302054, 0.4019331453886428, 0.40596053161498186, 0.4091824405960532, 0.4111961337092227], 'f1': [0.19973020403381153, 0.21604162997075227, 0.2213377135883438, 0.22457840142643623, 0.22635365395493948, 0.22760526213587484, 0.23208687160753827, 0.23864536283262872, 0.24112500307520635, 0.24674907673320814, 0.24632076907571412, 0.2475509906670644, 0.25114768261816794, 0.2530221862215474, 0.2529723913605707, 0.2560330852678076, 0.2609068678943832, 0.26405729714706, 0.2661634621458126, 0.2735716903018957, 0.27780742458695823, 0.28361985312879817, 0.28832715396226133, 0.29135913836471455, 0.29833454284703187, 0.3068299598080815, 0.31066734075623365, 0.31614205910008447, 0.32103453513458136, 0.32303410546437183]}

