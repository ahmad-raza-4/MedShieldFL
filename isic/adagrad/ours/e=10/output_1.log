nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/11/2025 11:38:41:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/11/2025 11:38:41:DEBUG:ChannelConnectivity.IDLE
02/11/2025 11:38:41:DEBUG:ChannelConnectivity.CONNECTING
02/11/2025 11:38:41:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739302721.772924 1491856 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/11/2025 11:39:02:INFO:
[92mINFO [0m:      Received: train message 6bb05f4a-a09e-48a8-9035-a773e4a6d670
02/11/2025 11:39:02:INFO:Received: train message 6bb05f4a-a09e-48a8-9035-a773e4a6d670
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 12:08:53:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:09:33:INFO:
[92mINFO [0m:      Received: evaluate message 37ed5e0d-9505-4205-a7e2-0a28cd9287a4
02/11/2025 12:09:33:INFO:Received: evaluate message 37ed5e0d-9505-4205-a7e2-0a28cd9287a4
[92mINFO [0m:      Sent reply
02/11/2025 12:13:33:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:14:00:INFO:
[92mINFO [0m:      Received: train message 30537f78-43c4-43c2-8fd0-a4616b365944
02/11/2025 12:14:00:INFO:Received: train message 30537f78-43c4-43c2-8fd0-a4616b365944
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 12:43:33:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:43:57:INFO:
[92mINFO [0m:      Received: evaluate message c14dc956-5c10-4689-9032-24f0bdb7c7c8
02/11/2025 12:43:57:INFO:Received: evaluate message c14dc956-5c10-4689-9032-24f0bdb7c7c8
[92mINFO [0m:      Sent reply
02/11/2025 12:47:50:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 12:48:29:INFO:
[92mINFO [0m:      Received: train message 71f8ea92-d18a-4546-af40-3effc0329b74
02/11/2025 12:48:29:INFO:Received: train message 71f8ea92-d18a-4546-af40-3effc0329b74
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 13:21:49:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:22:18:INFO:
[92mINFO [0m:      Received: evaluate message 301ec183-530d-4cd1-9f5f-1570fbaed158
02/11/2025 13:22:18:INFO:Received: evaluate message 301ec183-530d-4cd1-9f5f-1570fbaed158
[92mINFO [0m:      Sent reply
02/11/2025 13:26:26:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:27:04:INFO:
[92mINFO [0m:      Received: train message 2aca5c59-1258-4167-a0d8-da8a2fff74cd
02/11/2025 13:27:04:INFO:Received: train message 2aca5c59-1258-4167-a0d8-da8a2fff74cd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 13:56:17:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 13:56:48:INFO:
[92mINFO [0m:      Received: evaluate message 84d9106f-b20c-4477-978e-681e8fc6240a
02/11/2025 13:56:48:INFO:Received: evaluate message 84d9106f-b20c-4477-978e-681e8fc6240a
[92mINFO [0m:      Sent reply
02/11/2025 14:00:54:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:01:32:INFO:
[92mINFO [0m:      Received: train message b1e55c59-e85a-4534-a395-76af78fbe7f4
02/11/2025 14:01:32:INFO:Received: train message b1e55c59-e85a-4534-a395-76af78fbe7f4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 14:30:56:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:31:51:INFO:
[92mINFO [0m:      Received: evaluate message 46f97930-f382-4df8-b9f6-936426b04ecd
02/11/2025 14:31:51:INFO:Received: evaluate message 46f97930-f382-4df8-b9f6-936426b04ecd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 14:36:03:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 14:36:37:INFO:
[92mINFO [0m:      Received: train message 0e347549-7f53-4a16-9df5-be225a3500d1
02/11/2025 14:36:37:INFO:Received: train message 0e347549-7f53-4a16-9df5-be225a3500d1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 15:06:05:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:06:39:INFO:
[92mINFO [0m:      Received: evaluate message 7b42b9c3-1c8d-4178-8ef6-524ee2f816c2
02/11/2025 15:06:39:INFO:Received: evaluate message 7b42b9c3-1c8d-4178-8ef6-524ee2f816c2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 15:10:43:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:11:15:INFO:
[92mINFO [0m:      Received: train message 7ecfe814-59cd-4d2c-8831-771897567bc0
02/11/2025 15:11:15:INFO:Received: train message 7ecfe814-59cd-4d2c-8831-771897567bc0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 15:40:38:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:41:16:INFO:
[92mINFO [0m:      Received: evaluate message 78190271-5b3f-4bad-a736-b26d326841ed
02/11/2025 15:41:16:INFO:Received: evaluate message 78190271-5b3f-4bad-a736-b26d326841ed
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adagrad/e=10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192], 'accuracy': [0.25654450261780104], 'auc': [0.5097526219963764], 'precision': [0.25488184534737934], 'recall': [0.25654450261780104], 'f1': [0.19218762720861446]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877], 'accuracy': [0.25654450261780104, 0.29883205799436163], 'auc': [0.5097526219963764, 0.5400268636526322], 'precision': [0.25488184534737934, 0.31012842353432646], 'recall': [0.25654450261780104, 0.29883205799436163], 'f1': [0.19218762720861446, 0.20880157085225062]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 15:45:11:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 15:45:33:INFO:
[92mINFO [0m:      Received: train message f2a86952-85e2-408a-9e44-ab6b2cf7e2b1
02/11/2025 15:45:33:INFO:Received: train message f2a86952-85e2-408a-9e44-ab6b2cf7e2b1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 16:15:01:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:15:38:INFO:
[92mINFO [0m:      Received: evaluate message cf61a51d-e771-4b5c-a13d-007917479250
02/11/2025 16:15:38:INFO:Received: evaluate message cf61a51d-e771-4b5c-a13d-007917479250
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 16:19:47:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:20:22:INFO:
[92mINFO [0m:      Received: train message b65c8143-567b-433a-903d-f33ce2516ef7
02/11/2025 16:20:22:INFO:Received: train message b65c8143-567b-433a-903d-f33ce2516ef7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 16:52:51:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:53:30:INFO:
[92mINFO [0m:      Received: evaluate message 002e2557-f07c-4ec5-a8d0-c920e2a2d1bc
02/11/2025 16:53:30:INFO:Received: evaluate message 002e2557-f07c-4ec5-a8d0-c920e2a2d1bc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 16:57:29:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 16:57:41:INFO:
[92mINFO [0m:      Received: train message 0b6e82e8-b1a0-44c1-b465-8b582425cde8
02/11/2025 16:57:41:INFO:Received: train message 0b6e82e8-b1a0-44c1-b465-8b582425cde8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 17:26:04:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 17:26:31:INFO:
[92mINFO [0m:      Received: evaluate message 7be07332-9ec2-4176-87b1-3e3661de6f19
02/11/2025 17:26:31:INFO:Received: evaluate message 7be07332-9ec2-4176-87b1-3e3661de6f19
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 17:30:33:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 17:31:17:INFO:
[92mINFO [0m:      Received: train message 3c31c408-3fc1-4b48-9061-9e3f5c82f7fa
02/11/2025 17:31:17:INFO:Received: train message 3c31c408-3fc1-4b48-9061-9e3f5c82f7fa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 18:00:26:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:00:56:INFO:
[92mINFO [0m:      Received: evaluate message d554d12f-aadb-41ff-a2b8-96e802fe2efb
02/11/2025 18:00:56:INFO:Received: evaluate message d554d12f-aadb-41ff-a2b8-96e802fe2efb

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 18:04:57:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:05:24:INFO:
[92mINFO [0m:      Received: train message b547bb15-2704-446b-81a2-36deefe78c2a
02/11/2025 18:05:24:INFO:Received: train message b547bb15-2704-446b-81a2-36deefe78c2a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 18:33:37:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:34:12:INFO:
[92mINFO [0m:      Received: evaluate message 95e835b5-cb2b-42cd-83c1-c923af45f434
02/11/2025 18:34:12:INFO:Received: evaluate message 95e835b5-cb2b-42cd-83c1-c923af45f434
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 18:38:15:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 18:38:43:INFO:
[92mINFO [0m:      Received: train message a23add55-49fc-4848-94db-174a2354a228
02/11/2025 18:38:43:INFO:Received: train message a23add55-49fc-4848-94db-174a2354a228
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 19:09:31:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:09:51:INFO:
[92mINFO [0m:      Received: evaluate message b637e116-0247-4538-99ef-0682350b4e54
02/11/2025 19:09:51:INFO:Received: evaluate message b637e116-0247-4538-99ef-0682350b4e54
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 19:13:05:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:14:41:INFO:
[92mINFO [0m:      Received: train message 780b830e-0fb4-4e50-a7e3-9e530376339d
02/11/2025 19:14:41:INFO:Received: train message 780b830e-0fb4-4e50-a7e3-9e530376339d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 19:43:35:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:44:12:INFO:
[92mINFO [0m:      Received: evaluate message fbd9d774-1699-4407-a2e2-0ede3282e352
02/11/2025 19:44:12:INFO:Received: evaluate message fbd9d774-1699-4407-a2e2-0ede3282e352

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715]}

Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 19:48:12:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 19:48:45:INFO:
[92mINFO [0m:      Received: train message 0e627b12-6763-42c5-bb77-96783121d3b2
02/11/2025 19:48:45:INFO:Received: train message 0e627b12-6763-42c5-bb77-96783121d3b2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 20:19:16:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:19:40:INFO:
[92mINFO [0m:      Received: evaluate message 20659c7b-e0fa-4a5c-a5f1-74acd40620a7
02/11/2025 20:19:40:INFO:Received: evaluate message 20659c7b-e0fa-4a5c-a5f1-74acd40620a7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 20:23:16:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:24:15:INFO:
[92mINFO [0m:      Received: train message b4cd4d35-f4b1-4622-bc1e-214e6964e630
02/11/2025 20:24:15:INFO:Received: train message b4cd4d35-f4b1-4622-bc1e-214e6964e630
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 20:52:41:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:53:05:INFO:
[92mINFO [0m:      Received: evaluate message 89d5e35b-74b0-437e-8fa6-b5bd83ddedca
02/11/2025 20:53:05:INFO:Received: evaluate message 89d5e35b-74b0-437e-8fa6-b5bd83ddedca
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 20:56:55:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 20:57:34:INFO:
[92mINFO [0m:      Received: train message d2b22929-cc58-41be-a0db-2373f90bbba7
02/11/2025 20:57:34:INFO:Received: train message d2b22929-cc58-41be-a0db-2373f90bbba7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 21:28:27:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 21:29:15:INFO:
[92mINFO [0m:      Received: evaluate message c2803dbc-68ec-417f-89d2-45c29662b208
02/11/2025 21:29:15:INFO:Received: evaluate message c2803dbc-68ec-417f-89d2-45c29662b208

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 21:33:22:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 21:33:54:INFO:
[92mINFO [0m:      Received: train message 6e99e1fd-b2a7-4dfb-84d9-008d1af25f09
02/11/2025 21:33:54:INFO:Received: train message 6e99e1fd-b2a7-4dfb-84d9-008d1af25f09
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 22:02:38:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:03:05:INFO:
[92mINFO [0m:      Received: evaluate message 2d2e13b8-f6bc-402a-9cd5-3dc9423728b3
02/11/2025 22:03:05:INFO:Received: evaluate message 2d2e13b8-f6bc-402a-9cd5-3dc9423728b3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 22:07:09:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:07:48:INFO:
[92mINFO [0m:      Received: train message 6868b481-430b-48a3-bb6f-f729e5066480
02/11/2025 22:07:48:INFO:Received: train message 6868b481-430b-48a3-bb6f-f729e5066480
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 22:38:26:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:39:04:INFO:
[92mINFO [0m:      Received: evaluate message e1d14d5f-7999-4811-b3a1-701e5bd67376
02/11/2025 22:39:04:INFO:Received: evaluate message e1d14d5f-7999-4811-b3a1-701e5bd67376
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 22:42:59:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 22:43:34:INFO:
[92mINFO [0m:      Received: train message c658a18d-ce7b-4330-84ba-6f2e731175ad
02/11/2025 22:43:34:INFO:Received: train message c658a18d-ce7b-4330-84ba-6f2e731175ad

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 23:11:57:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:12:23:INFO:
[92mINFO [0m:      Received: evaluate message 97489b81-9e3a-4419-8878-5c5389fd1f53
02/11/2025 23:12:23:INFO:Received: evaluate message 97489b81-9e3a-4419-8878-5c5389fd1f53
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 23:16:22:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:17:01:INFO:
[92mINFO [0m:      Received: train message 7e6cf82d-4eaa-42c7-a7d8-08c949b545de
02/11/2025 23:17:01:INFO:Received: train message 7e6cf82d-4eaa-42c7-a7d8-08c949b545de
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/11/2025 23:46:24:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:47:00:INFO:
[92mINFO [0m:      Received: evaluate message 23ff1933-97a9-44d4-a4fe-74d76d9744d8
02/11/2025 23:47:00:INFO:Received: evaluate message 23ff1933-97a9-44d4-a4fe-74d76d9744d8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/11/2025 23:51:11:INFO:Sent reply
[92mINFO [0m:      
02/11/2025 23:51:41:INFO:
[92mINFO [0m:      Received: train message 5698bdf8-55e5-41ed-99b6-0bcfcee1ae79
02/11/2025 23:51:41:INFO:Received: train message 5698bdf8-55e5-41ed-99b6-0bcfcee1ae79
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 00:21:30:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:21:44:INFO:
[92mINFO [0m:      Received: evaluate message 8010a82b-1c56-4c92-b9a1-1d7cab1fcf4e
02/12/2025 00:21:44:INFO:Received: evaluate message 8010a82b-1c56-4c92-b9a1-1d7cab1fcf4e
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 00:24:43:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:26:38:INFO:
[92mINFO [0m:      Received: train message b41c2412-0a31-4fed-bdf8-f7dd72e5bb27
02/12/2025 00:26:38:INFO:Received: train message b41c2412-0a31-4fed-bdf8-f7dd72e5bb27
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 00:56:15:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 00:56:53:INFO:
[92mINFO [0m:      Received: evaluate message b1d1522b-3631-4525-bc66-861e4c4f4ed3
02/12/2025 00:56:53:INFO:Received: evaluate message b1d1522b-3631-4525-bc66-861e4c4f4ed3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 01:00:45:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:00:59:INFO:
[92mINFO [0m:      Received: train message 3125ecaa-b1fb-441c-aa5f-174d3b24c5e0
02/12/2025 01:00:59:INFO:Received: train message 3125ecaa-b1fb-441c-aa5f-174d3b24c5e0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 01:29:57:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:30:25:INFO:
[92mINFO [0m:      Received: evaluate message a233cefc-93d8-4622-b34e-17dfe7d40c8d
02/12/2025 01:30:25:INFO:Received: evaluate message a233cefc-93d8-4622-b34e-17dfe7d40c8d

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 01:34:21:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 01:35:07:INFO:
[92mINFO [0m:      Received: train message 59f37c38-895e-45c7-8a4d-9eadebb3e7b3
02/12/2025 01:35:07:INFO:Received: train message 59f37c38-895e-45c7-8a4d-9eadebb3e7b3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 02:03:26:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:04:02:INFO:
[92mINFO [0m:      Received: evaluate message fd5723a2-c444-4688-b981-3b261dc8413c
02/12/2025 02:04:02:INFO:Received: evaluate message fd5723a2-c444-4688-b981-3b261dc8413c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 02:08:07:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:08:17:INFO:
[92mINFO [0m:      Received: train message 2e7a22fb-a718-4a36-975a-035de46ea056
02/12/2025 02:08:17:INFO:Received: train message 2e7a22fb-a718-4a36-975a-035de46ea056
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 02:38:55:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:39:43:INFO:
[92mINFO [0m:      Received: evaluate message af645958-251a-42fd-8506-c1b6ad6f9d2b
02/12/2025 02:39:43:INFO:Received: evaluate message af645958-251a-42fd-8506-c1b6ad6f9d2b

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 02:43:48:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 02:44:12:INFO:
[92mINFO [0m:      Received: train message 34e40246-8107-4e9a-8d57-90473d6e8899
02/12/2025 02:44:12:INFO:Received: train message 34e40246-8107-4e9a-8d57-90473d6e8899
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 03:12:50:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:13:20:INFO:
[92mINFO [0m:      Received: evaluate message 536b32ab-a175-4830-b27d-2328ad764a51
02/12/2025 03:13:20:INFO:Received: evaluate message 536b32ab-a175-4830-b27d-2328ad764a51
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 03:17:30:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:18:04:INFO:
[92mINFO [0m:      Received: train message 59e6f142-0393-4f0e-8088-6b5946553e4b
02/12/2025 03:18:04:INFO:Received: train message 59e6f142-0393-4f0e-8088-6b5946553e4b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 03:47:19:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:47:53:INFO:
[92mINFO [0m:      Received: evaluate message 0fff6392-0c53-4849-944f-bd86ab7d73a3
02/12/2025 03:47:53:INFO:Received: evaluate message 0fff6392-0c53-4849-944f-bd86ab7d73a3

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 03:52:11:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 03:52:46:INFO:
[92mINFO [0m:      Received: train message 4484404a-0aea-4c9e-9a91-675406f67c1c
02/12/2025 03:52:46:INFO:Received: train message 4484404a-0aea-4c9e-9a91-675406f67c1c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 04:24:40:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 04:25:08:INFO:
[92mINFO [0m:      Received: evaluate message 4b6417cb-af04-45ef-8c9c-1f1bcd74867d
02/12/2025 04:25:08:INFO:Received: evaluate message 4b6417cb-af04-45ef-8c9c-1f1bcd74867d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 04:28:56:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 04:29:51:INFO:
[92mINFO [0m:      Received: train message ac4e52da-cca6-4ea1-aa50-e5acd11694ba
02/12/2025 04:29:51:INFO:Received: train message ac4e52da-cca6-4ea1-aa50-e5acd11694ba

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/12/2025 05:01:13:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 05:01:50:INFO:
[92mINFO [0m:      Received: evaluate message 6b68753e-13cb-4fcf-af2e-1403cea5dfa9
02/12/2025 05:01:50:INFO:Received: evaluate message 6b68753e-13cb-4fcf-af2e-1403cea5dfa9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/12/2025 05:05:54:INFO:Sent reply
[92mINFO [0m:      
02/12/2025 05:05:54:INFO:
[92mINFO [0m:      Received: reconnect message 0cf6e1dd-0b0f-4d5f-8da8-518f05cdb140
02/12/2025 05:05:54:INFO:Received: reconnect message 0cf6e1dd-0b0f-4d5f-8da8-518f05cdb140
02/12/2025 05:05:54:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/12/2025 05:05:54:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972, 1.5554112734162802], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845, 0.7592226722304614], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026, 0.4915718981258468], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098, 0.33214967061441997]}



Final client history:
{'loss': [2.036031061826192, 1.981395454306877, 1.939764735058244, 1.9064510066423788, 1.875968724117133, 1.849598651493071, 1.8258984195000214, 1.8036534529369874, 1.7827254533287598, 1.7658085937803425, 1.7487531566735097, 1.733521282888928, 1.7191492897629785, 1.705060879069377, 1.6920306080058847, 1.6800016942544662, 1.6681843541871624, 1.6564685214300383, 1.6458035973930283, 1.636080804871687, 1.6272131464820305, 1.6181067660408925, 1.609158669562768, 1.6012264389306174, 1.5924123911621395, 1.5849327492800347, 1.5768285225919307, 1.570052482310774, 1.5624736956996972, 1.5554112734162802], 'accuracy': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'auc': [0.5097526219963764, 0.5400268636526322, 0.5638312260798812, 0.5836383814480943, 0.6000766660712902, 0.6155294148498756, 0.628583824666378, 0.6394677843075485, 0.6492091480528263, 0.6575832526580466, 0.6664262634285236, 0.6739198560343704, 0.6807552130310778, 0.6865459595901796, 0.6919479912889411, 0.6973626391552714, 0.7024598903389732, 0.7074405218090025, 0.7122176252156023, 0.7170978022850238, 0.7215792654412574, 0.7256361396826667, 0.7294186655637883, 0.7332968853449526, 0.7375824684554882, 0.7418382109898753, 0.746146820602791, 0.750229025814285, 0.7548385345101845, 0.7592226722304614], 'precision': [0.25488184534737934, 0.31012842353432646, 0.35154533374168384, 0.3762154412917688, 0.3781923652455598, 0.3910069884360289, 0.4006972243508654, 0.42325156571431183, 0.41402414269848964, 0.43645442742739443, 0.44605771076295964, 0.45564609346923035, 0.45377648530455017, 0.4609125499882643, 0.45446322527640987, 0.4554481838437277, 0.45412843963593474, 0.45905522799700427, 0.4602239634628819, 0.4621037953041488, 0.47223313664421834, 0.4726037256613511, 0.47705100140628115, 0.47912071045299004, 0.4806735848275344, 0.4878484550089377, 0.49348417581265097, 0.4896953329155652, 0.4922153427747026, 0.4915718981258468], 'recall': [0.25654450261780104, 0.29883205799436163, 0.31614981876761983, 0.3282319774466371, 0.33709222714458315, 0.3419250906161901, 0.3495771244462344, 0.3572291582762787, 0.3600483286347161, 0.36689488521949254, 0.3729359645590012, 0.3789770438985099, 0.38219895287958117, 0.38582360048328634, 0.38622633910592025, 0.3894482480869915, 0.39428111155859846, 0.39750302053966974, 0.4003221908981071, 0.4015304067660089, 0.40555779299234795, 0.4067660088602497, 0.4095851792186871, 0.4115988723318566, 0.41320982682239227, 0.41643173580346354, 0.4204591220298027, 0.4204591220298027, 0.4224728151429722, 0.4216673378977044], 'f1': [0.19218762720861446, 0.20880157085225062, 0.21360422103594487, 0.21798026042115545, 0.22307476684442026, 0.22585199322274613, 0.23102619133571567, 0.24081870198965466, 0.24335372295022117, 0.2539603844307945, 0.25976799140801715, 0.26925592493399914, 0.27260557730015395, 0.2775383483394011, 0.27797469314360845, 0.28285485714130487, 0.29024552803845133, 0.2941004741428168, 0.29836607179966473, 0.3006170908615403, 0.3070006894399559, 0.3089002820806132, 0.3133437200837893, 0.31654673711213727, 0.3193173825439073, 0.3243260555742572, 0.3298319407721134, 0.33000907755766645, 0.3333449595270098, 0.33214967061441997]}

