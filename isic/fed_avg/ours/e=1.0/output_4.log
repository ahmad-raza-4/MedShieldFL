nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/07/2025 20:08:10:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/07/2025 20:08:10:DEBUG:ChannelConnectivity.IDLE
02/07/2025 20:08:10:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738987690.239823 1674356 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/07/2025 20:16:47:INFO:
[92mINFO [0m:      Received: train message 3d827bec-de3d-4844-96d7-833fbbf8a529
02/07/2025 20:16:47:INFO:Received: train message 3d827bec-de3d-4844-96d7-833fbbf8a529
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 20:30:36:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 20:57:35:INFO:
[92mINFO [0m:      Received: evaluate message fafaf836-d41b-4f86-833e-53e0218c0a16
02/07/2025 20:57:35:INFO:Received: evaluate message fafaf836-d41b-4f86-833e-53e0218c0a16
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:02:00:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:02:57:INFO:
[92mINFO [0m:      Received: train message eecbf5cd-889e-41b8-ab27-abcc30346f6c
02/07/2025 21:02:57:INFO:Received: train message eecbf5cd-889e-41b8-ab27-abcc30346f6c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 21:15:48:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:44:02:INFO:
[92mINFO [0m:      Received: evaluate message e8ad6881-ac7f-4844-bfa3-bfdad28bb0c6
02/07/2025 21:44:02:INFO:Received: evaluate message e8ad6881-ac7f-4844-bfa3-bfdad28bb0c6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:49:12:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:49:48:INFO:
[92mINFO [0m:      Received: train message 86199779-1fcf-4f15-ad9d-c50a26dcb7f7
02/07/2025 21:49:48:INFO:Received: train message 86199779-1fcf-4f15-ad9d-c50a26dcb7f7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 22:03:22:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:28:58:INFO:
[92mINFO [0m:      Received: evaluate message 6dcc9b82-b9cf-4636-ab1e-702b6cd14f90
02/07/2025 22:28:58:INFO:Received: evaluate message 6dcc9b82-b9cf-4636-ab1e-702b6cd14f90
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 22:34:42:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:35:07:INFO:
[92mINFO [0m:      Received: train message 35d3ce2c-789e-46c4-bb16-76b253940ad6
02/07/2025 22:35:07:INFO:Received: train message 35d3ce2c-789e-46c4-bb16-76b253940ad6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 22:52:07:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:18:18:INFO:
[92mINFO [0m:      Received: evaluate message 6f149b01-a491-4e64-90ec-c2bb0080f859
02/07/2025 23:18:18:INFO:Received: evaluate message 6f149b01-a491-4e64-90ec-c2bb0080f859
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 23:23:05:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:23:44:INFO:
[92mINFO [0m:      Received: train message aa1d5574-ddcb-4340-9341-728943bd3101
02/07/2025 23:23:44:INFO:Received: train message aa1d5574-ddcb-4340-9341-728943bd3101
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 23:36:39:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:05:18:INFO:
[92mINFO [0m:      Received: evaluate message aba184cc-658b-4252-a1fc-2846e9054282
02/08/2025 00:05:18:INFO:Received: evaluate message aba184cc-658b-4252-a1fc-2846e9054282
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:10:11:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:10:33:INFO:
[92mINFO [0m:      Received: train message bda3cb6e-5422-46d4-9407-661065515085
02/08/2025 00:10:33:INFO:Received: train message bda3cb6e-5422-46d4-9407-661065515085
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 00:23:49:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:52:11:INFO:
[92mINFO [0m:      Received: evaluate message 127add73-06ca-49c0-a3a9-ce85da4d11c9
02/08/2025 00:52:11:INFO:Received: evaluate message 127add73-06ca-49c0-a3a9-ce85da4d11c9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:57:28:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:58:06:INFO:
[92mINFO [0m:      Received: train message ebad3630-974d-4e31-a2fa-f109c7f4ff38
02/08/2025 00:58:06:INFO:Received: train message ebad3630-974d-4e31-a2fa-f109c7f4ff38
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 01:12:29:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:38:06:INFO:
[92mINFO [0m:      Received: evaluate message c02426e7-56ab-4530-be12-8b4854b67d1f
02/08/2025 01:38:06:INFO:Received: evaluate message c02426e7-56ab-4530-be12-8b4854b67d1f
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794], 'accuracy': [0.47643979057591623], 'auc': [0.7113553522460114], 'precision': [0.4601909378305515], 'recall': [0.47643979057591623], 'f1': [0.39597907648163966]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782], 'accuracy': [0.47643979057591623, 0.5501409585179219], 'auc': [0.7113553522460114, 0.7558628759678585], 'precision': [0.4601909378305515, 0.4857540893933654], 'recall': [0.47643979057591623, 0.5501409585179219], 'f1': [0.39597907648163966, 0.4846537523434621]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 01:42:43:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:43:37:INFO:
[92mINFO [0m:      Received: train message 029be7a7-72ce-4cc2-872d-10b355f2c3fb
02/08/2025 01:43:37:INFO:Received: train message 029be7a7-72ce-4cc2-872d-10b355f2c3fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 01:58:42:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:24:33:INFO:
[92mINFO [0m:      Received: evaluate message 9447b33e-a712-4fdf-a2df-45e594eff7ab
02/08/2025 02:24:33:INFO:Received: evaluate message 9447b33e-a712-4fdf-a2df-45e594eff7ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 02:29:11:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:29:59:INFO:
[92mINFO [0m:      Received: train message cd55b944-47a9-4014-8da6-c48ee32c46a6
02/08/2025 02:29:59:INFO:Received: train message cd55b944-47a9-4014-8da6-c48ee32c46a6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 02:43:24:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:12:02:INFO:
[92mINFO [0m:      Received: evaluate message 97368448-7882-4c52-a388-0870b12f4566
02/08/2025 03:12:02:INFO:Received: evaluate message 97368448-7882-4c52-a388-0870b12f4566
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 03:17:07:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:17:32:INFO:
[92mINFO [0m:      Received: train message ba2859a9-9840-4659-90c3-5ed4f72275b4
02/08/2025 03:17:32:INFO:Received: train message ba2859a9-9840-4659-90c3-5ed4f72275b4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 03:30:38:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:58:05:INFO:
[92mINFO [0m:      Received: evaluate message f8ff4f34-ac60-45f9-8dad-36afda0d9b82
02/08/2025 03:58:05:INFO:Received: evaluate message f8ff4f34-ac60-45f9-8dad-36afda0d9b82
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 04:03:25:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:03:52:INFO:
[92mINFO [0m:      Received: train message c6997563-c23c-4883-a0e2-011535dc39f2
02/08/2025 04:03:52:INFO:Received: train message c6997563-c23c-4883-a0e2-011535dc39f2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 04:18:21:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:43:18:INFO:
[92mINFO [0m:      Received: evaluate message b562c380-82cb-452d-8d61-b36044f8dcb7
02/08/2025 04:43:18:INFO:Received: evaluate message b562c380-82cb-452d-8d61-b36044f8dcb7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 04:48:01:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:48:26:INFO:
[92mINFO [0m:      Received: train message 9905317e-1569-4b24-9da9-68da21502bad
02/08/2025 04:48:26:INFO:Received: train message 9905317e-1569-4b24-9da9-68da21502bad

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 05:02:16:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:30:03:INFO:
[92mINFO [0m:      Received: evaluate message f619e54a-5c5d-4714-a6a1-f95a7954c258
02/08/2025 05:30:03:INFO:Received: evaluate message f619e54a-5c5d-4714-a6a1-f95a7954c258
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 05:34:44:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:35:04:INFO:
[92mINFO [0m:      Received: train message adf9a3aa-eafd-447e-9de0-8f358c709f34
02/08/2025 05:35:04:INFO:Received: train message adf9a3aa-eafd-447e-9de0-8f358c709f34
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 05:48:50:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:16:43:INFO:
[92mINFO [0m:      Received: evaluate message 0338ddfe-e240-42b9-a13a-576363dac5df
02/08/2025 06:16:43:INFO:Received: evaluate message 0338ddfe-e240-42b9-a13a-576363dac5df
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 06:21:36:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:21:49:INFO:
[92mINFO [0m:      Received: train message 025a2bac-d8e1-401c-bddd-a24782319d4c
02/08/2025 06:21:49:INFO:Received: train message 025a2bac-d8e1-401c-bddd-a24782319d4c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 06:35:59:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:01:38:INFO:
[92mINFO [0m:      Received: evaluate message 90cf50c7-aa69-4a05-a4f9-53b36155c775
02/08/2025 07:01:38:INFO:Received: evaluate message 90cf50c7-aa69-4a05-a4f9-53b36155c775
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 07:06:23:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:07:16:INFO:
[92mINFO [0m:      Received: train message 26140b96-b9df-4420-9597-6ad1344d4e0a
02/08/2025 07:07:16:INFO:Received: train message 26140b96-b9df-4420-9597-6ad1344d4e0a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 07:21:48:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:48:05:INFO:
[92mINFO [0m:      Received: evaluate message a32086ae-e32f-44a6-973e-192b96fb39ec
02/08/2025 07:48:05:INFO:Received: evaluate message a32086ae-e32f-44a6-973e-192b96fb39ec
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 07:52:30:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:53:26:INFO:
[92mINFO [0m:      Received: train message 2f9c5273-1722-4397-b5a4-78fbf734dcd4
02/08/2025 07:53:26:INFO:Received: train message 2f9c5273-1722-4397-b5a4-78fbf734dcd4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 08:06:50:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:35:26:INFO:
[92mINFO [0m:      Received: evaluate message 3762bc57-d1f8-494d-920e-331cfc703e1b
02/08/2025 08:35:26:INFO:Received: evaluate message 3762bc57-d1f8-494d-920e-331cfc703e1b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 08:40:14:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:40:43:INFO:
[92mINFO [0m:      Received: train message 4e2c1b7b-0475-4874-b11b-538595e1306d
02/08/2025 08:40:43:INFO:Received: train message 4e2c1b7b-0475-4874-b11b-538595e1306d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 08:53:48:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:20:08:INFO:
[92mINFO [0m:      Received: evaluate message 18130e11-5c9a-4585-8fee-0cb5c701b56a
02/08/2025 09:20:08:INFO:Received: evaluate message 18130e11-5c9a-4585-8fee-0cb5c701b56a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 09:25:11:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:25:55:INFO:
[92mINFO [0m:      Received: train message 4b2967f3-db44-4e63-96bc-2717adba3921
02/08/2025 09:25:55:INFO:Received: train message 4b2967f3-db44-4e63-96bc-2717adba3921
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 09:40:15:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:04:52:INFO:
[92mINFO [0m:      Received: evaluate message 4ccda9ab-3d37-4b3b-8b7b-3ce13882112b
02/08/2025 10:04:52:INFO:Received: evaluate message 4ccda9ab-3d37-4b3b-8b7b-3ce13882112b

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 10:09:28:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:09:55:INFO:
[92mINFO [0m:      Received: train message 75f718bf-0521-4bb2-9270-6896a75e6798
02/08/2025 10:09:55:INFO:Received: train message 75f718bf-0521-4bb2-9270-6896a75e6798
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 10:24:08:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:50:12:INFO:
[92mINFO [0m:      Received: evaluate message e4c935d7-8670-4005-baf3-41391788e34d
02/08/2025 10:50:12:INFO:Received: evaluate message e4c935d7-8670-4005-baf3-41391788e34d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 10:54:57:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:55:26:INFO:
[92mINFO [0m:      Received: train message eaea7dc7-2f38-4b52-a6e3-348289b310ac
02/08/2025 10:55:26:INFO:Received: train message eaea7dc7-2f38-4b52-a6e3-348289b310ac
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 11:09:16:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:34:55:INFO:
[92mINFO [0m:      Received: evaluate message 606d4efa-e0ba-43bc-ba49-c7f089c02d67
02/08/2025 11:34:55:INFO:Received: evaluate message 606d4efa-e0ba-43bc-ba49-c7f089c02d67

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 11:40:08:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:40:45:INFO:
[92mINFO [0m:      Received: train message 62e38a0c-0104-4601-91a1-dffb4fb8933e
02/08/2025 11:40:45:INFO:Received: train message 62e38a0c-0104-4601-91a1-dffb4fb8933e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 11:55:01:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:19:06:INFO:
[92mINFO [0m:      Received: evaluate message 85719530-d2fe-438b-bb00-b890d19e44c8
02/08/2025 12:19:06:INFO:Received: evaluate message 85719530-d2fe-438b-bb00-b890d19e44c8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 12:23:46:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:24:24:INFO:
[92mINFO [0m:      Received: train message 6f1c6ac8-6f9b-48cb-a892-db1bb2249985
02/08/2025 12:24:24:INFO:Received: train message 6f1c6ac8-6f9b-48cb-a892-db1bb2249985
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 12:39:03:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:05:10:INFO:
[92mINFO [0m:      Received: evaluate message 9e86360e-26db-4a0c-9b8b-bdd545962efd
02/08/2025 13:05:10:INFO:Received: evaluate message 9e86360e-26db-4a0c-9b8b-bdd545962efd

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 13:10:19:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:11:02:INFO:
[92mINFO [0m:      Received: train message 692d7c03-0d31-4f2c-bbcf-301f95914b03
02/08/2025 13:11:02:INFO:Received: train message 692d7c03-0d31-4f2c-bbcf-301f95914b03
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 13:24:38:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:51:54:INFO:
[92mINFO [0m:      Received: evaluate message bd807f3d-93ef-4198-9c91-d8a10c971101
02/08/2025 13:51:54:INFO:Received: evaluate message bd807f3d-93ef-4198-9c91-d8a10c971101
[92mINFO [0m:      Sent reply
02/08/2025 13:56:46:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:57:43:INFO:
[92mINFO [0m:      Received: train message fcadccad-6c2c-49f2-940f-be76703898d7
02/08/2025 13:57:43:INFO:Received: train message fcadccad-6c2c-49f2-940f-be76703898d7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 14:11:52:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:38:40:INFO:
[92mINFO [0m:      Received: evaluate message a2fee760-0931-4823-92d2-fb2d0ebd1f9f
02/08/2025 14:38:40:INFO:Received: evaluate message a2fee760-0931-4823-92d2-fb2d0ebd1f9f

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 14:43:38:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:44:22:INFO:
[92mINFO [0m:      Received: train message f7235d03-59b1-48d4-863c-2f71d03f3bf0
02/08/2025 14:44:22:INFO:Received: train message f7235d03-59b1-48d4-863c-2f71d03f3bf0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 14:57:59:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:24:11:INFO:
[92mINFO [0m:      Received: evaluate message 356f7d4c-4dcd-474c-a1cd-29393d54405b
02/08/2025 15:24:11:INFO:Received: evaluate message 356f7d4c-4dcd-474c-a1cd-29393d54405b
[92mINFO [0m:      Sent reply
02/08/2025 15:29:14:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:29:41:INFO:
[92mINFO [0m:      Received: train message 555d8d20-f8f3-4c22-8cb0-4c0ba47b207b
02/08/2025 15:29:41:INFO:Received: train message 555d8d20-f8f3-4c22-8cb0-4c0ba47b207b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 15:44:28:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:10:14:INFO:
[92mINFO [0m:      Received: evaluate message 92589a5e-1da9-4251-8a9a-4789b5592e27
02/08/2025 16:10:14:INFO:Received: evaluate message 92589a5e-1da9-4251-8a9a-4789b5592e27

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 16:14:57:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:15:25:INFO:
[92mINFO [0m:      Received: train message 966b89cb-55d8-471f-88f8-fc063d9127f5
02/08/2025 16:15:25:INFO:Received: train message 966b89cb-55d8-471f-88f8-fc063d9127f5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 16:30:31:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:56:18:INFO:
[92mINFO [0m:      Received: evaluate message 2b62ac4f-2935-4ec7-8e5f-3ebd57e76966
02/08/2025 16:56:18:INFO:Received: evaluate message 2b62ac4f-2935-4ec7-8e5f-3ebd57e76966
[92mINFO [0m:      Sent reply
02/08/2025 17:01:05:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:01:39:INFO:
[92mINFO [0m:      Received: train message 1b4fe0c8-adcd-48dc-bf85-4f6f3e8b275a
02/08/2025 17:01:39:INFO:Received: train message 1b4fe0c8-adcd-48dc-bf85-4f6f3e8b275a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 17:17:15:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:41:47:INFO:
[92mINFO [0m:      Received: evaluate message 6f597766-8bb1-47c1-abc3-2d1cab7cda93
02/08/2025 17:41:47:INFO:Received: evaluate message 6f597766-8bb1-47c1-abc3-2d1cab7cda93

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 17:46:39:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:47:05:INFO:
[92mINFO [0m:      Received: train message adfbf317-b6a9-4bc9-ae29-2e014865e2d6
02/08/2025 17:47:05:INFO:Received: train message adfbf317-b6a9-4bc9-ae29-2e014865e2d6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:02:30:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:27:44:INFO:
[92mINFO [0m:      Received: evaluate message acac351c-d091-4d0f-b795-fcf8c8f1ea08
02/08/2025 18:27:44:INFO:Received: evaluate message acac351c-d091-4d0f-b795-fcf8c8f1ea08
[92mINFO [0m:      Sent reply
02/08/2025 18:32:28:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:32:45:INFO:
[92mINFO [0m:      Received: train message d3343192-68d6-4f15-92de-239bb2b6984d
02/08/2025 18:32:45:INFO:Received: train message d3343192-68d6-4f15-92de-239bb2b6984d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:47:29:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:14:20:INFO:
[92mINFO [0m:      Received: evaluate message 58769c21-aa1b-4c7c-8d51-fbe5bbd74c03
02/08/2025 19:14:20:INFO:Received: evaluate message 58769c21-aa1b-4c7c-8d51-fbe5bbd74c03

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 19:18:48:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:18:49:INFO:
[92mINFO [0m:      Received: reconnect message 0d8fc89c-4cfa-4cc1-bc45-79ff42b058e2
02/08/2025 19:18:49:INFO:Received: reconnect message 0d8fc89c-4cfa-4cc1-bc45-79ff42b058e2
02/08/2025 19:18:49:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/08/2025 19:18:49:INFO:Disconnect and shut down

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712, 1.3589135335666058], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445, 0.8710482896241687], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899, 0.6162404554774186], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089, 0.6075857344583624]}



Final client history:
{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712, 1.3589135335666058], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445, 0.8710482896241687], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899, 0.6162404554774186], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089, 0.6075857344583624]}

