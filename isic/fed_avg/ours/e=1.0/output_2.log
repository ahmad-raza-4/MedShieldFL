nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/07/2025 20:11:09:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/07/2025 20:11:09:DEBUG:ChannelConnectivity.IDLE
02/07/2025 20:11:09:DEBUG:ChannelConnectivity.CONNECTING
02/07/2025 20:11:09:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
02/07/2025 20:16:34:INFO:
[92mINFO [0m:      Received: train message 0d8fc771-b1e4-41a2-800a-aa83bc64e6bd
02/07/2025 20:16:34:INFO:Received: train message 0d8fc771-b1e4-41a2-800a-aa83bc64e6bd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 20:38:34:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 20:57:49:INFO:
[92mINFO [0m:      Received: evaluate message 521adc37-b43d-45e2-a89b-5b5f903ebbd4
02/07/2025 20:57:49:INFO:Received: evaluate message 521adc37-b43d-45e2-a89b-5b5f903ebbd4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:02:32:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:03:17:INFO:
[92mINFO [0m:      Received: train message d738a354-1bca-4389-afef-b629b6c46021
02/07/2025 21:03:17:INFO:Received: train message d738a354-1bca-4389-afef-b629b6c46021
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 21:23:11:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:44:08:INFO:
[92mINFO [0m:      Received: evaluate message 293a40bf-d33a-425f-9869-2c91db0f9460
02/07/2025 21:44:08:INFO:Received: evaluate message 293a40bf-d33a-425f-9869-2c91db0f9460
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:48:54:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:49:53:INFO:
[92mINFO [0m:      Received: train message 87985b90-4a82-49b6-b7fc-a2b8617c2bdf
02/07/2025 21:49:53:INFO:Received: train message 87985b90-4a82-49b6-b7fc-a2b8617c2bdf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 22:10:07:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:29:02:INFO:
[92mINFO [0m:      Received: evaluate message 3dcc48b9-a505-43ba-97e1-a3c1c29dce35
02/07/2025 22:29:02:INFO:Received: evaluate message 3dcc48b9-a505-43ba-97e1-a3c1c29dce35
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 22:34:27:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:34:57:INFO:
[92mINFO [0m:      Received: train message 970b2610-c9fc-4114-9b10-4727293c487c
02/07/2025 22:34:57:INFO:Received: train message 970b2610-c9fc-4114-9b10-4727293c487c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 22:59:33:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:18:16:INFO:
[92mINFO [0m:      Received: evaluate message 32241c7a-9291-4fa2-bb1f-1f949812a826
02/07/2025 23:18:16:INFO:Received: evaluate message 32241c7a-9291-4fa2-bb1f-1f949812a826
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 23:23:07:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:23:50:INFO:
[92mINFO [0m:      Received: train message 6feea54d-e87b-4297-87bb-617d09f30f71
02/07/2025 23:23:50:INFO:Received: train message 6feea54d-e87b-4297-87bb-617d09f30f71
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 23:44:19:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:05:15:INFO:
[92mINFO [0m:      Received: evaluate message d2b28901-345d-43ac-8359-44f7d6b8f7c2
02/08/2025 00:05:15:INFO:Received: evaluate message d2b28901-345d-43ac-8359-44f7d6b8f7c2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:10:07:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:10:47:INFO:
[92mINFO [0m:      Received: train message 9c3d60c8-422e-47dc-b47c-3c04992084fd
02/08/2025 00:10:47:INFO:Received: train message 9c3d60c8-422e-47dc-b47c-3c04992084fd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 00:30:27:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:52:10:INFO:
[92mINFO [0m:      Received: evaluate message 469ed2df-04e7-4bb0-88b8-26f718ab8a3d
02/08/2025 00:52:10:INFO:Received: evaluate message 469ed2df-04e7-4bb0-88b8-26f718ab8a3d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:57:06:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:57:55:INFO:
[92mINFO [0m:      Received: train message 88c33528-45c9-46da-b515-d00461dd481e
02/08/2025 00:57:55:INFO:Received: train message 88c33528-45c9-46da-b515-d00461dd481e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 01:18:04:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:38:14:INFO:
[92mINFO [0m:      Received: evaluate message 9a623446-3269-49fe-bcda-7e0b8b02d823
02/08/2025 01:38:14:INFO:Received: evaluate message 9a623446-3269-49fe-bcda-7e0b8b02d823
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794], 'accuracy': [0.47643979057591623], 'auc': [0.7113553522460114], 'precision': [0.4601909378305515], 'recall': [0.47643979057591623], 'f1': [0.39597907648163966]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782], 'accuracy': [0.47643979057591623, 0.5501409585179219], 'auc': [0.7113553522460114, 0.7558628759678585], 'precision': [0.4601909378305515, 0.4857540893933654], 'recall': [0.47643979057591623, 0.5501409585179219], 'f1': [0.39597907648163966, 0.4846537523434621]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 01:42:52:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:43:27:INFO:
[92mINFO [0m:      Received: train message f474722b-d303-4680-a843-88a1d7ad3f62
02/08/2025 01:43:27:INFO:Received: train message f474722b-d303-4680-a843-88a1d7ad3f62
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 02:04:43:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:24:47:INFO:
[92mINFO [0m:      Received: evaluate message 93cdfaac-19a0-40fd-83b9-6c24c9b674fe
02/08/2025 02:24:47:INFO:Received: evaluate message 93cdfaac-19a0-40fd-83b9-6c24c9b674fe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 02:29:26:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:30:00:INFO:
[92mINFO [0m:      Received: train message f0e24931-f083-46df-b757-da5397473bd0
02/08/2025 02:30:00:INFO:Received: train message f0e24931-f083-46df-b757-da5397473bd0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 02:49:33:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:11:57:INFO:
[92mINFO [0m:      Received: evaluate message 76c79f0e-d47e-4cf7-915e-7002bbea930d
02/08/2025 03:11:57:INFO:Received: evaluate message 76c79f0e-d47e-4cf7-915e-7002bbea930d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 03:16:40:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:17:44:INFO:
[92mINFO [0m:      Received: train message d11c28ed-1bb0-4464-b67c-d5ff397ac39e
02/08/2025 03:17:44:INFO:Received: train message d11c28ed-1bb0-4464-b67c-d5ff397ac39e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 03:36:50:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:58:13:INFO:
[92mINFO [0m:      Received: evaluate message d3f4afc7-d4a2-4b75-b532-91ce3f839e80
02/08/2025 03:58:13:INFO:Received: evaluate message d3f4afc7-d4a2-4b75-b532-91ce3f839e80
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 04:03:16:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:04:00:INFO:
[92mINFO [0m:      Received: train message 4f8403dd-1df3-4dc4-9ab5-a95d0f0a5cbf
02/08/2025 04:04:00:INFO:Received: train message 4f8403dd-1df3-4dc4-9ab5-a95d0f0a5cbf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 04:23:32:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:43:14:INFO:
[92mINFO [0m:      Received: evaluate message 01ba8cb1-0d89-4a7b-8f6c-3d9025b3de4a
02/08/2025 04:43:14:INFO:Received: evaluate message 01ba8cb1-0d89-4a7b-8f6c-3d9025b3de4a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 04:47:50:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:48:23:INFO:
[92mINFO [0m:      Received: train message 0ae04b3b-ad9e-42df-baa6-777552686bb4
02/08/2025 04:48:23:INFO:Received: train message 0ae04b3b-ad9e-42df-baa6-777552686bb4

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 05:08:31:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:30:01:INFO:
[92mINFO [0m:      Received: evaluate message 4dc73f8f-b194-41c1-8c81-8867cc06d73e
02/08/2025 05:30:01:INFO:Received: evaluate message 4dc73f8f-b194-41c1-8c81-8867cc06d73e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 05:34:39:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:35:17:INFO:
[92mINFO [0m:      Received: train message b96cfdfc-6c3a-41fa-97a6-cbcbd3d06476
02/08/2025 05:35:17:INFO:Received: train message b96cfdfc-6c3a-41fa-97a6-cbcbd3d06476
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 05:54:47:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:16:23:INFO:
[92mINFO [0m:      Received: evaluate message 6bc27034-b4db-45e9-bc2a-c3a7949306f1
02/08/2025 06:16:23:INFO:Received: evaluate message 6bc27034-b4db-45e9-bc2a-c3a7949306f1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 06:20:44:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:22:13:INFO:
[92mINFO [0m:      Received: train message 6a46ac7c-e48c-4263-aba1-5959fdaf5395
02/08/2025 06:22:13:INFO:Received: train message 6a46ac7c-e48c-4263-aba1-5959fdaf5395
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 06:41:42:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:01:57:INFO:
[92mINFO [0m:      Received: evaluate message d974d4f8-9504-408b-a94b-0f630dca371f
02/08/2025 07:01:57:INFO:Received: evaluate message d974d4f8-9504-408b-a94b-0f630dca371f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 07:06:34:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:07:32:INFO:
[92mINFO [0m:      Received: train message 3640582b-d967-434c-9a83-1e71133dbf69
02/08/2025 07:07:32:INFO:Received: train message 3640582b-d967-434c-9a83-1e71133dbf69
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 07:28:14:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:48:21:INFO:
[92mINFO [0m:      Received: evaluate message df347aa7-5850-43c6-bbcc-8f57b41d16ff
02/08/2025 07:48:21:INFO:Received: evaluate message df347aa7-5850-43c6-bbcc-8f57b41d16ff
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 07:52:48:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:53:03:INFO:
[92mINFO [0m:      Received: train message 9d21f5fd-329d-4669-8bbc-daac53f72fde
02/08/2025 07:53:03:INFO:Received: train message 9d21f5fd-329d-4669-8bbc-daac53f72fde
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 08:12:57:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:35:39:INFO:
[92mINFO [0m:      Received: evaluate message b785afcc-9db3-49dd-815f-95715cb33ba8
02/08/2025 08:35:39:INFO:Received: evaluate message b785afcc-9db3-49dd-815f-95715cb33ba8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 08:40:15:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:40:47:INFO:
[92mINFO [0m:      Received: train message 8f55f95c-aa28-4b3c-a82a-1fdd04be7235
02/08/2025 08:40:47:INFO:Received: train message 8f55f95c-aa28-4b3c-a82a-1fdd04be7235
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 08:59:51:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:19:58:INFO:
[92mINFO [0m:      Received: evaluate message 1683bf50-a18c-45d4-9a80-63f44681173f
02/08/2025 09:19:58:INFO:Received: evaluate message 1683bf50-a18c-45d4-9a80-63f44681173f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 09:24:26:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:25:44:INFO:
[92mINFO [0m:      Received: train message 95ebcb0a-9512-42b0-8500-f01115d821f3
02/08/2025 09:25:44:INFO:Received: train message 95ebcb0a-9512-42b0-8500-f01115d821f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 09:45:54:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:05:00:INFO:
[92mINFO [0m:      Received: evaluate message 6ed55ace-37fc-4f56-9342-8b09f6d07a96
02/08/2025 10:05:00:INFO:Received: evaluate message 6ed55ace-37fc-4f56-9342-8b09f6d07a96

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 10:09:36:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:09:58:INFO:
[92mINFO [0m:      Received: train message 7d029af9-d981-40ac-9139-8a2fc14a81cf
02/08/2025 10:09:58:INFO:Received: train message 7d029af9-d981-40ac-9139-8a2fc14a81cf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 10:30:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:49:51:INFO:
[92mINFO [0m:      Received: evaluate message 3fee193b-b39d-44af-84c9-2d7d9982faa8
02/08/2025 10:49:51:INFO:Received: evaluate message 3fee193b-b39d-44af-84c9-2d7d9982faa8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 10:54:20:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:55:32:INFO:
[92mINFO [0m:      Received: train message 68c562cf-cf2b-4198-b1ae-faf5bc899690
02/08/2025 10:55:32:INFO:Received: train message 68c562cf-cf2b-4198-b1ae-faf5bc899690
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 11:14:59:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:34:36:INFO:
[92mINFO [0m:      Received: evaluate message cac0ad06-61e4-49a7-bc51-37979fc21d41
02/08/2025 11:34:36:INFO:Received: evaluate message cac0ad06-61e4-49a7-bc51-37979fc21d41

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 11:39:41:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:40:49:INFO:
[92mINFO [0m:      Received: train message 7c48dd1e-4d3a-41fd-a118-9b888c94a8fe
02/08/2025 11:40:49:INFO:Received: train message 7c48dd1e-4d3a-41fd-a118-9b888c94a8fe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 12:00:38:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:18:48:INFO:
[92mINFO [0m:      Received: evaluate message d62c2381-1266-47a7-9647-3098f27899cd
02/08/2025 12:18:48:INFO:Received: evaluate message d62c2381-1266-47a7-9647-3098f27899cd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 12:23:31:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:24:19:INFO:
[92mINFO [0m:      Received: train message 73e5dae9-f0c2-4b00-84a8-88e54c510f59
02/08/2025 12:24:19:INFO:Received: train message 73e5dae9-f0c2-4b00-84a8-88e54c510f59
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 12:45:58:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:05:14:INFO:
[92mINFO [0m:      Received: evaluate message fa2f5407-9ddf-4cee-96ea-617b1a709d3d
02/08/2025 13:05:14:INFO:Received: evaluate message fa2f5407-9ddf-4cee-96ea-617b1a709d3d

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 13:10:11:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:10:37:INFO:
[92mINFO [0m:      Received: train message 99c48659-fc5d-4614-b93b-e8a18c12ce8a
02/08/2025 13:10:37:INFO:Received: train message 99c48659-fc5d-4614-b93b-e8a18c12ce8a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 13:31:16:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:52:14:INFO:
[92mINFO [0m:      Received: evaluate message 04dee2ee-3664-431b-9e06-d0c4549a1dfb
02/08/2025 13:52:14:INFO:Received: evaluate message 04dee2ee-3664-431b-9e06-d0c4549a1dfb
[92mINFO [0m:      Sent reply
02/08/2025 13:56:51:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:57:24:INFO:
[92mINFO [0m:      Received: train message 62a7e0a5-e975-4cf8-9d8a-699cea17a586
02/08/2025 13:57:24:INFO:Received: train message 62a7e0a5-e975-4cf8-9d8a-699cea17a586
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 14:17:36:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:38:30:INFO:
[92mINFO [0m:      Received: evaluate message ad8e5ff8-0b91-4ec1-b79b-0c98e7bd150d
02/08/2025 14:38:30:INFO:Received: evaluate message ad8e5ff8-0b91-4ec1-b79b-0c98e7bd150d

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 14:42:55:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:44:12:INFO:
[92mINFO [0m:      Received: train message 7f8e7aeb-063e-4dfa-a145-645a92a10ed1
02/08/2025 14:44:12:INFO:Received: train message 7f8e7aeb-063e-4dfa-a145-645a92a10ed1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 15:04:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:24:15:INFO:
[92mINFO [0m:      Received: evaluate message 27a15b2f-ae72-4f75-99a8-22a7ee70dea2
02/08/2025 15:24:15:INFO:Received: evaluate message 27a15b2f-ae72-4f75-99a8-22a7ee70dea2
[92mINFO [0m:      Sent reply
02/08/2025 15:28:51:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:29:50:INFO:
[92mINFO [0m:      Received: train message 577fd657-1fc2-42ba-8743-d3195b525b75
02/08/2025 15:29:50:INFO:Received: train message 577fd657-1fc2-42ba-8743-d3195b525b75
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 15:50:20:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:10:26:INFO:
[92mINFO [0m:      Received: evaluate message 414692e3-306b-49fb-873d-ca64632d46d8
02/08/2025 16:10:26:INFO:Received: evaluate message 414692e3-306b-49fb-873d-ca64632d46d8

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 16:14:47:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:15:43:INFO:
[92mINFO [0m:      Received: train message dbdabef4-62e3-4b03-a6de-8678ceb2d7c9
02/08/2025 16:15:43:INFO:Received: train message dbdabef4-62e3-4b03-a6de-8678ceb2d7c9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 16:37:13:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:56:06:INFO:
[92mINFO [0m:      Received: evaluate message 7a089961-a936-48f4-875a-be888915dacf
02/08/2025 16:56:06:INFO:Received: evaluate message 7a089961-a936-48f4-875a-be888915dacf
[92mINFO [0m:      Sent reply
02/08/2025 17:00:23:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:01:26:INFO:
[92mINFO [0m:      Received: train message 8b05f98c-648a-4469-9eb7-59338156676d
02/08/2025 17:01:26:INFO:Received: train message 8b05f98c-648a-4469-9eb7-59338156676d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 17:22:28:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:41:34:INFO:
[92mINFO [0m:      Received: evaluate message fb277f37-0b8a-4398-b8ca-f30d376c99b7
02/08/2025 17:41:34:INFO:Received: evaluate message fb277f37-0b8a-4398-b8ca-f30d376c99b7

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 17:45:44:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:47:06:INFO:
[92mINFO [0m:      Received: train message 16f2376c-8c3d-4c9d-baf8-17566c44561b
02/08/2025 17:47:06:INFO:Received: train message 16f2376c-8c3d-4c9d-baf8-17566c44561b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:08:23:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:27:24:INFO:
[92mINFO [0m:      Received: evaluate message 8b654b39-72a9-4452-8a25-94704dc9aaf4
02/08/2025 18:27:24:INFO:Received: evaluate message 8b654b39-72a9-4452-8a25-94704dc9aaf4
[92mINFO [0m:      Sent reply
02/08/2025 18:31:00:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:33:06:INFO:
[92mINFO [0m:      Received: train message e12d98ea-998a-4b80-8764-a1907556ec96
02/08/2025 18:33:06:INFO:Received: train message e12d98ea-998a-4b80-8764-a1907556ec96
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:54:43:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:14:04:INFO:
[92mINFO [0m:      Received: evaluate message c8495922-1e2d-4dbd-b077-68e84aaf7672
02/08/2025 19:14:04:INFO:Received: evaluate message c8495922-1e2d-4dbd-b077-68e84aaf7672

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 19:18:13:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:18:49:INFO:
[92mINFO [0m:      Received: reconnect message 96ab8777-e940-47bc-8b81-bd9fb49921ce
02/08/2025 19:18:49:INFO:Received: reconnect message 96ab8777-e940-47bc-8b81-bd9fb49921ce
02/08/2025 19:18:49:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/08/2025 19:18:49:INFO:Disconnect and shut down

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712, 1.3589135335666058], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445, 0.8710482896241687], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899, 0.6162404554774186], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089, 0.6075857344583624]}



Final client history:
{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712, 1.3589135335666058], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445, 0.8710482896241687], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899, 0.6162404554774186], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089, 0.6075857344583624]}

