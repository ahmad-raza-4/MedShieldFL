nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/07/2025 20:06:03:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/07/2025 20:06:03:DEBUG:ChannelConnectivity.IDLE
02/07/2025 20:06:03:DEBUG:ChannelConnectivity.CONNECTING
[92mINFO [0m:      
02/07/2025 20:06:03:INFO:
[92mINFO [0m:      Received: get_parameters message aceb4fff-b06e-4a6c-b6ec-edf0178e9cb4
02/07/2025 20:06:03:INFO:Received: get_parameters message aceb4fff-b06e-4a6c-b6ec-edf0178e9cb4
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738987563.666747 1672849 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
02/07/2025 20:06:03:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      Sent reply
02/07/2025 20:06:11:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 20:16:49:INFO:
[92mINFO [0m:      Received: train message a8a9d42e-3d98-4893-9b81-bbdf416ca655
02/07/2025 20:16:49:INFO:Received: train message a8a9d42e-3d98-4893-9b81-bbdf416ca655
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 20:20:39:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 20:57:54:INFO:
[92mINFO [0m:      Received: evaluate message 9f7da528-a309-4e47-b6ef-7edef22bdca1
02/07/2025 20:57:54:INFO:Received: evaluate message 9f7da528-a309-4e47-b6ef-7edef22bdca1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:02:37:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:02:51:INFO:
[92mINFO [0m:      Received: train message ca27d308-8392-46e3-b7d2-ad8751337e03
02/07/2025 21:02:51:INFO:Received: train message ca27d308-8392-46e3-b7d2-ad8751337e03
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 21:06:01:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:43:56:INFO:
[92mINFO [0m:      Received: evaluate message ea1bc743-c731-4707-88df-9f18934f6991
02/07/2025 21:43:56:INFO:Received: evaluate message ea1bc743-c731-4707-88df-9f18934f6991
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:48:42:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:49:52:INFO:
[92mINFO [0m:      Received: train message 13196e55-4e2f-49fd-91bc-7370772e383d
02/07/2025 21:49:52:INFO:Received: train message 13196e55-4e2f-49fd-91bc-7370772e383d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 21:53:32:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:29:02:INFO:
[92mINFO [0m:      Received: evaluate message 5175cccd-352c-4951-9d84-a0e24612643a
02/07/2025 22:29:02:INFO:Received: evaluate message 5175cccd-352c-4951-9d84-a0e24612643a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 22:34:29:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:35:19:INFO:
[92mINFO [0m:      Received: train message 66378e2b-e435-49be-9f27-990d5ba2d8fc
02/07/2025 22:35:19:INFO:Received: train message 66378e2b-e435-49be-9f27-990d5ba2d8fc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 22:39:02:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:18:18:INFO:
[92mINFO [0m:      Received: evaluate message d56e4236-b67e-449c-9d89-79c662fd2bcf
02/07/2025 23:18:18:INFO:Received: evaluate message d56e4236-b67e-449c-9d89-79c662fd2bcf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 23:23:06:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:23:26:INFO:
[92mINFO [0m:      Received: train message bdd2a67b-69f6-4527-b2b1-f4551177f979
02/07/2025 23:23:26:INFO:Received: train message bdd2a67b-69f6-4527-b2b1-f4551177f979
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 23:26:31:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:05:17:INFO:
[92mINFO [0m:      Received: evaluate message e7de6645-78c9-4946-ab9d-126dca6623ae
02/08/2025 00:05:17:INFO:Received: evaluate message e7de6645-78c9-4946-ab9d-126dca6623ae
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:10:07:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:10:49:INFO:
[92mINFO [0m:      Received: train message 000d7483-6f97-4e98-9f9a-5efb9d772c93
02/08/2025 00:10:49:INFO:Received: train message 000d7483-6f97-4e98-9f9a-5efb9d772c93
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 00:14:27:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:52:10:INFO:
[92mINFO [0m:      Received: evaluate message d956e366-e926-4606-b69e-d5a7d1034250
02/08/2025 00:52:10:INFO:Received: evaluate message d956e366-e926-4606-b69e-d5a7d1034250
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:57:03:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:57:42:INFO:
[92mINFO [0m:      Received: train message d9231c43-1117-4cb0-88d9-7ac373e0f03c
02/08/2025 00:57:42:INFO:Received: train message d9231c43-1117-4cb0-88d9-7ac373e0f03c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 01:00:56:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:38:19:INFO:
[92mINFO [0m:      Received: evaluate message 869cce57-df3f-4b56-b302-6d694a619028
02/08/2025 01:38:19:INFO:Received: evaluate message 869cce57-df3f-4b56-b302-6d694a619028
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794], 'accuracy': [0.47643979057591623], 'auc': [0.7113553522460114], 'precision': [0.4601909378305515], 'recall': [0.47643979057591623], 'f1': [0.39597907648163966]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782], 'accuracy': [0.47643979057591623, 0.5501409585179219], 'auc': [0.7113553522460114, 0.7558628759678585], 'precision': [0.4601909378305515, 0.4857540893933654], 'recall': [0.47643979057591623, 0.5501409585179219], 'f1': [0.39597907648163966, 0.4846537523434621]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 01:42:58:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:43:34:INFO:
[92mINFO [0m:      Received: train message 5a56dd19-a433-4121-b7aa-9c3432cc3607
02/08/2025 01:43:34:INFO:Received: train message 5a56dd19-a433-4121-b7aa-9c3432cc3607
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 01:47:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:24:46:INFO:
[92mINFO [0m:      Received: evaluate message 52ccd094-2053-464e-91d0-682cae067518
02/08/2025 02:24:46:INFO:Received: evaluate message 52ccd094-2053-464e-91d0-682cae067518
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 02:29:23:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:29:42:INFO:
[92mINFO [0m:      Received: train message 674dbf39-18f8-4560-b2e6-7631f1fadafe
02/08/2025 02:29:42:INFO:Received: train message 674dbf39-18f8-4560-b2e6-7631f1fadafe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 02:32:40:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:12:01:INFO:
[92mINFO [0m:      Received: evaluate message 34ac4238-cb39-4b63-bb20-e31461216077
02/08/2025 03:12:01:INFO:Received: evaluate message 34ac4238-cb39-4b63-bb20-e31461216077
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 03:16:45:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:17:27:INFO:
[92mINFO [0m:      Received: train message e20c1c72-83d5-4cb3-b764-f805364f5122
02/08/2025 03:17:27:INFO:Received: train message e20c1c72-83d5-4cb3-b764-f805364f5122
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 03:20:40:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:58:15:INFO:
[92mINFO [0m:      Received: evaluate message 15334e59-7f41-46d2-ad30-8c9ff7a674d0
02/08/2025 03:58:15:INFO:Received: evaluate message 15334e59-7f41-46d2-ad30-8c9ff7a674d0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 04:03:20:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:04:00:INFO:
[92mINFO [0m:      Received: train message cfeed22d-acd9-4926-8d1a-a86da1c1129e
02/08/2025 04:04:00:INFO:Received: train message cfeed22d-acd9-4926-8d1a-a86da1c1129e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 04:07:25:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:43:17:INFO:
[92mINFO [0m:      Received: evaluate message 3865d810-b384-4da3-93f1-3b97b0351a4c
02/08/2025 04:43:17:INFO:Received: evaluate message 3865d810-b384-4da3-93f1-3b97b0351a4c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 04:47:52:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:48:36:INFO:
[92mINFO [0m:      Received: train message 1c9c6a38-0d9a-4352-aea8-b0d0f32e6aed
02/08/2025 04:48:36:INFO:Received: train message 1c9c6a38-0d9a-4352-aea8-b0d0f32e6aed

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477]}

Step 1b: Recomputing FIM for epoch 12
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 04:52:03:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:30:00:INFO:
[92mINFO [0m:      Received: evaluate message a735ec6b-c405-4b1c-a01a-cf6a6ad7fa13
02/08/2025 05:30:00:INFO:Received: evaluate message a735ec6b-c405-4b1c-a01a-cf6a6ad7fa13
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 05:34:34:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:35:00:INFO:
[92mINFO [0m:      Received: train message 83666c51-1f2c-4c4d-a87b-69320d52f273
02/08/2025 05:35:00:INFO:Received: train message 83666c51-1f2c-4c4d-a87b-69320d52f273
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 05:37:52:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:16:19:INFO:
[92mINFO [0m:      Received: evaluate message 38983fdb-86e8-4bd7-9b59-4136582086ff
02/08/2025 06:16:19:INFO:Received: evaluate message 38983fdb-86e8-4bd7-9b59-4136582086ff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 06:20:40:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:22:11:INFO:
[92mINFO [0m:      Received: train message aca1d4eb-7ae9-439c-a04a-b3d9ed4ffcd8
02/08/2025 06:22:11:INFO:Received: train message aca1d4eb-7ae9-439c-a04a-b3d9ed4ffcd8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 06:25:56:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:02:02:INFO:
[92mINFO [0m:      Received: evaluate message de3d13ed-28ff-4b7d-aeab-7652bbdb6324
02/08/2025 07:02:02:INFO:Received: evaluate message de3d13ed-28ff-4b7d-aeab-7652bbdb6324
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 07:06:43:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:07:28:INFO:
[92mINFO [0m:      Received: train message 6b400cf6-6111-4098-a49b-fb18283b9e2c
02/08/2025 07:07:28:INFO:Received: train message 6b400cf6-6111-4098-a49b-fb18283b9e2c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 07:11:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:48:11:INFO:
[92mINFO [0m:      Received: evaluate message bfafc6b2-ea18-4691-a0f8-4f324fc071b8
02/08/2025 07:48:11:INFO:Received: evaluate message bfafc6b2-ea18-4691-a0f8-4f324fc071b8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 07:52:26:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:53:28:INFO:
[92mINFO [0m:      Received: train message 29529941-a452-490a-864c-f1fc7ab63ff4
02/08/2025 07:53:28:INFO:Received: train message 29529941-a452-490a-864c-f1fc7ab63ff4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 07:57:01:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:35:36:INFO:
[92mINFO [0m:      Received: evaluate message 1d3c4af2-0c9e-40d6-922b-4ac9a6239224
02/08/2025 08:35:36:INFO:Received: evaluate message 1d3c4af2-0c9e-40d6-922b-4ac9a6239224
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 08:40:14:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:40:48:INFO:
[92mINFO [0m:      Received: train message d5657385-01fd-442a-969d-db9eb8e0c11b
02/08/2025 08:40:48:INFO:Received: train message d5657385-01fd-442a-969d-db9eb8e0c11b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 08:44:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:20:06:INFO:
[92mINFO [0m:      Received: evaluate message 352397d9-4f43-4a05-a4e3-e39e52cfc6a8
02/08/2025 09:20:06:INFO:Received: evaluate message 352397d9-4f43-4a05-a4e3-e39e52cfc6a8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 09:24:54:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:25:52:INFO:
[92mINFO [0m:      Received: train message 8dbae5d6-27f3-43a3-8025-8400bed26c17
02/08/2025 09:25:52:INFO:Received: train message 8dbae5d6-27f3-43a3-8025-8400bed26c17
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 09:29:26:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:04:48:INFO:
[92mINFO [0m:      Received: evaluate message 86676225-e141-4240-8484-380e053cec3a
02/08/2025 10:04:48:INFO:Received: evaluate message 86676225-e141-4240-8484-380e053cec3a

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 10:09:24:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:10:12:INFO:
[92mINFO [0m:      Received: train message a96262b6-9266-4a0f-ad27-43bdaa46e4f2
02/08/2025 10:10:12:INFO:Received: train message a96262b6-9266-4a0f-ad27-43bdaa46e4f2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 10:13:41:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:50:06:INFO:
[92mINFO [0m:      Received: evaluate message 4ebe9563-0a59-47d0-b12b-e6b7b790317f
02/08/2025 10:50:06:INFO:Received: evaluate message 4ebe9563-0a59-47d0-b12b-e6b7b790317f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 10:54:38:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:55:30:INFO:
[92mINFO [0m:      Received: train message eab22c2c-cb75-4bd8-8bd8-be03b270aa5b
02/08/2025 10:55:30:INFO:Received: train message eab22c2c-cb75-4bd8-8bd8-be03b270aa5b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 10:58:53:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:34:57:INFO:
[92mINFO [0m:      Received: evaluate message e2b97108-1aff-42b7-a15f-e294e45ece24
02/08/2025 11:34:57:INFO:Received: evaluate message e2b97108-1aff-42b7-a15f-e294e45ece24

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 11:39:59:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:40:37:INFO:
[92mINFO [0m:      Received: train message 7e511311-aff5-4a6c-af65-3ae8c2551611
02/08/2025 11:40:37:INFO:Received: train message 7e511311-aff5-4a6c-af65-3ae8c2551611
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 11:44:07:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:18:41:INFO:
[92mINFO [0m:      Received: evaluate message 52c6f335-16a3-4590-943f-612b6bb3a0bd
02/08/2025 12:18:41:INFO:Received: evaluate message 52c6f335-16a3-4590-943f-612b6bb3a0bd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 12:23:21:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:24:04:INFO:
[92mINFO [0m:      Received: train message 65f0b5ae-066c-4013-ab12-a30956b22965
02/08/2025 12:24:04:INFO:Received: train message 65f0b5ae-066c-4013-ab12-a30956b22965
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 12:27:20:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:05:20:INFO:
[92mINFO [0m:      Received: evaluate message 63ccbb65-c469-4c49-8eea-658e35aafc5f
02/08/2025 13:05:20:INFO:Received: evaluate message 63ccbb65-c469-4c49-8eea-658e35aafc5f

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 13:10:14:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:11:01:INFO:
[92mINFO [0m:      Received: train message dc9e0805-d4e8-4c17-932b-ce0329da4335
02/08/2025 13:11:01:INFO:Received: train message dc9e0805-d4e8-4c17-932b-ce0329da4335
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 13:14:38:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:52:17:INFO:
[92mINFO [0m:      Received: evaluate message db74c5c0-3d45-4fc1-b5dc-f60999f0710d
02/08/2025 13:52:17:INFO:Received: evaluate message db74c5c0-3d45-4fc1-b5dc-f60999f0710d
[92mINFO [0m:      Sent reply
02/08/2025 13:56:58:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:57:33:INFO:
[92mINFO [0m:      Received: train message d90f1cd9-7717-4209-bef7-0913458777e3
02/08/2025 13:57:33:INFO:Received: train message d90f1cd9-7717-4209-bef7-0913458777e3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 14:00:47:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:38:39:INFO:
[92mINFO [0m:      Received: evaluate message 9b2b9858-fafe-417a-9861-ea98da2d1f7e
02/08/2025 14:38:39:INFO:Received: evaluate message 9b2b9858-fafe-417a-9861-ea98da2d1f7e

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 14:43:13:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:44:18:INFO:
[92mINFO [0m:      Received: train message e98ecc67-5189-450c-a0d9-1e61951e6f52
02/08/2025 14:44:18:INFO:Received: train message e98ecc67-5189-450c-a0d9-1e61951e6f52
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 14:47:40:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:24:15:INFO:
[92mINFO [0m:      Received: evaluate message b4fccc96-3e90-4baf-a8ea-629e044939c5
02/08/2025 15:24:15:INFO:Received: evaluate message b4fccc96-3e90-4baf-a8ea-629e044939c5
[92mINFO [0m:      Sent reply
02/08/2025 15:28:50:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:29:44:INFO:
[92mINFO [0m:      Received: train message cc5be65a-1c24-453a-99be-3192caa5b8fa
02/08/2025 15:29:44:INFO:Received: train message cc5be65a-1c24-453a-99be-3192caa5b8fa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 15:32:56:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:10:24:INFO:
[92mINFO [0m:      Received: evaluate message 28e85f13-cb34-4756-99ab-14a33104802f
02/08/2025 16:10:24:INFO:Received: evaluate message 28e85f13-cb34-4756-99ab-14a33104802f

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 16:14:47:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:15:16:INFO:
[92mINFO [0m:      Received: train message 60e2132f-74bd-47cd-a83b-5bb3e4425013
02/08/2025 16:15:16:INFO:Received: train message 60e2132f-74bd-47cd-a83b-5bb3e4425013
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 16:18:28:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:56:20:INFO:
[92mINFO [0m:      Received: evaluate message 433adcdb-78aa-4a64-9e89-5926c25514a9
02/08/2025 16:56:20:INFO:Received: evaluate message 433adcdb-78aa-4a64-9e89-5926c25514a9
[92mINFO [0m:      Sent reply
02/08/2025 17:00:40:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:01:34:INFO:
[92mINFO [0m:      Received: train message d9babe23-d773-4914-b46f-1dd5d4b68237
02/08/2025 17:01:34:INFO:Received: train message d9babe23-d773-4914-b46f-1dd5d4b68237
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 17:04:44:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:41:43:INFO:
[92mINFO [0m:      Received: evaluate message 745fadea-cbb3-489f-8f41-eb4302594804
02/08/2025 17:41:43:INFO:Received: evaluate message 745fadea-cbb3-489f-8f41-eb4302594804

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 17:46:11:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:47:09:INFO:
[92mINFO [0m:      Received: train message 3adc3807-c64d-46bf-89d9-22f3631202de
02/08/2025 17:47:09:INFO:Received: train message 3adc3807-c64d-46bf-89d9-22f3631202de
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 17:50:14:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:27:41:INFO:
[92mINFO [0m:      Received: evaluate message be6474bf-e513-400c-a87b-efdc25379093
02/08/2025 18:27:41:INFO:Received: evaluate message be6474bf-e513-400c-a87b-efdc25379093
[92mINFO [0m:      Sent reply
02/08/2025 18:32:09:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:33:04:INFO:
[92mINFO [0m:      Received: train message ef6f501c-d985-4ad9-ab4f-e835a55c5955
02/08/2025 18:33:04:INFO:Received: train message ef6f501c-d985-4ad9-ab4f-e835a55c5955
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:36:27:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:14:16:INFO:
[92mINFO [0m:      Received: evaluate message e56097f5-f0ac-4833-b248-b1a7b4a65b72
02/08/2025 19:14:16:INFO:Received: evaluate message e56097f5-f0ac-4833-b248-b1a7b4a65b72

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 19:18:35:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:18:49:INFO:
[92mINFO [0m:      Received: reconnect message d6c8786b-79a4-400b-ac64-1dd3e9289ef1
02/08/2025 19:18:49:INFO:Received: reconnect message d6c8786b-79a4-400b-ac64-1dd3e9289ef1
02/08/2025 19:18:49:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/08/2025 19:18:49:INFO:Disconnect and shut down

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712, 1.3589135335666058], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445, 0.8710482896241687], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899, 0.6162404554774186], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089, 0.6075857344583624]}



Final client history:
{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712, 1.3589135335666058], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445, 0.8710482896241687], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899, 0.6162404554774186], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089, 0.6075857344583624]}

