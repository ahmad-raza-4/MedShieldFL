nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=10.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/07/2025 20:11:22:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/07/2025 20:11:22:DEBUG:ChannelConnectivity.IDLE
02/07/2025 20:11:22:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738987882.297839 1677767 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/07/2025 20:16:58:INFO:
[92mINFO [0m:      Received: train message 7dd6fcdf-3834-4959-8507-02fb060fa4fb
02/07/2025 20:16:58:INFO:Received: train message 7dd6fcdf-3834-4959-8507-02fb060fa4fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 20:38:21:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 20:57:28:INFO:
[92mINFO [0m:      Received: evaluate message 1280747a-2b62-4c29-bcd8-c7582d3ad83b
02/07/2025 20:57:28:INFO:Received: evaluate message 1280747a-2b62-4c29-bcd8-c7582d3ad83b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:01:59:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:02:37:INFO:
[92mINFO [0m:      Received: train message 79c8d2f6-3a74-49b7-a532-067ddadb3446
02/07/2025 21:02:37:INFO:Received: train message 79c8d2f6-3a74-49b7-a532-067ddadb3446
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 21:22:21:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:42:43:INFO:
[92mINFO [0m:      Received: evaluate message a0622e1a-cdbf-4455-a153-0c98bfb90cfe
02/07/2025 21:42:43:INFO:Received: evaluate message a0622e1a-cdbf-4455-a153-0c98bfb90cfe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:47:23:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:48:05:INFO:
[92mINFO [0m:      Received: train message c1a03ab2-00de-4e72-863f-09ab4060d650
02/07/2025 21:48:05:INFO:Received: train message c1a03ab2-00de-4e72-863f-09ab4060d650
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 22:08:12:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:26:00:INFO:
[92mINFO [0m:      Received: evaluate message 85f0df45-2ca8-496b-9965-99b3e0172310
02/07/2025 22:26:00:INFO:Received: evaluate message 85f0df45-2ca8-496b-9965-99b3e0172310
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 22:29:54:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:31:37:INFO:
[92mINFO [0m:      Received: train message 86327a49-81b2-4077-a01b-c5cb09186961
02/07/2025 22:31:37:INFO:Received: train message 86327a49-81b2-4077-a01b-c5cb09186961
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 22:56:07:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:14:59:INFO:
[92mINFO [0m:      Received: evaluate message 9287bcae-903b-4b1d-8a25-c6e9ea9f9fb5
02/07/2025 23:14:59:INFO:Received: evaluate message 9287bcae-903b-4b1d-8a25-c6e9ea9f9fb5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 23:19:18:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:19:40:INFO:
[92mINFO [0m:      Received: train message 0cf3fdc4-3bf9-44f7-a158-6386d8435ec1
02/07/2025 23:19:40:INFO:Received: train message 0cf3fdc4-3bf9-44f7-a158-6386d8435ec1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 23:40:17:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:00:43:INFO:
[92mINFO [0m:      Received: evaluate message 4c293ebb-eb3e-4210-8d6b-17a71199ba63
02/08/2025 00:00:43:INFO:Received: evaluate message 4c293ebb-eb3e-4210-8d6b-17a71199ba63
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:05:00:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:05:46:INFO:
[92mINFO [0m:      Received: train message c67a43f0-77b7-4131-9ca5-f2754586e56a
02/08/2025 00:05:46:INFO:Received: train message c67a43f0-77b7-4131-9ca5-f2754586e56a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 00:26:07:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:44:24:INFO:
[92mINFO [0m:      Received: evaluate message 5d71707e-a8cf-4a4e-8b21-b25a5b1b7734
02/08/2025 00:44:24:INFO:Received: evaluate message 5d71707e-a8cf-4a4e-8b21-b25a5b1b7734
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:48:24:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:49:47:INFO:
[92mINFO [0m:      Received: train message a8f7935a-eb10-45ef-9cd2-6f73aa6a0d7a
02/08/2025 00:49:47:INFO:Received: train message a8f7935a-eb10-45ef-9cd2-6f73aa6a0d7a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 01:10:43:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:30:25:INFO:
[92mINFO [0m:      Received: evaluate message 55155e15-4332-43c9-bbc8-134bd260cb92
02/08/2025 01:30:25:INFO:Received: evaluate message 55155e15-4332-43c9-bbc8-134bd260cb92
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=10.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=10.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232], 'accuracy': [0.5553765606121627], 'auc': [0.7952128841995612], 'precision': [0.5630504832476843], 'recall': [0.5553765606121627], 'f1': [0.5014109976972659]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068], 'accuracy': [0.5553765606121627, 0.6004832863471606], 'auc': [0.7952128841995612, 0.8346505194429128], 'precision': [0.5630504832476843, 0.588172573519457], 'recall': [0.5553765606121627, 0.6004832863471606], 'f1': [0.5014109976972659, 0.559282479633486]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 01:34:48:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:35:19:INFO:
[92mINFO [0m:      Received: train message 7b304efa-ea9a-45b9-94ea-ac32ecb250ba
02/08/2025 01:35:19:INFO:Received: train message 7b304efa-ea9a-45b9-94ea-ac32ecb250ba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 01:57:16:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:16:45:INFO:
[92mINFO [0m:      Received: evaluate message 4007f919-386e-437f-967b-6136bdabb31b
02/08/2025 02:16:45:INFO:Received: evaluate message 4007f919-386e-437f-967b-6136bdabb31b
[92mINFO [0m:      Sent reply
02/08/2025 02:20:46:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:21:35:INFO:
[92mINFO [0m:      Received: train message d83bc078-f824-463c-8257-149320af26f5
02/08/2025 02:21:35:INFO:Received: train message d83bc078-f824-463c-8257-149320af26f5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 02:41:34:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:01:44:INFO:
[92mINFO [0m:      Received: evaluate message 66020242-977c-44fa-af5f-9216e81a9f7f
02/08/2025 03:01:44:INFO:Received: evaluate message 66020242-977c-44fa-af5f-9216e81a9f7f
[92mINFO [0m:      Sent reply
02/08/2025 03:06:00:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:06:35:INFO:
[92mINFO [0m:      Received: train message ba47d87d-b7da-4897-8a1c-915c4b604fff
02/08/2025 03:06:35:INFO:Received: train message ba47d87d-b7da-4897-8a1c-915c4b604fff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 03:26:44:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:45:14:INFO:
[92mINFO [0m:      Received: evaluate message 64e8c6f0-a003-414a-9347-3c626e33f507
02/08/2025 03:45:14:INFO:Received: evaluate message 64e8c6f0-a003-414a-9347-3c626e33f507
[92mINFO [0m:      Sent reply
02/08/2025 03:49:09:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:49:57:INFO:
[92mINFO [0m:      Received: train message 7b0569e2-636a-48dd-a5a5-c0d656bd79cd
02/08/2025 03:49:57:INFO:Received: train message 7b0569e2-636a-48dd-a5a5-c0d656bd79cd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 04:10:36:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:32:43:INFO:
[92mINFO [0m:      Received: evaluate message f0c2b67f-48cc-4740-99f1-bc6144d0b9ec
02/08/2025 04:32:43:INFO:Received: evaluate message f0c2b67f-48cc-4740-99f1-bc6144d0b9ec
[92mINFO [0m:      Sent reply
02/08/2025 04:36:55:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:37:40:INFO:
[92mINFO [0m:      Received: train message 3e90d1dc-a84c-4c77-8bfd-f26002a04b9e
02/08/2025 04:37:40:INFO:Received: train message 3e90d1dc-a84c-4c77-8bfd-f26002a04b9e

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986]}

Step 1b: Recomputing FIM for epoch 12
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 04:57:07:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:21:10:INFO:
[92mINFO [0m:      Received: evaluate message e57392de-50fb-4fc7-8b81-ffb03ea3d8c7
02/08/2025 05:21:10:INFO:Received: evaluate message e57392de-50fb-4fc7-8b81-ffb03ea3d8c7
[92mINFO [0m:      Sent reply
02/08/2025 05:25:18:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:25:55:INFO:
[92mINFO [0m:      Received: train message d911d5a7-8aaa-495d-8712-d4b57ebaedfc
02/08/2025 05:25:55:INFO:Received: train message d911d5a7-8aaa-495d-8712-d4b57ebaedfc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 05:44:56:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:05:38:INFO:
[92mINFO [0m:      Received: evaluate message 9efd024e-df42-4249-ba71-514d219c40c4
02/08/2025 06:05:38:INFO:Received: evaluate message 9efd024e-df42-4249-ba71-514d219c40c4
[92mINFO [0m:      Sent reply
02/08/2025 06:09:59:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:10:55:INFO:
[92mINFO [0m:      Received: train message 0711432e-6435-4e6d-9f60-2f554cbf5e59
02/08/2025 06:10:55:INFO:Received: train message 0711432e-6435-4e6d-9f60-2f554cbf5e59
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 06:31:11:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:52:12:INFO:
[92mINFO [0m:      Received: evaluate message 94199f06-5a3a-4cac-a552-7c296f995cde
02/08/2025 06:52:12:INFO:Received: evaluate message 94199f06-5a3a-4cac-a552-7c296f995cde
[92mINFO [0m:      Sent reply
02/08/2025 06:56:28:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:56:56:INFO:
[92mINFO [0m:      Received: train message 3b3658ac-eb7f-4284-8497-c7446b28489e
02/08/2025 06:56:56:INFO:Received: train message 3b3658ac-eb7f-4284-8497-c7446b28489e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 07:17:38:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:39:59:INFO:
[92mINFO [0m:      Received: evaluate message c5e7111f-c79d-4409-8fe4-01edfd4c752c
02/08/2025 07:39:59:INFO:Received: evaluate message c5e7111f-c79d-4409-8fe4-01edfd4c752c
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 07:44:00:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:44:37:INFO:
[92mINFO [0m:      Received: train message aee37e70-fe04-4db4-9b13-26a6fcb8afb1
02/08/2025 07:44:37:INFO:Received: train message aee37e70-fe04-4db4-9b13-26a6fcb8afb1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 08:04:01:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:26:08:INFO:
[92mINFO [0m:      Received: evaluate message ed7edb5e-abc4-4b00-8862-0ee98f8e5d22
02/08/2025 08:26:08:INFO:Received: evaluate message ed7edb5e-abc4-4b00-8862-0ee98f8e5d22
[92mINFO [0m:      Sent reply
02/08/2025 08:30:28:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:30:50:INFO:
[92mINFO [0m:      Received: train message 26522a20-46b7-43b1-9444-c5b72ca9bd95
02/08/2025 08:30:50:INFO:Received: train message 26522a20-46b7-43b1-9444-c5b72ca9bd95
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 08:50:11:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:09:59:INFO:
[92mINFO [0m:      Received: evaluate message ce35a9c1-f1f2-4556-b0ad-9bca94a146d7
02/08/2025 09:09:59:INFO:Received: evaluate message ce35a9c1-f1f2-4556-b0ad-9bca94a146d7
[92mINFO [0m:      Sent reply
02/08/2025 09:14:10:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:14:41:INFO:
[92mINFO [0m:      Received: train message aadd3a76-0a0f-4195-b8f2-cc6ad7acc90b
02/08/2025 09:14:41:INFO:Received: train message aadd3a76-0a0f-4195-b8f2-cc6ad7acc90b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 09:35:24:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:57:03:INFO:
[92mINFO [0m:      Received: evaluate message 674dd3bc-6ec2-47d1-a099-73b273954ef4
02/08/2025 09:57:03:INFO:Received: evaluate message 674dd3bc-6ec2-47d1-a099-73b273954ef4

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 10:01:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:01:44:INFO:
[92mINFO [0m:      Received: train message 8bb50bd9-d406-4e8b-aab0-b18f7b8b4280
02/08/2025 10:01:44:INFO:Received: train message 8bb50bd9-d406-4e8b-aab0-b18f7b8b4280
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 10:21:31:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:45:02:INFO:
[92mINFO [0m:      Received: evaluate message 9f47f54e-7b40-463f-a705-86a91a14d9fb
02/08/2025 10:45:02:INFO:Received: evaluate message 9f47f54e-7b40-463f-a705-86a91a14d9fb
[92mINFO [0m:      Sent reply
02/08/2025 10:49:14:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:49:44:INFO:
[92mINFO [0m:      Received: train message 914006a1-febc-40a6-ba85-6c42d39d9d9b
02/08/2025 10:49:44:INFO:Received: train message 914006a1-febc-40a6-ba85-6c42d39d9d9b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 11:09:14:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:29:47:INFO:
[92mINFO [0m:      Received: evaluate message 24d7d808-52b0-455c-8b7a-ad7b15c46757
02/08/2025 11:29:47:INFO:Received: evaluate message 24d7d808-52b0-455c-8b7a-ad7b15c46757

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 11:34:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:34:52:INFO:
[92mINFO [0m:      Received: train message 5a51ba6d-44a5-4386-b9d8-6f802202e80d
02/08/2025 11:34:52:INFO:Received: train message 5a51ba6d-44a5-4386-b9d8-6f802202e80d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 11:55:41:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:15:01:INFO:
[92mINFO [0m:      Received: evaluate message 90d7159d-08f2-48bb-a070-cd6987d2d015
02/08/2025 12:15:01:INFO:Received: evaluate message 90d7159d-08f2-48bb-a070-cd6987d2d015
[92mINFO [0m:      Sent reply
02/08/2025 12:19:21:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:19:53:INFO:
[92mINFO [0m:      Received: train message 28831750-adfb-49fe-a418-6cd922bfbc43
02/08/2025 12:19:53:INFO:Received: train message 28831750-adfb-49fe-a418-6cd922bfbc43
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 12:41:21:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:02:50:INFO:
[92mINFO [0m:      Received: evaluate message 422ea427-212c-411a-9efa-94d812e21cab
02/08/2025 13:02:50:INFO:Received: evaluate message 422ea427-212c-411a-9efa-94d812e21cab

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 13:07:16:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:08:19:INFO:
[92mINFO [0m:      Received: train message 8273873b-bae9-4379-9064-c8e8356f79d0
02/08/2025 13:08:19:INFO:Received: train message 8273873b-bae9-4379-9064-c8e8356f79d0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 13:28:20:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:51:42:INFO:
[92mINFO [0m:      Received: evaluate message ee5089f0-ace7-4122-bb6f-5a1164840bfd
02/08/2025 13:51:42:INFO:Received: evaluate message ee5089f0-ace7-4122-bb6f-5a1164840bfd
[92mINFO [0m:      Sent reply
02/08/2025 13:55:59:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:56:41:INFO:
[92mINFO [0m:      Received: train message 2602c6d4-e45d-45a4-83e6-9a4465f0611b
02/08/2025 13:56:41:INFO:Received: train message 2602c6d4-e45d-45a4-83e6-9a4465f0611b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 14:16:14:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:39:38:INFO:
[92mINFO [0m:      Received: evaluate message 71bc7004-e51b-4ac6-8abe-115059ca7145
02/08/2025 14:39:38:INFO:Received: evaluate message 71bc7004-e51b-4ac6-8abe-115059ca7145

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 14:44:06:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:44:53:INFO:
[92mINFO [0m:      Received: train message 534f0646-1873-4bbe-8ddb-adbeab847acb
02/08/2025 14:44:53:INFO:Received: train message 534f0646-1873-4bbe-8ddb-adbeab847acb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 15:04:00:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:26:48:INFO:
[92mINFO [0m:      Received: evaluate message 028cd273-d9ac-4194-bbc4-2c37cc0cf208
02/08/2025 15:26:48:INFO:Received: evaluate message 028cd273-d9ac-4194-bbc4-2c37cc0cf208
[92mINFO [0m:      Sent reply
02/08/2025 15:31:08:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:32:17:INFO:
[92mINFO [0m:      Received: train message 7f51f74a-0aa9-4650-996f-b14da967dc9c
02/08/2025 15:32:17:INFO:Received: train message 7f51f74a-0aa9-4650-996f-b14da967dc9c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 15:51:45:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:15:32:INFO:
[92mINFO [0m:      Received: evaluate message 5d0a5bbf-ff38-4963-9fbc-11ef85a3fe8e
02/08/2025 16:15:32:INFO:Received: evaluate message 5d0a5bbf-ff38-4963-9fbc-11ef85a3fe8e

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 16:19:58:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:21:13:INFO:
[92mINFO [0m:      Received: train message 7dadad77-f3ae-4a8a-884d-97abcb55018a
02/08/2025 16:21:13:INFO:Received: train message 7dadad77-f3ae-4a8a-884d-97abcb55018a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 16:40:20:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:03:49:INFO:
[92mINFO [0m:      Received: evaluate message f18cdc7b-2c9d-43e0-9804-bfbca2ccb76a
02/08/2025 17:03:49:INFO:Received: evaluate message f18cdc7b-2c9d-43e0-9804-bfbca2ccb76a
[92mINFO [0m:      Sent reply
02/08/2025 17:08:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:09:15:INFO:
[92mINFO [0m:      Received: train message e37880f4-0789-43ba-9787-c694c34837d9
02/08/2025 17:09:15:INFO:Received: train message e37880f4-0789-43ba-9787-c694c34837d9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 17:28:01:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:53:08:INFO:
[92mINFO [0m:      Received: evaluate message 115df7ee-ca78-43e5-a60d-17542d07237b
02/08/2025 17:53:08:INFO:Received: evaluate message 115df7ee-ca78-43e5-a60d-17542d07237b

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213, 1.310995662039535], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718, 0.8949018310594852], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313, 0.6584166731871604], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051, 0.6380738299672268]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213, 1.310995662039535, 1.264396644987697], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718, 0.8949018310594852, 0.898103397330748], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313, 0.6584166731871604, 0.6631704669167207], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051, 0.6380738299672268, 0.6435018680920499]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 17:57:35:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:58:57:INFO:
[92mINFO [0m:      Received: train message 970296ee-d6f5-4f1d-8d12-1db995f34721
02/08/2025 17:58:57:INFO:Received: train message 970296ee-d6f5-4f1d-8d12-1db995f34721
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:17:20:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:42:27:INFO:
[92mINFO [0m:      Received: evaluate message 2344772d-f35e-4d14-870a-64e714f3786e
02/08/2025 18:42:27:INFO:Received: evaluate message 2344772d-f35e-4d14-870a-64e714f3786e
[92mINFO [0m:      Sent reply
02/08/2025 18:46:59:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:47:54:INFO:
[92mINFO [0m:      Received: train message 71d5d22b-9283-4d01-a716-f5481c9aa021
02/08/2025 18:47:54:INFO:Received: train message 71d5d22b-9283-4d01-a716-f5481c9aa021
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 19:06:28:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:27:35:INFO:
[92mINFO [0m:      Received: evaluate message 6090b97f-ea5a-43cd-b45d-b8918617a905
02/08/2025 19:27:35:INFO:Received: evaluate message 6090b97f-ea5a-43cd-b45d-b8918617a905

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213, 1.310995662039535, 1.264396644987697, 1.2701550575211027], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718, 0.8949018310594852, 0.898103397330748, 0.8990099129770881], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313, 0.6584166731871604, 0.6631704669167207, 0.6583154352356761], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051, 0.6380738299672268, 0.6435018680920499, 0.6458983891160318]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213, 1.310995662039535, 1.264396644987697, 1.2701550575211027, 1.2586158710144433], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542, 0.6612968183648812], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718, 0.8949018310594852, 0.898103397330748, 0.8990099129770881, 0.9004977288641947], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313, 0.6584166731871604, 0.6631704669167207, 0.6583154352356761, 0.6579979095347669], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542, 0.6612968183648812], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051, 0.6380738299672268, 0.6435018680920499, 0.6458983891160318, 0.6490721132841767]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 19:31:43:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:31:59:INFO:
[92mINFO [0m:      Received: reconnect message 447e305f-78ba-40c3-a9fb-e85cbfc34110
02/08/2025 19:31:59:INFO:Received: reconnect message 447e305f-78ba-40c3-a9fb-e85cbfc34110
02/08/2025 19:31:59:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/08/2025 19:31:59:INFO:Disconnect and shut down

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213, 1.310995662039535, 1.264396644987697, 1.2701550575211027, 1.2586158710144433, 1.3398930698309126], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542, 0.6612968183648812, 0.6455900120821587], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718, 0.8949018310594852, 0.898103397330748, 0.8990099129770881, 0.9004977288641947, 0.9001085710167956], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313, 0.6584166731871604, 0.6631704669167207, 0.6583154352356761, 0.6579979095347669, 0.6540454901821245], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542, 0.6612968183648812, 0.6455900120821587], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051, 0.6380738299672268, 0.6435018680920499, 0.6458983891160318, 0.6490721132841767, 0.6330882930625233]}



Final client history:
{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213, 1.310995662039535, 1.264396644987697, 1.2701550575211027, 1.2586158710144433, 1.3398930698309126], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542, 0.6612968183648812, 0.6455900120821587], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718, 0.8949018310594852, 0.898103397330748, 0.8990099129770881, 0.9004977288641947, 0.9001085710167956], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313, 0.6584166731871604, 0.6631704669167207, 0.6583154352356761, 0.6579979095347669, 0.6540454901821245], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542, 0.6612968183648812, 0.6455900120821587], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051, 0.6380738299672268, 0.6435018680920499, 0.6458983891160318, 0.6490721132841767, 0.6330882930625233]}

