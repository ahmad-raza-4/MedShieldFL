nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=30.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/07/2025 20:13:58:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/07/2025 20:13:58:DEBUG:ChannelConnectivity.IDLE
02/07/2025 20:13:58:DEBUG:ChannelConnectivity.CONNECTING
02/07/2025 20:13:58:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738988038.685588 1679292 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/07/2025 20:26:00:INFO:
[92mINFO [0m:      Received: train message 4aad5452-5172-421b-be78-cf6f51b7dc45
02/07/2025 20:26:00:INFO:Received: train message 4aad5452-5172-421b-be78-cf6f51b7dc45
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 20:43:21:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:25:33:INFO:
[92mINFO [0m:      Received: evaluate message 16a6a5f8-59c1-44e5-b7d8-69deabeed404
02/07/2025 21:25:33:INFO:Received: evaluate message 16a6a5f8-59c1-44e5-b7d8-69deabeed404
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:31:01:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:31:39:INFO:
[92mINFO [0m:      Received: train message df795210-aa85-49cf-b425-0321f5578f7d
02/07/2025 21:31:39:INFO:Received: train message df795210-aa85-49cf-b425-0321f5578f7d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 21:48:52:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:28:17:INFO:
[92mINFO [0m:      Received: evaluate message 992e6fd5-cf68-449c-8027-18a87ace0f09
02/07/2025 22:28:17:INFO:Received: evaluate message 992e6fd5-cf68-449c-8027-18a87ace0f09
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 22:34:48:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:35:47:INFO:
[92mINFO [0m:      Received: train message b9db29c5-dbc0-40cf-9f9d-a7e19e34ebbd
02/07/2025 22:35:47:INFO:Received: train message b9db29c5-dbc0-40cf-9f9d-a7e19e34ebbd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 22:55:47:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:37:09:INFO:
[92mINFO [0m:      Received: evaluate message 61eb30a8-eb38-4ca1-9efc-6324c1bd9571
02/07/2025 23:37:09:INFO:Received: evaluate message 61eb30a8-eb38-4ca1-9efc-6324c1bd9571
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 23:43:22:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:44:18:INFO:
[92mINFO [0m:      Received: train message 6773a31d-0fec-4e5f-883b-d317a633622b
02/07/2025 23:44:18:INFO:Received: train message 6773a31d-0fec-4e5f-883b-d317a633622b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 00:00:19:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:40:33:INFO:
[92mINFO [0m:      Received: evaluate message df76b04a-1b1f-40c5-ba10-2786229b0708
02/08/2025 00:40:33:INFO:Received: evaluate message df76b04a-1b1f-40c5-ba10-2786229b0708
[92mINFO [0m:      Sent reply
02/08/2025 00:46:18:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:47:21:INFO:
[92mINFO [0m:      Received: train message ad092fe6-2036-4d71-a2a6-04941f9e57ba
02/08/2025 00:47:21:INFO:Received: train message ad092fe6-2036-4d71-a2a6-04941f9e57ba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 01:05:56:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:43:26:INFO:
[92mINFO [0m:      Received: evaluate message be1baa39-f01d-4876-96c9-94b93b8e4df0
02/08/2025 01:43:26:INFO:Received: evaluate message be1baa39-f01d-4876-96c9-94b93b8e4df0
[92mINFO [0m:      Sent reply
02/08/2025 01:49:27:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:49:46:INFO:
[92mINFO [0m:      Received: train message d425363d-8c11-4cc8-b314-c89c0a680f1c
02/08/2025 01:49:46:INFO:Received: train message d425363d-8c11-4cc8-b314-c89c0a680f1c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 02:06:40:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:48:08:INFO:
[92mINFO [0m:      Received: evaluate message 28d337a6-6cda-402c-a1d7-2de1558aecb8
02/08/2025 02:48:08:INFO:Received: evaluate message 28d337a6-6cda-402c-a1d7-2de1558aecb8
[92mINFO [0m:      Sent reply
02/08/2025 02:54:02:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:54:43:INFO:
[92mINFO [0m:      Received: train message 09e170f3-20c1-4ba9-aefc-6031638707b7
02/08/2025 02:54:43:INFO:Received: train message 09e170f3-20c1-4ba9-aefc-6031638707b7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 03:11:50:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:51:57:INFO:
[92mINFO [0m:      Received: evaluate message 5df02c10-af71-43b8-bdbc-b4155a531fec
02/08/2025 03:51:57:INFO:Received: evaluate message 5df02c10-af71-43b8-bdbc-b4155a531fec
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=30.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=30.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415], 'accuracy': [0.5734997986306887], 'auc': [0.8209263333238899], 'precision': [0.5896199585980895], 'recall': [0.5734997986306887], 'f1': [0.5251729041892587]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146], 'accuracy': [0.5734997986306887, 0.6061216270640355], 'auc': [0.8209263333238899, 0.8524348169466368], 'precision': [0.5896199585980895, 0.6033971678272441], 'recall': [0.5734997986306887, 0.6061216270640355], 'f1': [0.5251729041892587, 0.5708840472582581]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 03:57:44:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:58:36:INFO:
[92mINFO [0m:      Received: train message 3f1a21dc-a890-4d2e-9b85-d9fda5c7565b
02/08/2025 03:58:36:INFO:Received: train message 3f1a21dc-a890-4d2e-9b85-d9fda5c7565b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 04:16:33:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:57:36:INFO:
[92mINFO [0m:      Received: evaluate message c46c2f77-fb42-41ea-95c3-79e2bdc3ff22
02/08/2025 04:57:36:INFO:Received: evaluate message c46c2f77-fb42-41ea-95c3-79e2bdc3ff22
[92mINFO [0m:      Sent reply
02/08/2025 05:03:09:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:03:46:INFO:
[92mINFO [0m:      Received: train message 7c256d70-6296-4f82-8b69-3d4a8cf53907
02/08/2025 05:03:46:INFO:Received: train message 7c256d70-6296-4f82-8b69-3d4a8cf53907
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 05:19:37:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:01:25:INFO:
[92mINFO [0m:      Received: evaluate message 0254a4aa-571e-4018-b225-790e36190bd5
02/08/2025 06:01:25:INFO:Received: evaluate message 0254a4aa-571e-4018-b225-790e36190bd5
[92mINFO [0m:      Sent reply
02/08/2025 06:07:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:07:55:INFO:
[92mINFO [0m:      Received: train message df9825a5-50e1-4c19-843b-d89118885173
02/08/2025 06:07:55:INFO:Received: train message df9825a5-50e1-4c19-843b-d89118885173
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 06:25:34:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:05:12:INFO:
[92mINFO [0m:      Received: evaluate message d82af315-897a-44d3-b5a9-6abc4ae422e1
02/08/2025 07:05:12:INFO:Received: evaluate message d82af315-897a-44d3-b5a9-6abc4ae422e1
[92mINFO [0m:      Sent reply
02/08/2025 07:11:29:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:11:54:INFO:
[92mINFO [0m:      Received: train message 07a9ca18-ed6f-4309-a7ea-f516a41e2a6f
02/08/2025 07:11:54:INFO:Received: train message 07a9ca18-ed6f-4309-a7ea-f516a41e2a6f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 07:28:42:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:10:21:INFO:
[92mINFO [0m:      Received: evaluate message 054be485-e235-449f-88ed-686ecc34c286
02/08/2025 08:10:21:INFO:Received: evaluate message 054be485-e235-449f-88ed-686ecc34c286
[92mINFO [0m:      Sent reply
02/08/2025 08:15:40:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:16:41:INFO:
[92mINFO [0m:      Received: train message 003d6780-11c0-41fa-a137-3420827883b1
02/08/2025 08:16:41:INFO:Received: train message 003d6780-11c0-41fa-a137-3420827883b1

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482]}

Step 1b: Recomputing FIM for epoch 12
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 08:33:05:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:13:37:INFO:
[92mINFO [0m:      Received: evaluate message 1b2d63c7-5b1f-453a-816f-52cc8dcddaec
02/08/2025 09:13:37:INFO:Received: evaluate message 1b2d63c7-5b1f-453a-816f-52cc8dcddaec
[92mINFO [0m:      Sent reply
02/08/2025 09:19:25:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:20:02:INFO:
[92mINFO [0m:      Received: train message baba523f-2302-4248-b640-b1a729ec9b11
02/08/2025 09:20:02:INFO:Received: train message baba523f-2302-4248-b640-b1a729ec9b11
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 09:38:10:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:18:58:INFO:
[92mINFO [0m:      Received: evaluate message 8fd2b272-b092-48ae-b2eb-f366720280c1
02/08/2025 10:18:58:INFO:Received: evaluate message 8fd2b272-b092-48ae-b2eb-f366720280c1
[92mINFO [0m:      Sent reply
02/08/2025 10:24:18:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:25:28:INFO:
[92mINFO [0m:      Received: train message 9e1336f4-c3b0-4cca-8bdd-ab853445711f
02/08/2025 10:25:28:INFO:Received: train message 9e1336f4-c3b0-4cca-8bdd-ab853445711f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 10:41:31:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:23:45:INFO:
[92mINFO [0m:      Received: evaluate message 98e2bb76-a8d7-4d9b-86be-47a925bcb1f5
02/08/2025 11:23:45:INFO:Received: evaluate message 98e2bb76-a8d7-4d9b-86be-47a925bcb1f5
[92mINFO [0m:      Sent reply
02/08/2025 11:29:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:29:36:INFO:
[92mINFO [0m:      Received: train message 7f85a3cb-af69-4af7-8afe-78d4a928f0c5
02/08/2025 11:29:36:INFO:Received: train message 7f85a3cb-af69-4af7-8afe-78d4a928f0c5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 11:47:26:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:28:04:INFO:
[92mINFO [0m:      Received: evaluate message cd077eb3-2f68-40f0-aa21-774d3d76bfa1
02/08/2025 12:28:04:INFO:Received: evaluate message cd077eb3-2f68-40f0-aa21-774d3d76bfa1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 12:34:13:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:34:39:INFO:
[92mINFO [0m:      Received: train message e6b36077-5a0a-4917-a4ac-55da3af5d24a
02/08/2025 12:34:39:INFO:Received: train message e6b36077-5a0a-4917-a4ac-55da3af5d24a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 12:50:41:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:30:31:INFO:
[92mINFO [0m:      Received: evaluate message 687e3741-e3df-41bb-9fe8-9edd9f0b8077
02/08/2025 13:30:31:INFO:Received: evaluate message 687e3741-e3df-41bb-9fe8-9edd9f0b8077
[92mINFO [0m:      Sent reply
02/08/2025 13:34:53:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:35:31:INFO:
[92mINFO [0m:      Received: train message 258e664b-38d8-4b10-ba37-e46bfa765856
02/08/2025 13:35:31:INFO:Received: train message 258e664b-38d8-4b10-ba37-e46bfa765856
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 13:48:04:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:21:00:INFO:
[92mINFO [0m:      Received: evaluate message 63c94c00-725c-4729-aadc-42365faec72f
02/08/2025 14:21:00:INFO:Received: evaluate message 63c94c00-725c-4729-aadc-42365faec72f
[92mINFO [0m:      Sent reply
02/08/2025 14:25:35:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:26:11:INFO:
[92mINFO [0m:      Received: train message 142a8815-5611-442c-96b8-c134b1979ba6
02/08/2025 14:26:11:INFO:Received: train message 142a8815-5611-442c-96b8-c134b1979ba6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 14:39:18:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:11:16:INFO:
[92mINFO [0m:      Received: evaluate message bfd36ebf-a175-424f-be75-35a1c68be221
02/08/2025 15:11:16:INFO:Received: evaluate message bfd36ebf-a175-424f-be75-35a1c68be221

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 15:15:40:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:16:13:INFO:
[92mINFO [0m:      Received: train message a2d189ad-9845-4fdc-98a8-ca6627c7faee
02/08/2025 15:16:13:INFO:Received: train message a2d189ad-9845-4fdc-98a8-ca6627c7faee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 15:30:23:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:00:53:INFO:
[92mINFO [0m:      Received: evaluate message 392cd9a9-57d8-4ee4-ae26-45e5ef164c49
02/08/2025 16:00:53:INFO:Received: evaluate message 392cd9a9-57d8-4ee4-ae26-45e5ef164c49
[92mINFO [0m:      Sent reply
02/08/2025 16:05:16:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:05:52:INFO:
[92mINFO [0m:      Received: train message a148b52d-9529-4468-8dea-dbdbe4edd318
02/08/2025 16:05:52:INFO:Received: train message a148b52d-9529-4468-8dea-dbdbe4edd318
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 16:20:45:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:50:22:INFO:
[92mINFO [0m:      Received: evaluate message 009ace0f-4d76-4295-af97-a60edb33d059
02/08/2025 16:50:22:INFO:Received: evaluate message 009ace0f-4d76-4295-af97-a60edb33d059

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 16:55:00:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:55:37:INFO:
[92mINFO [0m:      Received: train message 12e9ef2d-549b-4b47-8bce-821dbb0a6ef8
02/08/2025 16:55:37:INFO:Received: train message 12e9ef2d-549b-4b47-8bce-821dbb0a6ef8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 17:10:55:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:39:21:INFO:
[92mINFO [0m:      Received: evaluate message 503b4544-7234-410a-8284-81f757296241
02/08/2025 17:39:21:INFO:Received: evaluate message 503b4544-7234-410a-8284-81f757296241
[92mINFO [0m:      Sent reply
02/08/2025 17:44:00:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:44:24:INFO:
[92mINFO [0m:      Received: train message d49765e8-6f76-4492-8fb8-e3fff3c74746
02/08/2025 17:44:24:INFO:Received: train message d49765e8-6f76-4492-8fb8-e3fff3c74746
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 17:59:09:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:28:01:INFO:
[92mINFO [0m:      Received: evaluate message 96bc9219-abcf-4ae6-9068-7f3e27727a31
02/08/2025 18:28:01:INFO:Received: evaluate message 96bc9219-abcf-4ae6-9068-7f3e27727a31

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 18:32:51:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:33:33:INFO:
[92mINFO [0m:      Received: train message bc6da9ba-4e49-41d3-94db-358c34211d7d
02/08/2025 18:33:33:INFO:Received: train message bc6da9ba-4e49-41d3-94db-358c34211d7d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:48:20:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:16:58:INFO:
[92mINFO [0m:      Received: evaluate message 28c495a4-89dd-41e6-9df8-e736c1994b58
02/08/2025 19:16:58:INFO:Received: evaluate message 28c495a4-89dd-41e6-9df8-e736c1994b58
[92mINFO [0m:      Sent reply
02/08/2025 19:21:42:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:22:29:INFO:
[92mINFO [0m:      Received: train message 422506f9-5085-4b02-9273-a9e2a881f294
02/08/2025 19:22:29:INFO:Received: train message 422506f9-5085-4b02-9273-a9e2a881f294
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 19:35:20:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:55:25:INFO:
[92mINFO [0m:      Received: evaluate message 76643999-ac87-4bc0-b81f-af968d6fe19c
02/08/2025 19:55:25:INFO:Received: evaluate message 76643999-ac87-4bc0-b81f-af968d6fe19c

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 19:59:34:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:59:53:INFO:
[92mINFO [0m:      Received: train message 24233d11-d91c-4f0e-a49d-d32916bcd06f
02/08/2025 19:59:53:INFO:Received: train message 24233d11-d91c-4f0e-a49d-d32916bcd06f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 20:11:42:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 20:32:03:INFO:
[92mINFO [0m:      Received: evaluate message 2bb9f33b-115d-4a71-ba4a-55f04fcba4f3
02/08/2025 20:32:03:INFO:Received: evaluate message 2bb9f33b-115d-4a71-ba4a-55f04fcba4f3
[92mINFO [0m:      Sent reply
02/08/2025 20:36:04:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 20:36:49:INFO:
[92mINFO [0m:      Received: train message ccde96db-4c57-43b9-b861-b1b1c270c924
02/08/2025 20:36:49:INFO:Received: train message ccde96db-4c57-43b9-b861-b1b1c270c924
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 20:48:58:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 21:08:58:INFO:
[92mINFO [0m:      Received: evaluate message 7ae266f0-8612-41e9-b4a0-4dec750d2fb4
02/08/2025 21:08:58:INFO:Received: evaluate message 7ae266f0-8612-41e9-b4a0-4dec750d2fb4

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 21:13:13:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 21:13:43:INFO:
[92mINFO [0m:      Received: train message bab2158e-0e5b-468b-96c7-351ec198ee50
02/08/2025 21:13:43:INFO:Received: train message bab2158e-0e5b-468b-96c7-351ec198ee50
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 21:25:56:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 21:45:49:INFO:
[92mINFO [0m:      Received: evaluate message 23f953f7-bbb3-4bc1-8d8c-5e930fc93f5f
02/08/2025 21:45:49:INFO:Received: evaluate message 23f953f7-bbb3-4bc1-8d8c-5e930fc93f5f
[92mINFO [0m:      Sent reply
02/08/2025 21:49:54:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 21:50:30:INFO:
[92mINFO [0m:      Received: train message 09e43b74-4d2f-461b-b52d-b31a53215885
02/08/2025 21:50:30:INFO:Received: train message 09e43b74-4d2f-461b-b52d-b31a53215885
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 22:02:22:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 22:23:11:INFO:
[92mINFO [0m:      Received: evaluate message e515118f-dab2-48bc-be80-a130a57eb35a
02/08/2025 22:23:11:INFO:Received: evaluate message e515118f-dab2-48bc-be80-a130a57eb35a

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708, 1.3185475385097838], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325, 0.9027586538608232], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927, 0.678026646197293], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682, 0.6509285601262981]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708, 1.3185475385097838, 1.2741964666823251], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325, 0.9027586538608232, 0.9043753022262412], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927, 0.678026646197293, 0.6766303262499491], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682, 0.6509285601262981, 0.6513272226171777]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 22:27:16:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 22:27:54:INFO:
[92mINFO [0m:      Received: train message 6461ddf2-f61c-493d-8b90-f72747ef700e
02/08/2025 22:27:54:INFO:Received: train message 6461ddf2-f61c-493d-8b90-f72747ef700e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 22:39:52:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 23:00:25:INFO:
[92mINFO [0m:      Received: evaluate message aee6667c-d86e-4c32-811f-344020c82592
02/08/2025 23:00:25:INFO:Received: evaluate message aee6667c-d86e-4c32-811f-344020c82592
[92mINFO [0m:      Sent reply
02/08/2025 23:04:30:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 23:05:02:INFO:
[92mINFO [0m:      Received: train message f0b4ce8b-c4cf-4009-98a7-6f08cc4d34e6
02/08/2025 23:05:02:INFO:Received: train message f0b4ce8b-c4cf-4009-98a7-6f08cc4d34e6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 23:17:06:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 23:37:26:INFO:
[92mINFO [0m:      Received: evaluate message 82cd7603-3a51-48d1-bef5-6bfdcaf87e9c
02/08/2025 23:37:26:INFO:Received: evaluate message 82cd7603-3a51-48d1-bef5-6bfdcaf87e9c

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708, 1.3185475385097838, 1.2741964666823251, 1.2812168351642748], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325, 0.9027586538608232, 0.9043753022262412, 0.9049618852789625], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927, 0.678026646197293, 0.6766303262499491, 0.6708704491403805], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682, 0.6509285601262981, 0.6513272226171777, 0.6526233140714557]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708, 1.3185475385097838, 1.2741964666823251, 1.2812168351642748, 1.2479766714107208], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438, 0.6697543294401933], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325, 0.9027586538608232, 0.9043753022262412, 0.9049618852789625, 0.9066339823013814], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927, 0.678026646197293, 0.6766303262499491, 0.6708704491403805, 0.6722172714652418], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438, 0.6697543294401933], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682, 0.6509285601262981, 0.6513272226171777, 0.6526233140714557, 0.6613982788101018]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 23:41:33:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 23:41:37:INFO:
[92mINFO [0m:      Received: reconnect message 16fa3935-12df-41c1-b717-7fd08adb4747
02/08/2025 23:41:37:INFO:Received: reconnect message 16fa3935-12df-41c1-b717-7fd08adb4747
02/08/2025 23:41:37:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/08/2025 23:41:37:INFO:Disconnect and shut down

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708, 1.3185475385097838, 1.2741964666823251, 1.2812168351642748, 1.2479766714107208, 1.3531703354608517], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438, 0.6697543294401933, 0.6508256141763995], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325, 0.9027586538608232, 0.9043753022262412, 0.9049618852789625, 0.9066339823013814, 0.9051206490422763], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927, 0.678026646197293, 0.6766303262499491, 0.6708704491403805, 0.6722172714652418, 0.6712632027903389], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438, 0.6697543294401933, 0.6508256141763995], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682, 0.6509285601262981, 0.6513272226171777, 0.6526233140714557, 0.6613982788101018, 0.6422848655738752]}



Final client history:
{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708, 1.3185475385097838, 1.2741964666823251, 1.2812168351642748, 1.2479766714107208, 1.3531703354608517], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438, 0.6697543294401933, 0.6508256141763995], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325, 0.9027586538608232, 0.9043753022262412, 0.9049618852789625, 0.9066339823013814, 0.9051206490422763], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927, 0.678026646197293, 0.6766303262499491, 0.6708704491403805, 0.6722172714652418, 0.6712632027903389], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438, 0.6697543294401933, 0.6508256141763995], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682, 0.6509285601262981, 0.6513272226171777, 0.6526233140714557, 0.6613982788101018, 0.6422848655738752]}

