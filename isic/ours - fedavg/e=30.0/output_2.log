nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=30.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/07/2025 20:16:25:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/07/2025 20:16:25:DEBUG:ChannelConnectivity.IDLE
02/07/2025 20:16:25:DEBUG:ChannelConnectivity.CONNECTING
02/07/2025 20:16:25:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738988185.993162 1681282 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/07/2025 20:26:10:INFO:
[92mINFO [0m:      Received: train message 0099237d-127c-4447-ad19-94a7b3047c97
02/07/2025 20:26:10:INFO:Received: train message 0099237d-127c-4447-ad19-94a7b3047c97
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 20:52:13:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:25:29:INFO:
[92mINFO [0m:      Received: evaluate message c607ecf1-e965-4322-b1be-867fffe82873
02/07/2025 21:25:29:INFO:Received: evaluate message c607ecf1-e965-4322-b1be-867fffe82873
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:30:55:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:31:29:INFO:
[92mINFO [0m:      Received: train message 12bfab79-c232-4b2f-bd61-052b3fa9b2c9
02/07/2025 21:31:29:INFO:Received: train message 12bfab79-c232-4b2f-bd61-052b3fa9b2c9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 21:58:33:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:28:19:INFO:
[92mINFO [0m:      Received: evaluate message 51d7705c-be8c-4bca-bf60-e608e22bc79e
02/07/2025 22:28:19:INFO:Received: evaluate message 51d7705c-be8c-4bca-bf60-e608e22bc79e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 22:34:50:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:35:53:INFO:
[92mINFO [0m:      Received: train message e2ddeb63-3884-404b-993d-af6e85970648
02/07/2025 22:35:53:INFO:Received: train message e2ddeb63-3884-404b-993d-af6e85970648
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 23:04:33:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:37:12:INFO:
[92mINFO [0m:      Received: evaluate message c6ef8cd8-b7b8-4787-9660-05f87a6a6560
02/07/2025 23:37:12:INFO:Received: evaluate message c6ef8cd8-b7b8-4787-9660-05f87a6a6560
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 23:43:34:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:44:18:INFO:
[92mINFO [0m:      Received: train message c5fa0815-d3d9-4c2d-bf88-8078bc0fe4c7
02/07/2025 23:44:18:INFO:Received: train message c5fa0815-d3d9-4c2d-bf88-8078bc0fe4c7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 00:10:16:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:40:52:INFO:
[92mINFO [0m:      Received: evaluate message 46210344-7892-405a-989d-7ec00051cccf
02/08/2025 00:40:52:INFO:Received: evaluate message 46210344-7892-405a-989d-7ec00051cccf
[92mINFO [0m:      Sent reply
02/08/2025 00:46:41:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:47:10:INFO:
[92mINFO [0m:      Received: train message 7141c0a1-1d2a-4ea4-85bb-c0f940829a11
02/08/2025 00:47:10:INFO:Received: train message 7141c0a1-1d2a-4ea4-85bb-c0f940829a11
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 01:14:37:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:43:14:INFO:
[92mINFO [0m:      Received: evaluate message 4a6f2d7c-5e4e-4847-be5a-6c9085c316e1
02/08/2025 01:43:14:INFO:Received: evaluate message 4a6f2d7c-5e4e-4847-be5a-6c9085c316e1
[92mINFO [0m:      Sent reply
02/08/2025 01:49:08:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:50:07:INFO:
[92mINFO [0m:      Received: train message 575caf52-120a-4ffc-867e-bbd69edf62ac
02/08/2025 01:50:07:INFO:Received: train message 575caf52-120a-4ffc-867e-bbd69edf62ac
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 02:15:32:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:48:09:INFO:
[92mINFO [0m:      Received: evaluate message 2159be3a-68f6-4872-8323-de8a3bbcec92
02/08/2025 02:48:09:INFO:Received: evaluate message 2159be3a-68f6-4872-8323-de8a3bbcec92
[92mINFO [0m:      Sent reply
02/08/2025 02:54:05:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:54:43:INFO:
[92mINFO [0m:      Received: train message c8eec570-fba3-47c2-a19f-4609d07e46a0
02/08/2025 02:54:43:INFO:Received: train message c8eec570-fba3-47c2-a19f-4609d07e46a0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 03:21:55:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:51:57:INFO:
[92mINFO [0m:      Received: evaluate message 0db8b563-370a-4095-b7bb-8e2c45560088
02/08/2025 03:51:57:INFO:Received: evaluate message 0db8b563-370a-4095-b7bb-8e2c45560088
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=30.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=30.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415], 'accuracy': [0.5734997986306887], 'auc': [0.8209263333238899], 'precision': [0.5896199585980895], 'recall': [0.5734997986306887], 'f1': [0.5251729041892587]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146], 'accuracy': [0.5734997986306887, 0.6061216270640355], 'auc': [0.8209263333238899, 0.8524348169466368], 'precision': [0.5896199585980895, 0.6033971678272441], 'recall': [0.5734997986306887, 0.6061216270640355], 'f1': [0.5251729041892587, 0.5708840472582581]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 03:57:51:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:58:40:INFO:
[92mINFO [0m:      Received: train message a26bb4f0-f271-4c51-80fc-011780d416d3
02/08/2025 03:58:40:INFO:Received: train message a26bb4f0-f271-4c51-80fc-011780d416d3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 04:25:06:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:57:14:INFO:
[92mINFO [0m:      Received: evaluate message eeb2bf3f-5fc8-4fb8-97be-9f6585e8e1d6
02/08/2025 04:57:14:INFO:Received: evaluate message eeb2bf3f-5fc8-4fb8-97be-9f6585e8e1d6
[92mINFO [0m:      Sent reply
02/08/2025 05:02:49:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:03:27:INFO:
[92mINFO [0m:      Received: train message d7ae6aa8-bbfe-4982-9c76-20b109973d34
02/08/2025 05:03:27:INFO:Received: train message d7ae6aa8-bbfe-4982-9c76-20b109973d34
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 05:28:29:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:01:28:INFO:
[92mINFO [0m:      Received: evaluate message 79acedfb-560a-4faa-88d0-fba522b4cbb7
02/08/2025 06:01:28:INFO:Received: evaluate message 79acedfb-560a-4faa-88d0-fba522b4cbb7
[92mINFO [0m:      Sent reply
02/08/2025 06:07:18:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:07:41:INFO:
[92mINFO [0m:      Received: train message 67ce9722-100e-4dc1-b2f6-4c755b7a8c21
02/08/2025 06:07:41:INFO:Received: train message 67ce9722-100e-4dc1-b2f6-4c755b7a8c21
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 06:35:05:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:05:02:INFO:
[92mINFO [0m:      Received: evaluate message d44a9077-889f-4d84-9afe-a40df92fd042
02/08/2025 07:05:02:INFO:Received: evaluate message d44a9077-889f-4d84-9afe-a40df92fd042
[92mINFO [0m:      Sent reply
02/08/2025 07:11:20:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:12:02:INFO:
[92mINFO [0m:      Received: train message caba3752-37a9-4300-9613-f7786f475743
02/08/2025 07:12:02:INFO:Received: train message caba3752-37a9-4300-9613-f7786f475743
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 07:36:46:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:10:28:INFO:
[92mINFO [0m:      Received: evaluate message 1107f7ba-a773-4455-b1b0-1c843641a29c
02/08/2025 08:10:28:INFO:Received: evaluate message 1107f7ba-a773-4455-b1b0-1c843641a29c
[92mINFO [0m:      Sent reply
02/08/2025 08:15:50:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:16:35:INFO:
[92mINFO [0m:      Received: train message b973ea16-02a0-4ab7-8023-d42f8ecae9e0
02/08/2025 08:16:35:INFO:Received: train message b973ea16-02a0-4ab7-8023-d42f8ecae9e0

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482]}

Step 1b: Recomputing FIM for epoch 12
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 08:42:58:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:13:57:INFO:
[92mINFO [0m:      Received: evaluate message 3ac90f78-5b05-4163-9903-d22faa71a4e0
02/08/2025 09:13:57:INFO:Received: evaluate message 3ac90f78-5b05-4163-9903-d22faa71a4e0
[92mINFO [0m:      Sent reply
02/08/2025 09:19:46:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:20:11:INFO:
[92mINFO [0m:      Received: train message ed700f5d-679e-4148-903b-853c51ada79e
02/08/2025 09:20:11:INFO:Received: train message ed700f5d-679e-4148-903b-853c51ada79e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 09:46:52:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:19:28:INFO:
[92mINFO [0m:      Received: evaluate message 0f20f806-d3fe-4823-9973-8ba96b645712
02/08/2025 10:19:28:INFO:Received: evaluate message 0f20f806-d3fe-4823-9973-8ba96b645712
[92mINFO [0m:      Sent reply
02/08/2025 10:24:53:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:25:30:INFO:
[92mINFO [0m:      Received: train message 5cabb1ca-092d-4490-83e7-713ff5fb630e
02/08/2025 10:25:30:INFO:Received: train message 5cabb1ca-092d-4490-83e7-713ff5fb630e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 10:49:15:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:23:51:INFO:
[92mINFO [0m:      Received: evaluate message f3e804aa-8e0a-4c3e-8d9e-02a7b84c735a
02/08/2025 11:23:51:INFO:Received: evaluate message f3e804aa-8e0a-4c3e-8d9e-02a7b84c735a
[92mINFO [0m:      Sent reply
02/08/2025 11:29:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:29:38:INFO:
[92mINFO [0m:      Received: train message 61b80ae0-bd50-45ec-99a6-d3722422fe59
02/08/2025 11:29:38:INFO:Received: train message 61b80ae0-bd50-45ec-99a6-d3722422fe59
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 11:57:06:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:28:07:INFO:
[92mINFO [0m:      Received: evaluate message 93801483-7262-414f-93d7-4eb2b2c79466
02/08/2025 12:28:07:INFO:Received: evaluate message 93801483-7262-414f-93d7-4eb2b2c79466
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 12:34:17:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:34:54:INFO:
[92mINFO [0m:      Received: train message c9d31b51-f8f9-488c-abc5-5954c9d7be90
02/08/2025 12:34:54:INFO:Received: train message c9d31b51-f8f9-488c-abc5-5954c9d7be90
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 12:58:51:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:30:36:INFO:
[92mINFO [0m:      Received: evaluate message 8abb7a3c-8012-40ea-9927-cd03afbbf37f
02/08/2025 13:30:36:INFO:Received: evaluate message 8abb7a3c-8012-40ea-9927-cd03afbbf37f
[92mINFO [0m:      Sent reply
02/08/2025 13:34:56:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:35:13:INFO:
[92mINFO [0m:      Received: train message 004dbf26-edfb-489a-a3c6-3d057f718371
02/08/2025 13:35:13:INFO:Received: train message 004dbf26-edfb-489a-a3c6-3d057f718371
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 13:55:22:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:20:53:INFO:
[92mINFO [0m:      Received: evaluate message 86293a33-a361-44df-a190-a4bf214aa947
02/08/2025 14:20:53:INFO:Received: evaluate message 86293a33-a361-44df-a190-a4bf214aa947
[92mINFO [0m:      Sent reply
02/08/2025 14:25:29:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:26:13:INFO:
[92mINFO [0m:      Received: train message 3b25bbbc-37d2-4c5c-aeb7-3b81502b186e
02/08/2025 14:26:13:INFO:Received: train message 3b25bbbc-37d2-4c5c-aeb7-3b81502b186e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 14:47:49:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:11:13:INFO:
[92mINFO [0m:      Received: evaluate message 01cac030-0735-4786-bdcc-66380198393d
02/08/2025 15:11:13:INFO:Received: evaluate message 01cac030-0735-4786-bdcc-66380198393d

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 15:15:38:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:16:19:INFO:
[92mINFO [0m:      Received: train message 9b8ae37e-792f-42f3-8ab6-8d77cead9561
02/08/2025 15:16:19:INFO:Received: train message 9b8ae37e-792f-42f3-8ab6-8d77cead9561
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 15:38:39:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:00:29:INFO:
[92mINFO [0m:      Received: evaluate message 52ccf9ae-d111-4e53-9416-4e5bfb8775bb
02/08/2025 16:00:29:INFO:Received: evaluate message 52ccf9ae-d111-4e53-9416-4e5bfb8775bb
[92mINFO [0m:      Sent reply
02/08/2025 16:04:45:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:05:42:INFO:
[92mINFO [0m:      Received: train message 6cea9c82-c9ce-4d0a-929a-86f8528cbc7a
02/08/2025 16:05:42:INFO:Received: train message 6cea9c82-c9ce-4d0a-929a-86f8528cbc7a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 16:28:48:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:50:11:INFO:
[92mINFO [0m:      Received: evaluate message 3086a8ec-7d5c-490a-af82-a56db60d89b9
02/08/2025 16:50:11:INFO:Received: evaluate message 3086a8ec-7d5c-490a-af82-a56db60d89b9

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 16:54:41:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:55:27:INFO:
[92mINFO [0m:      Received: train message 3eb45cf2-90b5-41db-87d0-51bb5667d441
02/08/2025 16:55:27:INFO:Received: train message 3eb45cf2-90b5-41db-87d0-51bb5667d441
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 17:18:44:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:39:14:INFO:
[92mINFO [0m:      Received: evaluate message c082c9e3-a2cc-4e21-b6b7-3196e11c0fec
02/08/2025 17:39:14:INFO:Received: evaluate message c082c9e3-a2cc-4e21-b6b7-3196e11c0fec
[92mINFO [0m:      Sent reply
02/08/2025 17:43:52:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:44:36:INFO:
[92mINFO [0m:      Received: train message 36e35f64-3a4e-4956-b124-80b6d82434ae
02/08/2025 17:44:36:INFO:Received: train message 36e35f64-3a4e-4956-b124-80b6d82434ae
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:07:30:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:27:47:INFO:
[92mINFO [0m:      Received: evaluate message 7972bc5c-a430-4a3d-a87b-6edf68936e67
02/08/2025 18:27:47:INFO:Received: evaluate message 7972bc5c-a430-4a3d-a87b-6edf68936e67

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 18:32:37:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:33:19:INFO:
[92mINFO [0m:      Received: train message 559c7be9-d15a-4c79-89b7-9474ea6b9420
02/08/2025 18:33:19:INFO:Received: train message 559c7be9-d15a-4c79-89b7-9474ea6b9420
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:55:59:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:17:05:INFO:
[92mINFO [0m:      Received: evaluate message a2045943-7a68-4ca7-89d0-6a7a36cbe3f5
02/08/2025 19:17:05:INFO:Received: evaluate message a2045943-7a68-4ca7-89d0-6a7a36cbe3f5
[92mINFO [0m:      Sent reply
02/08/2025 19:21:52:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:22:29:INFO:
[92mINFO [0m:      Received: train message cb6ba27e-3f0f-4d66-85e4-cce6e6096fda
02/08/2025 19:22:29:INFO:Received: train message cb6ba27e-3f0f-4d66-85e4-cce6e6096fda
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 19:40:46:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:55:23:INFO:
[92mINFO [0m:      Received: evaluate message ab84bb30-6354-4eae-8561-dca9f915cdaa
02/08/2025 19:55:23:INFO:Received: evaluate message ab84bb30-6354-4eae-8561-dca9f915cdaa

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 19:59:34:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 20:00:01:INFO:
[92mINFO [0m:      Received: train message e06e8fbd-50ca-4503-9c73-e45706ab8d90
02/08/2025 20:00:01:INFO:Received: train message e06e8fbd-50ca-4503-9c73-e45706ab8d90
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 20:17:32:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 20:32:14:INFO:
[92mINFO [0m:      Received: evaluate message 3e02fbd4-000d-463d-ba93-226b94c93114
02/08/2025 20:32:14:INFO:Received: evaluate message 3e02fbd4-000d-463d-ba93-226b94c93114
[92mINFO [0m:      Sent reply
02/08/2025 20:36:22:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 20:36:54:INFO:
[92mINFO [0m:      Received: train message 15f8e886-8599-4947-b6f9-baa9e5db7e99
02/08/2025 20:36:54:INFO:Received: train message 15f8e886-8599-4947-b6f9-baa9e5db7e99
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 20:54:18:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 21:08:56:INFO:
[92mINFO [0m:      Received: evaluate message 603a5fd8-bbbd-469c-8e87-7009da31e9e4
02/08/2025 21:08:56:INFO:Received: evaluate message 603a5fd8-bbbd-469c-8e87-7009da31e9e4

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 21:13:04:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 21:13:47:INFO:
[92mINFO [0m:      Received: train message a94ab7a6-ba95-4b9e-9dbf-ab86a3560f55
02/08/2025 21:13:47:INFO:Received: train message a94ab7a6-ba95-4b9e-9dbf-ab86a3560f55
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 21:31:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 21:45:43:INFO:
[92mINFO [0m:      Received: evaluate message d8e19597-7774-4d01-a854-c4d8c3615240
02/08/2025 21:45:43:INFO:Received: evaluate message d8e19597-7774-4d01-a854-c4d8c3615240
[92mINFO [0m:      Sent reply
02/08/2025 21:49:47:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 21:50:30:INFO:
[92mINFO [0m:      Received: train message 3b0af8e5-f14d-406a-a434-7f7a54de3d5e
02/08/2025 21:50:30:INFO:Received: train message 3b0af8e5-f14d-406a-a434-7f7a54de3d5e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 22:07:48:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 22:22:52:INFO:
[92mINFO [0m:      Received: evaluate message 2a6373fb-598e-46e1-b5a6-47db7a695be3
02/08/2025 22:22:52:INFO:Received: evaluate message 2a6373fb-598e-46e1-b5a6-47db7a695be3

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708, 1.3185475385097838], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325, 0.9027586538608232], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927, 0.678026646197293], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682, 0.6509285601262981]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708, 1.3185475385097838, 1.2741964666823251], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325, 0.9027586538608232, 0.9043753022262412], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927, 0.678026646197293, 0.6766303262499491], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682, 0.6509285601262981, 0.6513272226171777]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 22:26:15:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 22:27:32:INFO:
[92mINFO [0m:      Received: train message ddebb86c-9672-4dd3-97be-380f4bc1aacc
02/08/2025 22:27:32:INFO:Received: train message ddebb86c-9672-4dd3-97be-380f4bc1aacc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 22:44:58:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 23:00:23:INFO:
[92mINFO [0m:      Received: evaluate message ee08377f-86eb-4caa-be82-bdde614c53a3
02/08/2025 23:00:23:INFO:Received: evaluate message ee08377f-86eb-4caa-be82-bdde614c53a3
[92mINFO [0m:      Sent reply
02/08/2025 23:04:29:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 23:04:56:INFO:
[92mINFO [0m:      Received: train message bc1533f5-d75c-4043-ab03-d38da18c7b2d
02/08/2025 23:04:56:INFO:Received: train message bc1533f5-d75c-4043-ab03-d38da18c7b2d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 23:22:30:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 23:37:13:INFO:
[92mINFO [0m:      Received: evaluate message 4f217525-daf1-408e-830f-ad8d57dd9140
02/08/2025 23:37:13:INFO:Received: evaluate message 4f217525-daf1-408e-830f-ad8d57dd9140

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708, 1.3185475385097838, 1.2741964666823251, 1.2812168351642748], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325, 0.9027586538608232, 0.9043753022262412, 0.9049618852789625], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927, 0.678026646197293, 0.6766303262499491, 0.6708704491403805], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682, 0.6509285601262981, 0.6513272226171777, 0.6526233140714557]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708, 1.3185475385097838, 1.2741964666823251, 1.2812168351642748, 1.2479766714107208], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438, 0.6697543294401933], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325, 0.9027586538608232, 0.9043753022262412, 0.9049618852789625, 0.9066339823013814], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927, 0.678026646197293, 0.6766303262499491, 0.6708704491403805, 0.6722172714652418], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438, 0.6697543294401933], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682, 0.6509285601262981, 0.6513272226171777, 0.6526233140714557, 0.6613982788101018]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 23:40:49:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 23:41:37:INFO:
[92mINFO [0m:      Received: reconnect message b6c2ba3c-6438-479e-b258-de5ac380d57b
02/08/2025 23:41:37:INFO:Received: reconnect message b6c2ba3c-6438-479e-b258-de5ac380d57b
02/08/2025 23:41:38:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/08/2025 23:41:38:INFO:Disconnect and shut down

{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708, 1.3185475385097838, 1.2741964666823251, 1.2812168351642748, 1.2479766714107208, 1.3531703354608517], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438, 0.6697543294401933, 0.6508256141763995], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325, 0.9027586538608232, 0.9043753022262412, 0.9049618852789625, 0.9066339823013814, 0.9051206490422763], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927, 0.678026646197293, 0.6766303262499491, 0.6708704491403805, 0.6722172714652418, 0.6712632027903389], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438, 0.6697543294401933, 0.6508256141763995], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682, 0.6509285601262981, 0.6513272226171777, 0.6526233140714557, 0.6613982788101018, 0.6422848655738752]}



Final client history:
{'loss': [1.3763256583826415, 1.3285103418680146, 1.3373394124795004, 1.3422987826447597, 1.3102117085764113, 1.3099978007331186, 1.329323493997464, 1.2717419178121954, 1.2958960361745728, 1.2888935617367394, 1.2881886062965824, 1.2887983894905062, 1.312919552965698, 1.3169054160231406, 1.2907440043292697, 1.321296288416936, 1.2718990556760705, 1.290925056474419, 1.2610760287944702, 1.276824944666495, 1.2620323399262614, 1.298693851794728, 1.2951653664312774, 1.3044213814405103, 1.2847175358573708, 1.3185475385097838, 1.2741964666823251, 1.2812168351642748, 1.2479766714107208, 1.3531703354608517], 'accuracy': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438, 0.6697543294401933, 0.6508256141763995], 'auc': [0.8209263333238899, 0.8524348169466368, 0.8667376756683434, 0.8745173398074795, 0.8782702551231041, 0.8801935054025718, 0.88263935537671, 0.887688678788253, 0.8889845428036838, 0.8908908422457996, 0.8926486526794739, 0.8938847058924138, 0.8931904755600379, 0.8945284681199597, 0.8958333912742964, 0.8964178056890348, 0.8976175956966725, 0.8973934394207038, 0.8978345893454193, 0.8987692165307118, 0.8997203127063986, 0.8996870757485629, 0.9007796017681694, 0.9011216759205583, 0.9029478773060325, 0.9027586538608232, 0.9043753022262412, 0.9049618852789625, 0.9066339823013814, 0.9051206490422763], 'precision': [0.5896199585980895, 0.6033971678272441, 0.62144158439859, 0.6301441218311281, 0.6218734892459606, 0.6328067685281672, 0.640615794509741, 0.6441346852042644, 0.644807241713974, 0.6415755643940348, 0.6430752057990011, 0.640544675079917, 0.64713648551798, 0.6460003433712027, 0.6491245310723439, 0.6396580476596774, 0.6599851599103794, 0.6533956895846124, 0.6515813903201264, 0.6549487231913611, 0.6657130767185281, 0.6669345697944524, 0.6664606460734918, 0.666695373703402, 0.6730571937320927, 0.678026646197293, 0.6766303262499491, 0.6708704491403805, 0.6722172714652418, 0.6712632027903389], 'recall': [0.5734997986306887, 0.6061216270640355, 0.6190092629883206, 0.6294804671768023, 0.6347160692710431, 0.6383407168747482, 0.6415626258558196, 0.6512283527990335, 0.6480064438179621, 0.6455900120821587, 0.6472009665726943, 0.644381796214257, 0.6492146596858639, 0.6472009665726943, 0.6528393072895691, 0.63954893274265, 0.6560612162706404, 0.6508256141763995, 0.6580749093838099, 0.6608940797422472, 0.6568666935159082, 0.6596858638743456, 0.6633105114780508, 0.662505034232783, 0.6608940797422472, 0.6608940797422472, 0.657672170761176, 0.6584776480064438, 0.6697543294401933, 0.6508256141763995], 'f1': [0.5251729041892587, 0.5708840472582581, 0.5892360851580035, 0.598800274008997, 0.6070932447135345, 0.6151881228635566, 0.6158988570268031, 0.6321393128720301, 0.6275179048620037, 0.6277193351722631, 0.6275907055431482, 0.6262783389599704, 0.6306495462552276, 0.6313109858859152, 0.6359831572121898, 0.6234353526377916, 0.6445620112605964, 0.6382335618121616, 0.6448924165569366, 0.6467796320689707, 0.6493960416087778, 0.6455838373810933, 0.6490171879912949, 0.6478382917736635, 0.6519532596938682, 0.6509285601262981, 0.6513272226171777, 0.6526233140714557, 0.6613982788101018, 0.6422848655738752]}

