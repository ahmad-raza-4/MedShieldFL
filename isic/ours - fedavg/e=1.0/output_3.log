nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/07/2025 20:10:30:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/07/2025 20:10:30:DEBUG:ChannelConnectivity.IDLE
02/07/2025 20:10:30:DEBUG:ChannelConnectivity.CONNECTING
02/07/2025 20:10:30:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738987830.786802 1677083 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/07/2025 20:16:46:INFO:
[92mINFO [0m:      Received: train message 16badb81-7823-4c69-8bac-43285a9fbe12
02/07/2025 20:16:46:INFO:Received: train message 16badb81-7823-4c69-8bac-43285a9fbe12
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 20:36:41:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 20:57:49:INFO:
[92mINFO [0m:      Received: evaluate message 42522341-f644-4bec-88d2-2b709ca86194
02/07/2025 20:57:49:INFO:Received: evaluate message 42522341-f644-4bec-88d2-2b709ca86194
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:02:31:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:03:07:INFO:
[92mINFO [0m:      Received: train message 25ee5f9a-0cb8-474b-ad21-7d8a3fa9236d
02/07/2025 21:03:07:INFO:Received: train message 25ee5f9a-0cb8-474b-ad21-7d8a3fa9236d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 21:21:14:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:44:14:INFO:
[92mINFO [0m:      Received: evaluate message a2ddf0ec-94e9-43c2-a909-cb3261e22700
02/07/2025 21:44:14:INFO:Received: evaluate message a2ddf0ec-94e9-43c2-a909-cb3261e22700
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:49:01:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:49:41:INFO:
[92mINFO [0m:      Received: train message 086b836c-2458-4cd2-90b3-883f88649200
02/07/2025 21:49:41:INFO:Received: train message 086b836c-2458-4cd2-90b3-883f88649200
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 22:08:14:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:28:48:INFO:
[92mINFO [0m:      Received: evaluate message 8b7b35ce-1267-4e1a-87e4-96af395ccac8
02/07/2025 22:28:48:INFO:Received: evaluate message 8b7b35ce-1267-4e1a-87e4-96af395ccac8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 22:34:24:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:35:21:INFO:
[92mINFO [0m:      Received: train message 97fc803c-57bf-42ae-8d86-7a0e6ed27ede
02/07/2025 22:35:21:INFO:Received: train message 97fc803c-57bf-42ae-8d86-7a0e6ed27ede
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 22:57:48:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:18:22:INFO:
[92mINFO [0m:      Received: evaluate message 288b7a34-dc1e-4e1b-82a5-c6d5877d1223
02/07/2025 23:18:22:INFO:Received: evaluate message 288b7a34-dc1e-4e1b-82a5-c6d5877d1223
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 23:23:10:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:23:50:INFO:
[92mINFO [0m:      Received: train message 2d344f5c-d5e2-45e7-90a9-b05e00b93c3c
02/07/2025 23:23:50:INFO:Received: train message 2d344f5c-d5e2-45e7-90a9-b05e00b93c3c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 23:42:16:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:04:55:INFO:
[92mINFO [0m:      Received: evaluate message e6bd5202-fc4d-4ab1-8537-d9db6c88cacd
02/08/2025 00:04:55:INFO:Received: evaluate message e6bd5202-fc4d-4ab1-8537-d9db6c88cacd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:09:32:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:10:50:INFO:
[92mINFO [0m:      Received: train message 52a2da14-e180-4ed1-a5e8-89ecd1467dd9
02/08/2025 00:10:50:INFO:Received: train message 52a2da14-e180-4ed1-a5e8-89ecd1467dd9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 00:28:36:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:52:07:INFO:
[92mINFO [0m:      Received: evaluate message 15a67766-08a8-4039-98ce-d1b778db0176
02/08/2025 00:52:07:INFO:Received: evaluate message 15a67766-08a8-4039-98ce-d1b778db0176
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:57:15:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:58:00:INFO:
[92mINFO [0m:      Received: train message c088caa9-d918-4d1b-bf2a-feba202374de
02/08/2025 00:58:00:INFO:Received: train message c088caa9-d918-4d1b-bf2a-feba202374de
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 01:16:26:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:38:20:INFO:
[92mINFO [0m:      Received: evaluate message 2179660c-7254-4003-a70c-4418c6fc7e72
02/08/2025 01:38:20:INFO:Received: evaluate message 2179660c-7254-4003-a70c-4418c6fc7e72
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794], 'accuracy': [0.47643979057591623], 'auc': [0.7113553522460114], 'precision': [0.4601909378305515], 'recall': [0.47643979057591623], 'f1': [0.39597907648163966]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782], 'accuracy': [0.47643979057591623, 0.5501409585179219], 'auc': [0.7113553522460114, 0.7558628759678585], 'precision': [0.4601909378305515, 0.4857540893933654], 'recall': [0.47643979057591623, 0.5501409585179219], 'f1': [0.39597907648163966, 0.4846537523434621]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 01:42:55:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:43:18:INFO:
[92mINFO [0m:      Received: train message 997b57df-9f38-4679-98c5-280d5dd2b7a8
02/08/2025 01:43:18:INFO:Received: train message 997b57df-9f38-4679-98c5-280d5dd2b7a8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 02:02:26:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:24:37:INFO:
[92mINFO [0m:      Received: evaluate message fa2a0f44-f762-46ca-a27e-f22ce31357b9
02/08/2025 02:24:37:INFO:Received: evaluate message fa2a0f44-f762-46ca-a27e-f22ce31357b9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 02:29:11:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:30:04:INFO:
[92mINFO [0m:      Received: train message 1fae8074-e0fb-4811-b857-df0263bb0ee1
02/08/2025 02:30:04:INFO:Received: train message 1fae8074-e0fb-4811-b857-df0263bb0ee1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 02:47:27:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:12:04:INFO:
[92mINFO [0m:      Received: evaluate message 3383644f-41a4-4133-b477-865da4ce1458
02/08/2025 03:12:04:INFO:Received: evaluate message 3383644f-41a4-4133-b477-865da4ce1458
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 03:16:47:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:17:47:INFO:
[92mINFO [0m:      Received: train message 5dc77dd3-9b33-41f1-9250-8103c15ceb34
02/08/2025 03:17:47:INFO:Received: train message 5dc77dd3-9b33-41f1-9250-8103c15ceb34
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 03:35:17:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:58:14:INFO:
[92mINFO [0m:      Received: evaluate message a9e960c7-4148-4bf4-8cec-e207889055e1
02/08/2025 03:58:14:INFO:Received: evaluate message a9e960c7-4148-4bf4-8cec-e207889055e1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 04:03:19:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:03:40:INFO:
[92mINFO [0m:      Received: train message e6a8974b-f315-498c-a374-f229909989ba
02/08/2025 04:03:40:INFO:Received: train message e6a8974b-f315-498c-a374-f229909989ba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 04:21:15:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:43:06:INFO:
[92mINFO [0m:      Received: evaluate message 45dc70dc-9f8c-45b9-8f32-ef776417543d
02/08/2025 04:43:06:INFO:Received: evaluate message 45dc70dc-9f8c-45b9-8f32-ef776417543d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 04:47:32:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:48:33:INFO:
[92mINFO [0m:      Received: train message ac0cbaa8-5d2e-4311-beed-6fe29a5cc105
02/08/2025 04:48:33:INFO:Received: train message ac0cbaa8-5d2e-4311-beed-6fe29a5cc105

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 05:06:47:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:29:51:INFO:
[92mINFO [0m:      Received: evaluate message 0583e7f2-36bc-4383-94de-d61a4daef55e
02/08/2025 05:29:51:INFO:Received: evaluate message 0583e7f2-36bc-4383-94de-d61a4daef55e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 05:34:32:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:35:19:INFO:
[92mINFO [0m:      Received: train message 0b1ec65f-8a1a-46dc-a5e2-fa31d3eb9046
02/08/2025 05:35:19:INFO:Received: train message 0b1ec65f-8a1a-46dc-a5e2-fa31d3eb9046
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 05:52:59:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:16:37:INFO:
[92mINFO [0m:      Received: evaluate message 41dc64c2-ae2a-4c45-9572-0965ccaadd2b
02/08/2025 06:16:37:INFO:Received: evaluate message 41dc64c2-ae2a-4c45-9572-0965ccaadd2b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 06:21:08:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:21:55:INFO:
[92mINFO [0m:      Received: train message cd190886-9c2a-47ca-b3e6-3e52e6be48b6
02/08/2025 06:21:55:INFO:Received: train message cd190886-9c2a-47ca-b3e6-3e52e6be48b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 06:39:42:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:01:48:INFO:
[92mINFO [0m:      Received: evaluate message 5a7a63f2-308d-4ca7-92e5-27cf8de626f8
02/08/2025 07:01:48:INFO:Received: evaluate message 5a7a63f2-308d-4ca7-92e5-27cf8de626f8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 07:06:27:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:07:08:INFO:
[92mINFO [0m:      Received: train message 7206f3db-69b9-48a7-91af-b4e2143c0437
02/08/2025 07:07:08:INFO:Received: train message 7206f3db-69b9-48a7-91af-b4e2143c0437
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 07:26:14:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:48:21:INFO:
[92mINFO [0m:      Received: evaluate message e4178e6f-1fc7-483d-8669-e6ecad0cc486
02/08/2025 07:48:21:INFO:Received: evaluate message e4178e6f-1fc7-483d-8669-e6ecad0cc486
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 07:52:51:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:53:26:INFO:
[92mINFO [0m:      Received: train message 3976b6b5-51f8-4fbb-8e33-e6b3029e5cc7
02/08/2025 07:53:26:INFO:Received: train message 3976b6b5-51f8-4fbb-8e33-e6b3029e5cc7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 08:11:35:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:35:35:INFO:
[92mINFO [0m:      Received: evaluate message ea544091-a393-40ef-814b-f48d170c8762
02/08/2025 08:35:35:INFO:Received: evaluate message ea544091-a393-40ef-814b-f48d170c8762
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 08:40:14:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:40:50:INFO:
[92mINFO [0m:      Received: train message baa68db4-8727-4225-9b7b-dfdbbc54e02c
02/08/2025 08:40:50:INFO:Received: train message baa68db4-8727-4225-9b7b-dfdbbc54e02c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 08:58:13:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:20:18:INFO:
[92mINFO [0m:      Received: evaluate message 663e6bb6-3f69-4c50-985e-5fa01b690641
02/08/2025 09:20:18:INFO:Received: evaluate message 663e6bb6-3f69-4c50-985e-5fa01b690641
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 09:24:58:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:25:49:INFO:
[92mINFO [0m:      Received: train message d5228c4c-7897-45e6-8266-6385ed3af3e3
02/08/2025 09:25:49:INFO:Received: train message d5228c4c-7897-45e6-8266-6385ed3af3e3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 09:44:01:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:05:02:INFO:
[92mINFO [0m:      Received: evaluate message b7655c51-499f-41cf-b4ca-0801720d9ba0
02/08/2025 10:05:02:INFO:Received: evaluate message b7655c51-499f-41cf-b4ca-0801720d9ba0

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 10:09:35:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:10:10:INFO:
[92mINFO [0m:      Received: train message b38022a2-e09d-460c-8c78-a450a9e1868a
02/08/2025 10:10:10:INFO:Received: train message b38022a2-e09d-460c-8c78-a450a9e1868a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 10:28:15:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:50:18:INFO:
[92mINFO [0m:      Received: evaluate message e4316155-292f-4c08-94d0-c22db06b86d8
02/08/2025 10:50:18:INFO:Received: evaluate message e4316155-292f-4c08-94d0-c22db06b86d8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 10:54:47:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:55:22:INFO:
[92mINFO [0m:      Received: train message e16e98c6-235b-49e8-9217-c913c2f833ba
02/08/2025 10:55:22:INFO:Received: train message e16e98c6-235b-49e8-9217-c913c2f833ba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 11:12:44:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:34:46:INFO:
[92mINFO [0m:      Received: evaluate message ef3e52d4-f402-4d1f-b54e-c39c73771d98
02/08/2025 11:34:46:INFO:Received: evaluate message ef3e52d4-f402-4d1f-b54e-c39c73771d98

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 11:39:53:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:40:48:INFO:
[92mINFO [0m:      Received: train message c99013ab-f2af-4603-b376-03d9c9f2ad76
02/08/2025 11:40:48:INFO:Received: train message c99013ab-f2af-4603-b376-03d9c9f2ad76
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 11:59:04:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:19:04:INFO:
[92mINFO [0m:      Received: evaluate message 8ffa3dbb-7586-474b-a144-728a479e7c7b
02/08/2025 12:19:04:INFO:Received: evaluate message 8ffa3dbb-7586-474b-a144-728a479e7c7b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 12:23:42:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:24:23:INFO:
[92mINFO [0m:      Received: train message 3f1302af-9131-43f2-89e1-4bc6a86f975f
02/08/2025 12:24:23:INFO:Received: train message 3f1302af-9131-43f2-89e1-4bc6a86f975f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 12:43:55:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:04:58:INFO:
[92mINFO [0m:      Received: evaluate message ea7f636a-2d53-47d3-90e5-fd630e08baf8
02/08/2025 13:04:58:INFO:Received: evaluate message ea7f636a-2d53-47d3-90e5-fd630e08baf8

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 13:09:58:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:11:04:INFO:
[92mINFO [0m:      Received: train message 50cba3b5-cb30-4fdb-9d28-8b90218d5348
02/08/2025 13:11:04:INFO:Received: train message 50cba3b5-cb30-4fdb-9d28-8b90218d5348
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 13:29:25:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:52:01:INFO:
[92mINFO [0m:      Received: evaluate message 2e43f1b6-2a9f-4f4b-8c5f-0af10141cc60
02/08/2025 13:52:01:INFO:Received: evaluate message 2e43f1b6-2a9f-4f4b-8c5f-0af10141cc60
[92mINFO [0m:      Sent reply
02/08/2025 13:56:31:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:57:34:INFO:
[92mINFO [0m:      Received: train message 958f3292-5eac-4a61-b32a-d0b10e5a5dce
02/08/2025 13:57:34:INFO:Received: train message 958f3292-5eac-4a61-b32a-d0b10e5a5dce
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 14:15:59:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:38:36:INFO:
[92mINFO [0m:      Received: evaluate message 8fd70d11-de04-4569-93ea-b0767f85a935
02/08/2025 14:38:36:INFO:Received: evaluate message 8fd70d11-de04-4569-93ea-b0767f85a935

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 14:43:04:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:44:18:INFO:
[92mINFO [0m:      Received: train message 76118478-34ac-4a11-b377-efaf8d98015a
02/08/2025 14:44:18:INFO:Received: train message 76118478-34ac-4a11-b377-efaf8d98015a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 15:02:08:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:24:03:INFO:
[92mINFO [0m:      Received: evaluate message d53d3111-01b1-4536-992b-f59c818d84d5
02/08/2025 15:24:03:INFO:Received: evaluate message d53d3111-01b1-4536-992b-f59c818d84d5
[92mINFO [0m:      Sent reply
02/08/2025 15:28:29:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:29:35:INFO:
[92mINFO [0m:      Received: train message 46a3f5f6-e9f3-4c25-88d1-465b9c502698
02/08/2025 15:29:35:INFO:Received: train message 46a3f5f6-e9f3-4c25-88d1-465b9c502698
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 15:48:05:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:10:06:INFO:
[92mINFO [0m:      Received: evaluate message 58dd5226-f1a1-4e7a-96db-6c86d6a0efd2
02/08/2025 16:10:06:INFO:Received: evaluate message 58dd5226-f1a1-4e7a-96db-6c86d6a0efd2

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 16:14:18:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:15:39:INFO:
[92mINFO [0m:      Received: train message 0343ddea-0685-46fe-ab39-f46f767d2ad7
02/08/2025 16:15:39:INFO:Received: train message 0343ddea-0685-46fe-ab39-f46f767d2ad7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 16:34:56:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:56:15:INFO:
[92mINFO [0m:      Received: evaluate message 392d6080-343b-499d-92d9-5214b5749d6b
02/08/2025 16:56:15:INFO:Received: evaluate message 392d6080-343b-499d-92d9-5214b5749d6b
[92mINFO [0m:      Sent reply
02/08/2025 17:00:38:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:01:17:INFO:
[92mINFO [0m:      Received: train message 7bf5c975-2fbd-49e9-b354-fb158391704f
02/08/2025 17:01:17:INFO:Received: train message 7bf5c975-2fbd-49e9-b354-fb158391704f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 17:20:17:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:41:47:INFO:
[92mINFO [0m:      Received: evaluate message 5b0c1188-65c8-4f0e-80be-3e3d87198653
02/08/2025 17:41:47:INFO:Received: evaluate message 5b0c1188-65c8-4f0e-80be-3e3d87198653

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 17:46:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:47:17:INFO:
[92mINFO [0m:      Received: train message a8d6e9de-d43e-46fb-8823-5ab933615699
02/08/2025 17:47:17:INFO:Received: train message a8d6e9de-d43e-46fb-8823-5ab933615699
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:06:13:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:27:51:INFO:
[92mINFO [0m:      Received: evaluate message 818ef0f3-82e1-49bc-b26d-88fb446a295a
02/08/2025 18:27:51:INFO:Received: evaluate message 818ef0f3-82e1-49bc-b26d-88fb446a295a
[92mINFO [0m:      Sent reply
02/08/2025 18:32:18:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:33:09:INFO:
[92mINFO [0m:      Received: train message cd5be330-eebb-42e0-884e-e8295d086a4b
02/08/2025 18:33:09:INFO:Received: train message cd5be330-eebb-42e0-884e-e8295d086a4b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:52:01:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:14:19:INFO:
[92mINFO [0m:      Received: evaluate message 5e384add-b186-4880-a041-62f1d0c3fc59
02/08/2025 19:14:19:INFO:Received: evaluate message 5e384add-b186-4880-a041-62f1d0c3fc59

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 19:18:38:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:18:49:INFO:
[92mINFO [0m:      Received: reconnect message 869c2404-1ffd-4c2d-8a56-e5db2e636931
02/08/2025 19:18:49:INFO:Received: reconnect message 869c2404-1ffd-4c2d-8a56-e5db2e636931
02/08/2025 19:18:49:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/08/2025 19:18:49:INFO:Disconnect and shut down

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712, 1.3589135335666058], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445, 0.8710482896241687], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899, 0.6162404554774186], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089, 0.6075857344583624]}



Final client history:
{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712, 1.3589135335666058], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445, 0.8710482896241687], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899, 0.6162404554774186], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089, 0.6075857344583624]}

