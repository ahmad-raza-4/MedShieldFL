nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 07:17:48:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 07:17:48:DEBUG:ChannelConnectivity.IDLE
01/29/2025 07:17:48:DEBUG:ChannelConnectivity.CONNECTING
01/29/2025 07:17:48:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 07:18:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:18:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a990cbc5-c51b-4788-a89c-db94ebb3726d
01/29/2025 07:18:33:INFO:Received: train message a990cbc5-c51b-4788-a89c-db94ebb3726d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 07:46:11:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:46:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:46:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 74b512bb-b169-4301-97cd-e7deb5a1f3f7
01/29/2025 07:46:32:INFO:Received: evaluate message 74b512bb-b169-4301-97cd-e7deb5a1f3f7
[92mINFO [0m:      Sent reply
01/29/2025 07:49:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:51:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:51:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5cb04d0b-333b-4758-9d10-71a07ebcb650
01/29/2025 07:51:28:INFO:Received: train message 5cb04d0b-333b-4758-9d10-71a07ebcb650
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:15:05:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:18:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:18:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 60499733-5a44-4247-9612-2c8f6005c5e8
01/29/2025 08:18:34:INFO:Received: evaluate message 60499733-5a44-4247-9612-2c8f6005c5e8
[92mINFO [0m:      Sent reply
01/29/2025 08:22:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:23:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:23:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e8db1a26-48e3-4962-b215-f86c04df2037
01/29/2025 08:23:11:INFO:Received: train message e8db1a26-48e3-4962-b215-f86c04df2037
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:46:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:47:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:47:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1e5bc363-fe7e-469c-817c-ee7aeda7aa27
01/29/2025 08:47:16:INFO:Received: evaluate message 1e5bc363-fe7e-469c-817c-ee7aeda7aa27
[92mINFO [0m:      Sent reply
01/29/2025 08:51:21:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:51:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:51:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3993c3dd-93f5-46af-a3bb-9809b4f3a32a
01/29/2025 08:51:54:INFO:Received: train message 3993c3dd-93f5-46af-a3bb-9809b4f3a32a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:15:42:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:19:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:19:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4feeb16d-faf4-4e3a-95c8-2721cdff4385
01/29/2025 09:19:08:INFO:Received: evaluate message 4feeb16d-faf4-4e3a-95c8-2721cdff4385
[92mINFO [0m:      Sent reply
01/29/2025 09:25:34:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:26:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:26:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3b8b3d56-945b-4444-828d-491debd1e2b8
01/29/2025 09:26:29:INFO:Received: train message 3b8b3d56-945b-4444-828d-491debd1e2b8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:53:10:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:58:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:58:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7f715255-9d24-4a16-8b68-b22f6cdd1b5d
01/29/2025 09:58:18:INFO:Received: evaluate message 7f715255-9d24-4a16-8b68-b22f6cdd1b5d
[92mINFO [0m:      Sent reply
01/29/2025 10:03:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:04:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:04:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a6a2f958-d666-4205-8a36-f62d84b13ba5
01/29/2025 10:04:08:INFO:Received: train message a6a2f958-d666-4205-8a36-f62d84b13ba5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 10:26:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:26:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:26:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ae3501c0-8bc8-420c-967b-6f165a67a3bc
01/29/2025 10:26:45:INFO:Received: evaluate message ae3501c0-8bc8-420c-967b-6f165a67a3bc
[92mINFO [0m:      Sent reply
01/29/2025 10:31:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:31:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:31:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3269af71-8956-404e-bda5-c1cf46ea025a
01/29/2025 10:31:32:INFO:Received: train message 3269af71-8956-404e-bda5-c1cf46ea025a
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657], 'accuracy': [0.517921868707209], 'auc': [0.7610659482721037]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953], 'accuracy': [0.517921868707209, 0.5819573097060008], 'auc': [0.7610659482721037, 0.8125282828523395]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 10:54:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:55:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:55:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message da8bd6a6-0667-4ce5-a7c7-37639b7c864e
01/29/2025 10:55:21:INFO:Received: evaluate message da8bd6a6-0667-4ce5-a7c7-37639b7c864e
[92mINFO [0m:      Sent reply
01/29/2025 10:59:39:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:00:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:00:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7447892d-e41d-42cd-b06c-17923fdc4dd5
01/29/2025 11:00:06:INFO:Received: train message 7447892d-e41d-42cd-b06c-17923fdc4dd5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:26:02:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:26:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:26:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bb92c1e7-cd2e-47ad-86c7-a63571cc3f41
01/29/2025 11:26:51:INFO:Received: evaluate message bb92c1e7-cd2e-47ad-86c7-a63571cc3f41
[92mINFO [0m:      Sent reply
01/29/2025 11:32:21:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:32:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:32:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 36d87ae1-8db0-430e-99d6-7cc420b200eb
01/29/2025 11:32:51:INFO:Received: train message 36d87ae1-8db0-430e-99d6-7cc420b200eb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:12:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:13:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:13:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8e1c7fc9-e962-4b16-aa44-0a700aafd0dc
01/29/2025 12:13:48:INFO:Received: evaluate message 8e1c7fc9-e962-4b16-aa44-0a700aafd0dc
[92mINFO [0m:      Sent reply
01/29/2025 12:18:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:19:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:19:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6ce174a2-8e28-48b8-af15-556b11bb316e
01/29/2025 12:19:05:INFO:Received: train message 6ce174a2-8e28-48b8-af15-556b11bb316e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:49:32:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:50:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:50:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1d5b335c-d38e-4bc7-ac88-98febdf822e9
01/29/2025 12:50:14:INFO:Received: evaluate message 1d5b335c-d38e-4bc7-ac88-98febdf822e9
[92mINFO [0m:      Sent reply
01/29/2025 12:55:02:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:55:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:55:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 177ee476-88f2-4359-b5c5-0963cd1f93b7
01/29/2025 12:55:53:INFO:Received: train message 177ee476-88f2-4359-b5c5-0963cd1f93b7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:31:05:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:31:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:31:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7ea7e53a-315c-4549-87f3-0285bd971798
01/29/2025 13:31:52:INFO:Received: evaluate message 7ea7e53a-315c-4549-87f3-0285bd971798
[92mINFO [0m:      Sent reply
01/29/2025 13:36:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:37:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:37:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9a78f43a-72c8-4771-bfb2-3ad69df53af0
01/29/2025 13:37:40:INFO:Received: train message 9a78f43a-72c8-4771-bfb2-3ad69df53af0
[0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:07:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:07:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:07:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7479821b-5775-49ae-b841-00fe8f52880a
01/29/2025 14:07:44:INFO:Received: evaluate message 7479821b-5775-49ae-b841-00fe8f52880a
[92mINFO [0m:      Sent reply
01/29/2025 14:13:28:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:14:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:14:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 757c078c-4db4-4cdb-90e9-c8a8a944a370
01/29/2025 14:14:30:INFO:Received: train message 757c078c-4db4-4cdb-90e9-c8a8a944a370
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:50:44:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:51:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:51:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d5699fec-237f-47f0-aa96-f32acc3fe897
01/29/2025 14:51:29:INFO:Received: evaluate message d5699fec-237f-47f0-aa96-f32acc3fe897
[92mINFO [0m:      Sent reply
01/29/2025 14:55:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:57:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:57:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6c2f884c-46a0-4ccb-b50d-a20cdd567b15
01/29/2025 14:57:24:INFO:Received: train message 6c2f884c-46a0-4ccb-b50d-a20cdd567b15
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:30:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:30:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:30:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f45195c9-e680-46ea-9476-0626210d03b3
01/29/2025 15:30:43:INFO:Received: evaluate message f45195c9-e680-46ea-9476-0626210d03b3
[92mINFO [0m:      Sent reply
01/29/2025 15:35:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:36:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:36:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 17a7f47a-1481-4e73-8bcd-1127b1f2340d
01/29/2025 15:36:43:INFO:Received: train message 17a7f47a-1481-4e73-8bcd-1127b1f2340d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:08:01:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:08:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:08:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fee047af-ec0d-4c8d-a105-370ae4440f3e
01/29/2025 16:08:32:INFO:Received: evaluate message fee047af-ec0d-4c8d-a105-370ae4440f3e
[92mINFO [0m:      Sent reply
01/29/2025 16:13:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ef9ac4b9-f6a7-4076-be5c-b4fcb1841aec
01/29/2025 16:15:26:INFO:Received: train message ef9ac4b9-f6a7-4076-be5c-b4fcb1841aec
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:48:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:48:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:48:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 83d2835e-eadf-4e1b-9034-2894a447fb2b
01/29/2025 16:48:46:INFO:Received: evaluate message 83d2835e-eadf-4e1b-9034-2894a447fb2b
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 16:55:17:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:57:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:57:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d84774bb-78e5-464a-8acd-59fb5759b70d
01/29/2025 16:57:05:INFO:Received: train message d84774bb-78e5-464a-8acd-59fb5759b70d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:30:34:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:31:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:31:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4862b9fb-b561-49fb-9264-cb9f4c09e024
01/29/2025 17:31:14:INFO:Received: evaluate message 4862b9fb-b561-49fb-9264-cb9f4c09e024
[92mINFO [0m:      Sent reply
01/29/2025 17:35:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:36:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:36:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4a6f33ec-0219-4b1b-9832-156d3157634d
01/29/2025 17:36:30:INFO:Received: train message 4a6f33ec-0219-4b1b-9832-156d3157634d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:12:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:12:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:12:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e54d9476-69db-440d-894a-0d749886a741
01/29/2025 18:12:55:INFO:Received: evaluate message e54d9476-69db-440d-894a-0d749886a741
[92mINFO [0m:      Sent reply
01/29/2025 18:17:49:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:18:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:18:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1718c81d-42dc-4057-86c4-77cad82de2f9
01/29/2025 18:18:38:INFO:Received: train message 1718c81d-42dc-4057-86c4-77cad82de2f9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:51:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:51:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:51:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8ca32b02-1ea8-444a-a07f-4e2b245d0d7a
01/29/2025 18:51:38:INFO:Received: evaluate message 8ca32b02-1ea8-444a-a07f-4e2b245d0d7a
[92mINFO [0m:      Sent reply
01/29/2025 18:55:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:57:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:57:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 97ba0173-d2e3-440e-92c9-1ff945b37f33
01/29/2025 18:57:00:INFO:Received: train message 97ba0173-d2e3-440e-92c9-1ff945b37f33
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:29:41:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:30:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:30:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b3b8de1b-11de-45dc-9738-2e1c2ff7ce4e
01/29/2025 19:30:18:INFO:Received: evaluate message b3b8de1b-11de-45dc-9738-2e1c2ff7ce4e

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 19:34:44:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:35:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:35:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 45ae71b4-bb45-4ae9-8184-a61270790426
01/29/2025 19:35:15:INFO:Received: train message 45ae71b4-bb45-4ae9-8184-a61270790426
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:09:08:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:09:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:09:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6d411529-28b7-4a34-ae5d-0c16fc080307
01/29/2025 20:09:45:INFO:Received: evaluate message 6d411529-28b7-4a34-ae5d-0c16fc080307
[92mINFO [0m:      Sent reply
01/29/2025 20:14:23:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:15:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:15:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 16238713-f8d2-4771-a305-4b895b3a8591
01/29/2025 20:15:05:INFO:Received: train message 16238713-f8d2-4771-a305-4b895b3a8591
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:47:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7a00a35b-f3eb-4749-a3f6-2dba6c7664ec
01/29/2025 20:48:28:INFO:Received: evaluate message 7a00a35b-f3eb-4749-a3f6-2dba6c7664ec
[92mINFO [0m:      Sent reply
01/29/2025 20:53:29:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:54:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:54:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 861b4626-933c-4d64-b541-587936f88851
01/29/2025 20:54:05:INFO:Received: train message 861b4626-933c-4d64-b541-587936f88851
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:28:01:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:29:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:29:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 112fba05-4a2d-4fd9-a3d4-97c939e5fca3
01/29/2025 21:29:08:INFO:Received: evaluate message 112fba05-4a2d-4fd9-a3d4-97c939e5fca3
[92mINFO [0m:      Sent reply
01/29/2025 21:34:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:35:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:35:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a7ab33bf-b428-40d7-a146-e86cfcd313ff
01/29/2025 21:35:33:INFO:Received: train message a7ab33bf-b428-40d7-a146-e86cfcd313ff

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:10:33:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:11:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:11:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3d63cf30-6a86-4b10-bac3-3ff6133e1b20
01/29/2025 22:11:22:INFO:Received: evaluate message 3d63cf30-6a86-4b10-bac3-3ff6133e1b20
[92mINFO [0m:      Sent reply
01/29/2025 22:16:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:17:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:17:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3feabd33-2d61-4a0e-a3d2-29847faf32e7
01/29/2025 22:17:24:INFO:Received: train message 3feabd33-2d61-4a0e-a3d2-29847faf32e7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:53:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:53:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:53:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8bd62a1d-b666-4d01-b0c7-a7a00e3b1f11
01/29/2025 22:53:59:INFO:Received: evaluate message 8bd62a1d-b666-4d01-b0c7-a7a00e3b1f11
[92mINFO [0m:      Sent reply
01/29/2025 22:58:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:59:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:59:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6ac7d02a-53b0-4e03-aaf1-dbf31885b2f7
01/29/2025 22:59:40:INFO:Received: train message 6ac7d02a-53b0-4e03-aaf1-dbf31885b2f7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:38:02:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:39:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:39:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0555b197-fd10-4979-a554-7d4c157daf04
01/29/2025 23:39:02:INFO:Received: evaluate message 0555b197-fd10-4979-a554-7d4c157daf04
[92mINFO [0m:      Sent reply
01/29/2025 23:43:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:45:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:45:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 58035fb9-c6f9-4a4c-b9c3-ab9fcb895655
01/29/2025 23:45:30:INFO:Received: train message 58035fb9-c6f9-4a4c-b9c3-ab9fcb895655
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:20:15:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:21:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:21:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 693f5991-85a5-4465-8b58-edbd4db07266
01/30/2025 00:21:05:INFO:Received: evaluate message 693f5991-85a5-4465-8b58-edbd4db07266
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally[92mINFO [0m:      Sent reply
01/30/2025 00:26:39:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:27:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:27:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cc543546-8d3f-4463-9ad4-0834ab94bc22
01/30/2025 00:27:19:INFO:Received: train message cc543546-8d3f-4463-9ad4-0834ab94bc22
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:07:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:08:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:08:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a8528726-9f06-4f9c-a8a8-99568ad17791
01/30/2025 01:08:04:INFO:Received: evaluate message a8528726-9f06-4f9c-a8a8-99568ad17791
[92mINFO [0m:      Sent reply
01/30/2025 01:13:20:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:13:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:13:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c9a7153c-8ba1-42ad-b592-c78351232713
01/30/2025 01:13:51:INFO:Received: train message c9a7153c-8ba1-42ad-b592-c78351232713
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:54:53:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:55:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:55:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f4e3cf9f-d7d3-460d-ba7c-032777428347
01/30/2025 01:55:22:INFO:Received: evaluate message f4e3cf9f-d7d3-460d-ba7c-032777428347
[92mINFO [0m:      Sent reply
01/30/2025 01:59:43:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:00:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:00:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 34dd0f37-7c5c-4170-a376-d7bc5583846d
01/30/2025 02:00:04:INFO:Received: train message 34dd0f37-7c5c-4170-a376-d7bc5583846d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:36:52:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:37:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:37:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 49016520-1b38-44e1-8bf0-a425dcf2e040
01/30/2025 02:37:30:INFO:Received: evaluate message 49016520-1b38-44e1-8bf0-a425dcf2e040


{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.4951506555080414, 0.7463480234146118, 0.4994523227214813, 0.17625199258327484, 0.22207118570804596, 0.0960950180888176, 0.10982655733823776, 0.09631598740816116]
Noise Multiplier after list and tensor:  0.305188967846334
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 02:42:15:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:42:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:42:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 97839c98-b9b5-4ae0-a484-130426e23356
01/30/2025 02:42:27:INFO:Received: reconnect message 97839c98-b9b5-4ae0-a484-130426e23356
01/30/2025 02:42:27:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 02:42:27:INFO:Disconnect and shut down

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047, 1.3677138108857638], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804, 0.6331051147805075], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232, 0.8855101957814356]}



Final client history:
{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047, 1.3677138108857638], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804, 0.6331051147805075], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232, 0.8855101957814356]}

