nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 07:12:02:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 07:12:02:DEBUG:ChannelConnectivity.IDLE
01/29/2025 07:12:02:DEBUG:ChannelConnectivity.CONNECTING
01/29/2025 07:12:02:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 07:18:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:18:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 59054b12-d40a-46cc-ba3d-438718770630
01/29/2025 07:18:30:INFO:Received: train message 59054b12-d40a-46cc-ba3d-438718770630
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 07:31:02:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:46:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:46:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f6ca6011-9e8f-4970-965f-cc679bdcba66
01/29/2025 07:46:52:INFO:Received: evaluate message f6ca6011-9e8f-4970-965f-cc679bdcba66
[92mINFO [0m:      Sent reply
01/29/2025 07:50:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:51:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:51:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1a2f866a-785f-4c86-8216-13f0faafa544
01/29/2025 07:51:13:INFO:Received: train message 1a2f866a-785f-4c86-8216-13f0faafa544
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:01:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:18:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:18:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6bc3f07d-f75e-4a72-855f-d86add49ddb6
01/29/2025 08:18:34:INFO:Received: evaluate message 6bc3f07d-f75e-4a72-855f-d86add49ddb6
[92mINFO [0m:      Sent reply
01/29/2025 08:22:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:23:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:23:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a6b29524-12dc-4048-9db1-79723f3bd2b5
01/29/2025 08:23:24:INFO:Received: train message a6b29524-12dc-4048-9db1-79723f3bd2b5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:32:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:47:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:47:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 10cd3b28-20dd-49ef-b656-e94aeec29597
01/29/2025 08:47:13:INFO:Received: evaluate message 10cd3b28-20dd-49ef-b656-e94aeec29597
[92mINFO [0m:      Sent reply
01/29/2025 08:51:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:51:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:51:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bdd04b6a-7eeb-427b-97d6-494a0c7a689a
01/29/2025 08:51:52:INFO:Received: train message bdd04b6a-7eeb-427b-97d6-494a0c7a689a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:03:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:19:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:19:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0af476b2-3f86-4858-ab8e-4d85610168e1
01/29/2025 09:19:08:INFO:Received: evaluate message 0af476b2-3f86-4858-ab8e-4d85610168e1
[92mINFO [0m:      Sent reply
01/29/2025 09:25:44:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:26:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:26:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cae9a287-055a-4f73-a3f4-9fc816fb0bce
01/29/2025 09:26:20:INFO:Received: train message cae9a287-055a-4f73-a3f4-9fc816fb0bce
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:37:15:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:58:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:58:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c2310bd3-f227-48b4-b92a-e091352983f4
01/29/2025 09:58:35:INFO:Received: evaluate message c2310bd3-f227-48b4-b92a-e091352983f4
[92mINFO [0m:      Sent reply
01/29/2025 10:03:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:04:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:04:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ca1257d5-ec8c-42b0-a783-39cab21daac3
01/29/2025 10:04:07:INFO:Received: train message ca1257d5-ec8c-42b0-a783-39cab21daac3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 10:13:42:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:26:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:26:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fd9d441c-6320-411f-908f-034e887b6cba
01/29/2025 10:26:47:INFO:Received: evaluate message fd9d441c-6320-411f-908f-034e887b6cba
[92mINFO [0m:      Sent reply
01/29/2025 10:31:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:31:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:31:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 98c24576-d943-4d22-b58c-c4b8c549cf24
01/29/2025 10:31:42:INFO:Received: train message 98c24576-d943-4d22-b58c-c4b8c549cf24
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657], 'accuracy': [0.517921868707209], 'auc': [0.7610659482721037]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953], 'accuracy': [0.517921868707209, 0.5819573097060008], 'auc': [0.7610659482721037, 0.8125282828523395]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 10:41:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:55:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:55:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3f7861c8-b2f5-4323-b32b-c36960f826bf
01/29/2025 10:55:20:INFO:Received: evaluate message 3f7861c8-b2f5-4323-b32b-c36960f826bf
[92mINFO [0m:      Sent reply
01/29/2025 10:59:34:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:00:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:00:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e63e1b38-b238-43ae-926e-dd796551a334
01/29/2025 11:00:23:INFO:Received: train message e63e1b38-b238-43ae-926e-dd796551a334
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:10:38:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:26:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:26:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d4574277-3436-4fd0-a394-fe562b779634
01/29/2025 11:26:54:INFO:Received: evaluate message d4574277-3436-4fd0-a394-fe562b779634
[92mINFO [0m:      Sent reply
01/29/2025 11:32:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:33:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:33:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9b6d1a49-7a12-41d7-8fc4-1a1b9f83d375
01/29/2025 11:33:23:INFO:Received: train message 9b6d1a49-7a12-41d7-8fc4-1a1b9f83d375
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:50:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:13:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:13:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fff94476-effa-418b-8c2e-e67144b1dc17
01/29/2025 12:13:32:INFO:Received: evaluate message fff94476-effa-418b-8c2e-e67144b1dc17
[92mINFO [0m:      Sent reply
01/29/2025 12:18:36:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:19:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:19:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 34098760-5055-4f23-9f50-3918e517eb53
01/29/2025 12:19:28:INFO:Received: train message 34098760-5055-4f23-9f50-3918e517eb53
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:30:43:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:50:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:50:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 03e9e124-5c8a-4cc6-bd10-8290fae5ada1
01/29/2025 12:50:03:INFO:Received: evaluate message 03e9e124-5c8a-4cc6-bd10-8290fae5ada1
[92mINFO [0m:      Sent reply
01/29/2025 12:54:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:55:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:55:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 23a087c9-9930-4b68-abb7-9fda2ed039c8
01/29/2025 12:55:42:INFO:Received: train message 23a087c9-9930-4b68-abb7-9fda2ed039c8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:07:07:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:31:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:31:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 897d9d94-79ee-4878-a746-357209949ef8
01/29/2025 13:31:56:INFO:Received: evaluate message 897d9d94-79ee-4878-a746-357209949ef8
[92mINFO [0m:      Sent reply
01/29/2025 13:36:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:37:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:37:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0b6cca98-8a3f-4b81-883b-889bb1d222e5
01/29/2025 13:37:27:INFO:Received: train message 0b6cca98-8a3f-4b81-883b-889bb1d222e5
[0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:50:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:07:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:07:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3410dbda-983c-45ba-b6e7-1a2175deda02
01/29/2025 14:07:58:INFO:Received: evaluate message 3410dbda-983c-45ba-b6e7-1a2175deda02
[92mINFO [0m:      Sent reply
01/29/2025 14:13:43:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:14:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:14:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2b4effbf-dd80-4ae5-87f6-d54641f7c12e
01/29/2025 14:14:40:INFO:Received: train message 2b4effbf-dd80-4ae5-87f6-d54641f7c12e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:25:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:51:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:51:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 465aaeaf-2b11-418f-9411-12e8fdb38d12
01/29/2025 14:51:48:INFO:Received: evaluate message 465aaeaf-2b11-418f-9411-12e8fdb38d12
[92mINFO [0m:      Sent reply
01/29/2025 14:56:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:57:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:57:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 267a1e2e-7eb4-44c7-8ffc-e0f803183fc4
01/29/2025 14:57:42:INFO:Received: train message 267a1e2e-7eb4-44c7-8ffc-e0f803183fc4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:12:31:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:31:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:31:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 74c0a6df-d1d1-481d-8b99-c7901a2c55d5
01/29/2025 15:31:00:INFO:Received: evaluate message 74c0a6df-d1d1-481d-8b99-c7901a2c55d5
[92mINFO [0m:      Sent reply
01/29/2025 15:35:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:36:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:36:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 58b44a18-5f42-4b9b-9bf8-d7598b26ec87
01/29/2025 15:36:45:INFO:Received: train message 58b44a18-5f42-4b9b-9bf8-d7598b26ec87
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:48:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:08:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:08:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6b2e91ea-9dab-4f9b-a9be-c8b4ffae1409
01/29/2025 16:08:41:INFO:Received: evaluate message 6b2e91ea-9dab-4f9b-a9be-c8b4ffae1409
[92mINFO [0m:      Sent reply
01/29/2025 16:14:27:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3852e4b1-a335-4562-8695-9debbf4494eb
01/29/2025 16:15:28:INFO:Received: train message 3852e4b1-a335-4562-8695-9debbf4494eb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:32:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:48:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:48:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 358094dc-7712-471d-b354-d2292307e103
01/29/2025 16:48:51:INFO:Received: evaluate message 358094dc-7712-471d-b354-d2292307e103
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 16:55:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:57:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:57:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1897a4a5-f3f5-4e8f-96ee-ff225961e40f
01/29/2025 16:57:04:INFO:Received: train message 1897a4a5-f3f5-4e8f-96ee-ff225961e40f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:08:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:30:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:30:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9447432c-ce8d-40bb-8c7b-309a0f965e8d
01/29/2025 17:30:59:INFO:Received: evaluate message 9447432c-ce8d-40bb-8c7b-309a0f965e8d
[92mINFO [0m:      Sent reply
01/29/2025 17:35:34:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:36:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:36:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 71785264-b614-423d-af1f-93a16f36127a
01/29/2025 17:36:12:INFO:Received: train message 71785264-b614-423d-af1f-93a16f36127a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:48:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:12:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:12:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 64ab78e1-f2d2-453d-b84b-5d20d75ce142
01/29/2025 18:12:53:INFO:Received: evaluate message 64ab78e1-f2d2-453d-b84b-5d20d75ce142
[92mINFO [0m:      Sent reply
01/29/2025 18:17:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:18:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:18:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6b3f3728-e7d0-403c-9a30-7dd9cb967e6b
01/29/2025 18:18:30:INFO:Received: train message 6b3f3728-e7d0-403c-9a30-7dd9cb967e6b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:29:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:51:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:51:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1ec94a4d-c47a-408d-ad59-28660aaaae00
01/29/2025 18:51:43:INFO:Received: evaluate message 1ec94a4d-c47a-408d-ad59-28660aaaae00
[92mINFO [0m:      Sent reply
01/29/2025 18:56:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:57:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:57:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9f9f7dd3-2147-411c-868d-4aead2cee3c0
01/29/2025 18:57:14:INFO:Received: train message 9f9f7dd3-2147-411c-868d-4aead2cee3c0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:10:28:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:30:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:30:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4bf4da48-2b20-441c-86eb-e10878856351
01/29/2025 19:30:22:INFO:Received: evaluate message 4bf4da48-2b20-441c-86eb-e10878856351

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 19:34:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:35:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:35:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 938bdde1-ae06-48f6-b7a7-e08ee60307d2
01/29/2025 19:35:15:INFO:Received: train message 938bdde1-ae06-48f6-b7a7-e08ee60307d2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:47:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:09:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:09:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f7d47803-b50a-43c7-8b97-a7a9f968c5a4
01/29/2025 20:09:40:INFO:Received: evaluate message f7d47803-b50a-43c7-8b97-a7a9f968c5a4
[92mINFO [0m:      Sent reply
01/29/2025 20:14:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:15:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:15:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 569595a3-5db3-4e2d-9f3f-e098f36e5f28
01/29/2025 20:15:06:INFO:Received: train message 569595a3-5db3-4e2d-9f3f-e098f36e5f28
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:28:41:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 79542b5f-6e07-4469-8a44-21aa9cf819e2
01/29/2025 20:48:21:INFO:Received: evaluate message 79542b5f-6e07-4469-8a44-21aa9cf819e2
[92mINFO [0m:      Sent reply
01/29/2025 20:53:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:54:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:54:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1533724f-d899-440b-84ba-aaadb16d581a
01/29/2025 20:54:12:INFO:Received: train message 1533724f-d899-440b-84ba-aaadb16d581a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:06:35:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:29:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:29:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d073756e-9a5d-43e3-aa07-1ea082a3e66c
01/29/2025 21:29:06:INFO:Received: evaluate message d073756e-9a5d-43e3-aa07-1ea082a3e66c
[92mINFO [0m:      Sent reply
01/29/2025 21:34:17:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:35:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:35:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d6194b00-cd35-4a0a-a7fa-3dc296329854
01/29/2025 21:35:21:INFO:Received: train message d6194b00-cd35-4a0a-a7fa-3dc296329854

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:46:49:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:10:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:10:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 10226ffc-71d1-4cd2-8795-a070accd539e
01/29/2025 22:10:59:INFO:Received: evaluate message 10226ffc-71d1-4cd2-8795-a070accd539e
[92mINFO [0m:      Sent reply
01/29/2025 22:16:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:17:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:17:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 289b63e1-be18-4ed3-88bb-500ba88151b5
01/29/2025 22:17:20:INFO:Received: train message 289b63e1-be18-4ed3-88bb-500ba88151b5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:27:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:53:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:53:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4e063a3f-6e40-4a60-be69-d1620c51fd54
01/29/2025 22:53:54:INFO:Received: evaluate message 4e063a3f-6e40-4a60-be69-d1620c51fd54
[92mINFO [0m:      Sent reply
01/29/2025 22:58:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:59:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:59:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3afc9ba4-753c-4b10-8852-3d93c64f3c4a
01/29/2025 22:59:35:INFO:Received: train message 3afc9ba4-753c-4b10-8852-3d93c64f3c4a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:12:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:39:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:39:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5c2191da-1eaf-43b8-a57f-fd72acd02408
01/29/2025 23:39:16:INFO:Received: evaluate message 5c2191da-1eaf-43b8-a57f-fd72acd02408
[92mINFO [0m:      Sent reply
01/29/2025 23:43:39:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:45:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:45:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bf40ca4c-7130-4c57-818e-76eb22b5b74a
01/29/2025 23:45:40:INFO:Received: train message bf40ca4c-7130-4c57-818e-76eb22b5b74a
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:55:32:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:20:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:20:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d2a16c17-baa6-4e15-a4f5-c5cf2242ed51
01/30/2025 00:20:58:INFO:Received: evaluate message d2a16c17-baa6-4e15-a4f5-c5cf2242ed51
[92mINFO [0m:      Sent reply
01/30/2025 00:26:36:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:27:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:27:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 52c6e8ff-adaa-406a-b6e8-0112c6382d75
01/30/2025 00:27:17:INFO:Received: train message 52c6e8ff-adaa-406a-b6e8-0112c6382d75
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:37:19:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:07:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:07:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 519377a3-babf-4b89-ad76-c60048b19ed6
01/30/2025 01:07:50:INFO:Received: evaluate message 519377a3-babf-4b89-ad76-c60048b19ed6
[92mINFO [0m:      Sent reply
01/30/2025 01:13:05:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:13:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:13:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message be6f50b5-dd17-4f11-a8a9-92278ff60d0d
01/30/2025 01:13:56:INFO:Received: train message be6f50b5-dd17-4f11-a8a9-92278ff60d0d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:24:05:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:55:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:55:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4973edc7-1e12-4443-a7fb-f1c2b9093ee1
01/30/2025 01:55:28:INFO:Received: evaluate message 4973edc7-1e12-4443-a7fb-f1c2b9093ee1
[92mINFO [0m:      Sent reply
01/30/2025 01:59:46:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:00:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:00:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0f86781b-0500-4a14-a947-56c2482fab30
01/30/2025 02:00:12:INFO:Received: train message 0f86781b-0500-4a14-a947-56c2482fab30
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.3828125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.03018909879028797, 0.07560770213603973, 0.025630546733736992, 0.01641574129462242, 0.042994674295186996, 0.0077919005416333675, 0.002709162188693881, 0.017343338578939438]
Noise Multiplier after list and tensor:  0.0273352705698926
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:11:18:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:37:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:37:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 76b5e685-0b53-44bc-99bb-6628d42081dc
01/30/2025 02:37:34:INFO:Received: evaluate message 76b5e685-0b53-44bc-99bb-6628d42081dc
[92mINFO [0m:      Sent reply
01/30/2025 02:42:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:42:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:42:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message bd411cff-6636-466c-b878-6ac87bb520d1
01/30/2025 02:42:27:INFO:Received: reconnect message bd411cff-6636-466c-b878-6ac87bb520d1
01/30/2025 02:42:27:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 02:42:27:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047, 1.3677138108857638], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804, 0.6331051147805075], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232, 0.8855101957814356]}



Final client history:
{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047, 1.3677138108857638], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804, 0.6331051147805075], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232, 0.8855101957814356]}

