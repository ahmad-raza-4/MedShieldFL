nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 11:17:58:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 11:17:58:DEBUG:ChannelConnectivity.IDLE
01/29/2025 11:17:58:DEBUG:ChannelConnectivity.CONNECTING
01/29/2025 11:17:58:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 11:18:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:18:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 96d124e9-a9a1-4924-806c-28ff21e6fb61
01/29/2025 11:18:33:INFO:Received: train message 96d124e9-a9a1-4924-806c-28ff21e6fb61
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:08:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:09:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:09:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 59e3221a-0b2b-449b-a77a-98d8f99c2215
01/29/2025 12:09:56:INFO:Received: evaluate message 59e3221a-0b2b-449b-a77a-98d8f99c2215
[92mINFO [0m:      Sent reply
01/29/2025 12:14:08:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:15:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:15:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6721460a-4ae7-4eea-bd96-d57b5340e4c5
01/29/2025 12:15:10:INFO:Received: train message 6721460a-4ae7-4eea-bd96-d57b5340e4c5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:46:35:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:47:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:47:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e30484be-28f9-472b-96bf-176e29be78e8
01/29/2025 12:47:15:INFO:Received: evaluate message e30484be-28f9-472b-96bf-176e29be78e8
[92mINFO [0m:      Sent reply
01/29/2025 12:51:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:52:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:52:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 60523eb8-7462-48c5-a9f4-e3f0b36c51ff
01/29/2025 12:52:25:INFO:Received: train message 60523eb8-7462-48c5-a9f4-e3f0b36c51ff
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:30:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:30:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:30:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d0d21af3-d9d0-4f79-887f-9415030a4e51
01/29/2025 13:30:59:INFO:Received: evaluate message d0d21af3-d9d0-4f79-887f-9415030a4e51
[92mINFO [0m:      Sent reply
01/29/2025 13:35:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:36:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:36:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 96869384-7803-4acd-ab7d-26237143659e
01/29/2025 13:36:33:INFO:Received: train message 96869384-7803-4acd-ab7d-26237143659e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:07:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:08:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:08:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2af88883-c5de-4e54-9e32-89a4e2c5ef10
01/29/2025 14:08:31:INFO:Received: evaluate message 2af88883-c5de-4e54-9e32-89a4e2c5ef10
[92mINFO [0m:      Sent reply
01/29/2025 14:14:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:14:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:14:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 39b7b0a8-5543-4281-b5fd-3ff757a0c0c6
01/29/2025 14:14:52:INFO:Received: train message 39b7b0a8-5543-4281-b5fd-3ff757a0c0c6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:48:36:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:50:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:50:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ad2a02c4-defb-448e-b076-1390ec1004af
01/29/2025 14:50:30:INFO:Received: evaluate message ad2a02c4-defb-448e-b076-1390ec1004af
[92mINFO [0m:      Sent reply
01/29/2025 14:55:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:55:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:55:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 07fbe042-84a4-4413-b058-64fa8448b136
01/29/2025 14:55:59:INFO:Received: train message 07fbe042-84a4-4413-b058-64fa8448b136
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:29:17:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:29:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:29:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 16879a6e-c6bf-41d6-9ed9-edf707f8b771
01/29/2025 15:29:45:INFO:Received: evaluate message 16879a6e-c6bf-41d6-9ed9-edf707f8b771
[92mINFO [0m:      Sent reply
01/29/2025 15:34:10:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:34:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:34:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 171ba2cc-49ce-4129-9f73-9d0cb2a961ed
01/29/2025 15:34:56:INFO:Received: train message 171ba2cc-49ce-4129-9f73-9d0cb2a961ed
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345], 'accuracy': [0.5690696737817157], 'auc': [0.830051779313479]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765], 'accuracy': [0.5690696737817157, 0.6153846153846154], 'auc': [0.830051779313479, 0.8663737657560014]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:07:50:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:08:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:08:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 517e5b3c-f296-44ae-89e6-80f3c7d27a44
01/29/2025 16:08:27:INFO:Received: evaluate message 517e5b3c-f296-44ae-89e6-80f3c7d27a44
[92mINFO [0m:      Sent reply
01/29/2025 16:14:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5fba6614-c93d-44ca-9078-25c9406a691a
01/29/2025 16:15:51:INFO:Received: train message 5fba6614-c93d-44ca-9078-25c9406a691a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:48:31:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:49:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:49:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eed3ccfc-2178-448b-abf7-9da6cea7d4c8
01/29/2025 16:49:50:INFO:Received: evaluate message eed3ccfc-2178-448b-abf7-9da6cea7d4c8
[92mINFO [0m:      Sent reply
01/29/2025 16:57:01:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:57:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:57:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 824dc34d-d5d0-4223-9533-3eab2498b6e4
01/29/2025 16:57:29:INFO:Received: train message 824dc34d-d5d0-4223-9533-3eab2498b6e4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:30:34:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:31:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:31:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message df2e7d1a-61b7-4f0c-8fea-375c5d19f790
01/29/2025 17:31:54:INFO:Received: evaluate message df2e7d1a-61b7-4f0c-8fea-375c5d19f790
[92mINFO [0m:      Sent reply
01/29/2025 17:36:16:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:36:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:36:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4dada29e-fb12-4bcb-8cbd-79b3cbd0dadb
01/29/2025 17:36:54:INFO:Received: train message 4dada29e-fb12-4bcb-8cbd-79b3cbd0dadb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:12:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:13:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:13:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5fecf9d9-14af-4a4b-a8f1-d6680f55488d
01/29/2025 18:13:20:INFO:Received: evaluate message 5fecf9d9-14af-4a4b-a8f1-d6680f55488d
[92mINFO [0m:      Sent reply
01/29/2025 18:18:03:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:19:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:19:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8e7d43a4-73ab-42e1-9b4a-80a6b315aca8
01/29/2025 18:19:32:INFO:Received: train message 8e7d43a4-73ab-42e1-9b4a-80a6b315aca8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:51:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:53:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:53:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a7a59591-b56d-40ef-b835-f6c6048deb44
01/29/2025 18:53:01:INFO:Received: evaluate message a7a59591-b56d-40ef-b835-f6c6048deb44
[92mINFO [0m:      Sent reply
01/29/2025 18:57:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:58:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:58:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1a27893b-51b9-4900-8340-7117377c94cf
01/29/2025 18:58:25:INFO:Received: train message 1a27893b-51b9-4900-8340-7117377c94cf
[0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:31:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:31:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:31:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f2b8179a-e1a6-460b-af76-dd88bf0c9eee
01/29/2025 19:31:42:INFO:Received: evaluate message f2b8179a-e1a6-460b-af76-dd88bf0c9eee
[92mINFO [0m:      Sent reply
01/29/2025 19:36:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:36:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:36:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 302dee03-5200-4f12-b129-aaf8a6aa6ee6
01/29/2025 19:36:52:INFO:Received: train message 302dee03-5200-4f12-b129-aaf8a6aa6ee6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:09:31:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:10:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:10:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9364a65f-1c66-41fd-8ded-78b6f8f76e7d
01/29/2025 20:10:45:INFO:Received: evaluate message 9364a65f-1c66-41fd-8ded-78b6f8f76e7d
[92mINFO [0m:      Sent reply
01/29/2025 20:15:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:16:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:16:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ff4bd9bb-74a7-4e45-9e12-f23f1e77171e
01/29/2025 20:16:18:INFO:Received: train message ff4bd9bb-74a7-4e45-9e12-f23f1e77171e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:47:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 21c7ab7b-fea3-471f-bbe1-9031b80e7c22
01/29/2025 20:48:28:INFO:Received: evaluate message 21c7ab7b-fea3-471f-bbe1-9031b80e7c22
[92mINFO [0m:      Sent reply
01/29/2025 20:53:11:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:53:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:53:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 47f40dc4-3147-4577-9b9e-6f8a3c3f4f85
01/29/2025 20:53:28:INFO:Received: train message 47f40dc4-3147-4577-9b9e-6f8a3c3f4f85
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:26:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:26:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:26:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a7d826be-014d-4a73-95de-8b0866217014
01/29/2025 21:26:33:INFO:Received: evaluate message a7d826be-014d-4a73-95de-8b0866217014
[92mINFO [0m:      Sent reply
01/29/2025 21:31:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:33:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:33:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message acda2f59-39f9-4970-bc13-e366fa02dcbc
01/29/2025 21:33:03:INFO:Received: train message acda2f59-39f9-4970-bc13-e366fa02dcbc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:05:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:05:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:05:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 67543db4-e3fc-4611-bdef-9f80520009a2
01/29/2025 22:05:58:INFO:Received: evaluate message 67543db4-e3fc-4611-bdef-9f80520009a2
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 22:11:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:12:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:12:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6a30e417-e6d9-4798-a285-63e5328e759c
01/29/2025 22:12:11:INFO:Received: train message 6a30e417-e6d9-4798-a285-63e5328e759c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:43:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:44:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:44:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 199cdf8d-5b86-41a7-bd1e-5e5b69c381b4
01/29/2025 22:44:28:INFO:Received: evaluate message 199cdf8d-5b86-41a7-bd1e-5e5b69c381b4
[92mINFO [0m:      Sent reply
01/29/2025 22:49:33:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:50:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:50:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7ca89846-9adc-4195-8b2e-fe1993cb3981
01/29/2025 22:50:20:INFO:Received: train message 7ca89846-9adc-4195-8b2e-fe1993cb3981
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:26:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:27:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:27:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b54386eb-a32b-457e-ba44-5a3e4f68daf3
01/29/2025 23:27:54:INFO:Received: evaluate message b54386eb-a32b-457e-ba44-5a3e4f68daf3
[92mINFO [0m:      Sent reply
01/29/2025 23:31:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:34:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:34:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2fbc7fa8-5f77-4702-a3c0-972951dde29f
01/29/2025 23:34:06:INFO:Received: train message 2fbc7fa8-5f77-4702-a3c0-972951dde29f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:06:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:07:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:07:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a76ff5f4-3c7d-4ba1-8f16-c59055055e0d
01/30/2025 00:07:25:INFO:Received: evaluate message a76ff5f4-3c7d-4ba1-8f16-c59055055e0d
[92mINFO [0m:      Sent reply
01/30/2025 00:11:09:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:13:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:13:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4f9d2fee-a7cb-428f-9014-7bff02e5e578
01/30/2025 00:13:03:INFO:Received: train message 4f9d2fee-a7cb-428f-9014-7bff02e5e578
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:46:18:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:46:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:46:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 84063b99-876f-4a32-bc80-d21c85f58ad2
01/30/2025 00:46:49:INFO:Received: evaluate message 84063b99-876f-4a32-bc80-d21c85f58ad2

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 00:52:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:53:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:53:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 05ea326b-38a9-415b-bf32-d94ac49beeb8
01/30/2025 00:53:20:INFO:Received: train message 05ea326b-38a9-415b-bf32-d94ac49beeb8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:32:52:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:33:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:33:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 84c5b39e-f015-4a3f-bcdf-5e3d01c78548
01/30/2025 01:33:48:INFO:Received: evaluate message 84c5b39e-f015-4a3f-bcdf-5e3d01c78548
[92mINFO [0m:      Sent reply
01/30/2025 01:39:01:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:39:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:39:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 73d800e7-57e7-4301-b79f-b206d66c1c5f
01/30/2025 01:39:43:INFO:Received: train message 73d800e7-57e7-4301-b79f-b206d66c1c5f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:14:43:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:15:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:15:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 46c598ce-b315-44c8-b3af-d663bd5472bd
01/30/2025 02:15:14:INFO:Received: evaluate message 46c598ce-b315-44c8-b3af-d663bd5472bd
[92mINFO [0m:      Sent reply
01/30/2025 02:19:31:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:21:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:21:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 78fa95f3-67bc-4e10-beaa-6bd11c5ba496
01/30/2025 02:21:22:INFO:Received: train message 78fa95f3-67bc-4e10-beaa-6bd11c5ba496
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:55:13:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:56:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:56:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f72332ec-afae-4be5-97d4-b2180987b720
01/30/2025 02:56:03:INFO:Received: evaluate message f72332ec-afae-4be5-97d4-b2180987b720
[92mINFO [0m:      Sent reply
01/30/2025 03:01:28:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:02:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:02:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c8b2e9b3-b23d-46e0-ad35-7639c28cc1fb
01/30/2025 03:02:20:INFO:Received: train message c8b2e9b3-b23d-46e0-ad35-7639c28cc1fb

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:31:12:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:32:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:32:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b729607d-4e51-4c2b-b790-332d37ec0fdd
01/30/2025 03:32:11:INFO:Received: evaluate message b729607d-4e51-4c2b-b790-332d37ec0fdd
[92mINFO [0m:      Sent reply
01/30/2025 03:36:17:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:37:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:37:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 084d70b8-8a33-4c67-83ff-bd6649614b04
01/30/2025 03:37:19:INFO:Received: train message 084d70b8-8a33-4c67-83ff-bd6649614b04
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:10:30:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:11:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:11:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f653c226-04a9-474d-9378-697f554b8e71
01/30/2025 04:11:17:INFO:Received: evaluate message f653c226-04a9-474d-9378-697f554b8e71
[92mINFO [0m:      Sent reply
01/30/2025 04:16:36:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:17:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:17:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 669e6732-6ae4-41a3-8ad8-33be14b6c3a0
01/30/2025 04:17:25:INFO:Received: train message 669e6732-6ae4-41a3-8ad8-33be14b6c3a0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:53:48:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:54:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:54:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b4fe4c58-b146-4518-b77c-75ddce05989c
01/30/2025 04:54:23:INFO:Received: evaluate message b4fe4c58-b146-4518-b77c-75ddce05989c
[92mINFO [0m:      Sent reply
01/30/2025 04:59:36:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:01:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:01:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b8cc9d1c-ee84-434f-ac01-cb30a82921ff
01/30/2025 05:01:05:INFO:Received: train message b8cc9d1c-ee84-434f-ac01-cb30a82921ff
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:39:50:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:40:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:40:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4f27c96f-2318-49e8-a4ea-74d7a28d3c12
01/30/2025 05:40:51:INFO:Received: evaluate message 4f27c96f-2318-49e8-a4ea-74d7a28d3c12
[92mINFO [0m:      Sent reply
01/30/2025 05:46:04:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:46:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:46:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8164c204-9499-4885-b58b-a4849e70e92f
01/30/2025 05:46:49:INFO:Received: train message 8164c204-9499-4885-b58b-a4849e70e92f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 06:33:19:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:34:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:34:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a8c3d130-d1c1-4163-b380-93a2abcef094
01/30/2025 06:34:11:INFO:Received: evaluate message a8c3d130-d1c1-4163-b380-93a2abcef094
[92mINFO [0m:      Sent reply
01/30/2025 06:38:49:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:39:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:39:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 51243789-ba76-4ddc-9c1b-bdfbe5809e07
01/30/2025 06:39:46:INFO:Received: train message 51243789-ba76-4ddc-9c1b-bdfbe5809e07
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 07:10:13:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:10:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:10:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b20dd41c-5b81-4040-aa1e-6f905cbfefa6
01/30/2025 07:10:45:INFO:Received: evaluate message b20dd41c-5b81-4040-aa1e-6f905cbfefa6
[92mINFO [0m:      Sent reply
01/30/2025 07:15:05:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:16:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:16:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 44ba2b7f-6e5d-464a-8da8-82f0bdc2f7ad
01/30/2025 07:16:28:INFO:Received: train message 44ba2b7f-6e5d-464a-8da8-82f0bdc2f7ad
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434, 1.3048963505944273], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351, 0.653242045912203], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195, 0.9039743690802352]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434, 1.3048963505944273, 1.329263680124302], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351, 0.653242045912203, 0.6472009665726943], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195, 0.9039743690802352, 0.9046808053879412]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434, 1.3048963505944273, 1.329263680124302, 1.34127657160469], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351, 0.653242045912203, 0.6472009665726943, 0.6484091824405961], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195, 0.9039743690802352, 0.9046808053879412, 0.9038223419023863]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.84228515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.08679565787315369, 0.1300283968448639, 0.08396735042333603, 0.027786701917648315, 0.039344578981399536, 0.00919430237263441, 0.011667607352137566, 0.024326274171471596]
Noise Multiplier after list and tensor:  0.05163885874208063
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 07:50:45:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:51:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:51:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 33d6b27c-95c3-4c11-87b8-2f6e8946a2c1
01/30/2025 07:51:40:INFO:Received: evaluate message 33d6b27c-95c3-4c11-87b8-2f6e8946a2c1
[92mINFO [0m:      Sent reply
01/30/2025 07:58:37:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:58:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:58:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 959c810e-fe18-4382-933c-b92ff917b067
01/30/2025 07:58:38:INFO:Received: reconnect message 959c810e-fe18-4382-933c-b92ff917b067
01/30/2025 07:58:38:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 07:58:38:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434, 1.3048963505944273, 1.329263680124302, 1.34127657160469, 1.3089735003858145], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351, 0.653242045912203, 0.6472009665726943, 0.6484091824405961, 0.6604913411196134], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195, 0.9039743690802352, 0.9046808053879412, 0.9038223419023863, 0.9054790163872304]}



Final client history:
{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434, 1.3048963505944273, 1.329263680124302, 1.34127657160469, 1.3089735003858145], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351, 0.653242045912203, 0.6472009665726943, 0.6484091824405961, 0.6604913411196134], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195, 0.9039743690802352, 0.9046808053879412, 0.9038223419023863, 0.9054790163872304]}

