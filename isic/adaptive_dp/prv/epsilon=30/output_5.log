nohup: ignoring input
Traceback (most recent call last):
  File "client_5.py", line 16, in <module>
    from flamby.datasets.fed_isic2019 import FedIsic2019
ModuleNotFoundError: No module named 'flamby'
nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adaptive_dp/prv/epsilon=30/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/30/2025 12:31:31:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/30/2025 12:31:31:DEBUG:ChannelConnectivity.IDLE
01/30/2025 12:31:31:DEBUG:ChannelConnectivity.CONNECTING
01/30/2025 12:31:31:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/30/2025 12:40:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 12:40:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 85518e4c-6959-4b14-b696-06e34a8c9bdc
01/30/2025 12:40:10:INFO:Received: train message 85518e4c-6959-4b14-b696-06e34a8c9bdc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 12:44:18:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 13:06:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 13:06:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2aacd7a9-85ca-4a64-812c-767ec719fdd3
01/30/2025 13:06:43:INFO:Received: evaluate message 2aacd7a9-85ca-4a64-812c-767ec719fdd3
[92mINFO [0m:      Sent reply
01/30/2025 13:10:49:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 13:11:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 13:11:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2e8bc99d-187e-46b0-81d7-3494f2aaf006
01/30/2025 13:11:21:INFO:Received: train message 2e8bc99d-187e-46b0-81d7-3494f2aaf006
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 13:15:39:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 13:40:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 13:40:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 245aebf7-5bc3-44aa-a86a-3e939de8535b
01/30/2025 13:40:47:INFO:Received: evaluate message 245aebf7-5bc3-44aa-a86a-3e939de8535b
[92mINFO [0m:      Sent reply
01/30/2025 13:45:06:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 13:45:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 13:45:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6cff62ef-a15d-41c3-a7e8-b72437db8dd8
01/30/2025 13:45:30:INFO:Received: train message 6cff62ef-a15d-41c3-a7e8-b72437db8dd8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 13:49:26:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 14:09:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 14:09:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5b4d9046-ac59-4b8a-87b5-660adfe86471
01/30/2025 14:09:57:INFO:Received: evaluate message 5b4d9046-ac59-4b8a-87b5-660adfe86471
[92mINFO [0m:      Sent reply
01/30/2025 14:13:57:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 14:14:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 14:14:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eba94709-0273-4a23-98eb-ff7b04bc22bd
01/30/2025 14:14:34:INFO:Received: train message eba94709-0273-4a23-98eb-ff7b04bc22bd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 14:18:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 14:44:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 14:44:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ce16ec4f-c38b-45de-8c1c-49abf3a69a31
01/30/2025 14:44:35:INFO:Received: evaluate message ce16ec4f-c38b-45de-8c1c-49abf3a69a31
[92mINFO [0m:      Sent reply
01/30/2025 14:48:37:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 14:49:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 14:49:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 39500a63-1e53-495b-a46e-7419cc566d61
01/30/2025 14:49:20:INFO:Received: train message 39500a63-1e53-495b-a46e-7419cc566d61
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 14:53:24:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 15:16:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 15:16:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3d8771f1-889e-4ea0-8564-237c80651b74
01/30/2025 15:16:33:INFO:Received: evaluate message 3d8771f1-889e-4ea0-8564-237c80651b74
[92mINFO [0m:      Sent reply
01/30/2025 15:20:31:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 15:21:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 15:21:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5a7ff11e-1867-4e77-9423-02a7c3b60bce
01/30/2025 15:21:21:INFO:Received: train message 5a7ff11e-1867-4e77-9423-02a7c3b60bce
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 15:25:20:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 15:46:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 15:46:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 12cc0266-dd4e-4974-87e4-e3e1e0985dbf
01/30/2025 15:46:36:INFO:Received: evaluate message 12cc0266-dd4e-4974-87e4-e3e1e0985dbf
[92mINFO [0m:      Sent reply
01/30/2025 15:50:30:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 15:51:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 15:51:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 515b7399-e5d7-45f2-aeb3-b223f219b0b6
01/30/2025 15:51:22:INFO:Received: train message 515b7399-e5d7-45f2-aeb3-b223f219b0b6
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adaptive_dp/prv/epsilon=30', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adaptive_dp/prv/epsilon=30']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671], 'accuracy': [0.5730970600080548], 'auc': [0.8431309076755067]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373], 'accuracy': [0.5730970600080548, 0.6198147402335884], 'auc': [0.8431309076755067, 0.8753916239274806]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 15:55:39:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 16:20:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 16:20:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ac97ffb9-e30c-41cf-ba24-97d6dc47a7e8
01/30/2025 16:20:57:INFO:Received: evaluate message ac97ffb9-e30c-41cf-ba24-97d6dc47a7e8
[92mINFO [0m:      Sent reply
01/30/2025 16:24:36:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 16:25:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 16:25:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 70951150-49fe-4d3e-b0fc-bf84f2bf45ce
01/30/2025 16:25:28:INFO:Received: train message 70951150-49fe-4d3e-b0fc-bf84f2bf45ce
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 16:29:24:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 16:49:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 16:49:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9f59ed47-8893-4dd9-b645-e1e602e99479
01/30/2025 16:49:14:INFO:Received: evaluate message 9f59ed47-8893-4dd9-b645-e1e602e99479
[92mINFO [0m:      Sent reply
01/30/2025 16:53:34:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 16:54:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 16:54:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 63545470-a943-4519-8426-0f0a52d05123
01/30/2025 16:54:08:INFO:Received: train message 63545470-a943-4519-8426-0f0a52d05123
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 16:58:45:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 17:26:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 17:26:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0ee48db7-3019-4a44-bb6d-a6259c69ab30
01/30/2025 17:26:08:INFO:Received: evaluate message 0ee48db7-3019-4a44-bb6d-a6259c69ab30
[92mINFO [0m:      Sent reply
01/30/2025 17:30:15:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 17:30:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 17:30:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c564f421-10d9-4ab5-9e02-f729848d8a9e
01/30/2025 17:30:47:INFO:Received: train message c564f421-10d9-4ab5-9e02-f729848d8a9e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 17:35:10:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 17:54:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 17:54:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 800c1f46-6a89-486e-9ffc-68390969cf33
01/30/2025 17:54:53:INFO:Received: evaluate message 800c1f46-6a89-486e-9ffc-68390969cf33
[92mINFO [0m:      Sent reply
01/30/2025 17:59:04:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 17:59:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 17:59:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 26a4aa77-cb39-4215-9109-2f7645591c45
01/30/2025 17:59:29:INFO:Received: train message 26a4aa77-cb39-4215-9109-2f7645591c45
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 18:03:57:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 18:27:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 18:27:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 43006ed5-a0a9-4395-a027-599ee6a90a50
01/30/2025 18:27:56:INFO:Received: evaluate message 43006ed5-a0a9-4395-a027-599ee6a90a50
[92mINFO [0m:      Sent reply
01/30/2025 18:32:01:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 18:32:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 18:32:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5f966d6a-151c-463b-9d62-93b605cc693e
01/30/2025 18:32:29:INFO:Received: train message 5f966d6a-151c-463b-9d62-93b605cc693e
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 18:36:34:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 18:56:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 18:56:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 62767d60-2ac2-4dfb-a772-ad59b21ccce4
01/30/2025 18:56:27:INFO:Received: evaluate message 62767d60-2ac2-4dfb-a772-ad59b21ccce4
[92mINFO [0m:      Sent reply
01/30/2025 19:00:48:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 19:01:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 19:01:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ee88fc83-fd52-4c2d-a0fd-4c215dac31cc
01/30/2025 19:01:15:INFO:Received: train message ee88fc83-fd52-4c2d-a0fd-4c215dac31cc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 19:05:25:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 19:30:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 19:30:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 19075108-6951-4b79-94b9-63d88c289343
01/30/2025 19:30:13:INFO:Received: evaluate message 19075108-6951-4b79-94b9-63d88c289343
[92mINFO [0m:      Sent reply
01/30/2025 19:34:22:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 19:34:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 19:34:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9fe7d415-d439-4d6d-9453-fed358c70480
01/30/2025 19:34:56:INFO:Received: train message 9fe7d415-d439-4d6d-9453-fed358c70480
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 19:39:14:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 19:58:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 19:58:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7aa1ca80-342e-4e08-a20e-c63aaf026547
01/30/2025 19:58:31:INFO:Received: evaluate message 7aa1ca80-342e-4e08-a20e-c63aaf026547
[92mINFO [0m:      Sent reply
01/30/2025 20:02:35:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 20:03:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 20:03:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ad55d419-0de7-4b21-8a3d-3d9bfbf39052
01/30/2025 20:03:22:INFO:Received: train message ad55d419-0de7-4b21-8a3d-3d9bfbf39052
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 20:07:31:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 20:32:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 20:32:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2f5aa434-f199-496e-9ad0-a11eea1f6e63
01/30/2025 20:32:15:INFO:Received: evaluate message 2f5aa434-f199-496e-9ad0-a11eea1f6e63
[92mINFO [0m:      Sent reply
01/30/2025 20:36:17:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 20:36:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 20:36:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b049aa76-2bda-4d7c-89b8-f3439163434c
01/30/2025 20:36:49:INFO:Received: train message b049aa76-2bda-4d7c-89b8-f3439163434c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 20:40:58:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 21:01:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 21:01:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 166055f0-509b-468a-ab11-c95542b77b18
01/30/2025 21:01:00:INFO:Received: evaluate message 166055f0-509b-468a-ab11-c95542b77b18
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 21:04:57:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 21:05:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 21:05:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1236ce64-5250-434a-804c-f4439f0593cc
01/30/2025 21:05:50:INFO:Received: train message 1236ce64-5250-434a-804c-f4439f0593cc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 21:10:07:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 21:32:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 21:32:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4fa44ad5-1ad8-4ddd-9446-474301d61a98
01/30/2025 21:32:26:INFO:Received: evaluate message 4fa44ad5-1ad8-4ddd-9446-474301d61a98
[92mINFO [0m:      Sent reply
01/30/2025 21:36:14:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 21:37:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 21:37:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message df8a958e-44e3-42b5-b48f-fc2cb6ec27b6
01/30/2025 21:37:31:INFO:Received: train message df8a958e-44e3-42b5-b48f-fc2cb6ec27b6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 21:41:46:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 22:07:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 22:07:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6b0cbd8e-433c-4300-8304-8dcc9f9e4502
01/30/2025 22:07:17:INFO:Received: evaluate message 6b0cbd8e-433c-4300-8304-8dcc9f9e4502
[92mINFO [0m:      Sent reply
01/30/2025 22:11:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 22:12:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 22:12:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d82d0b46-f0fb-44de-8d52-6e39211beb9b
01/30/2025 22:12:20:INFO:Received: train message d82d0b46-f0fb-44de-8d52-6e39211beb9b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 22:16:36:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 22:47:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 22:47:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 55e7e054-a35b-4a22-9511-e555dc690425
01/30/2025 22:47:30:INFO:Received: evaluate message 55e7e054-a35b-4a22-9511-e555dc690425
[92mINFO [0m:      Sent reply
01/30/2025 22:51:28:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 22:52:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 22:52:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 44eeb79e-71ab-4006-a836-be7b5a78b1cb
01/30/2025 22:52:43:INFO:Received: train message 44eeb79e-71ab-4006-a836-be7b5a78b1cb

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 22:56:43:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 23:20:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 23:20:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eb6632d4-cbd3-4eab-b9ea-ff480339f830
01/30/2025 23:20:01:INFO:Received: evaluate message eb6632d4-cbd3-4eab-b9ea-ff480339f830
[92mINFO [0m:      Sent reply
01/30/2025 23:23:56:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 23:25:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 23:25:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 45f7c360-f770-4d0c-ba7b-8d9f910d314e
01/30/2025 23:25:32:INFO:Received: train message 45f7c360-f770-4d0c-ba7b-8d9f910d314e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 23:30:01:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 23:56:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 23:56:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8095d472-e1ae-459c-9b66-ea241963b5eb
01/30/2025 23:56:54:INFO:Received: evaluate message 8095d472-e1ae-459c-9b66-ea241963b5eb
[92mINFO [0m:      Sent reply
01/31/2025 00:01:17:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 00:02:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 00:02:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8f2ea476-7593-49b2-9ad5-f934b1e6a216
01/31/2025 00:02:23:INFO:Received: train message 8f2ea476-7593-49b2-9ad5-f934b1e6a216
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 00:07:04:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 00:38:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 00:38:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 006c0c08-f1ed-4213-8fff-f45259d5aec0
01/31/2025 00:38:58:INFO:Received: evaluate message 006c0c08-f1ed-4213-8fff-f45259d5aec0
[92mINFO [0m:      Sent reply
01/31/2025 00:43:22:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 00:44:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 00:44:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 573d35f1-7bc8-41b9-a3e6-c2321cb81360
01/31/2025 00:44:17:INFO:Received: train message 573d35f1-7bc8-41b9-a3e6-c2321cb81360
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 00:48:22:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 01:13:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 01:13:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6519c449-634a-4280-afc1-7f09b662d6dd
01/31/2025 01:13:07:INFO:Received: evaluate message 6519c449-634a-4280-afc1-7f09b662d6dd
Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 01:17:25:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 01:17:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 01:17:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d65188ed-0b9b-45d7-bc83-3320714f0753
01/31/2025 01:17:58:INFO:Received: train message d65188ed-0b9b-45d7-bc83-3320714f0753
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 01:21:44:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 01:42:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 01:42:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4d1c95b0-635a-4e2b-8284-c26f4f9a8302
01/31/2025 01:42:55:INFO:Received: evaluate message 4d1c95b0-635a-4e2b-8284-c26f4f9a8302
[92mINFO [0m:      Sent reply
01/31/2025 01:46:42:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 01:47:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 01:47:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d4ac0e75-2136-4f67-8f5b-383b59a66ddd
01/31/2025 01:47:31:INFO:Received: train message d4ac0e75-2136-4f67-8f5b-383b59a66ddd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 01:51:43:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 02:12:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 02:12:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9e0023ff-c471-45a7-a616-94e91e424459
01/31/2025 02:12:11:INFO:Received: evaluate message 9e0023ff-c471-45a7-a616-94e91e424459
[92mINFO [0m:      Sent reply
01/31/2025 02:16:05:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 02:16:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 02:16:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4b86788e-467a-4f79-bdaf-c01aa3481248
01/31/2025 02:16:49:INFO:Received: train message 4b86788e-467a-4f79-bdaf-c01aa3481248
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 02:20:47:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 02:38:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 02:38:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 64323744-8d50-4d08-9b31-a74051876d95
01/31/2025 02:38:09:INFO:Received: evaluate message 64323744-8d50-4d08-9b31-a74051876d95

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 02:42:17:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 02:43:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 02:43:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c3be4ae9-7fdd-4dca-834c-b7801e2430b4
01/31/2025 02:43:04:INFO:Received: train message c3be4ae9-7fdd-4dca-834c-b7801e2430b4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 02:47:04:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:06:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:06:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f287ecc4-240e-44a6-a759-ca5e52644ab1
01/31/2025 03:06:23:INFO:Received: evaluate message f287ecc4-240e-44a6-a759-ca5e52644ab1
[92mINFO [0m:      Sent reply
01/31/2025 03:10:13:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:10:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:10:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a0a34013-7688-414a-be47-057e08702449
01/31/2025 03:10:58:INFO:Received: train message a0a34013-7688-414a-be47-057e08702449
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 03:15:10:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:33:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:33:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1c378950-5911-4d39-87f9-6cac57d33831
01/31/2025 03:33:08:INFO:Received: evaluate message 1c378950-5911-4d39-87f9-6cac57d33831
[92mINFO [0m:      Sent reply
01/31/2025 03:37:05:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:37:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:37:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 70984e14-4d9f-4a9d-bf35-fd19e61cc064
01/31/2025 03:37:46:INFO:Received: train message 70984e14-4d9f-4a9d-bf35-fd19e61cc064
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 03:42:07:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:59:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:59:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e8ca6015-737f-4b84-a63d-30a9476c9e07
01/31/2025 03:59:53:INFO:Received: evaluate message e8ca6015-737f-4b84-a63d-30a9476c9e07

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338, 1.3332027150074224], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742, 0.6544502617801047], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267, 0.9093262993126497]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 04:03:56:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 04:04:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 04:04:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 46310ec8-c89d-4588-ba62-f5437cada1ea
01/31/2025 04:04:21:INFO:Received: train message 46310ec8-c89d-4588-ba62-f5437cada1ea
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 04:08:21:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 04:28:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 04:28:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a631cfe7-ab0b-4238-bd33-fbe982c9da4e
01/31/2025 04:28:28:INFO:Received: evaluate message a631cfe7-ab0b-4238-bd33-fbe982c9da4e
[92mINFO [0m:      Sent reply
01/31/2025 04:32:40:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 04:32:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 04:32:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 82add1b5-ee39-4367-a79f-cb8f08030347
01/31/2025 04:32:45:INFO:Received: reconnect message 82add1b5-ee39-4367-a79f-cb8f08030347
01/31/2025 04:32:45:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/31/2025 04:32:45:INFO:Disconnect and shut down

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338, 1.3332027150074224, 1.3677619359352107], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742, 0.6544502617801047, 0.6480064438179621], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267, 0.9093262993126497, 0.9082844410647312]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0028989966958761215, 0.0052146692760288715, 0.00014830751752015203, 0.0004834300489164889, 0.0024501762818545103, 0.0005216352292336524, 0.00025411584647372365, 0.00012027876800857484]
Noise Multiplier after list and tensor:  0.0015114512079890119
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338, 1.3332027150074224, 1.3677619359352107, 1.339125461219253], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742, 0.6544502617801047, 0.6480064438179621, 0.662505034232783], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267, 0.9093262993126497, 0.9082844410647312, 0.909541772155786]}



Final client history:
{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338, 1.3332027150074224, 1.3677619359352107, 1.339125461219253], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742, 0.6544502617801047, 0.6480064438179621, 0.662505034232783], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267, 0.9093262993126497, 0.9082844410647312, 0.909541772155786]}

