nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/07/2025 20:16:14:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/07/2025 20:16:14:DEBUG:ChannelConnectivity.IDLE
02/07/2025 20:16:14:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738988174.201363 1681060 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/07/2025 20:16:34:INFO:
[92mINFO [0m:      Received: train message 9a569a9f-fd5b-40e4-ab4f-ef05f9204ded
02/07/2025 20:16:34:INFO:Received: train message 9a569a9f-fd5b-40e4-ab4f-ef05f9204ded
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 20:57:05:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 20:57:51:INFO:
[92mINFO [0m:      Received: evaluate message c6b79b05-4a21-4dfa-a954-4477cfb4a11d
02/07/2025 20:57:51:INFO:Received: evaluate message c6b79b05-4a21-4dfa-a954-4477cfb4a11d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:02:37:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:03:10:INFO:
[92mINFO [0m:      Received: train message 064375c7-8951-4576-a073-601426555592
02/07/2025 21:03:10:INFO:Received: train message 064375c7-8951-4576-a073-601426555592
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 21:43:24:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:44:14:INFO:
[92mINFO [0m:      Received: evaluate message 9cac02de-7c4f-40ab-95ac-2cae3a359c26
02/07/2025 21:44:14:INFO:Received: evaluate message 9cac02de-7c4f-40ab-95ac-2cae3a359c26
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:49:08:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:49:44:INFO:
[92mINFO [0m:      Received: train message 49412adb-b6e8-4fbf-a27c-96218a774aec
02/07/2025 21:49:44:INFO:Received: train message 49412adb-b6e8-4fbf-a27c-96218a774aec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 22:28:09:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:28:42:INFO:
[92mINFO [0m:      Received: evaluate message 24b8e094-45a5-47fe-bd29-98dcb25a3bbd
02/07/2025 22:28:42:INFO:Received: evaluate message 24b8e094-45a5-47fe-bd29-98dcb25a3bbd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 22:34:02:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:35:19:INFO:
[92mINFO [0m:      Received: train message 539fe5c5-94e4-401b-a7df-88934eae7ea3
02/07/2025 22:35:19:INFO:Received: train message 539fe5c5-94e4-401b-a7df-88934eae7ea3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 23:17:35:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:18:22:INFO:
[92mINFO [0m:      Received: evaluate message da91f14b-bac4-4238-91c4-b759e29d6179
02/07/2025 23:18:22:INFO:Received: evaluate message da91f14b-bac4-4238-91c4-b759e29d6179
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 23:23:11:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:23:36:INFO:
[92mINFO [0m:      Received: train message 951c2c2f-ca20-49d1-afd0-ac726fced32e
02/07/2025 23:23:36:INFO:Received: train message 951c2c2f-ca20-49d1-afd0-ac726fced32e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 00:04:34:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:05:04:INFO:
[92mINFO [0m:      Received: evaluate message e2d81c90-b4c2-478d-8893-183b2d792276
02/08/2025 00:05:04:INFO:Received: evaluate message e2d81c90-b4c2-478d-8893-183b2d792276
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:09:45:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:10:33:INFO:
[92mINFO [0m:      Received: train message 009398c9-90f7-4fc5-811c-978db6610161
02/08/2025 00:10:33:INFO:Received: train message 009398c9-90f7-4fc5-811c-978db6610161
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 00:51:22:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:52:10:INFO:
[92mINFO [0m:      Received: evaluate message a8154f1a-11c9-4fb7-a440-cf2f0d1e535f
02/08/2025 00:52:10:INFO:Received: evaluate message a8154f1a-11c9-4fb7-a440-cf2f0d1e535f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:57:07:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:58:04:INFO:
[92mINFO [0m:      Received: train message 423e5353-6e83-4607-8048-ad9b02f77fee
02/08/2025 00:58:04:INFO:Received: train message 423e5353-6e83-4607-8048-ad9b02f77fee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 01:37:33:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:38:15:INFO:
[92mINFO [0m:      Received: evaluate message 486108b5-e71a-4d91-94d0-1d489e7dc967
02/08/2025 01:38:15:INFO:Received: evaluate message 486108b5-e71a-4d91-94d0-1d489e7dc967
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=1.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794], 'accuracy': [0.47643979057591623], 'auc': [0.7113553522460114], 'precision': [0.4601909378305515], 'recall': [0.47643979057591623], 'f1': [0.39597907648163966]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782], 'accuracy': [0.47643979057591623, 0.5501409585179219], 'auc': [0.7113553522460114, 0.7558628759678585], 'precision': [0.4601909378305515, 0.4857540893933654], 'recall': [0.47643979057591623, 0.5501409585179219], 'f1': [0.39597907648163966, 0.4846537523434621]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 01:42:52:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:43:40:INFO:
[92mINFO [0m:      Received: train message 647d3179-06f2-42c9-aeb3-669d5d1a162f
02/08/2025 01:43:40:INFO:Received: train message 647d3179-06f2-42c9-aeb3-669d5d1a162f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 02:24:00:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:24:48:INFO:
[92mINFO [0m:      Received: evaluate message 53cf8844-d57b-404e-93d0-c5251d21adf0
02/08/2025 02:24:48:INFO:Received: evaluate message 53cf8844-d57b-404e-93d0-c5251d21adf0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 02:29:28:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:29:52:INFO:
[92mINFO [0m:      Received: train message 54b96d7b-b8cd-4bac-9ac9-72bfde12eda8
02/08/2025 02:29:52:INFO:Received: train message 54b96d7b-b8cd-4bac-9ac9-72bfde12eda8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 03:11:11:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:11:48:INFO:
[92mINFO [0m:      Received: evaluate message 84d4789b-127b-4f95-8e74-51f1368086fd
02/08/2025 03:11:48:INFO:Received: evaluate message 84d4789b-127b-4f95-8e74-51f1368086fd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 03:16:16:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:17:44:INFO:
[92mINFO [0m:      Received: train message dfa3e6bb-de77-4e76-bc49-9a815ea19cf2
02/08/2025 03:17:44:INFO:Received: train message dfa3e6bb-de77-4e76-bc49-9a815ea19cf2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 03:57:26:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:58:14:INFO:
[92mINFO [0m:      Received: evaluate message edc59807-c732-4b5a-8ef0-8022514f9415
02/08/2025 03:58:14:INFO:Received: evaluate message edc59807-c732-4b5a-8ef0-8022514f9415
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 04:03:21:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:04:03:INFO:
[92mINFO [0m:      Received: train message 8b8d5520-ce6d-4c71-aa03-e4b122c0b32f
02/08/2025 04:04:03:INFO:Received: train message 8b8d5520-ce6d-4c71-aa03-e4b122c0b32f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 04:42:34:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:43:20:INFO:
[92mINFO [0m:      Received: evaluate message 18bee0a3-1f3c-4450-afc2-78b9821b4691
02/08/2025 04:43:20:INFO:Received: evaluate message 18bee0a3-1f3c-4450-afc2-78b9821b4691
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 04:47:54:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:48:26:INFO:
[92mINFO [0m:      Received: train message f272a325-165c-43e6-93f9-cec7ecd41bc6
02/08/2025 04:48:26:INFO:Received: train message f272a325-165c-43e6-93f9-cec7ecd41bc6

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 05:29:18:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:29:50:INFO:
[92mINFO [0m:      Received: evaluate message e7eafa8a-d18e-44e1-a6c1-9a5d1c20f38d
02/08/2025 05:29:50:INFO:Received: evaluate message e7eafa8a-d18e-44e1-a6c1-9a5d1c20f38d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 05:34:19:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:35:04:INFO:
[92mINFO [0m:      Received: train message 5792517f-02a4-44a6-aef0-c603514910ff
02/08/2025 05:35:04:INFO:Received: train message 5792517f-02a4-44a6-aef0-c603514910ff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 06:15:54:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:16:43:INFO:
[92mINFO [0m:      Received: evaluate message ac54b070-f619-4ebb-9f3a-cd17cc85bd66
02/08/2025 06:16:43:INFO:Received: evaluate message ac54b070-f619-4ebb-9f3a-cd17cc85bd66
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 06:21:14:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:22:13:INFO:
[92mINFO [0m:      Received: train message 24be28e7-f844-4ebf-87a4-982d42b23239
02/08/2025 06:22:13:INFO:Received: train message 24be28e7-f844-4ebf-87a4-982d42b23239
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 07:01:15:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:02:02:INFO:
[92mINFO [0m:      Received: evaluate message ac41350b-2c91-404b-9234-d3f4bcd005ee
02/08/2025 07:02:02:INFO:Received: evaluate message ac41350b-2c91-404b-9234-d3f4bcd005ee
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 07:06:48:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:07:30:INFO:
[92mINFO [0m:      Received: train message e7403192-70e9-450c-aadd-8df077df9d63
02/08/2025 07:07:30:INFO:Received: train message e7403192-70e9-450c-aadd-8df077df9d63
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 07:47:36:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:48:21:INFO:
[92mINFO [0m:      Received: evaluate message 618bc743-4b2e-4a85-8d6a-4a8222ca5400
02/08/2025 07:48:21:INFO:Received: evaluate message 618bc743-4b2e-4a85-8d6a-4a8222ca5400
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 07:52:52:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:53:18:INFO:
[92mINFO [0m:      Received: train message 7e1c7814-633b-423f-bbe3-6becde85ef85
02/08/2025 07:53:18:INFO:Received: train message 7e1c7814-633b-423f-bbe3-6becde85ef85
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 08:34:50:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:35:11:INFO:
[92mINFO [0m:      Received: evaluate message da4f59c4-fa14-43bd-a700-cd7c121e3223
02/08/2025 08:35:11:INFO:Received: evaluate message da4f59c4-fa14-43bd-a700-cd7c121e3223
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 08:39:42:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:40:48:INFO:
[92mINFO [0m:      Received: train message 28666962-7715-420c-9681-97d736b8e475
02/08/2025 08:40:48:INFO:Received: train message 28666962-7715-420c-9681-97d736b8e475
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 09:19:28:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:20:20:INFO:
[92mINFO [0m:      Received: evaluate message cd71ace2-0ae4-4762-bf7e-56d5d16870cb
02/08/2025 09:20:20:INFO:Received: evaluate message cd71ace2-0ae4-4762-bf7e-56d5d16870cb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 09:25:06:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:25:52:INFO:
[92mINFO [0m:      Received: train message 42bb3e40-db39-4ee5-9ae8-00d077aab4c8
02/08/2025 09:25:52:INFO:Received: train message 42bb3e40-db39-4ee5-9ae8-00d077aab4c8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 10:04:18:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:04:56:INFO:
[92mINFO [0m:      Received: evaluate message 08ed9276-1b29-4180-a3b1-26387c53d4fb
02/08/2025 10:04:56:INFO:Received: evaluate message 08ed9276-1b29-4180-a3b1-26387c53d4fb

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 10:09:35:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:10:02:INFO:
[92mINFO [0m:      Received: train message 5ef60ed2-e8e8-4603-8877-0a3edf6f7de7
02/08/2025 10:10:02:INFO:Received: train message 5ef60ed2-e8e8-4603-8877-0a3edf6f7de7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 10:49:31:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:50:18:INFO:
[92mINFO [0m:      Received: evaluate message fc2aaa28-4b70-4ea5-8942-04ce3c10a64b
02/08/2025 10:50:18:INFO:Received: evaluate message fc2aaa28-4b70-4ea5-8942-04ce3c10a64b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 10:54:51:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:55:13:INFO:
[92mINFO [0m:      Received: train message f7d2a198-17fe-4c6b-8059-4dc5f7f61398
02/08/2025 10:55:13:INFO:Received: train message f7d2a198-17fe-4c6b-8059-4dc5f7f61398
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 11:34:13:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:34:32:INFO:
[92mINFO [0m:      Received: evaluate message b3556d4b-5d9e-4399-9c83-0f4b515e9148
02/08/2025 11:34:32:INFO:Received: evaluate message b3556d4b-5d9e-4399-9c83-0f4b515e9148

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 11:39:30:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:40:51:INFO:
[92mINFO [0m:      Received: train message 721e9f0b-1559-41df-a10f-2ec087d0f754
02/08/2025 11:40:51:INFO:Received: train message 721e9f0b-1559-41df-a10f-2ec087d0f754
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 12:18:18:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:19:02:INFO:
[92mINFO [0m:      Received: evaluate message bb422c7c-c5f3-4cb7-8170-0974a29dbb1d
02/08/2025 12:19:02:INFO:Received: evaluate message bb422c7c-c5f3-4cb7-8170-0974a29dbb1d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 12:23:41:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:24:15:INFO:
[92mINFO [0m:      Received: train message ddbd4cc3-6501-4d0a-babe-eb49869a10cb
02/08/2025 12:24:15:INFO:Received: train message ddbd4cc3-6501-4d0a-babe-eb49869a10cb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 13:04:19:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:05:09:INFO:
[92mINFO [0m:      Received: evaluate message ab9c913e-031a-443d-886b-f27af26901ab
02/08/2025 13:05:09:INFO:Received: evaluate message ab9c913e-031a-443d-886b-f27af26901ab

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 13:10:07:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:10:45:INFO:
[92mINFO [0m:      Received: train message 89884fcd-eca8-467f-916d-278d71ea75c7
02/08/2025 13:10:45:INFO:Received: train message 89884fcd-eca8-467f-916d-278d71ea75c7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 13:51:34:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:52:24:INFO:
[92mINFO [0m:      Received: evaluate message 3bf3b16b-c64f-4246-bb00-2dde550e748e
02/08/2025 13:52:24:INFO:Received: evaluate message 3bf3b16b-c64f-4246-bb00-2dde550e748e
[92mINFO [0m:      Sent reply
02/08/2025 13:57:02:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:57:48:INFO:
[92mINFO [0m:      Received: train message e68c9e0a-8df6-483d-a595-df1a2b24c4a2
02/08/2025 13:57:48:INFO:Received: train message e68c9e0a-8df6-483d-a595-df1a2b24c4a2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 14:37:53:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:38:15:INFO:
[92mINFO [0m:      Received: evaluate message 967cb5af-c34c-45bc-8f74-d41669092546
02/08/2025 14:38:15:INFO:Received: evaluate message 967cb5af-c34c-45bc-8f74-d41669092546

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 14:42:18:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:44:22:INFO:
[92mINFO [0m:      Received: train message 4f476d7b-174a-474b-bb13-ba6eb17636f6
02/08/2025 14:44:22:INFO:Received: train message 4f476d7b-174a-474b-bb13-ba6eb17636f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 15:23:27:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:23:55:INFO:
[92mINFO [0m:      Received: evaluate message 366e2a70-f3fd-422c-a891-7e837d33802e
02/08/2025 15:23:55:INFO:Received: evaluate message 366e2a70-f3fd-422c-a891-7e837d33802e
[92mINFO [0m:      Sent reply
02/08/2025 15:28:11:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:29:50:INFO:
[92mINFO [0m:      Received: train message a57165b7-7175-4490-a571-0f27168ad3fa
02/08/2025 15:29:50:INFO:Received: train message a57165b7-7175-4490-a571-0f27168ad3fa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 16:09:41:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:10:08:INFO:
[92mINFO [0m:      Received: evaluate message 9ff2fb8a-00c0-4834-953b-a651cadbe705
02/08/2025 16:10:08:INFO:Received: evaluate message 9ff2fb8a-00c0-4834-953b-a651cadbe705

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 16:14:28:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:15:40:INFO:
[92mINFO [0m:      Received: train message 116a199a-577a-418d-8b7f-9a3a6f88ea0b
02/08/2025 16:15:40:INFO:Received: train message 116a199a-577a-418d-8b7f-9a3a6f88ea0b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 16:55:30:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:55:56:INFO:
[92mINFO [0m:      Received: evaluate message 95b87eef-77c4-40d1-8940-70b293d9a125
02/08/2025 16:55:56:INFO:Received: evaluate message 95b87eef-77c4-40d1-8940-70b293d9a125
[92mINFO [0m:      Sent reply
02/08/2025 17:00:05:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:01:44:INFO:
[92mINFO [0m:      Received: train message 18dc9ede-16fd-48d3-882c-dbfa3987f681
02/08/2025 17:01:44:INFO:Received: train message 18dc9ede-16fd-48d3-882c-dbfa3987f681
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 17:41:04:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:41:52:INFO:
[92mINFO [0m:      Received: evaluate message e03d3d91-2189-4d81-92f8-28a6bcabb7e1
02/08/2025 17:41:52:INFO:Received: evaluate message e03d3d91-2189-4d81-92f8-28a6bcabb7e1

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 17:46:19:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:47:15:INFO:
[92mINFO [0m:      Received: train message fedc071e-e8f4-427f-ad45-9add2ae82452
02/08/2025 17:47:15:INFO:Received: train message fedc071e-e8f4-427f-ad45-9add2ae82452
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:27:06:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:27:48:INFO:
[92mINFO [0m:      Received: evaluate message 86623e4f-5a02-4c9f-8ddd-d7896df5bc53
02/08/2025 18:27:48:INFO:Received: evaluate message 86623e4f-5a02-4c9f-8ddd-d7896df5bc53
[92mINFO [0m:      Sent reply
02/08/2025 18:32:16:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:33:03:INFO:
[92mINFO [0m:      Received: train message 925820d8-d5e9-4c8d-afcf-b56123afbd01
02/08/2025 18:33:03:INFO:Received: train message 925820d8-d5e9-4c8d-afcf-b56123afbd01
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 19:13:35:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:14:15:INFO:
[92mINFO [0m:      Received: evaluate message 6a064752-dd25-4bac-9d2a-fa6fb20224d3
02/08/2025 19:14:15:INFO:Received: evaluate message 6a064752-dd25-4bac-9d2a-fa6fb20224d3

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 19:18:33:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:18:49:INFO:
[92mINFO [0m:      Received: reconnect message 8d942618-34ae-4d57-9ee8-702fca5c651f
02/08/2025 19:18:49:INFO:Received: reconnect message 8d942618-34ae-4d57-9ee8-702fca5c651f
02/08/2025 19:18:49:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/08/2025 19:18:49:INFO:Disconnect and shut down

{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712, 1.3589135335666058], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445, 0.8710482896241687], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899, 0.6162404554774186], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089, 0.6075857344583624]}



Final client history:
{'loss': [1.6662151453910794, 1.5297159701645782, 1.551863261014283, 1.5058985817014714, 1.502028143602756, 1.5154600723395653, 1.4822706440692461, 1.464383297325669, 1.4763472311159702, 1.472784026659068, 1.4691647884109258, 1.45691324197329, 1.449523841235314, 1.437218246323612, 1.4469232554883185, 1.4338654322437927, 1.4200311099335496, 1.4161966695835453, 1.3913126350697422, 1.4152062544887987, 1.3926140606235315, 1.4000321165045277, 1.4029377634564668, 1.3948988419024637, 1.3893109979861886, 1.3888976879288868, 1.3626970973794517, 1.3771219619137598, 1.3507838928790712, 1.3589135335666058], 'accuracy': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'auc': [0.7113553522460114, 0.7558628759678585, 0.7725894638722273, 0.7876926757378253, 0.7939092717312701, 0.7940878214441569, 0.802629846122829, 0.8098284398796789, 0.8140964547603464, 0.8128923485226616, 0.8195022044174174, 0.8246702567554322, 0.8272553477470276, 0.8326360690848916, 0.8334649620278298, 0.8376225717283172, 0.8393377557951005, 0.8399682240589387, 0.8398582023154366, 0.8443204186923237, 0.8457597680527605, 0.8470305526057175, 0.8481692318586358, 0.849876589529544, 0.8525685976558717, 0.8555754862850858, 0.8602040502950337, 0.8639697614827273, 0.8681789708511445, 0.8710482896241687], 'precision': [0.4601909378305515, 0.4857540893933654, 0.5179395504749758, 0.5619708755743729, 0.5570045105459634, 0.5412509487714587, 0.5519301972405665, 0.5546906034663777, 0.5583946636713808, 0.5572109780381591, 0.5532355915320368, 0.569842817718724, 0.581707921349307, 0.5797258470567845, 0.5927215192174056, 0.5911074069406417, 0.5889947316206251, 0.591519501734024, 0.5885827483721016, 0.5845985065382313, 0.5899201564502452, 0.596991273507988, 0.5947573968926, 0.6045802532663231, 0.6003238340427195, 0.6090818048510976, 0.6149304471814185, 0.6166674529175278, 0.6221956701122899, 0.6162404554774186], 'recall': [0.47643979057591623, 0.5501409585179219, 0.5626258558195731, 0.5755134917438582, 0.5795408779701974, 0.581151832460733, 0.5912202980265807, 0.5948449456302859, 0.5976641159887233, 0.5964559001208216, 0.5956504228755538, 0.6020942408376964, 0.6073298429319371, 0.6065243656866693, 0.6133709222714459, 0.6145791381393476, 0.6149818767619815, 0.616995569875151, 0.6186065243656866, 0.6161900926298832, 0.6161900926298832, 0.6182037857430528, 0.6198147402335884, 0.6242448650825614, 0.6298832057994361, 0.6270640354409988, 0.6331051147805075, 0.6327023761578735, 0.6391461941200162, 0.6290777285541683], 'f1': [0.39597907648163966, 0.4846537523434621, 0.5014585519194299, 0.5185958001267923, 0.5241021306683176, 0.5284484806602164, 0.5442051234564025, 0.5467840740686137, 0.5504479369912794, 0.5525628286109179, 0.5506875319567477, 0.5595981776779966, 0.5648141032720417, 0.5665643567942293, 0.5755159773018691, 0.5761601381091164, 0.5784824760887189, 0.5816825797990623, 0.584887022233394, 0.5808668158602136, 0.582217014833643, 0.5834507332516731, 0.5834477087391129, 0.5910586692961276, 0.5981992271718743, 0.5948632982420066, 0.6020400564488785, 0.6048181807269863, 0.6133549686102089, 0.6075857344583624]}

