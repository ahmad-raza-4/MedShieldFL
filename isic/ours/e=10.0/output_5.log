nohup: ignoring input
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=10.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
02/07/2025 20:07:15:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/07/2025 20:07:15:DEBUG:ChannelConnectivity.IDLE
02/07/2025 20:07:15:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738987635.099143 1673697 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/07/2025 20:16:59:INFO:
[92mINFO [0m:      Received: train message d83e6b81-f959-4ed3-92ab-d555444b3150
02/07/2025 20:16:59:INFO:Received: train message d83e6b81-f959-4ed3-92ab-d555444b3150
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 20:22:54:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 20:57:02:INFO:
[92mINFO [0m:      Received: evaluate message 39c11aa1-38f8-446d-9088-b2214ab1c412
02/07/2025 20:57:02:INFO:Received: evaluate message 39c11aa1-38f8-446d-9088-b2214ab1c412
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:00:26:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:02:20:INFO:
[92mINFO [0m:      Received: train message bb49d06f-2804-439b-82c9-eb05527faa45
02/07/2025 21:02:20:INFO:Received: train message bb49d06f-2804-439b-82c9-eb05527faa45
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 21:08:08:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:42:45:INFO:
[92mINFO [0m:      Received: evaluate message 45b1adc7-a165-4e97-8466-a6fdf8116afe
02/07/2025 21:42:45:INFO:Received: evaluate message 45b1adc7-a165-4e97-8466-a6fdf8116afe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 21:47:30:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 21:47:45:INFO:
[92mINFO [0m:      Received: train message f6149719-2278-423f-9f9d-a403e2a1e00d
02/07/2025 21:47:45:INFO:Received: train message f6149719-2278-423f-9f9d-a403e2a1e00d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 21:53:37:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:26:19:INFO:
[92mINFO [0m:      Received: evaluate message dd5799e8-ebc7-43bf-bfde-550c1a72da49
02/07/2025 22:26:19:INFO:Received: evaluate message dd5799e8-ebc7-43bf-bfde-550c1a72da49
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 22:30:53:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 22:31:28:INFO:
[92mINFO [0m:      Received: train message 91e0ebd5-e4ac-402e-b1af-16d46610f6ad
02/07/2025 22:31:28:INFO:Received: train message 91e0ebd5-e4ac-402e-b1af-16d46610f6ad
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 22:37:37:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:15:00:INFO:
[92mINFO [0m:      Received: evaluate message 1b2fa2e4-415a-4bde-80c8-085ea344670f
02/07/2025 23:15:00:INFO:Received: evaluate message 1b2fa2e4-415a-4bde-80c8-085ea344670f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/07/2025 23:19:17:INFO:Sent reply
[92mINFO [0m:      
02/07/2025 23:19:39:INFO:
[92mINFO [0m:      Received: train message 303d4272-df20-414d-bf1a-250730f4f8ae
02/07/2025 23:19:39:INFO:Received: train message 303d4272-df20-414d-bf1a-250730f4f8ae
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/07/2025 23:25:48:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:00:43:INFO:
[92mINFO [0m:      Received: evaluate message a1477d14-3640-4595-bc62-1a542a1a8bb5
02/08/2025 00:00:43:INFO:Received: evaluate message a1477d14-3640-4595-bc62-1a542a1a8bb5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:05:01:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:05:44:INFO:
[92mINFO [0m:      Received: train message 903cfb97-65b7-4346-a79a-2014d87d084a
02/08/2025 00:05:44:INFO:Received: train message 903cfb97-65b7-4346-a79a-2014d87d084a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 00:11:53:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:44:38:INFO:
[92mINFO [0m:      Received: evaluate message b33c6e85-fe3b-4d80-8b6c-548b9d3948b2
02/08/2025 00:44:38:INFO:Received: evaluate message b33c6e85-fe3b-4d80-8b6c-548b9d3948b2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/08/2025 00:49:05:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 00:49:55:INFO:
[92mINFO [0m:      Received: train message 0d1e6852-5341-4f17-98c2-85fe21732cd7
02/08/2025 00:49:55:INFO:Received: train message 0d1e6852-5341-4f17-98c2-85fe21732cd7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 00:55:56:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:30:19:INFO:
[92mINFO [0m:      Received: evaluate message 26ebc2c1-0bc1-4618-92cc-0416daa06165
02/08/2025 01:30:19:INFO:Received: evaluate message 26ebc2c1-0bc1-4618-92cc-0416daa06165
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=10.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/ours/e=10.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232], 'accuracy': [0.5553765606121627], 'auc': [0.7952128841995612], 'precision': [0.5630504832476843], 'recall': [0.5553765606121627], 'f1': [0.5014109976972659]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068], 'accuracy': [0.5553765606121627, 0.6004832863471606], 'auc': [0.7952128841995612, 0.8346505194429128], 'precision': [0.5630504832476843, 0.588172573519457], 'recall': [0.5553765606121627, 0.6004832863471606], 'f1': [0.5014109976972659, 0.559282479633486]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 01:34:44:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 01:35:10:INFO:
[92mINFO [0m:      Received: train message 3ad6f12f-c8e7-4bc8-9e01-c1239e038781
02/08/2025 01:35:10:INFO:Received: train message 3ad6f12f-c8e7-4bc8-9e01-c1239e038781
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 01:40:45:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:16:52:INFO:
[92mINFO [0m:      Received: evaluate message 65499e21-d77d-49a1-9960-8f3d8ea6b646
02/08/2025 02:16:52:INFO:Received: evaluate message 65499e21-d77d-49a1-9960-8f3d8ea6b646
[92mINFO [0m:      Sent reply
02/08/2025 02:20:52:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 02:21:38:INFO:
[92mINFO [0m:      Received: train message c9004491-a10a-46ec-9fa1-6aefa5838844
02/08/2025 02:21:38:INFO:Received: train message c9004491-a10a-46ec-9fa1-6aefa5838844
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 02:27:20:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:01:28:INFO:
[92mINFO [0m:      Received: evaluate message 30692860-9bb9-4155-954a-b00fd72351d3
02/08/2025 03:01:28:INFO:Received: evaluate message 30692860-9bb9-4155-954a-b00fd72351d3
[92mINFO [0m:      Sent reply
02/08/2025 03:05:26:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:06:44:INFO:
[92mINFO [0m:      Received: train message 433d81bd-25a7-49c6-8042-23a82929bde7
02/08/2025 03:06:44:INFO:Received: train message 433d81bd-25a7-49c6-8042-23a82929bde7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 03:12:22:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:45:23:INFO:
[92mINFO [0m:      Received: evaluate message 80fa50c1-295a-4890-ad2f-9a8224beba4b
02/08/2025 03:45:23:INFO:Received: evaluate message 80fa50c1-295a-4890-ad2f-9a8224beba4b
[92mINFO [0m:      Sent reply
02/08/2025 03:49:25:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 03:50:10:INFO:
[92mINFO [0m:      Received: train message 1b94ff07-ecdb-41f3-bcbd-f305ef5896a3
02/08/2025 03:50:10:INFO:Received: train message 1b94ff07-ecdb-41f3-bcbd-f305ef5896a3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 03:55:54:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:32:43:INFO:
[92mINFO [0m:      Received: evaluate message c53dcccc-3a30-4a0c-8b44-0bf88ba53e97
02/08/2025 04:32:43:INFO:Received: evaluate message c53dcccc-3a30-4a0c-8b44-0bf88ba53e97
[92mINFO [0m:      Sent reply
02/08/2025 04:36:53:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 04:37:42:INFO:
[92mINFO [0m:      Received: train message 791779ae-cc4e-4f0c-93b1-b1bb848b01b8
02/08/2025 04:37:42:INFO:Received: train message 791779ae-cc4e-4f0c-93b1-b1bb848b01b8

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986]}

Step 1b: Recomputing FIM for epoch 12
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 04:43:07:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:21:04:INFO:
[92mINFO [0m:      Received: evaluate message 8b9f9bb9-7dc5-413b-8ed8-54009756e2df
02/08/2025 05:21:04:INFO:Received: evaluate message 8b9f9bb9-7dc5-413b-8ed8-54009756e2df
[92mINFO [0m:      Sent reply
02/08/2025 05:25:07:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 05:25:40:INFO:
[92mINFO [0m:      Received: train message 0c0f7f23-78e7-44fd-bf60-de1388dfcdaa
02/08/2025 05:25:40:INFO:Received: train message 0c0f7f23-78e7-44fd-bf60-de1388dfcdaa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 05:30:54:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:05:28:INFO:
[92mINFO [0m:      Received: evaluate message 30e7a9b2-53a7-4500-bdff-b18c064f4a6f
02/08/2025 06:05:28:INFO:Received: evaluate message 30e7a9b2-53a7-4500-bdff-b18c064f4a6f
[92mINFO [0m:      Sent reply
02/08/2025 06:09:56:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:10:59:INFO:
[92mINFO [0m:      Received: train message dffb7e5e-d97a-4f45-977e-62f8b9cae101
02/08/2025 06:10:59:INFO:Received: train message dffb7e5e-d97a-4f45-977e-62f8b9cae101
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 06:16:31:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:52:08:INFO:
[92mINFO [0m:      Received: evaluate message 33d2d59a-1978-4ddb-82ab-5444521eb85f
02/08/2025 06:52:08:INFO:Received: evaluate message 33d2d59a-1978-4ddb-82ab-5444521eb85f
[92mINFO [0m:      Sent reply
02/08/2025 06:56:29:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 06:56:46:INFO:
[92mINFO [0m:      Received: train message ddc0c1b2-e7cf-42a1-84fa-b601da7038ed
02/08/2025 06:56:46:INFO:Received: train message ddc0c1b2-e7cf-42a1-84fa-b601da7038ed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 07:01:58:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:39:55:INFO:
[92mINFO [0m:      Received: evaluate message 1abfebf2-9de0-4dd3-af6e-1bfc39115680
02/08/2025 07:39:55:INFO:Received: evaluate message 1abfebf2-9de0-4dd3-af6e-1bfc39115680
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 07:44:01:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 07:44:25:INFO:
[92mINFO [0m:      Received: train message 2dda2a2a-d19a-4a5d-b78b-589725e4cf75
02/08/2025 07:44:25:INFO:Received: train message 2dda2a2a-d19a-4a5d-b78b-589725e4cf75
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 07:49:51:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:26:01:INFO:
[92mINFO [0m:      Received: evaluate message 0392a1b2-fa73-492e-b827-11e060f9c84e
02/08/2025 08:26:01:INFO:Received: evaluate message 0392a1b2-fa73-492e-b827-11e060f9c84e
[92mINFO [0m:      Sent reply
02/08/2025 08:30:19:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 08:31:00:INFO:
[92mINFO [0m:      Received: train message c13faff6-089f-46f9-8bfd-fc64c6dcaca8
02/08/2025 08:31:00:INFO:Received: train message c13faff6-089f-46f9-8bfd-fc64c6dcaca8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 08:36:24:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:10:02:INFO:
[92mINFO [0m:      Received: evaluate message 02ee859b-a69e-4608-962a-56dfb79a6d8c
02/08/2025 09:10:02:INFO:Received: evaluate message 02ee859b-a69e-4608-962a-56dfb79a6d8c
[92mINFO [0m:      Sent reply
02/08/2025 09:14:09:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:14:50:INFO:
[92mINFO [0m:      Received: train message d4e9be4a-356e-4f08-84f9-153fff724f92
02/08/2025 09:14:50:INFO:Received: train message d4e9be4a-356e-4f08-84f9-153fff724f92
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 09:20:50:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 09:56:50:INFO:
[92mINFO [0m:      Received: evaluate message b0ee65c6-12ed-49ae-8403-2c20750084c2
02/08/2025 09:56:50:INFO:Received: evaluate message b0ee65c6-12ed-49ae-8403-2c20750084c2

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 10:00:41:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:01:55:INFO:
[92mINFO [0m:      Received: train message 83a14608-6011-4597-bff1-a02be6629956
02/08/2025 10:01:55:INFO:Received: train message 83a14608-6011-4597-bff1-a02be6629956
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 10:07:36:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:45:02:INFO:
[92mINFO [0m:      Received: evaluate message bdb0589c-f596-4b5e-b09b-6e3f1ee45520
02/08/2025 10:45:02:INFO:Received: evaluate message bdb0589c-f596-4b5e-b09b-6e3f1ee45520
[92mINFO [0m:      Sent reply
02/08/2025 10:49:12:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 10:49:50:INFO:
[92mINFO [0m:      Received: train message c9e9c053-b7db-43c2-9fb8-f70c172e26e7
02/08/2025 10:49:50:INFO:Received: train message c9e9c053-b7db-43c2-9fb8-f70c172e26e7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 10:55:43:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:29:34:INFO:
[92mINFO [0m:      Received: evaluate message 2e210406-52e0-46d1-9834-6393a689ebce
02/08/2025 11:29:34:INFO:Received: evaluate message 2e210406-52e0-46d1-9834-6393a689ebce

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 11:33:46:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 11:34:57:INFO:
[92mINFO [0m:      Received: train message 484275d1-d6f1-427d-b40b-00300725ad99
02/08/2025 11:34:57:INFO:Received: train message 484275d1-d6f1-427d-b40b-00300725ad99
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 11:41:03:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:14:54:INFO:
[92mINFO [0m:      Received: evaluate message aa406b45-7ef5-4d1b-90d2-cd8449415519
02/08/2025 12:14:54:INFO:Received: evaluate message aa406b45-7ef5-4d1b-90d2-cd8449415519
[92mINFO [0m:      Sent reply
02/08/2025 12:19:03:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 12:19:37:INFO:
[92mINFO [0m:      Received: train message 22d5e362-9b0a-4d66-896e-011b6a3cf4ac
02/08/2025 12:19:37:INFO:Received: train message 22d5e362-9b0a-4d66-896e-011b6a3cf4ac
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 12:25:27:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:03:01:INFO:
[92mINFO [0m:      Received: evaluate message 9b43e750-c8d3-4449-afe2-53a863a364ca
02/08/2025 13:03:01:INFO:Received: evaluate message 9b43e750-c8d3-4449-afe2-53a863a364ca

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 13:07:36:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:08:02:INFO:
[92mINFO [0m:      Received: train message cea55cd9-8bd3-44ab-82b7-cce0f3c55da1
02/08/2025 13:08:02:INFO:Received: train message cea55cd9-8bd3-44ab-82b7-cce0f3c55da1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 13:13:30:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:51:34:INFO:
[92mINFO [0m:      Received: evaluate message afd51639-9801-4972-9dd1-7868a2db7c89
02/08/2025 13:51:34:INFO:Received: evaluate message afd51639-9801-4972-9dd1-7868a2db7c89
[92mINFO [0m:      Sent reply
02/08/2025 13:56:01:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 13:56:58:INFO:
[92mINFO [0m:      Received: train message 8e1293fd-7f2d-4e46-bd24-bb3e1d7e34ba
02/08/2025 13:56:58:INFO:Received: train message 8e1293fd-7f2d-4e46-bd24-bb3e1d7e34ba
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 14:02:42:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:39:36:INFO:
[92mINFO [0m:      Received: evaluate message afa593ec-e301-4c01-a38a-6d3e00de9c95
02/08/2025 14:39:36:INFO:Received: evaluate message afa593ec-e301-4c01-a38a-6d3e00de9c95

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 14:44:10:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 14:45:08:INFO:
[92mINFO [0m:      Received: train message 4d2bb03a-1644-451c-a3ab-6b73fb4273ab
02/08/2025 14:45:08:INFO:Received: train message 4d2bb03a-1644-451c-a3ab-6b73fb4273ab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 14:51:04:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:26:43:INFO:
[92mINFO [0m:      Received: evaluate message 2a445ca3-3a29-4e35-a811-bf1582f1e4ab
02/08/2025 15:26:43:INFO:Received: evaluate message 2a445ca3-3a29-4e35-a811-bf1582f1e4ab
[92mINFO [0m:      Sent reply
02/08/2025 15:31:13:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 15:32:28:INFO:
[92mINFO [0m:      Received: train message e65a3b90-9cf9-4f42-be17-7bc41cc4a412
02/08/2025 15:32:28:INFO:Received: train message e65a3b90-9cf9-4f42-be17-7bc41cc4a412
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 15:38:24:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:15:16:INFO:
[92mINFO [0m:      Received: evaluate message 2ec435a5-e1ed-42c2-a6a7-dbf5b7dc44c6
02/08/2025 16:15:16:INFO:Received: evaluate message 2ec435a5-e1ed-42c2-a6a7-dbf5b7dc44c6

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 16:19:30:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 16:20:51:INFO:
[92mINFO [0m:      Received: train message e75af632-8d3c-4d01-a8a5-4950ddbcd762
02/08/2025 16:20:51:INFO:Received: train message e75af632-8d3c-4d01-a8a5-4950ddbcd762
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 16:26:33:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:03:39:INFO:
[92mINFO [0m:      Received: evaluate message 01f44781-9e92-489d-888a-f69d9255c458
02/08/2025 17:03:39:INFO:Received: evaluate message 01f44781-9e92-489d-888a-f69d9255c458
[92mINFO [0m:      Sent reply
02/08/2025 17:08:02:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:09:06:INFO:
[92mINFO [0m:      Received: train message d660dca6-f936-4e09-a56f-80929c7bb0c3
02/08/2025 17:09:06:INFO:Received: train message d660dca6-f936-4e09-a56f-80929c7bb0c3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 17:14:48:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:53:11:INFO:
[92mINFO [0m:      Received: evaluate message c65104b3-2901-402b-aab3-ade2139b2c6f
02/08/2025 17:53:11:INFO:Received: evaluate message c65104b3-2901-402b-aab3-ade2139b2c6f

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213, 1.310995662039535], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718, 0.8949018310594852], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313, 0.6584166731871604], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051, 0.6380738299672268]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213, 1.310995662039535, 1.264396644987697], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718, 0.8949018310594852, 0.898103397330748], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313, 0.6584166731871604, 0.6631704669167207], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051, 0.6380738299672268, 0.6435018680920499]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 17:57:51:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 17:58:22:INFO:
[92mINFO [0m:      Received: train message 957be967-9983-486c-b2a4-71dd3bee3f48
02/08/2025 17:58:22:INFO:Received: train message 957be967-9983-486c-b2a4-71dd3bee3f48
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:03:51:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:42:10:INFO:
[92mINFO [0m:      Received: evaluate message 17d5c97f-1808-430f-9a9d-9b4e74578876
02/08/2025 18:42:10:INFO:Received: evaluate message 17d5c97f-1808-430f-9a9d-9b4e74578876
[92mINFO [0m:      Sent reply
02/08/2025 18:46:47:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 18:47:40:INFO:
[92mINFO [0m:      Received: train message c61a955d-9082-48bc-b658-9cec486d3a42
02/08/2025 18:47:40:INFO:Received: train message c61a955d-9082-48bc-b658-9cec486d3a42
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/08/2025 18:53:31:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:27:38:INFO:
[92mINFO [0m:      Received: evaluate message 10f723b1-7737-4992-a2e9-ef20de4c5bac
02/08/2025 19:27:38:INFO:Received: evaluate message 10f723b1-7737-4992-a2e9-ef20de4c5bac

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213, 1.310995662039535, 1.264396644987697, 1.2701550575211027], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718, 0.8949018310594852, 0.898103397330748, 0.8990099129770881], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313, 0.6584166731871604, 0.6631704669167207, 0.6583154352356761], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051, 0.6380738299672268, 0.6435018680920499, 0.6458983891160318]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213, 1.310995662039535, 1.264396644987697, 1.2701550575211027, 1.2586158710144433], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542, 0.6612968183648812], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718, 0.8949018310594852, 0.898103397330748, 0.8990099129770881, 0.9004977288641947], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313, 0.6584166731871604, 0.6631704669167207, 0.6583154352356761, 0.6579979095347669], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542, 0.6612968183648812], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051, 0.6380738299672268, 0.6435018680920499, 0.6458983891160318, 0.6490721132841767]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
02/08/2025 19:31:43:INFO:Sent reply
[92mINFO [0m:      
02/08/2025 19:31:59:INFO:
[92mINFO [0m:      Received: reconnect message 991e387d-e3b1-4bb6-98ad-6b6699cc98a8
02/08/2025 19:31:59:INFO:Received: reconnect message 991e387d-e3b1-4bb6-98ad-6b6699cc98a8
02/08/2025 19:31:59:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/08/2025 19:31:59:INFO:Disconnect and shut down

{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213, 1.310995662039535, 1.264396644987697, 1.2701550575211027, 1.2586158710144433, 1.3398930698309126], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542, 0.6612968183648812, 0.6455900120821587], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718, 0.8949018310594852, 0.898103397330748, 0.8990099129770881, 0.9004977288641947, 0.9001085710167956], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313, 0.6584166731871604, 0.6631704669167207, 0.6583154352356761, 0.6579979095347669, 0.6540454901821245], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542, 0.6612968183648812, 0.6455900120821587], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051, 0.6380738299672268, 0.6435018680920499, 0.6458983891160318, 0.6490721132841767, 0.6330882930625233]}



Final client history:
{'loss': [1.4341865640660232, 1.3278700841851068, 1.3695796489523535, 1.342881513467682, 1.3157564302725606, 1.3268978310553143, 1.3512491694487634, 1.31799615891384, 1.335788213769803, 1.3390306717582254, 1.3095185295785572, 1.304992672708598, 1.3292212398408452, 1.3222316344371017, 1.3027529884767397, 1.3187947198358505, 1.2865643985464454, 1.282628777931642, 1.2753643221400026, 1.288350668301085, 1.285291568314986, 1.298678616506651, 1.296516762113936, 1.3280433489582109, 1.307151519239213, 1.310995662039535, 1.264396644987697, 1.2701550575211027, 1.2586158710144433, 1.3398930698309126], 'accuracy': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542, 0.6612968183648812, 0.6455900120821587], 'auc': [0.7952128841995612, 0.8346505194429128, 0.8468441987990782, 0.8583597319477125, 0.8617931561932564, 0.8640858292676156, 0.8667492739261751, 0.8712091508108861, 0.8736215597217655, 0.8753301910581146, 0.8787145521687414, 0.882201747474201, 0.8812664892747069, 0.883909522960755, 0.8848483268866959, 0.8856810268106924, 0.886416361504327, 0.8870566670104403, 0.8876330416517124, 0.8889823366780979, 0.8896010307279275, 0.8904494497573925, 0.8915276688145444, 0.8920644830300674, 0.8937551436187718, 0.8949018310594852, 0.898103397330748, 0.8990099129770881, 0.9004977288641947, 0.9001085710167956], 'precision': [0.5630504832476843, 0.588172573519457, 0.6085488469660614, 0.6117725102754475, 0.6055459689475594, 0.6070590404441935, 0.619290541861942, 0.6249222310080221, 0.6287182135820377, 0.6261179834108211, 0.6271574912917494, 0.6490016637707274, 0.64344157383758, 0.6413825884603398, 0.6461429199246976, 0.6371906649904521, 0.6394345387110894, 0.6427513067497419, 0.640485194964392, 0.6378509635129688, 0.651810234487161, 0.6475338196630815, 0.6421394068320244, 0.6447551062442161, 0.6496039439722313, 0.6584166731871604, 0.6631704669167207, 0.6583154352356761, 0.6579979095347669, 0.6540454901821245], 'recall': [0.5553765606121627, 0.6004832863471606, 0.6073298429319371, 0.6242448650825614, 0.6250503423278292, 0.6294804671768023, 0.6322996375352397, 0.6415626258558196, 0.6391461941200162, 0.6359242851389448, 0.6419653644784535, 0.6476037051953283, 0.6451872734595248, 0.644381796214257, 0.6463954893274265, 0.6435763189689891, 0.643979057591623, 0.6488119210632299, 0.6484091824405961, 0.6500201369311317, 0.6512283527990335, 0.6476037051953283, 0.6516310914216673, 0.6472009665726943, 0.6492146596858639, 0.6524365686669351, 0.657269432138542, 0.657269432138542, 0.6612968183648812, 0.6455900120821587], 'f1': [0.5014109976972659, 0.559282479633486, 0.5687876854052006, 0.5881840652338696, 0.5911347625336894, 0.5953735250780595, 0.6009719733892815, 0.6148915310497657, 0.6111155202136914, 0.6107630100981116, 0.6165766754508986, 0.6252281878793747, 0.6220516268312504, 0.6239670774238675, 0.6253015036018132, 0.6226410102278951, 0.6274442905717673, 0.6306460583809372, 0.6324613158643079, 0.6319901358287863, 0.6368903606532035, 0.6308434079905596, 0.6309898982818102, 0.6288858572714934, 0.6345255824333051, 0.6380738299672268, 0.6435018680920499, 0.6458983891160318, 0.6490721132841767, 0.6330882930625233]}

