nohup: ignoring input
01/27/2025 01:39:23:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 01:39:23:DEBUG:ChannelConnectivity.IDLE
01/27/2025 01:39:23:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 01:39:23:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 01:40:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:40:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 252cc121-23cd-4a1f-a92f-ca85c0fc6fbf
01/27/2025 01:40:05:INFO:Received: train message 252cc121-23cd-4a1f-a92f-ca85c0fc6fbf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:40:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:41:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:41:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e5572aca-6c15-482c-8a0e-cecb987ebf3c
01/27/2025 01:41:45:INFO:Received: evaluate message e5572aca-6c15-482c-8a0e-cecb987ebf3c
[92mINFO [0m:      Sent reply
01/27/2025 01:41:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:42:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:42:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 072b88a1-ba1c-42b8-aaf3-34c349913334
01/27/2025 01:42:54:INFO:Received: train message 072b88a1-ba1c-42b8-aaf3-34c349913334
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:43:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:44:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:44:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eef949c9-08b8-431e-baf3-4aef32b2f4e2
01/27/2025 01:44:05:INFO:Received: evaluate message eef949c9-08b8-431e-baf3-4aef32b2f4e2
[92mINFO [0m:      Sent reply
01/27/2025 01:44:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:44:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:44:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7217f2e1-8948-46c6-ba2e-28fb1688ddbf
01/27/2025 01:44:36:INFO:Received: train message 7217f2e1-8948-46c6-ba2e-28fb1688ddbf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:44:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:45:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:45:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5f2410e4-b7b3-47b2-a369-d779fccaa1fb
01/27/2025 01:45:33:INFO:Received: evaluate message 5f2410e4-b7b3-47b2-a369-d779fccaa1fb
[92mINFO [0m:      Sent reply
01/27/2025 01:45:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:46:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:46:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ff1293ca-dd52-41b9-8a06-7ce3dd2250f7
01/27/2025 01:46:16:INFO:Received: train message ff1293ca-dd52-41b9-8a06-7ce3dd2250f7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:46:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:47:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:47:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1a5cb2d3-d55e-4233-abbc-06f0053ae2c9
01/27/2025 01:47:09:INFO:Received: evaluate message 1a5cb2d3-d55e-4233-abbc-06f0053ae2c9
[92mINFO [0m:      Sent reply
01/27/2025 01:47:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:47:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:47:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 10cdf562-2cc8-4895-9ab4-9653a84774e0
01/27/2025 01:47:44:INFO:Received: train message 10cdf562-2cc8-4895-9ab4-9653a84774e0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:48:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:48:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:48:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 92479d8f-8f92-4fe7-bdf5-5b7455030ee9
01/27/2025 01:48:36:INFO:Received: evaluate message 92479d8f-8f92-4fe7-bdf5-5b7455030ee9
[92mINFO [0m:      Sent reply
01/27/2025 01:48:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:49:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:49:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 66b34d4b-4032-48f0-a4cb-70550f164d11
01/27/2025 01:49:07:INFO:Received: train message 66b34d4b-4032-48f0-a4cb-70550f164d11
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:49:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:50:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:50:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 22febc04-29ee-4202-be70-aa917cd44973
01/27/2025 01:50:11:INFO:Received: evaluate message 22febc04-29ee-4202-be70-aa917cd44973
[92mINFO [0m:      Sent reply
01/27/2025 01:50:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:50:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:50:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0dbaef7f-29c2-4fe3-b31e-8a855481c945
01/27/2025 01:50:47:INFO:Received: train message 0dbaef7f-29c2-4fe3-b31e-8a855481c945
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:51:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:51:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:51:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message be1d8816-4aca-43ed-a21b-9d48d1c70937
01/27/2025 01:51:48:INFO:Received: evaluate message be1d8816-4aca-43ed-a21b-9d48d1c70937
[92mINFO [0m:      Sent reply
01/27/2025 01:51:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:52:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:52:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 57a602a0-3f11-4e66-a5d6-9020309155f4
01/27/2025 01:52:07:INFO:Received: train message 57a602a0-3f11-4e66-a5d6-9020309155f4
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379], 'accuracy': [0.5215011727912432], 'auc': [0.7229562347935747]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828], 'accuracy': [0.5215011727912432, 0.5175918686473807], 'auc': [0.7229562347935747, 0.7423497364044646]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:52:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:53:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:53:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c1554af6-ba82-4d34-8ff7-4aa15b351cb6
01/27/2025 01:53:12:INFO:Received: evaluate message c1554af6-ba82-4d34-8ff7-4aa15b351cb6
[92mINFO [0m:      Sent reply
01/27/2025 01:53:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:53:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:53:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6ce05ac3-0a45-48b3-b509-d1d9b4b5d53f
01/27/2025 01:53:45:INFO:Received: train message 6ce05ac3-0a45-48b3-b509-d1d9b4b5d53f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:54:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:54:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:54:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4365c4b9-f597-42a0-9fb1-bbab159f16a1
01/27/2025 01:54:48:INFO:Received: evaluate message 4365c4b9-f597-42a0-9fb1-bbab159f16a1
[92mINFO [0m:      Sent reply
01/27/2025 01:54:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:55:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:55:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 173af594-6488-47e5-873e-b4dffffbcee8
01/27/2025 01:55:18:INFO:Received: train message 173af594-6488-47e5-873e-b4dffffbcee8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:55:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:56:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:56:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f2729ee1-031a-4419-ab85-c7a5dacc4b91
01/27/2025 01:56:11:INFO:Received: evaluate message f2729ee1-031a-4419-ab85-c7a5dacc4b91
[92mINFO [0m:      Sent reply
01/27/2025 01:56:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:56:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:56:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5cbe4cac-f9e1-4ff5-8132-0a1cd85d35f9
01/27/2025 01:56:38:INFO:Received: train message 5cbe4cac-f9e1-4ff5-8132-0a1cd85d35f9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:56:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:57:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:57:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 53b968ec-319c-4560-8c34-37422135d6b8
01/27/2025 01:57:32:INFO:Received: evaluate message 53b968ec-319c-4560-8c34-37422135d6b8
[92mINFO [0m:      Sent reply
01/27/2025 01:57:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:58:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:58:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4ea8d813-1466-4efa-977b-9f1832053876
01/27/2025 01:58:03:INFO:Received: train message 4ea8d813-1466-4efa-977b-9f1832053876
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:58:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:59:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:59:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b53253d2-b533-4564-b9ed-6a1d896eeddf
01/27/2025 01:59:20:INFO:Received: evaluate message b53253d2-b533-4564-b9ed-6a1d896eeddf
[92mINFO [0m:      Sent reply
01/27/2025 01:59:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:59:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:59:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 980ce3ec-34f8-4e2a-87d1-04f224773839
01/27/2025 01:59:57:INFO:Received: train message 980ce3ec-34f8-4e2a-87d1-04f224773839
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:00:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:00:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:00:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a1d6cca8-09a9-4e6d-a79b-9298b26476e6
01/27/2025 02:00:55:INFO:Received: evaluate message a1d6cca8-09a9-4e6d-a79b-9298b26476e6
[92mINFO [0m:      Sent reply
01/27/2025 02:01:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:01:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:01:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 22b46ef9-4f5e-4575-976b-b8358b4dcbb5
01/27/2025 02:01:32:INFO:Received: train message 22b46ef9-4f5e-4575-976b-b8358b4dcbb5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:01:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:02:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:02:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c49ec132-8361-4ced-8257-7764bff66176
01/27/2025 02:02:44:INFO:Received: evaluate message c49ec132-8361-4ced-8257-7764bff66176
[92mINFO [0m:      Sent reply
01/27/2025 02:02:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:03:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:03:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4616d36d-5ca5-47f0-b4ad-1c106d9600a0
01/27/2025 02:03:31:INFO:Received: train message 4616d36d-5ca5-47f0-b4ad-1c106d9600a0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:03:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:04:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:04:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a65937e1-99a9-41eb-8591-3566044382a1
01/27/2025 02:04:24:INFO:Received: evaluate message a65937e1-99a9-41eb-8591-3566044382a1
[92mINFO [0m:      Sent reply
01/27/2025 02:04:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:05:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:05:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e6949e99-ffa4-435b-a3b1-f82421bcf17b
01/27/2025 02:05:11:INFO:Received: train message e6949e99-ffa4-435b-a3b1-f82421bcf17b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:05:41:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:06:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:06:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 755d27c2-2134-4223-b775-9f615ea26e2d
01/27/2025 02:06:49:INFO:Received: evaluate message 755d27c2-2134-4223-b775-9f615ea26e2d
[92mINFO [0m:      Sent reply
01/27/2025 02:06:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:07:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:07:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 83db120b-21ac-42a9-9e7e-0b81a6e70d0e
01/27/2025 02:07:37:INFO:Received: train message 83db120b-21ac-42a9-9e7e-0b81a6e70d0e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:08:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:08:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:08:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6890a623-a62a-404d-ad1c-e56ce70d739e
01/27/2025 02:08:59:INFO:Received: evaluate message 6890a623-a62a-404d-ad1c-e56ce70d739e
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:09:04:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:09:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:09:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 24527531-0146-43c5-8041-6464e1ed133e
01/27/2025 02:09:29:INFO:Received: train message 24527531-0146-43c5-8041-6464e1ed133e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:09:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:11:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:11:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 532302a9-7605-4e45-b78f-f50709ac5302
01/27/2025 02:11:21:INFO:Received: evaluate message 532302a9-7605-4e45-b78f-f50709ac5302
[92mINFO [0m:      Sent reply
01/27/2025 02:11:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:12:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:12:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 795db776-522f-4d36-a4fb-72f18f89378f
01/27/2025 02:12:01:INFO:Received: train message 795db776-522f-4d36-a4fb-72f18f89378f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:12:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:13:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:13:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8dffff98-2d59-4104-ad5d-172aba960621
01/27/2025 02:13:05:INFO:Received: evaluate message 8dffff98-2d59-4104-ad5d-172aba960621
[92mINFO [0m:      Sent reply
01/27/2025 02:13:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:13:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:13:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3e39757e-f7d1-453a-aa98-7f62c9b916ef
01/27/2025 02:13:47:INFO:Received: train message 3e39757e-f7d1-453a-aa98-7f62c9b916ef
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:14:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:14:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:14:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e4cc1694-9383-47dc-a1a2-b570d29fc2dc
01/27/2025 02:14:58:INFO:Received: evaluate message e4cc1694-9383-47dc-a1a2-b570d29fc2dc
[92mINFO [0m:      Sent reply
01/27/2025 02:15:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:15:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:15:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cdf9117f-5206-4de2-a08c-d5f465bed2a7
01/27/2025 02:15:33:INFO:Received: train message cdf9117f-5206-4de2-a08c-d5f465bed2a7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:15:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:16:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:16:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 71759f84-0358-4ee8-be68-eb1e29f88cea
01/27/2025 02:16:32:INFO:Received: evaluate message 71759f84-0358-4ee8-be68-eb1e29f88cea

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:16:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:17:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:17:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1e7a8874-b794-4dd9-90f3-7d26deb1d2f9
01/27/2025 02:17:05:INFO:Received: train message 1e7a8874-b794-4dd9-90f3-7d26deb1d2f9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:17:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:18:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:18:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message befe217d-9858-45ec-80e8-b44ad85da33e
01/27/2025 02:18:32:INFO:Received: evaluate message befe217d-9858-45ec-80e8-b44ad85da33e
[92mINFO [0m:      Sent reply
01/27/2025 02:18:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:19:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:19:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 89625760-88cc-481d-8240-bd9aef9ee609
01/27/2025 02:19:07:INFO:Received: train message 89625760-88cc-481d-8240-bd9aef9ee609
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:19:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:20:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:20:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4ed6ad79-6232-46f5-b97e-9610d5b1b7fb
01/27/2025 02:20:16:INFO:Received: evaluate message 4ed6ad79-6232-46f5-b97e-9610d5b1b7fb
[92mINFO [0m:      Sent reply
01/27/2025 02:20:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:20:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:20:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4674c610-3f7b-4cb7-b5bc-a72cfa23b419
01/27/2025 02:20:42:INFO:Received: train message 4674c610-3f7b-4cb7-b5bc-a72cfa23b419
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:21:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:22:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:22:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 61f52c60-062f-4ede-ab6f-651e59f01604
01/27/2025 02:22:15:INFO:Received: evaluate message 61f52c60-062f-4ede-ab6f-651e59f01604
[92mINFO [0m:      Sent reply
01/27/2025 02:22:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:22:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:22:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 037b7e22-a28b-4205-8933-c9cafd2b1a4c
01/27/2025 02:22:46:INFO:Received: train message 037b7e22-a28b-4205-8933-c9cafd2b1a4c

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:23:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:23:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:23:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9c4332ff-2b20-4692-8d62-0a6396668243
01/27/2025 02:23:52:INFO:Received: evaluate message 9c4332ff-2b20-4692-8d62-0a6396668243
[92mINFO [0m:      Sent reply
01/27/2025 02:23:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:24:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:24:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 365eea68-3b64-4715-b216-0afa13d489da
01/27/2025 02:24:31:INFO:Received: train message 365eea68-3b64-4715-b216-0afa13d489da
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:24:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:25:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:25:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eecdfe20-71dc-41b6-8c8d-2d70de58b7ae
01/27/2025 02:25:24:INFO:Received: evaluate message eecdfe20-71dc-41b6-8c8d-2d70de58b7ae
[92mINFO [0m:      Sent reply
01/27/2025 02:25:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:25:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:25:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6ee0980f-a63c-4ef9-8220-e3ef56cac61c
01/27/2025 02:25:49:INFO:Received: train message 6ee0980f-a63c-4ef9-8220-e3ef56cac61c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:26:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:27:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:27:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 172b24e3-67aa-4138-822a-ef926ca64b2f
01/27/2025 02:27:18:INFO:Received: evaluate message 172b24e3-67aa-4138-822a-ef926ca64b2f
[92mINFO [0m:      Sent reply
01/27/2025 02:27:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:27:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:27:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8abba90f-58ee-4536-887d-d0ed3c273db6
01/27/2025 02:27:36:INFO:Received: train message 8abba90f-58ee-4536-887d-d0ed3c273db6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:27:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:28:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:28:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a507b0cd-da8d-4818-989f-e6a39468475b
01/27/2025 02:28:54:INFO:Received: evaluate message a507b0cd-da8d-4818-989f-e6a39468475b
0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:28:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:29:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:29:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 709c6f01-10b9-4e01-9508-aabf58d11a5f
01/27/2025 02:29:41:INFO:Received: train message 709c6f01-10b9-4e01-9508-aabf58d11a5f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:30:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:30:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:30:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c6c54ce3-65cb-4a56-acc3-a912b5aca280
01/27/2025 02:30:49:INFO:Received: evaluate message c6c54ce3-65cb-4a56-acc3-a912b5aca280
[92mINFO [0m:      Sent reply
01/27/2025 02:30:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:31:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:31:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 37f3c5ca-5056-49b0-8e5b-896628cf29e3
01/27/2025 02:31:07:INFO:Received: train message 37f3c5ca-5056-49b0-8e5b-896628cf29e3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:31:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:32:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:32:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2540bab2-4f69-43a9-aa1b-dded5dfbc063
01/27/2025 02:32:22:INFO:Received: evaluate message 2540bab2-4f69-43a9-aa1b-dded5dfbc063
[92mINFO [0m:      Sent reply
01/27/2025 02:32:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:32:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:32:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 2957b36c-4256-4492-9719-347caba62586
01/27/2025 02:32:40:INFO:Received: reconnect message 2957b36c-4256-4492-9719-347caba62586
01/27/2025 02:32:41:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 02:32:41:INFO:Disconnect and shut down

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6793212890625
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.024755576625466347, 0.0029962826520204544, 0.006347412709146738, 0.01864243671298027]
Noise Multiplier after list and tensor:  0.013185427174903452
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648, 1.0215496112351496], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663, 0.5934323690383112], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105, 0.8078225087894171]}



Final client history:
{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648, 1.0215496112351496], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663, 0.5934323690383112], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105, 0.8078225087894171]}

