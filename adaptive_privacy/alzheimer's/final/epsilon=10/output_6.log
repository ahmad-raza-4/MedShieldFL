nohup: ignoring input
01/27/2025 01:39:22:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 01:39:22:DEBUG:ChannelConnectivity.IDLE
01/27/2025 01:39:22:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 01:39:22:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 01:39:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:39:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7e697106-6947-48ca-ab38-56b5c9fdde55
01/27/2025 01:39:59:INFO:Received: train message 7e697106-6947-48ca-ab38-56b5c9fdde55
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:40:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:41:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:41:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cc9ed40b-6d28-43c7-b3e3-65bfe15d42d9
01/27/2025 01:41:52:INFO:Received: evaluate message cc9ed40b-6d28-43c7-b3e3-65bfe15d42d9
[92mINFO [0m:      Sent reply
01/27/2025 01:41:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:42:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:42:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2741f6f0-80ff-456e-a46d-9413547eb335
01/27/2025 01:42:53:INFO:Received: train message 2741f6f0-80ff-456e-a46d-9413547eb335
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:43:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:43:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:43:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 77f9601c-22de-4614-8868-dd1bc5f3da4e
01/27/2025 01:43:56:INFO:Received: evaluate message 77f9601c-22de-4614-8868-dd1bc5f3da4e
[92mINFO [0m:      Sent reply
01/27/2025 01:43:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:44:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:44:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f8a1045e-6f29-4dc2-a0c9-0675f4f7726e
01/27/2025 01:44:42:INFO:Received: train message f8a1045e-6f29-4dc2-a0c9-0675f4f7726e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:45:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:45:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:45:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2735c063-12e6-4c8c-b2dc-6d2aec1e0d31
01/27/2025 01:45:41:INFO:Received: evaluate message 2735c063-12e6-4c8c-b2dc-6d2aec1e0d31
[92mINFO [0m:      Sent reply
01/27/2025 01:45:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:46:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:46:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2b8175e2-fc8d-484a-8cca-78896f939689
01/27/2025 01:46:06:INFO:Received: train message 2b8175e2-fc8d-484a-8cca-78896f939689
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:46:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:47:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:47:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 93f69af9-c596-4878-815a-be134bdad2c2
01/27/2025 01:47:00:INFO:Received: evaluate message 93f69af9-c596-4878-815a-be134bdad2c2
[92mINFO [0m:      Sent reply
01/27/2025 01:47:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:47:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:47:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d955c290-aa60-427a-848a-a1fed3c4b15c
01/27/2025 01:47:30:INFO:Received: train message d955c290-aa60-427a-848a-a1fed3c4b15c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:47:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:48:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:48:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5a4c9e9-e006-4fa4-9c7b-1c67b6bce9dc
01/27/2025 01:48:40:INFO:Received: evaluate message b5a4c9e9-e006-4fa4-9c7b-1c67b6bce9dc
[92mINFO [0m:      Sent reply
01/27/2025 01:48:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:49:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:49:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0f6d2003-9a93-4fa2-8a96-c722c4653874
01/27/2025 01:49:09:INFO:Received: train message 0f6d2003-9a93-4fa2-8a96-c722c4653874
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:49:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:49:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:49:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3490e7dd-c05e-4c13-a725-dbda41f1f3ce
01/27/2025 01:49:58:INFO:Received: evaluate message 3490e7dd-c05e-4c13-a725-dbda41f1f3ce
[92mINFO [0m:      Sent reply
01/27/2025 01:49:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:50:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:50:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message db637f0e-9d59-4224-8051-9d35ec617f00
01/27/2025 01:50:41:INFO:Received: train message db637f0e-9d59-4224-8051-9d35ec617f00
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:51:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:51:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:51:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 47edcd77-efc8-4250-95ea-60ec83bd9df2
01/27/2025 01:51:40:INFO:Received: evaluate message 47edcd77-efc8-4250-95ea-60ec83bd9df2
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379], 'accuracy': [0.5215011727912432], 'auc': [0.7229562347935747]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828], 'accuracy': [0.5215011727912432, 0.5175918686473807], 'auc': [0.7229562347935747, 0.7423497364044646]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:51:41:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:52:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:52:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dad90b86-80ab-4d98-8498-89eb216148d7
01/27/2025 01:52:20:INFO:Received: train message dad90b86-80ab-4d98-8498-89eb216148d7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:52:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:53:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:53:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f161e748-4b06-43e4-be5a-edaec32de1dc
01/27/2025 01:53:17:INFO:Received: evaluate message f161e748-4b06-43e4-be5a-edaec32de1dc
[92mINFO [0m:      Sent reply
01/27/2025 01:53:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:53:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:53:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 38531606-b043-4355-82c4-ea3a269ff02d
01/27/2025 01:53:50:INFO:Received: train message 38531606-b043-4355-82c4-ea3a269ff02d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:54:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:54:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:54:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3e6b1fd5-aa71-46f9-bde9-26581c03f834
01/27/2025 01:54:36:INFO:Received: evaluate message 3e6b1fd5-aa71-46f9-bde9-26581c03f834
[92mINFO [0m:      Sent reply
01/27/2025 01:54:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:55:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:55:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cba16af1-3324-4d2d-92ee-b57e1516d0a1
01/27/2025 01:55:12:INFO:Received: train message cba16af1-3324-4d2d-92ee-b57e1516d0a1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:55:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:56:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:56:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0b57a356-541f-49a7-b06d-cd5907757e72
01/27/2025 01:56:19:INFO:Received: evaluate message 0b57a356-541f-49a7-b06d-cd5907757e72
[92mINFO [0m:      Sent reply
01/27/2025 01:56:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:56:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:56:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5693d793-282d-41de-8f74-f88a5f63b1fd
01/27/2025 01:56:37:INFO:Received: train message 5693d793-282d-41de-8f74-f88a5f63b1fd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:56:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:57:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:57:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 23d1cf1c-2a12-447f-84b0-a8f27f1f76f8
01/27/2025 01:57:47:INFO:Received: evaluate message 23d1cf1c-2a12-447f-84b0-a8f27f1f76f8
[92mINFO [0m:      Sent reply
01/27/2025 01:57:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:58:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:58:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6bd84f0f-1b04-4554-8da7-04950b73cf5f
01/27/2025 01:58:12:INFO:Received: train message 6bd84f0f-1b04-4554-8da7-04950b73cf5f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:58:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:59:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:59:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3d22cb47-93e7-4e07-a0bd-c11bbb9af5ca
01/27/2025 01:59:21:INFO:Received: evaluate message 3d22cb47-93e7-4e07-a0bd-c11bbb9af5ca
[92mINFO [0m:      Sent reply
01/27/2025 01:59:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:59:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:59:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d0b091a8-1dfb-474c-99f5-a4c2e9e0ea8a
01/27/2025 01:59:43:INFO:Received: train message d0b091a8-1dfb-474c-99f5-a4c2e9e0ea8a

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:00:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:00:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:00:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 15d22a81-7d63-46b0-a5c4-f633816bbceb
01/27/2025 02:00:50:INFO:Received: evaluate message 15d22a81-7d63-46b0-a5c4-f633816bbceb
[92mINFO [0m:      Sent reply
01/27/2025 02:00:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:01:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:01:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 13eb26fb-cc21-4804-87d8-679206090fec
01/27/2025 02:01:38:INFO:Received: train message 13eb26fb-cc21-4804-87d8-679206090fec
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:02:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:02:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:02:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 329470d4-792a-4439-8fac-cde244ac815d
01/27/2025 02:02:46:INFO:Received: evaluate message 329470d4-792a-4439-8fac-cde244ac815d
[92mINFO [0m:      Sent reply
01/27/2025 02:02:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:03:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:03:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ce23880a-6d8b-4105-87c8-a1ed4dc3012d
01/27/2025 02:03:12:INFO:Received: train message ce23880a-6d8b-4105-87c8-a1ed4dc3012d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:03:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:04:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:04:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4322bfce-1021-4a9b-a1de-650a69e197cf
01/27/2025 02:04:34:INFO:Received: evaluate message 4322bfce-1021-4a9b-a1de-650a69e197cf
[92mINFO [0m:      Sent reply
01/27/2025 02:04:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:05:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:05:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9513ad6b-e6bc-4c2d-a2f1-953209ecfc98
01/27/2025 02:05:00:INFO:Received: train message 9513ad6b-e6bc-4c2d-a2f1-953209ecfc98
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:05:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:06:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:06:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b9d7a864-5e8b-4ab9-b9e9-8a17fb15764e
01/27/2025 02:06:42:INFO:Received: evaluate message b9d7a864-5e8b-4ab9-b9e9-8a17fb15764e
[92mINFO [0m:      Sent reply
01/27/2025 02:06:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:07:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:07:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 811fec09-c289-444c-a2d7-2da560f1c6bc
01/27/2025 02:07:14:INFO:Received: train message 811fec09-c289-444c-a2d7-2da560f1c6bc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:07:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:08:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:08:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fcb9825e-4641-4597-8083-eda073c7830e
01/27/2025 02:08:57:INFO:Received: evaluate message fcb9825e-4641-4597-8083-eda073c7830e
[0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:09:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:09:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:09:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 930a68da-59a2-49bf-b686-d75e3d22b08f
01/27/2025 02:09:29:INFO:Received: train message 930a68da-59a2-49bf-b686-d75e3d22b08f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:09:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:11:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:11:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 520748f8-5a9c-49ef-a157-29c03573e14f
01/27/2025 02:11:15:INFO:Received: evaluate message 520748f8-5a9c-49ef-a157-29c03573e14f
[92mINFO [0m:      Sent reply
01/27/2025 02:11:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:12:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:12:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2b47f965-f86b-4869-9082-93cbd4afc30e
01/27/2025 02:12:05:INFO:Received: train message 2b47f965-f86b-4869-9082-93cbd4afc30e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:12:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:13:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:13:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e1642c0f-b02b-4f1a-8116-7b74b4ccf001
01/27/2025 02:13:12:INFO:Received: evaluate message e1642c0f-b02b-4f1a-8116-7b74b4ccf001
[92mINFO [0m:      Sent reply
01/27/2025 02:13:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:13:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:13:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1b8165a6-f115-4131-a685-7a3e380f3cff
01/27/2025 02:13:47:INFO:Received: train message 1b8165a6-f115-4131-a685-7a3e380f3cff
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:14:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:14:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:14:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 71d2ab1b-4261-44f2-97c1-b13e50c6b956
01/27/2025 02:14:58:INFO:Received: evaluate message 71d2ab1b-4261-44f2-97c1-b13e50c6b956
[92mINFO [0m:      Sent reply
01/27/2025 02:15:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:15:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:15:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6abcba28-1c5d-47b2-b9e6-b681be8c4b42
01/27/2025 02:15:37:INFO:Received: train message 6abcba28-1c5d-47b2-b9e6-b681be8c4b42
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:16:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:16:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:16:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4fc38998-92a0-454d-8f76-1b68512577b6
01/27/2025 02:16:35:INFO:Received: evaluate message 4fc38998-92a0-454d-8f76-1b68512577b6

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:16:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:17:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:17:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1eb6150d-71a1-47e1-badc-ea4e322deacf
01/27/2025 02:17:02:INFO:Received: train message 1eb6150d-71a1-47e1-badc-ea4e322deacf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:17:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:18:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:18:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 238029a9-1741-4f2f-a1d4-a92c8e4d7829
01/27/2025 02:18:16:INFO:Received: evaluate message 238029a9-1741-4f2f-a1d4-a92c8e4d7829
[92mINFO [0m:      Sent reply
01/27/2025 02:18:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:18:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:18:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6a3b0baf-f625-4294-8c23-97d7765ab66e
01/27/2025 02:18:49:INFO:Received: train message 6a3b0baf-f625-4294-8c23-97d7765ab66e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:19:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:20:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:20:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8e61b5c4-0d04-434e-bbe5-a6af31e6d089
01/27/2025 02:20:20:INFO:Received: evaluate message 8e61b5c4-0d04-434e-bbe5-a6af31e6d089
[92mINFO [0m:      Sent reply
01/27/2025 02:20:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:20:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:20:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 98354eef-bf51-48f8-bf7c-c375b7874e32
01/27/2025 02:20:54:INFO:Received: train message 98354eef-bf51-48f8-bf7c-c375b7874e32
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:21:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:22:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:22:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1f06c701-be7b-4263-95ca-7f880b1a5152
01/27/2025 02:22:02:INFO:Received: evaluate message 1f06c701-be7b-4263-95ca-7f880b1a5152
[92mINFO [0m:      Sent reply
01/27/2025 02:22:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:22:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:22:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a337afa9-6ed1-45ec-9ab0-b4c0152841a5
01/27/2025 02:22:30:INFO:Received: train message a337afa9-6ed1-45ec-9ab0-b4c0152841a5

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:22:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:23:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:23:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 41714508-d052-4665-b9e6-1f0e10f6e1ba
01/27/2025 02:23:57:INFO:Received: evaluate message 41714508-d052-4665-b9e6-1f0e10f6e1ba
[92mINFO [0m:      Sent reply
01/27/2025 02:24:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:24:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:24:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ab83f7ce-5579-4776-9c80-ac4c3c698662
01/27/2025 02:24:27:INFO:Received: train message ab83f7ce-5579-4776-9c80-ac4c3c698662
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:24:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:25:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:25:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 081f61c2-099f-456d-aef3-10371e9a3cd8
01/27/2025 02:25:34:INFO:Received: evaluate message 081f61c2-099f-456d-aef3-10371e9a3cd8
[92mINFO [0m:      Sent reply
01/27/2025 02:25:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:26:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:26:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9a54bce9-7961-4259-bec3-50832d588ba2
01/27/2025 02:26:02:INFO:Received: train message 9a54bce9-7961-4259-bec3-50832d588ba2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:26:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:27:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:27:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4010cf19-55d0-44ea-a2a6-e9c675ea1d96
01/27/2025 02:27:18:INFO:Received: evaluate message 4010cf19-55d0-44ea-a2a6-e9c675ea1d96
[92mINFO [0m:      Sent reply
01/27/2025 02:27:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:27:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:27:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f5ec47af-3f24-430d-9dd3-747ec0fd5983
01/27/2025 02:27:36:INFO:Received: train message f5ec47af-3f24-430d-9dd3-747ec0fd5983
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:28:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:29:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:29:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fb807aa9-1263-4ae5-84c1-709574897167
01/27/2025 02:29:02:INFO:Received: evaluate message fb807aa9-1263-4ae5-84c1-709574897167
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 02:29:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:29:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:29:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c84501ed-af82-444a-bfa7-55771776c3e5
01/27/2025 02:29:27:INFO:Received: train message c84501ed-af82-444a-bfa7-55771776c3e5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:29:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:30:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:30:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 951b0222-b1f0-4d37-8544-861407aad56d
01/27/2025 02:30:45:INFO:Received: evaluate message 951b0222-b1f0-4d37-8544-861407aad56d
[92mINFO [0m:      Sent reply
01/27/2025 02:30:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:31:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:31:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c4df3768-27aa-4611-aba2-41692c726130
01/27/2025 02:31:21:INFO:Received: train message c4df3768-27aa-4611-aba2-41692c726130
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 02:31:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:32:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:32:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ae426c68-53b8-41d2-b5b4-feb9d8e7c355
01/27/2025 02:32:16:INFO:Received: evaluate message ae426c68-53b8-41d2-b5b4-feb9d8e7c355
[92mINFO [0m:      Sent reply
01/27/2025 02:32:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 02:32:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 02:32:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 862dc77f-92e2-4875-8649-47bdf6782ebd
01/27/2025 02:32:40:INFO:Received: reconnect message 862dc77f-92e2-4875-8649-47bdf6782ebd
01/27/2025 02:32:40:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 02:32:40:INFO:Disconnect and shut down

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.64727783203125
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0046102311462163925, 0.003876424627378583, 0.025301197543740273, 0.008865321055054665]
Noise Multiplier after list and tensor:  0.010663293593097478
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648, 1.0215496112351496], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663, 0.5934323690383112], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105, 0.8078225087894171]}



Final client history:
{'loss': [1.0710602096042379, 1.1517627028583828, 1.1502073602437786, 1.1004207033119322, 1.0772061186716648, 1.089008860703648, 1.0922733100249116, 1.088404753881101, 1.0892014965776171, 1.0883745094870476, 1.0229989906322965, 1.0652121436418827, 1.050048516726848, 1.0476347483444064, 1.0585798722305177, 1.0523276074590675, 1.0802337378612992, 1.0391119183554511, 1.055327892070454, 1.1200656794401889, 1.0540096241575931, 1.0522681916477719, 1.0575353733443766, 1.0563659273245263, 1.0124112074966072, 1.1022388778504588, 1.0422539782766442, 1.0229164813066294, 1.0136859894171648, 1.0215496112351496], 'accuracy': [0.5215011727912432, 0.5175918686473807, 0.5301016419077405, 0.5379202501954652, 0.5512118842845973, 0.5574667709147771, 0.5551211884284597, 0.5551211884284597, 0.562157935887412, 0.5543393275996873, 0.565285379202502, 0.5699765441751369, 0.5715402658326818, 0.5699765441751369, 0.5754495699765442, 0.5801407349491791, 0.5699765441751369, 0.5809225957779516, 0.5793588741204065, 0.5684128225175918, 0.5793588741204065, 0.5832681782642689, 0.581704456606724, 0.5871774824081314, 0.584831899921814, 0.5746677091477717, 0.5910867865519938, 0.5879593432369038, 0.5918686473807663, 0.5934323690383112], 'auc': [0.7229562347935747, 0.7423497364044646, 0.7524233594443471, 0.762121538800498, 0.7678654974851263, 0.7722122317578171, 0.7754424684442258, 0.7782690833909157, 0.7799773214605437, 0.783185476895956, 0.7871426834466828, 0.7887446048571481, 0.789714094055717, 0.789841007151616, 0.79099729777943, 0.7924627734705818, 0.793968087054631, 0.7955546660971148, 0.7951930746979026, 0.7957738263558304, 0.799082695934118, 0.799233048624971, 0.7997581157625151, 0.8003079842428056, 0.8034365061607835, 0.8029711108515283, 0.8054879333618542, 0.8064298303646438, 0.805661688863105, 0.8078225087894171]}

