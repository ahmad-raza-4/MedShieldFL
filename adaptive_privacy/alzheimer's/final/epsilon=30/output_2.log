nohup: ignoring input
01/26/2025 22:40:10:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/26/2025 22:40:10:DEBUG:ChannelConnectivity.IDLE
01/26/2025 22:40:10:DEBUG:ChannelConnectivity.CONNECTING
01/26/2025 22:40:10:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/26/2025 22:40:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:40:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f9c1e85d-4f84-4140-98cb-a82fbc4ff313
01/26/2025 22:40:33:INFO:Received: train message f9c1e85d-4f84-4140-98cb-a82fbc4ff313
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:40:58:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:42:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:42:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c766ed2f-9aaa-459e-a5ec-41c42859e994
01/26/2025 22:42:08:INFO:Received: evaluate message c766ed2f-9aaa-459e-a5ec-41c42859e994
[92mINFO [0m:      Sent reply
01/26/2025 22:42:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:42:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:42:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0ede591e-3d40-4a18-a5f0-3997695a3e82
01/26/2025 22:42:43:INFO:Received: train message 0ede591e-3d40-4a18-a5f0-3997695a3e82
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:43:11:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:44:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:44:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 669a747a-cc03-4c60-9c6c-4f008a4c6d9d
01/26/2025 22:44:03:INFO:Received: evaluate message 669a747a-cc03-4c60-9c6c-4f008a4c6d9d
[92mINFO [0m:      Sent reply
01/26/2025 22:44:07:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:44:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:44:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7390575f-01e3-4d08-9bab-3a225dc4082e
01/26/2025 22:44:28:INFO:Received: train message 7390575f-01e3-4d08-9bab-3a225dc4082e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:44:54:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:45:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:45:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7740866d-c6ad-4de2-bb5b-2a008822d304
01/26/2025 22:45:56:INFO:Received: evaluate message 7740866d-c6ad-4de2-bb5b-2a008822d304
[92mINFO [0m:      Sent reply
01/26/2025 22:46:06:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:46:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:46:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c1bdff9a-f4a1-484b-84e2-3a1089514138
01/26/2025 22:46:47:INFO:Received: train message c1bdff9a-f4a1-484b-84e2-3a1089514138
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:47:17:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:47:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:47:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 53cdcd7e-6114-4398-8386-996a009a753b
01/26/2025 22:47:52:INFO:Received: evaluate message 53cdcd7e-6114-4398-8386-996a009a753b
[92mINFO [0m:      Sent reply
01/26/2025 22:47:56:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:48:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:48:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 32cf8cdb-3070-4688-ba9e-779052eb2bd0
01/26/2025 22:48:50:INFO:Received: train message 32cf8cdb-3070-4688-ba9e-779052eb2bd0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:49:20:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:50:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:50:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1fb53ff0-0c0d-4e8e-b905-12c6a39dd4f4
01/26/2025 22:50:07:INFO:Received: evaluate message 1fb53ff0-0c0d-4e8e-b905-12c6a39dd4f4
[92mINFO [0m:      Sent reply
01/26/2025 22:50:09:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:50:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:50:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 057f285e-b24c-4192-a157-5e9add0ce251
01/26/2025 22:50:42:INFO:Received: train message 057f285e-b24c-4192-a157-5e9add0ce251
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:51:13:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:52:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:52:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 550a2a9a-c74b-4346-8ec2-2a2bbec39372
01/26/2025 22:52:12:INFO:Received: evaluate message 550a2a9a-c74b-4346-8ec2-2a2bbec39372
[92mINFO [0m:      Sent reply
01/26/2025 22:52:18:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:52:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:52:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 15ae385e-6384-4919-9374-e75a4f620c97
01/26/2025 22:52:37:INFO:Received: train message 15ae385e-6384-4919-9374-e75a4f620c97
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:53:08:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:54:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:54:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 037da12a-3eda-4c29-b282-07123cf83850
01/26/2025 22:54:25:INFO:Received: evaluate message 037da12a-3eda-4c29-b282-07123cf83850
[92mINFO [0m:      Sent reply
01/26/2025 22:54:30:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:54:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:54:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message db1a882f-9217-44ea-aa6c-534aed0e8eeb
01/26/2025 22:54:58:INFO:Received: train message db1a882f-9217-44ea-aa6c-534aed0e8eeb
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30, target_epsilon: 30, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544], 'accuracy': [0.5199374511336982], 'auc': [0.7254494785945842]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378], 'accuracy': [0.5199374511336982, 0.5238467552775606], 'auc': [0.7254494785945842, 0.7453794430136471]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653]}

/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:55:30:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:56:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:56:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 48e57e6b-2492-4c1f-88c2-b85e3100a767
01/26/2025 22:56:39:INFO:Received: evaluate message 48e57e6b-2492-4c1f-88c2-b85e3100a767
[92mINFO [0m:      Sent reply
01/26/2025 22:56:42:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:57:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:57:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b943a098-8716-4f85-9f37-a292e278729b
01/26/2025 22:57:21:INFO:Received: train message b943a098-8716-4f85-9f37-a292e278729b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:57:56:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:58:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:58:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9e8c143a-7c3e-427c-ac5a-dc70ae719b23
01/26/2025 22:58:28:INFO:Received: evaluate message 9e8c143a-7c3e-427c-ac5a-dc70ae719b23
[92mINFO [0m:      Sent reply
01/26/2025 22:58:33:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:59:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:59:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5dd4582f-3a86-4a4e-82b0-da7e5b5da9c2
01/26/2025 22:59:18:INFO:Received: train message 5dd4582f-3a86-4a4e-82b0-da7e5b5da9c2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:59:48:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:01:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:01:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 460db1ab-45f0-4540-a0d0-36c76e10935b
01/26/2025 23:01:00:INFO:Received: evaluate message 460db1ab-45f0-4540-a0d0-36c76e10935b
[92mINFO [0m:      Sent reply
01/26/2025 23:01:04:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:01:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:01:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f24610d9-cdf5-46e0-a285-2aced07117d0
01/26/2025 23:01:49:INFO:Received: train message f24610d9-cdf5-46e0-a285-2aced07117d0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:02:20:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:03:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:03:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 14063542-1001-4734-97a7-e1c3a035a92f
01/26/2025 23:03:09:INFO:Received: evaluate message 14063542-1001-4734-97a7-e1c3a035a92f
[92mINFO [0m:      Sent reply
01/26/2025 23:03:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:03:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:03:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a9ecca47-4b84-4f57-b348-c2db7a9f671d
01/26/2025 23:03:57:INFO:Received: train message a9ecca47-4b84-4f57-b348-c2db7a9f671d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:04:28:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:05:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:05:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e7f55385-9144-4190-be30-7b97c1ff5d44
01/26/2025 23:05:17:INFO:Received: evaluate message e7f55385-9144-4190-be30-7b97c1ff5d44
[92mINFO [0m:      Sent reply
01/26/2025 23:05:22:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:05:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:05:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4a679121-da55-4230-b13a-4a300971cf05
01/26/2025 23:05:49:INFO:Received: train message 4a679121-da55-4230-b13a-4a300971cf05
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:06:22:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:07:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:07:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4a4a1f2d-a425-49f7-be5b-c71041109578
01/26/2025 23:07:14:INFO:Received: evaluate message 4a4a1f2d-a425-49f7-be5b-c71041109578
[92mINFO [0m:      Sent reply
01/26/2025 23:07:17:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:07:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:07:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 626651a9-7270-40f5-a7e7-2f0c85c44d1c
01/26/2025 23:07:34:INFO:Received: train message 626651a9-7270-40f5-a7e7-2f0c85c44d1c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:08:03:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:09:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:09:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 250fd030-d51c-48c8-9d65-ef990ee78e82
01/26/2025 23:09:12:INFO:Received: evaluate message 250fd030-d51c-48c8-9d65-ef990ee78e82
[92mINFO [0m:      Sent reply
01/26/2025 23:09:17:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:09:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:09:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a40f827a-d0c4-4d99-9fa3-a33774706dfd
01/26/2025 23:09:28:INFO:Received: train message a40f827a-d0c4-4d99-9fa3-a33774706dfd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:09:51:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:11:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:11:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 10928a67-3e27-4673-850a-f9da079789d3
01/26/2025 23:11:04:INFO:Received: evaluate message 10928a67-3e27-4673-850a-f9da079789d3
[92mINFO [0m:      Sent reply
01/26/2025 23:11:09:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:11:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:11:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 69cbb17d-7755-4ad0-b8cf-8cae279f2001
01/26/2025 23:11:37:INFO:Received: train message 69cbb17d-7755-4ad0-b8cf-8cae279f2001
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:12:11:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:13:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:13:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 071a7ae4-2f71-481d-92d7-1af6f97d033d
01/26/2025 23:13:18:INFO:Received: evaluate message 071a7ae4-2f71-481d-92d7-1af6f97d033d
[92mINFO [0m:      Sent reply
01/26/2025 23:13:23:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:13:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:13:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 81257a44-8925-4f10-8b6f-0a273bff7b61
01/26/2025 23:13:49:INFO:Received: train message 81257a44-8925-4f10-8b6f-0a273bff7b61
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:14:18:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:15:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:15:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 07bfbb23-6366-44ba-aced-94229cb2aedf
01/26/2025 23:15:21:INFO:Received: evaluate message 07bfbb23-6366-44ba-aced-94229cb2aedf
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:15:24:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:16:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:16:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 74f49612-60cf-49cb-91a1-52ae9d16e871
01/26/2025 23:16:20:INFO:Received: train message 74f49612-60cf-49cb-91a1-52ae9d16e871
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:16:59:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:18:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:18:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 192ce71f-26cb-46d4-83fe-f2aa2a24d969
01/26/2025 23:18:00:INFO:Received: evaluate message 192ce71f-26cb-46d4-83fe-f2aa2a24d969
[92mINFO [0m:      Sent reply
01/26/2025 23:18:03:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:18:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:18:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 41bc9282-0f92-4ccb-9894-a54c815dc7d0
01/26/2025 23:18:52:INFO:Received: train message 41bc9282-0f92-4ccb-9894-a54c815dc7d0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:19:20:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:20:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:20:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f268fa43-791b-4fe4-81ac-71acbaa665b0
01/26/2025 23:20:27:INFO:Received: evaluate message f268fa43-791b-4fe4-81ac-71acbaa665b0
[92mINFO [0m:      Sent reply
01/26/2025 23:20:32:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:20:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:20:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3b901c5a-8c75-42bd-9dd8-e2a304c42c19
01/26/2025 23:20:53:INFO:Received: train message 3b901c5a-8c75-42bd-9dd8-e2a304c42c19
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:21:21:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:22:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:22:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f79da35d-bf5e-43ad-a1df-404a8592779f
01/26/2025 23:22:47:INFO:Received: evaluate message f79da35d-bf5e-43ad-a1df-404a8592779f
[92mINFO [0m:      Sent reply
01/26/2025 23:22:53:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:23:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:23:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 95a8d5fe-e0a4-421f-a5c7-8ab3dd535af4
01/26/2025 23:23:46:INFO:Received: train message 95a8d5fe-e0a4-421f-a5c7-8ab3dd535af4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:24:21:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:25:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:25:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1db61481-1e63-4064-9fa6-02b29f80366f
01/26/2025 23:25:12:INFO:Received: evaluate message 1db61481-1e63-4064-9fa6-02b29f80366f

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:25:17:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:25:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:25:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 80ff3bc4-c5d3-45b9-b7ff-5d77c66e2db4
01/26/2025 23:25:51:INFO:Received: train message 80ff3bc4-c5d3-45b9-b7ff-5d77c66e2db4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:26:21:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:26:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:26:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b671ef28-3053-49d6-9d98-3277f0823d09
01/26/2025 23:26:53:INFO:Received: evaluate message b671ef28-3053-49d6-9d98-3277f0823d09
[92mINFO [0m:      Sent reply
01/26/2025 23:26:57:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:27:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:27:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1a9b0c16-4252-4fe3-a263-f481078fb8f6
01/26/2025 23:27:38:INFO:Received: train message 1a9b0c16-4252-4fe3-a263-f481078fb8f6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:28:06:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:28:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:28:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f099a30c-68cb-41c7-a43e-78ef6273b9ff
01/26/2025 23:28:40:INFO:Received: evaluate message f099a30c-68cb-41c7-a43e-78ef6273b9ff
[92mINFO [0m:      Sent reply
01/26/2025 23:28:44:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:29:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:29:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c892c476-b9ea-4744-bd47-e1208f461d61
01/26/2025 23:29:29:INFO:Received: train message c892c476-b9ea-4744-bd47-e1208f461d61
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:29:57:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:30:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:30:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1bf032d4-a299-468a-abbc-d4bccac3cf0f
01/26/2025 23:30:37:INFO:Received: evaluate message 1bf032d4-a299-468a-abbc-d4bccac3cf0f
[92mINFO [0m:      Sent reply
01/26/2025 23:30:41:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:31:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:31:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c8a096b1-72d8-4a26-a85b-80275a489e81
01/26/2025 23:31:23:INFO:Received: train message c8a096b1-72d8-4a26-a85b-80275a489e81

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:31:53:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:32:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:32:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6ad048fe-3117-481f-a900-4b87f452b132
01/26/2025 23:32:21:INFO:Received: evaluate message 6ad048fe-3117-481f-a900-4b87f452b132
[92mINFO [0m:      Sent reply
01/26/2025 23:32:25:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:33:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:33:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3bef9538-1e1d-4242-b2fc-bbbac50152e1
01/26/2025 23:33:07:INFO:Received: train message 3bef9538-1e1d-4242-b2fc-bbbac50152e1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:33:44:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:34:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:34:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5616e77b-cb0a-4a0d-874e-323d59532379
01/26/2025 23:34:26:INFO:Received: evaluate message 5616e77b-cb0a-4a0d-874e-323d59532379
[92mINFO [0m:      Sent reply
01/26/2025 23:34:31:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:35:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:35:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 36170a6a-2b5c-4938-8c1d-6ea83cef3350
01/26/2025 23:35:16:INFO:Received: train message 36170a6a-2b5c-4938-8c1d-6ea83cef3350
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:35:45:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:36:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:36:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cb5bd76e-8e84-437f-8b16-65a629a763b8
01/26/2025 23:36:24:INFO:Received: evaluate message cb5bd76e-8e84-437f-8b16-65a629a763b8
[92mINFO [0m:      Sent reply
01/26/2025 23:36:28:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:36:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:36:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 32607fcc-e651-4b0f-bf66-c2a48d9a164f
01/26/2025 23:36:53:INFO:Received: train message 32607fcc-e651-4b0f-bf66-c2a48d9a164f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:37:24:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:38:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:38:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 37ce1618-9cc3-4e97-bb4f-9086808b3c71
01/26/2025 23:38:15:INFO:Received: evaluate message 37ce1618-9cc3-4e97-bb4f-9086808b3c71
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:38:20:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:38:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:38:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eb194a98-39fc-4034-831e-d9d38718219d
01/26/2025 23:38:50:INFO:Received: train message eb194a98-39fc-4034-831e-d9d38718219d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:39:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:40:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:40:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1fd8bedd-3972-4046-84d4-6fbc05c01431
01/26/2025 23:40:12:INFO:Received: evaluate message 1fd8bedd-3972-4046-84d4-6fbc05c01431
[92mINFO [0m:      Sent reply
01/26/2025 23:40:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:41:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:41:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 04dc5a66-9fdf-455b-a5a1-8bdfb1c53ac6
01/26/2025 23:41:02:INFO:Received: train message 04dc5a66-9fdf-455b-a5a1-8bdfb1c53ac6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:41:36:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:42:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:42:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c53eca5b-3af2-40d9-b1cf-bfe30c0177d9
01/26/2025 23:42:22:INFO:Received: evaluate message c53eca5b-3af2-40d9-b1cf-bfe30c0177d9
[92mINFO [0m:      Sent reply
01/26/2025 23:42:26:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:42:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:42:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 8c63aef4-dc7d-42d0-9c33-a7a1013ca265
01/26/2025 23:42:42:INFO:Received: reconnect message 8c63aef4-dc7d-42d0-9c33-a7a1013ca265
01/26/2025 23:42:42:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/26/2025 23:42:42:INFO:Disconnect and shut down

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.406341552734375
Data Scaling Factor: 0.12 where Client Data Size: 768
Noise Multiplier after Fisher Scaling:  [0.01480774860829115, 0.0017922507831826806, 0.0037967567332088947, 0.011151124723255634]
Noise Multiplier after list and tensor:  0.00788697021198459
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 768, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504, 1.0184915054580772], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836, 0.5918686473807663], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188, 0.8098659294281917]}



Final client history:
{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504, 1.0184915054580772], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836, 0.5918686473807663], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188, 0.8098659294281917]}

