nohup: ignoring input
01/26/2025 22:40:17:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/26/2025 22:40:17:DEBUG:ChannelConnectivity.IDLE
01/26/2025 22:40:17:DEBUG:ChannelConnectivity.CONNECTING
01/26/2025 22:40:17:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/26/2025 22:40:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:40:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message be180640-ca15-4cf8-8753-2d220e0f651a
01/26/2025 22:40:45:INFO:Received: train message be180640-ca15-4cf8-8753-2d220e0f651a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:41:28:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:42:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:42:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7cb0f88d-9648-48cb-ae14-c58c4e554a6b
01/26/2025 22:42:08:INFO:Received: evaluate message 7cb0f88d-9648-48cb-ae14-c58c4e554a6b
[92mINFO [0m:      Sent reply
01/26/2025 22:42:14:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:42:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:42:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eb89d546-bb58-4cf6-9bff-af53e86b6dd2
01/26/2025 22:42:47:INFO:Received: train message eb89d546-bb58-4cf6-9bff-af53e86b6dd2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:43:25:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:44:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:44:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message de873138-64ca-48c3-821e-44e2edf73223
01/26/2025 22:44:08:INFO:Received: evaluate message de873138-64ca-48c3-821e-44e2edf73223
[92mINFO [0m:      Sent reply
01/26/2025 22:44:12:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:44:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:44:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 349c2244-9ff7-40e1-b044-01640fd78d1e
01/26/2025 22:44:33:INFO:Received: train message 349c2244-9ff7-40e1-b044-01640fd78d1e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:45:08:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:46:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:46:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 080f1bc4-0a38-420a-a5fb-f33d08e75f4f
01/26/2025 22:46:14:INFO:Received: evaluate message 080f1bc4-0a38-420a-a5fb-f33d08e75f4f
[92mINFO [0m:      Sent reply
01/26/2025 22:46:19:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:46:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:46:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e01635fc-3120-4fa5-96f9-e4d27878500e
01/26/2025 22:46:46:INFO:Received: train message e01635fc-3120-4fa5-96f9-e4d27878500e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:47:25:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:48:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:48:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d1bb380a-9aa2-4f5c-8183-77ff804afbd7
01/26/2025 22:48:09:INFO:Received: evaluate message d1bb380a-9aa2-4f5c-8183-77ff804afbd7
[92mINFO [0m:      Sent reply
01/26/2025 22:48:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:48:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:48:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message abcb966e-0579-4502-89a1-69dd67f4ef95
01/26/2025 22:48:48:INFO:Received: train message abcb966e-0579-4502-89a1-69dd67f4ef95
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:49:30:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:50:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:50:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6f4e33ab-adfb-440e-a21f-9381e13159ba
01/26/2025 22:50:16:INFO:Received: evaluate message 6f4e33ab-adfb-440e-a21f-9381e13159ba
[92mINFO [0m:      Sent reply
01/26/2025 22:50:20:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:50:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:50:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 94342993-a750-4770-b778-e76ece238c1e
01/26/2025 22:50:42:INFO:Received: train message 94342993-a750-4770-b778-e76ece238c1e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:51:27:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:51:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:51:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d8cdc2a4-f9f8-4aef-9078-c6e1c5b5ec88
01/26/2025 22:51:54:INFO:Received: evaluate message d8cdc2a4-f9f8-4aef-9078-c6e1c5b5ec88
[92mINFO [0m:      Sent reply
01/26/2025 22:51:57:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:52:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:52:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3eb10ba7-19d3-415c-9f38-81be9a3d45ee
01/26/2025 22:52:57:INFO:Received: train message 3eb10ba7-19d3-415c-9f38-81be9a3d45ee
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:53:38:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:54:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:54:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message efa7e1ea-22ac-4d43-99ac-912cdd5a133f
01/26/2025 22:54:25:INFO:Received: evaluate message efa7e1ea-22ac-4d43-99ac-912cdd5a133f
[92mINFO [0m:      Sent reply
01/26/2025 22:54:31:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:55:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:55:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1f036fa6-8e00-45a1-8b7b-ff33c9cc4ff0
01/26/2025 22:55:13:INFO:Received: train message 1f036fa6-8e00-45a1-8b7b-ff33c9cc4ff0
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30, target_epsilon: 30, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544], 'accuracy': [0.5199374511336982], 'auc': [0.7254494785945842]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378], 'accuracy': [0.5199374511336982, 0.5238467552775606], 'auc': [0.7254494785945842, 0.7453794430136471]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653]}

/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:55:52:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:56:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:56:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fff2723b-5ce8-45d4-a59a-3fc39a48fa5d
01/26/2025 22:56:22:INFO:Received: evaluate message fff2723b-5ce8-45d4-a59a-3fc39a48fa5d
[92mINFO [0m:      Sent reply
01/26/2025 22:56:27:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:57:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:57:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 33f22014-1952-4b58-939b-122fe17c7075
01/26/2025 22:57:15:INFO:Received: train message 33f22014-1952-4b58-939b-122fe17c7075
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:58:02:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:58:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:58:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3061925a-58e4-4370-af02-8631723cc00f
01/26/2025 22:58:52:INFO:Received: evaluate message 3061925a-58e4-4370-af02-8631723cc00f
[92mINFO [0m:      Sent reply
01/26/2025 22:58:57:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:59:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:59:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7889746e-064d-4fec-919a-87298c8f7afd
01/26/2025 22:59:08:INFO:Received: train message 7889746e-064d-4fec-919a-87298c8f7afd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:59:47:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:01:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:01:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 912d8ec0-8a09-40f8-8c54-1be63d0f6606
01/26/2025 23:01:03:INFO:Received: evaluate message 912d8ec0-8a09-40f8-8c54-1be63d0f6606
[92mINFO [0m:      Sent reply
01/26/2025 23:01:08:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:01:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:01:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 29291cf9-9148-4827-a3d5-eb530ab67383
01/26/2025 23:01:49:INFO:Received: train message 29291cf9-9148-4827-a3d5-eb530ab67383
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:02:31:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:03:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:03:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message da292d33-ec12-4a69-8ad9-dee3e538218a
01/26/2025 23:03:15:INFO:Received: evaluate message da292d33-ec12-4a69-8ad9-dee3e538218a
[92mINFO [0m:      Sent reply
01/26/2025 23:03:19:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:03:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:03:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c087eba5-422c-4ea5-ba36-fd2b10dcf2d1
01/26/2025 23:03:36:INFO:Received: train message c087eba5-422c-4ea5-ba36-fd2b10dcf2d1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:04:10:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:05:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:05:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 55c4f30d-552e-4fed-94d0-ddc64f26de8a
01/26/2025 23:05:14:INFO:Received: evaluate message 55c4f30d-552e-4fed-94d0-ddc64f26de8a
[92mINFO [0m:      Sent reply
01/26/2025 23:05:19:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:05:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:05:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b44a401f-78d9-47fb-9384-536007a1fbb2
01/26/2025 23:05:54:INFO:Received: train message b44a401f-78d9-47fb-9384-536007a1fbb2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:06:34:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:07:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:07:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7f7604f2-9db5-4d03-be17-5143b8691c6f
01/26/2025 23:07:12:INFO:Received: evaluate message 7f7604f2-9db5-4d03-be17-5143b8691c6f
[92mINFO [0m:      Sent reply
01/26/2025 23:07:16:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:07:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:07:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 64e0e795-5524-45e0-a105-8631b75a7c2b
01/26/2025 23:07:53:INFO:Received: train message 64e0e795-5524-45e0-a105-8631b75a7c2b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:08:30:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:08:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:08:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 755d43dc-6a49-449d-b002-08dd20fcc213
01/26/2025 23:08:59:INFO:Received: evaluate message 755d43dc-6a49-449d-b002-08dd20fcc213
[92mINFO [0m:      Sent reply
01/26/2025 23:09:02:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:09:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:09:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 14f90334-9c3f-4b02-aefa-b8104f33991b
01/26/2025 23:09:44:INFO:Received: train message 14f90334-9c3f-4b02-aefa-b8104f33991b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:10:27:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:10:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:10:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 71938f94-3d1f-407c-9fa3-8dca88d8c275
01/26/2025 23:10:59:INFO:Received: evaluate message 71938f94-3d1f-407c-9fa3-8dca88d8c275
[92mINFO [0m:      Sent reply
01/26/2025 23:11:03:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:11:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:11:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a873a50b-56e3-4682-9d68-b8611022d0c3
01/26/2025 23:11:50:INFO:Received: train message a873a50b-56e3-4682-9d68-b8611022d0c3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:12:26:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:13:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:13:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d857921f-8076-4b9d-b545-82c81167cb61
01/26/2025 23:13:18:INFO:Received: evaluate message d857921f-8076-4b9d-b545-82c81167cb61
[92mINFO [0m:      Sent reply
01/26/2025 23:13:24:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:13:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:13:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fb89c3d2-ef76-4a8c-9672-a7d61f8c2f75
01/26/2025 23:13:58:INFO:Received: train message fb89c3d2-ef76-4a8c-9672-a7d61f8c2f75
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:14:44:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:15:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:15:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 71b0925f-00e6-4937-b968-00844fbf81b7
01/26/2025 23:15:35:INFO:Received: evaluate message 71b0925f-00e6-4937-b968-00844fbf81b7
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:15:38:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:16:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:16:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 77561fe0-8f15-4ce2-a02c-e0cc25f9e562
01/26/2025 23:16:15:INFO:Received: train message 77561fe0-8f15-4ce2-a02c-e0cc25f9e562
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:17:10:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:17:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:17:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 005722aa-4e95-48ba-9c74-1d3e68d0ec73
01/26/2025 23:17:31:INFO:Received: evaluate message 005722aa-4e95-48ba-9c74-1d3e68d0ec73
[92mINFO [0m:      Sent reply
01/26/2025 23:17:35:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:18:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:18:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 03f7e638-87dc-4d2d-a7c2-c6e3e31579bc
01/26/2025 23:18:54:INFO:Received: train message 03f7e638-87dc-4d2d-a7c2-c6e3e31579bc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:19:33:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:20:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:20:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1f5864e5-4871-4d63-b774-259696cfd11d
01/26/2025 23:20:04:INFO:Received: evaluate message 1f5864e5-4871-4d63-b774-259696cfd11d
[92mINFO [0m:      Sent reply
01/26/2025 23:20:07:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:20:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:20:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1b5f0e6d-595a-4a6d-9d19-8086cd6d0a25
01/26/2025 23:20:54:INFO:Received: train message 1b5f0e6d-595a-4a6d-9d19-8086cd6d0a25
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:21:33:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:22:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:22:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4111a242-b8f4-45b0-a92e-cc13ea732142
01/26/2025 23:22:45:INFO:Received: evaluate message 4111a242-b8f4-45b0-a92e-cc13ea732142
[92mINFO [0m:      Sent reply
01/26/2025 23:22:51:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:23:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:23:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b92ca6d9-f77a-41ec-a320-14de7eb7bffd
01/26/2025 23:23:48:INFO:Received: train message b92ca6d9-f77a-41ec-a320-14de7eb7bffd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:24:34:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:24:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:24:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5e3092cc-a97d-43b2-8958-2289906ce9ec
01/26/2025 23:24:51:INFO:Received: evaluate message 5e3092cc-a97d-43b2-8958-2289906ce9ec

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:24:53:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:25:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:25:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 71dad684-c87c-4d6a-8333-618d99576d4a
01/26/2025 23:25:45:INFO:Received: train message 71dad684-c87c-4d6a-8333-618d99576d4a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:26:26:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:27:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:27:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9ede7d12-32e2-4761-9a50-8079fff59542
01/26/2025 23:27:04:INFO:Received: evaluate message 9ede7d12-32e2-4761-9a50-8079fff59542
[92mINFO [0m:      Sent reply
01/26/2025 23:27:06:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:27:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:27:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 49f44953-c02c-47f2-81bf-2632e7e7ad40
01/26/2025 23:27:36:INFO:Received: train message 49f44953-c02c-47f2-81bf-2632e7e7ad40
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:28:17:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:28:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:28:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 85d4f337-092a-45f9-b57b-6732c782fdd0
01/26/2025 23:28:44:INFO:Received: evaluate message 85d4f337-092a-45f9-b57b-6732c782fdd0
[92mINFO [0m:      Sent reply
01/26/2025 23:28:49:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:29:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:29:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 21c5e1ca-0d3a-4a65-81be-afcbf311d4fd
01/26/2025 23:29:31:INFO:Received: train message 21c5e1ca-0d3a-4a65-81be-afcbf311d4fd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:30:11:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:30:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:30:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 537259ce-2ae2-461c-a059-834d0f8e08f6
01/26/2025 23:30:26:INFO:Received: evaluate message 537259ce-2ae2-461c-a059-834d0f8e08f6
[92mINFO [0m:      Sent reply
01/26/2025 23:30:28:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:31:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:31:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message df688305-e461-45ce-9d1e-5e846dd5af83
01/26/2025 23:31:17:INFO:Received: train message df688305-e461-45ce-9d1e-5e846dd5af83

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:32:01:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:32:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:32:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 68e8f97c-32e9-4ed4-9c8c-1184b6229fb3
01/26/2025 23:32:35:INFO:Received: evaluate message 68e8f97c-32e9-4ed4-9c8c-1184b6229fb3
[92mINFO [0m:      Sent reply
01/26/2025 23:32:38:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:33:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:33:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 402b44a9-59cf-4fc2-83ef-49382e4f9417
01/26/2025 23:33:10:INFO:Received: train message 402b44a9-59cf-4fc2-83ef-49382e4f9417
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:34:03:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:34:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:34:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c26cb31d-a274-4f76-8778-eb03e4b1ed5a
01/26/2025 23:34:40:INFO:Received: evaluate message c26cb31d-a274-4f76-8778-eb03e4b1ed5a
[92mINFO [0m:      Sent reply
01/26/2025 23:34:45:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:35:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:35:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f21c50b1-37b9-4b07-817a-804814352bcd
01/26/2025 23:35:00:INFO:Received: train message f21c50b1-37b9-4b07-817a-804814352bcd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:35:38:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:36:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:36:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 362833ac-e629-40ff-a7e7-6168a4091963
01/26/2025 23:36:21:INFO:Received: evaluate message 362833ac-e629-40ff-a7e7-6168a4091963
[92mINFO [0m:      Sent reply
01/26/2025 23:36:24:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:37:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:37:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message aef39342-9fea-40d6-bb71-7bf50b9e43fa
01/26/2025 23:37:02:INFO:Received: train message aef39342-9fea-40d6-bb71-7bf50b9e43fa
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:37:40:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:38:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:38:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 33d1f81e-e9c6-4425-bee2-826863735ba5
01/26/2025 23:38:00:INFO:Received: evaluate message 33d1f81e-e9c6-4425-bee2-826863735ba5
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:38:03:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:38:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:38:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 962b32d8-4fd7-45bc-8d58-09c23ea2b5c0
01/26/2025 23:38:52:INFO:Received: train message 962b32d8-4fd7-45bc-8d58-09c23ea2b5c0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:39:29:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:39:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:39:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bdf44aa8-f9e7-44ca-8abc-ed02981248b6
01/26/2025 23:39:54:INFO:Received: evaluate message bdf44aa8-f9e7-44ca-8abc-ed02981248b6
[92mINFO [0m:      Sent reply
01/26/2025 23:39:58:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:40:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:40:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 29eb46dd-d3b5-4eb5-8aa9-6e043fe2758b
01/26/2025 23:40:58:INFO:Received: train message 29eb46dd-d3b5-4eb5-8aa9-6e043fe2758b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:41:49:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:42:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:42:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8ad42228-2045-454c-be2e-485634fd90ea
01/26/2025 23:42:28:INFO:Received: evaluate message 8ad42228-2045-454c-be2e-485634fd90ea
[92mINFO [0m:      Sent reply
01/26/2025 23:42:31:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:42:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:42:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 68f60286-38cc-4b77-b788-d29d47d8313c
01/26/2025 23:42:42:INFO:Received: reconnect message 68f60286-38cc-4b77-b788-d29d47d8313c
01/26/2025 23:42:42:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/26/2025 23:42:42:INFO:Disconnect and shut down

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4254150390625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.00790428463369608, 0.002389282453805208, 0.03190857917070389, 0.03251965716481209]
Noise Multiplier after list and tensor:  0.018680450855754316
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504, 1.0184915054580772], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836, 0.5918686473807663], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188, 0.8098659294281917]}



Final client history:
{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504, 1.0184915054580772], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836, 0.5918686473807663], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188, 0.8098659294281917]}

