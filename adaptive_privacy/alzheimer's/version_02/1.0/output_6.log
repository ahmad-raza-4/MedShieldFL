nohup: ignoring input
01/31/2025 05:19:16:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/31/2025 05:19:16:DEBUG:ChannelConnectivity.IDLE
01/31/2025 05:19:16:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/31/2025 05:19:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:19:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message 1164dda7-fc77-4361-9e4d-2a7c665523b3
01/31/2025 05:19:16:INFO:Received: get_parameters message 1164dda7-fc77-4361-9e4d-2a7c665523b3
[92mINFO [0m:      Sent reply
01/31/2025 05:19:20:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:19:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:19:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 351edcb8-3c3e-40a9-ab8b-eb3a7fb8a842
01/31/2025 05:19:56:INFO:Received: train message 351edcb8-3c3e-40a9-ab8b-eb3a7fb8a842
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:20:11:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:20:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:20:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6f984d0b-c7d8-4620-b642-e01ced6b19d9
01/31/2025 05:20:59:INFO:Received: evaluate message 6f984d0b-c7d8-4620-b642-e01ced6b19d9
[92mINFO [0m:      Sent reply
01/31/2025 05:21:01:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:21:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:21:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7115fb69-91e0-43ac-b51e-debff8e01837
01/31/2025 05:21:33:INFO:Received: train message 7115fb69-91e0-43ac-b51e-debff8e01837
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:21:46:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:22:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:22:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5c124d04-e135-46ab-9d9b-ecea247b1027
01/31/2025 05:22:35:INFO:Received: evaluate message 5c124d04-e135-46ab-9d9b-ecea247b1027
[92mINFO [0m:      Sent reply
01/31/2025 05:22:37:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:22:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:22:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5d055e0f-048c-47d8-9b03-a8f8f6febd50
01/31/2025 05:22:55:INFO:Received: train message 5d055e0f-048c-47d8-9b03-a8f8f6febd50
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:23:04:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:24:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:24:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7a41db9d-794d-4772-b7ec-471ecd260276
01/31/2025 05:24:04:INFO:Received: evaluate message 7a41db9d-794d-4772-b7ec-471ecd260276
[92mINFO [0m:      Sent reply
01/31/2025 05:24:06:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:24:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:24:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 013649ac-4f6b-45fa-9d93-7636dc0cb5fb
01/31/2025 05:24:42:INFO:Received: train message 013649ac-4f6b-45fa-9d93-7636dc0cb5fb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:24:52:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:25:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:25:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d0c1bea8-1777-4425-a856-389d29f5a2ae
01/31/2025 05:25:42:INFO:Received: evaluate message d0c1bea8-1777-4425-a856-389d29f5a2ae
[92mINFO [0m:      Sent reply
01/31/2025 05:25:44:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:26:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:26:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d6d86e0b-7208-4593-8f78-e0c60f5559a4
01/31/2025 05:26:20:INFO:Received: train message d6d86e0b-7208-4593-8f78-e0c60f5559a4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:26:32:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:27:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:27:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 531c5495-b5e7-444a-bcb7-732a404f1c66
01/31/2025 05:27:10:INFO:Received: evaluate message 531c5495-b5e7-444a-bcb7-732a404f1c66
[92mINFO [0m:      Sent reply
01/31/2025 05:27:13:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:27:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:27:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ac2413b7-9a18-48b6-8aca-5e5695d4ca7f
01/31/2025 05:27:53:INFO:Received: train message ac2413b7-9a18-48b6-8aca-5e5695d4ca7f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:28:03:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:28:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:28:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ccb4b531-ff17-4936-86ff-5ab45faabff0
01/31/2025 05:28:51:INFO:Received: evaluate message ccb4b531-ff17-4936-86ff-5ab45faabff0
[92mINFO [0m:      Sent reply
01/31/2025 05:28:54:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:29:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:29:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 30ea40c5-4397-4e7d-bb39-8dea991a0def
01/31/2025 05:29:31:INFO:Received: train message 30ea40c5-4397-4e7d-bb39-8dea991a0def
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:29:41:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:30:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:30:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8e3b7958-ee52-4b83-9ae8-dbe1a2142198
01/31/2025 05:30:38:INFO:Received: evaluate message 8e3b7958-ee52-4b83-9ae8-dbe1a2142198
[92mINFO [0m:      Sent reply
01/31/2025 05:30:41:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:31:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:31:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bc1f477c-dfb2-4db4-ac8f-49d60ff61fae
01/31/2025 05:31:19:INFO:Received: train message bc1f477c-dfb2-4db4-ac8f-49d60ff61fae
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:31:27:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:32:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:32:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 49df4b6d-7ab8-4aa5-b32f-fc71bec7cb4c
01/31/2025 05:32:31:INFO:Received: evaluate message 49df4b6d-7ab8-4aa5-b32f-fc71bec7cb4c
[92mINFO [0m:      Sent reply
01/31/2025 05:32:34:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:33:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:33:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e2b85972-5564-48ef-9284-85dd3064c043
01/31/2025 05:33:06:INFO:Received: train message e2b85972-5564-48ef-9284-85dd3064c043
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:33:16:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:33:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:33:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6f540553-40c2-4464-a870-784dd080309a
01/31/2025 05:33:47:INFO:Received: evaluate message 6f540553-40c2-4464-a870-784dd080309a
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118], 'accuracy': [0.5175918686473807], 'auc': [0.737810384411014]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651], 'accuracy': [0.5175918686473807, 0.5215011727912432], 'auc': [0.737810384411014, 0.754622171691574]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 05:33:49:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:34:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:34:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a5b8583e-b96a-4467-9089-13d36c45bfcd
01/31/2025 05:34:41:INFO:Received: train message a5b8583e-b96a-4467-9089-13d36c45bfcd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:35:01:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:36:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:36:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5ef7e522-cdd6-49d8-8055-fdb2003e8ec9
01/31/2025 05:36:00:INFO:Received: evaluate message 5ef7e522-cdd6-49d8-8055-fdb2003e8ec9
[92mINFO [0m:      Sent reply
01/31/2025 05:36:03:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:36:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:36:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0286ef78-3256-4eab-9cbb-97a4be70044e
01/31/2025 05:36:47:INFO:Received: train message 0286ef78-3256-4eab-9cbb-97a4be70044e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:36:59:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:37:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:37:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ebb87dc4-9403-437b-a02f-b1fe95eb90d6
01/31/2025 05:37:44:INFO:Received: evaluate message ebb87dc4-9403-437b-a02f-b1fe95eb90d6
[92mINFO [0m:      Sent reply
01/31/2025 05:37:48:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:38:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:38:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 601cea63-79b7-4a1d-9c1a-b60705eac435
01/31/2025 05:38:38:INFO:Received: train message 601cea63-79b7-4a1d-9c1a-b60705eac435
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:38:54:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:39:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:39:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 845726a1-fb41-4571-9997-b2c9eb161af5
01/31/2025 05:39:37:INFO:Received: evaluate message 845726a1-fb41-4571-9997-b2c9eb161af5
[92mINFO [0m:      Sent reply
01/31/2025 05:39:40:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:40:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:40:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ca2e1e2f-045c-43b0-9138-fffee2e002b9
01/31/2025 05:40:14:INFO:Received: train message ca2e1e2f-045c-43b0-9138-fffee2e002b9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:40:24:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:41:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:41:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 82a8fdee-1f6d-44e6-80bf-b428d8b7c0cc
01/31/2025 05:41:24:INFO:Received: evaluate message 82a8fdee-1f6d-44e6-80bf-b428d8b7c0cc
[92mINFO [0m:      Sent reply
01/31/2025 05:41:27:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:42:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:42:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d468220f-7bd0-415e-9145-833157949cc1
01/31/2025 05:42:25:INFO:Received: train message d468220f-7bd0-415e-9145-833157949cc1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:42:34:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:43:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:43:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1f2e5bd0-240a-444f-8ab0-2f035b081452
01/31/2025 05:43:12:INFO:Received: evaluate message 1f2e5bd0-240a-444f-8ab0-2f035b081452
[92mINFO [0m:      Sent reply
01/31/2025 05:43:14:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:43:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:43:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8b072dd8-ca6f-4735-adfe-e420ebeee182
01/31/2025 05:43:51:INFO:Received: train message 8b072dd8-ca6f-4735-adfe-e420ebeee182
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:44:00:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:44:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:44:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d4910df8-9bbf-4a1b-809c-1760910b1000
01/31/2025 05:44:40:INFO:Received: evaluate message d4910df8-9bbf-4a1b-809c-1760910b1000

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 05:44:43:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:45:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:45:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 21a2efbe-2591-44ab-ab01-c124a0c176e6
01/31/2025 05:45:13:INFO:Received: train message 21a2efbe-2591-44ab-ab01-c124a0c176e6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:45:23:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:46:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:46:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5d9f9e39-b006-494e-9049-404447e42d21
01/31/2025 05:46:10:INFO:Received: evaluate message 5d9f9e39-b006-494e-9049-404447e42d21
[92mINFO [0m:      Sent reply
01/31/2025 05:46:12:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:46:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:46:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1b0fdf8b-7975-4ee8-a0db-c34487c5ff92
01/31/2025 05:46:43:INFO:Received: train message 1b0fdf8b-7975-4ee8-a0db-c34487c5ff92
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:46:55:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:47:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:47:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 036b7305-e2c0-418a-9557-c8377eb0e1e3
01/31/2025 05:47:35:INFO:Received: evaluate message 036b7305-e2c0-418a-9557-c8377eb0e1e3
[92mINFO [0m:      Sent reply
01/31/2025 05:47:38:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:47:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:47:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c4cbd2f7-1c8c-4e00-b531-f47ce54207d8
01/31/2025 05:47:55:INFO:Received: train message c4cbd2f7-1c8c-4e00-b531-f47ce54207d8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:48:04:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:49:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:49:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 183f549a-2959-4251-9728-0ebc6e80dcb1
01/31/2025 05:49:00:INFO:Received: evaluate message 183f549a-2959-4251-9728-0ebc6e80dcb1
[92mINFO [0m:      Sent reply
01/31/2025 05:49:03:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:49:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:49:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 05018fb6-d392-46d5-b117-3d10d02e2492
01/31/2025 05:49:35:INFO:Received: train message 05018fb6-d392-46d5-b117-3d10d02e2492
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:49:46:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:50:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:50:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 628123d3-d917-4b08-9a0a-c77e07837fdd
01/31/2025 05:50:18:INFO:Received: evaluate message 628123d3-d917-4b08-9a0a-c77e07837fdd
[92mINFO [0m:      Sent reply
01/31/2025 05:50:20:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:50:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:50:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 26ff82b2-41fc-4ed1-9a7b-a1fb24e65895
01/31/2025 05:50:46:INFO:Received: train message 26ff82b2-41fc-4ed1-9a7b-a1fb24e65895
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:50:54:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:51:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:51:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f1f23db0-9cc9-4d7b-985f-e98ef9a71077
01/31/2025 05:51:55:INFO:Received: evaluate message f1f23db0-9cc9-4d7b-985f-e98ef9a71077

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914, 1.0378118212843799], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5895230648944488], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436, 0.8001750832077643]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 05:51:58:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:52:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:52:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 57f668b3-df4f-4a8f-87f5-d6bb614f4128
01/31/2025 05:52:14:INFO:Received: train message 57f668b3-df4f-4a8f-87f5-d6bb614f4128
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:52:22:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:53:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:53:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 50e9ffe5-88e2-49eb-b8f9-4419f86c2cc6
01/31/2025 05:53:18:INFO:Received: evaluate message 50e9ffe5-88e2-49eb-b8f9-4419f86c2cc6
[92mINFO [0m:      Sent reply
01/31/2025 05:53:21:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:53:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:53:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 78e04730-264c-4acf-ad7b-59112aadbfaf
01/31/2025 05:53:53:INFO:Received: train message 78e04730-264c-4acf-ad7b-59112aadbfaf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:54:02:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:54:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:54:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 479348f1-1ff8-415d-b310-deda7261e195
01/31/2025 05:54:49:INFO:Received: evaluate message 479348f1-1ff8-415d-b310-deda7261e195
[92mINFO [0m:      Sent reply
01/31/2025 05:54:51:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:55:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:55:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 50c36f1f-3e86-4255-a73f-c1fa32116be9
01/31/2025 05:55:30:INFO:Received: train message 50c36f1f-3e86-4255-a73f-c1fa32116be9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:55:41:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:56:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:56:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a45b1f5b-03d1-48c2-89e6-e343cd3943ba
01/31/2025 05:56:34:INFO:Received: evaluate message a45b1f5b-03d1-48c2-89e6-e343cd3943ba
[92mINFO [0m:      Sent reply
01/31/2025 05:56:37:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:57:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:57:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 74762369-ab0f-4f27-a265-dac8c2ca3187
01/31/2025 05:57:14:INFO:Received: train message 74762369-ab0f-4f27-a265-dac8c2ca3187
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:57:25:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:58:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:58:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aec3ae61-f822-40a4-8e9a-472b98ad1a3f
01/31/2025 05:58:05:INFO:Received: evaluate message aec3ae61-f822-40a4-8e9a-472b98ad1a3f

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914, 1.0378118212843799, 1.105929083056148], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5895230648944488, 0.5777951524628616], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436, 0.8001750832077643, 0.8009661738399159]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914, 1.0378118212843799, 1.105929083056148, 1.0370387514183725], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5895230648944488, 0.5777951524628616, 0.5840500390930414], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436, 0.8001750832077643, 0.8009661738399159, 0.8038719238804488]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914, 1.0378118212843799, 1.105929083056148, 1.0370387514183725, 1.0404642454994386], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5895230648944488, 0.5777951524628616, 0.5840500390930414, 0.584831899921814], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436, 0.8001750832077643, 0.8009661738399159, 0.8038719238804488, 0.8044155986611784]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914, 1.0378118212843799, 1.105929083056148, 1.0370387514183725, 1.0404642454994386, 1.0359342383909635], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5895230648944488, 0.5777951524628616, 0.5840500390930414, 0.584831899921814, 0.5910867865519938], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436, 0.8001750832077643, 0.8009661738399159, 0.8038719238804488, 0.8044155986611784, 0.8048619192047883]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 05:58:07:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:58:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:58:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 953b782f-a577-47ce-881c-63923bf69232
01/31/2025 05:58:46:INFO:Received: train message 953b782f-a577-47ce-881c-63923bf69232
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 05:58:56:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 05:59:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 05:59:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3033233d-83c4-4a56-9926-26aa8701a3ce
01/31/2025 05:59:51:INFO:Received: evaluate message 3033233d-83c4-4a56-9926-26aa8701a3ce
[92mINFO [0m:      Sent reply
01/31/2025 05:59:53:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 06:00:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 06:00:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e2248fa2-2fe3-49ae-8558-ce5396696d69
01/31/2025 06:00:09:INFO:Received: train message e2248fa2-2fe3-49ae-8558-ce5396696d69
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 06:00:17:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 06:01:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 06:01:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5810582f-1ab9-4b73-8163-d9a6fb8c536b
01/31/2025 06:01:17:INFO:Received: evaluate message 5810582f-1ab9-4b73-8163-d9a6fb8c536b
[92mINFO [0m:      Sent reply
01/31/2025 06:01:21:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 06:01:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 06:01:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4ff13b34-39a6-4cbc-b2e9-b3328d58e64c
01/31/2025 06:01:39:INFO:Received: train message 4ff13b34-39a6-4cbc-b2e9-b3328d58e64c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 06:01:48:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 06:02:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 06:02:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e73137b2-a944-4ab7-aeb8-9f0a28ca3fa0
01/31/2025 06:02:48:INFO:Received: evaluate message e73137b2-a944-4ab7-aeb8-9f0a28ca3fa0
[92mINFO [0m:      Sent reply
01/31/2025 06:02:52:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 06:03:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 06:03:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9bf03361-1ae9-4251-bd4a-f753413b3403
01/31/2025 06:03:24:INFO:Received: train message 9bf03361-1ae9-4251-bd4a-f753413b3403

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914, 1.0378118212843799, 1.105929083056148, 1.0370387514183725, 1.0404642454994386, 1.0359342383909635, 1.036139934952134], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5895230648944488, 0.5777951524628616, 0.5840500390930414, 0.584831899921814, 0.5910867865519938, 0.5903049257232212], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436, 0.8001750832077643, 0.8009661738399159, 0.8038719238804488, 0.8044155986611784, 0.8048619192047883, 0.8059240550589555]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914, 1.0378118212843799, 1.105929083056148, 1.0370387514183725, 1.0404642454994386, 1.0359342383909635, 1.036139934952134, 0.9872238999516634], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5895230648944488, 0.5777951524628616, 0.5840500390930414, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436, 0.8001750832077643, 0.8009661738399159, 0.8038719238804488, 0.8044155986611784, 0.8048619192047883, 0.8059240550589555, 0.8094196018912208]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914, 1.0378118212843799, 1.105929083056148, 1.0370387514183725, 1.0404642454994386, 1.0359342383909635, 1.036139934952134, 0.9872238999516634, 1.0871558723457164], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5895230648944488, 0.5777951524628616, 0.5840500390930414, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461, 0.5863956215793589], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436, 0.8001750832077643, 0.8009661738399159, 0.8038719238804488, 0.8044155986611784, 0.8048619192047883, 0.8059240550589555, 0.8094196018912208, 0.809639705995446]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914, 1.0378118212843799, 1.105929083056148, 1.0370387514183725, 1.0404642454994386, 1.0359342383909635, 1.036139934952134, 0.9872238999516634, 1.0871558723457164, 1.0336675031563562], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5895230648944488, 0.5777951524628616, 0.5840500390930414, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461, 0.5863956215793589, 0.5942142298670836], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436, 0.8001750832077643, 0.8009661738399159, 0.8038719238804488, 0.8044155986611784, 0.8048619192047883, 0.8059240550589555, 0.8094196018912208, 0.809639705995446, 0.8109418186232445]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 06:03:36:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 06:04:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 06:04:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 71e2cf46-b0bc-4ae2-bd96-b998b6bc5939
01/31/2025 06:04:32:INFO:Received: evaluate message 71e2cf46-b0bc-4ae2-bd96-b998b6bc5939
[92mINFO [0m:      Sent reply
01/31/2025 06:04:34:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 06:05:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 06:05:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 140c0d33-68d9-465f-9c0e-34f5fab3ddc0
01/31/2025 06:05:05:INFO:Received: train message 140c0d33-68d9-465f-9c0e-34f5fab3ddc0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 06:05:17:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 06:06:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 06:06:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f694d497-9e17-40b9-9047-d350284d0c9c
01/31/2025 06:06:13:INFO:Received: evaluate message f694d497-9e17-40b9-9047-d350284d0c9c
[92mINFO [0m:      Sent reply
01/31/2025 06:06:15:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 06:06:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 06:06:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 88f8bfea-d2d8-4cc7-96ab-a843d75e8f67
01/31/2025 06:06:53:INFO:Received: train message 88f8bfea-d2d8-4cc7-96ab-a843d75e8f67
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 06:07:05:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 06:07:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 06:07:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ad9535c8-18f0-4cf7-94ee-ecc126e7a4d1
01/31/2025 06:07:59:INFO:Received: evaluate message ad9535c8-18f0-4cf7-94ee-ecc126e7a4d1
[92mINFO [0m:      Sent reply
01/31/2025 06:08:03:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 06:08:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 06:08:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 0c2c8306-4d26-4798-8874-f6780b6cbbd7
01/31/2025 06:08:23:INFO:Received: reconnect message 0c2c8306-4d26-4798-8874-f6780b6cbbd7
01/31/2025 06:08:23:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/31/2025 06:08:23:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914, 1.0378118212843799, 1.105929083056148, 1.0370387514183725, 1.0404642454994386, 1.0359342383909635, 1.036139934952134, 0.9872238999516634, 1.0871558723457164, 1.0336675031563562, 1.006800597398145], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5895230648944488, 0.5777951524628616, 0.5840500390930414, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461, 0.5863956215793589, 0.5942142298670836, 0.596559812353401], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436, 0.8001750832077643, 0.8009661738399159, 0.8038719238804488, 0.8044155986611784, 0.8048619192047883, 0.8059240550589555, 0.8094196018912208, 0.809639705995446, 0.8109418186232445, 0.8121464706217649]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914, 1.0378118212843799, 1.105929083056148, 1.0370387514183725, 1.0404642454994386, 1.0359342383909635, 1.036139934952134, 0.9872238999516634, 1.0871558723457164, 1.0336675031563562, 1.006800597398145, 1.0072966768836678], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5895230648944488, 0.5777951524628616, 0.5840500390930414, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461, 0.5863956215793589, 0.5942142298670836, 0.596559812353401, 0.5989053948397185], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436, 0.8001750832077643, 0.8009661738399159, 0.8038719238804488, 0.8044155986611784, 0.8048619192047883, 0.8059240550589555, 0.8094196018912208, 0.809639705995446, 0.8109418186232445, 0.8121464706217649, 0.8107583221748409]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914, 1.0378118212843799, 1.105929083056148, 1.0370387514183725, 1.0404642454994386, 1.0359342383909635, 1.036139934952134, 0.9872238999516634, 1.0871558723457164, 1.0336675031563562, 1.006800597398145, 1.0072966768836678, 1.014726576626161], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5895230648944488, 0.5777951524628616, 0.5840500390930414, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461, 0.5863956215793589, 0.5942142298670836, 0.596559812353401, 0.5989053948397185, 0.5989053948397185], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436, 0.8001750832077643, 0.8009661738399159, 0.8038719238804488, 0.8044155986611784, 0.8048619192047883, 0.8059240550589555, 0.8094196018912208, 0.809639705995446, 0.8109418186232445, 0.8121464706217649, 0.8107583221748409, 0.8129799795083197]}



Final client history:
{'loss': [1.0650320724177118, 1.1435031147092651, 1.14496373468493, 1.0871561519143356, 1.0580496810860145, 1.0860763859991174, 1.078478406238034, 1.075482842230629, 1.0735989590004331, 1.071916224156813, 1.008569647130601, 1.049823229997022, 1.032722747120846, 1.0393645761533115, 1.0436073726187283, 1.043382160499564, 1.0689515229591269, 1.0154109152617914, 1.0378118212843799, 1.105929083056148, 1.0370387514183725, 1.0404642454994386, 1.0359342383909635, 1.036139934952134, 0.9872238999516634, 1.0871558723457164, 1.0336675031563562, 1.006800597398145, 1.0072966768836678, 1.014726576626161], 'accuracy': [0.5175918686473807, 0.5215011727912432, 0.5340109460516028, 0.5433932759968726, 0.5543393275996873, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5856137607505864, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5895230648944488, 0.5777951524628616, 0.5840500390930414, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461, 0.5863956215793589, 0.5942142298670836, 0.596559812353401, 0.5989053948397185, 0.5989053948397185], 'auc': [0.737810384411014, 0.754622171691574, 0.7630398645800955, 0.7718321472507048, 0.777497085891427, 0.7804611864899138, 0.7828739786675091, 0.7847185316137854, 0.7860346211118455, 0.7884850994031911, 0.7928451880461926, 0.7949859176877196, 0.7949766829233569, 0.7937493853010754, 0.7958585238036657, 0.7975881541457897, 0.799110162433671, 0.8006139013113436, 0.8001750832077643, 0.8009661738399159, 0.8038719238804488, 0.8044155986611784, 0.8048619192047883, 0.8059240550589555, 0.8094196018912208, 0.809639705995446, 0.8109418186232445, 0.8121464706217649, 0.8107583221748409, 0.8129799795083197]}

