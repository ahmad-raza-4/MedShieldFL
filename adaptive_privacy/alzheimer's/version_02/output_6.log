nohup: ignoring input
01/30/2025 05:28:16:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/30/2025 05:28:16:DEBUG:ChannelConnectivity.IDLE
01/30/2025 05:28:16:DEBUG:ChannelConnectivity.CONNECTING
01/30/2025 05:28:16:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/30/2025 05:28:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:28:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 407c10f7-18e4-40a9-8d66-eb18ad026d99
01/30/2025 05:28:54:INFO:Received: train message 407c10f7-18e4-40a9-8d66-eb18ad026d99
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:29:28:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:30:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:30:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 48000432-7bc0-43b5-9c6d-9eb8841cb639
01/30/2025 05:30:38:INFO:Received: evaluate message 48000432-7bc0-43b5-9c6d-9eb8841cb639
[92mINFO [0m:      Sent reply
01/30/2025 05:30:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:31:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:31:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7a435b59-37d5-410e-bd4b-f6ac1619ca7d
01/30/2025 05:31:32:INFO:Received: train message 7a435b59-37d5-410e-bd4b-f6ac1619ca7d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:32:06:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:33:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:33:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0ecb581a-b6fe-4bb8-a130-6bfd549e75ac
01/30/2025 05:33:30:INFO:Received: evaluate message 0ecb581a-b6fe-4bb8-a130-6bfd549e75ac
[92mINFO [0m:      Sent reply
01/30/2025 05:33:35:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:34:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:34:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 16917f49-6be9-4008-8d7a-c76a59d30ca7
01/30/2025 05:34:51:INFO:Received: train message 16917f49-6be9-4008-8d7a-c76a59d30ca7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:35:27:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:36:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:36:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4a89f144-d02d-48e8-af36-d73d5035ff68
01/30/2025 05:36:42:INFO:Received: evaluate message 4a89f144-d02d-48e8-af36-d73d5035ff68
[92mINFO [0m:      Sent reply
01/30/2025 05:36:48:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:37:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:37:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 458e919f-37c6-4e2c-9f37-7b3f70df210f
01/30/2025 05:37:39:INFO:Received: train message 458e919f-37c6-4e2c-9f37-7b3f70df210f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:38:17:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:39:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:39:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 51da536b-893b-442b-9233-e6249396ddb3
01/30/2025 05:39:35:INFO:Received: evaluate message 51da536b-893b-442b-9233-e6249396ddb3
[92mINFO [0m:      Sent reply
01/30/2025 05:39:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:40:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:40:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3840601d-2d0d-49bb-b729-4608e73f82b0
01/30/2025 05:40:07:INFO:Received: train message 3840601d-2d0d-49bb-b729-4608e73f82b0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:40:38:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:42:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:42:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 58aac6aa-ac9a-4f08-a0bf-40afe4bddc55
01/30/2025 05:42:18:INFO:Received: evaluate message 58aac6aa-ac9a-4f08-a0bf-40afe4bddc55
[92mINFO [0m:      Sent reply
01/30/2025 05:42:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:43:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:43:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bd5ac610-d79c-4fbb-ba5a-9204f57bac70
01/30/2025 05:43:09:INFO:Received: train message bd5ac610-d79c-4fbb-ba5a-9204f57bac70
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:43:44:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:44:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:44:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 063a64c3-eed3-46ed-bd13-32ab6997a9ba
01/30/2025 05:44:53:INFO:Received: evaluate message 063a64c3-eed3-46ed-bd13-32ab6997a9ba
[92mINFO [0m:      Sent reply
01/30/2025 05:44:59:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:45:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:45:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2a3c6a2d-d99d-4822-93d0-5c651d922ae7
01/30/2025 05:45:55:INFO:Received: train message 2a3c6a2d-d99d-4822-93d0-5c651d922ae7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:46:26:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:47:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:47:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5a5c41c3-ee8f-4e56-918f-3558d16ee2a9
01/30/2025 05:47:43:INFO:Received: evaluate message 5a5c41c3-ee8f-4e56-918f-3558d16ee2a9
[92mINFO [0m:      Sent reply
01/30/2025 05:47:49:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:48:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:48:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e370c043-b3d3-4222-b3e9-a675cc36f13a
01/30/2025 05:48:38:INFO:Received: train message e370c043-b3d3-4222-b3e9-a675cc36f13a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:49:14:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:50:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:50:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 779aada7-fc63-40eb-894c-f4fb953f4d84
01/30/2025 05:50:17:INFO:Received: evaluate message 779aada7-fc63-40eb-894c-f4fb953f4d84
[92mINFO [0m:      Sent reply
01/30/2025 05:50:22:INFO:Sent reply
