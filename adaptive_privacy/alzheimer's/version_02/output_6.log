nohup: ignoring input
01/31/2025 07:59:37:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/31/2025 07:59:37:DEBUG:ChannelConnectivity.IDLE
01/31/2025 07:59:37:DEBUG:ChannelConnectivity.CONNECTING
01/31/2025 07:59:37:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/31/2025 07:59:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:59:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message a36a0937-735d-4115-a550-7a5ac0729edb
01/31/2025 07:59:37:INFO:Received: get_parameters message a36a0937-735d-4115-a550-7a5ac0729edb
[92mINFO [0m:      Sent reply
01/31/2025 07:59:42:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:00:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:00:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f816e835-16a5-4e0b-b367-c29a726f8285
01/31/2025 08:00:03:INFO:Received: train message f816e835-16a5-4e0b-b367-c29a726f8285
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:00:25:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:01:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:01:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9e756536-f969-428a-a29d-54e9fe53de5f
01/31/2025 08:01:27:INFO:Received: evaluate message 9e756536-f969-428a-a29d-54e9fe53de5f
[92mINFO [0m:      Sent reply
01/31/2025 08:01:31:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:02:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:02:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fc7d5a3f-7e6f-4efc-a430-93c35ea926b1
01/31/2025 08:02:04:INFO:Received: train message fc7d5a3f-7e6f-4efc-a430-93c35ea926b1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:02:31:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:03:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:03:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1e187fd5-1f71-429a-ae69-54d89cbc281b
01/31/2025 08:03:07:INFO:Received: evaluate message 1e187fd5-1f71-429a-ae69-54d89cbc281b
[92mINFO [0m:      Sent reply
01/31/2025 08:03:10:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:03:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:03:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3006c689-3dd9-47c5-a32f-1eb95b641976
01/31/2025 08:03:46:INFO:Received: train message 3006c689-3dd9-47c5-a32f-1eb95b641976
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:04:11:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:04:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:04:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d389cf56-5042-4ae4-8f35-9d3a1e234d61
01/31/2025 08:04:57:INFO:Received: evaluate message d389cf56-5042-4ae4-8f35-9d3a1e234d61
[92mINFO [0m:      Sent reply
01/31/2025 08:04:59:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:05:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:05:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5b8fe7ab-96f7-4a0d-8a17-249536401c65
01/31/2025 08:05:20:INFO:Received: train message 5b8fe7ab-96f7-4a0d-8a17-249536401c65
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:05:44:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:06:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:06:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d6d98be2-a558-4097-a071-68ae98a545bb
01/31/2025 08:06:32:INFO:Received: evaluate message d6d98be2-a558-4097-a071-68ae98a545bb
[92mINFO [0m:      Sent reply
01/31/2025 08:06:34:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:07:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:07:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 07c6ca14-1f46-43e6-9553-badd625ff295
01/31/2025 08:07:05:INFO:Received: train message 07c6ca14-1f46-43e6-9553-badd625ff295
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:07:28:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:08:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:08:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1a3196bd-ee06-4278-ba93-3cc41d86d2f5
01/31/2025 08:08:06:INFO:Received: evaluate message 1a3196bd-ee06-4278-ba93-3cc41d86d2f5
[92mINFO [0m:      Sent reply
01/31/2025 08:08:08:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:08:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:08:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 77b948df-f3bb-4f27-9e5d-8fee6b52f969
01/31/2025 08:08:49:INFO:Received: train message 77b948df-f3bb-4f27-9e5d-8fee6b52f969
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:09:11:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:10:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:10:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 644f9ceb-59c0-4bf6-85a3-065b151a5b2f
01/31/2025 08:10:00:INFO:Received: evaluate message 644f9ceb-59c0-4bf6-85a3-065b151a5b2f
[92mINFO [0m:      Sent reply
01/31/2025 08:10:05:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:10:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:10:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e6907286-0ff2-4d54-87eb-5cb1845f6a0f
01/31/2025 08:10:20:INFO:Received: train message e6907286-0ff2-4d54-87eb-5cb1845f6a0f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:10:39:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:11:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:11:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 757664f2-e708-4f05-b340-0b0529a9ef6d
01/31/2025 08:11:34:INFO:Received: evaluate message 757664f2-e708-4f05-b340-0b0529a9ef6d
[92mINFO [0m:      Sent reply
01/31/2025 08:11:36:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:12:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:12:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 99430bb5-a6f4-4218-aa42-9875a3016942
01/31/2025 08:12:13:INFO:Received: train message 99430bb5-a6f4-4218-aa42-9875a3016942
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:12:34:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:13:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:13:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fa67bc80-20c0-451e-8e54-68d9f5282b3b
01/31/2025 08:13:16:INFO:Received: evaluate message fa67bc80-20c0-451e-8e54-68d9f5282b3b
[92mINFO [0m:      Sent reply
01/31/2025 08:13:19:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:14:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:14:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 38455fda-b609-4600-a825-d1e583c57106
01/31/2025 08:14:01:INFO:Received: train message 38455fda-b609-4600-a825-d1e583c57106
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:14:25:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:15:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:15:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 39f23292-378d-480e-8f97-8288d088054c
01/31/2025 08:15:13:INFO:Received: evaluate message 39f23292-378d-480e-8f97-8288d088054c
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969], 'accuracy': [0.5183737294761532], 'auc': [0.7379489183581498]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479], 'accuracy': [0.5183737294761532, 0.5222830336200156], 'auc': [0.7379489183581498, 0.7546630873005361]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 08:15:15:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:15:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:15:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e4584742-16d6-423c-a53c-2cf02aa7b63e
01/31/2025 08:15:45:INFO:Received: train message e4584742-16d6-423c-a53c-2cf02aa7b63e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:16:10:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:16:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:16:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 78a2eb66-0dd1-43e0-970b-e4c4fbb8de25
01/31/2025 08:16:51:INFO:Received: evaluate message 78a2eb66-0dd1-43e0-970b-e4c4fbb8de25
[92mINFO [0m:      Sent reply
01/31/2025 08:16:55:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:17:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:17:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 239f1f07-4c92-4900-aa3a-b0775a4a6558
01/31/2025 08:17:40:INFO:Received: train message 239f1f07-4c92-4900-aa3a-b0775a4a6558
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:18:07:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:19:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:19:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 86821e17-b2ee-4b44-a40a-e075f8f2724c
01/31/2025 08:19:06:INFO:Received: evaluate message 86821e17-b2ee-4b44-a40a-e075f8f2724c
[92mINFO [0m:      Sent reply
01/31/2025 08:19:09:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:19:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:19:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 45a1ad78-3a18-4738-9c2c-c1dc37fad138
01/31/2025 08:19:48:INFO:Received: train message 45a1ad78-3a18-4738-9c2c-c1dc37fad138
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:20:17:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:21:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:21:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e733043f-1b3c-41c6-a3f3-796ea4f7ef10
01/31/2025 08:21:21:INFO:Received: evaluate message e733043f-1b3c-41c6-a3f3-796ea4f7ef10
[92mINFO [0m:      Sent reply
01/31/2025 08:21:24:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:21:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:21:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b7ac2017-abf8-4fe8-b9a6-a88a20c8e4bc
01/31/2025 08:21:49:INFO:Received: train message b7ac2017-abf8-4fe8-b9a6-a88a20c8e4bc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:22:10:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:22:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:22:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 83141a15-b9ea-4e70-89e4-cdb3a53e1325
01/31/2025 08:22:58:INFO:Received: evaluate message 83141a15-b9ea-4e70-89e4-cdb3a53e1325
[92mINFO [0m:      Sent reply
01/31/2025 08:22:59:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:23:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:23:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7e867bb0-c446-4663-a10d-a5008937667a
01/31/2025 08:23:20:INFO:Received: train message 7e867bb0-c446-4663-a10d-a5008937667a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:23:40:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:24:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:24:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ece9381f-b354-4f5d-91b1-1e7f282291fc
01/31/2025 08:24:16:INFO:Received: evaluate message ece9381f-b354-4f5d-91b1-1e7f282291fc
[92mINFO [0m:      Sent reply
01/31/2025 08:24:18:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:25:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:25:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 21209574-44b2-4c69-af3c-f6447a0228bc
01/31/2025 08:25:02:INFO:Received: train message 21209574-44b2-4c69-af3c-f6447a0228bc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:25:22:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:25:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:25:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e9161c30-f6c3-4f74-a034-36de21ffa184
01/31/2025 08:25:56:INFO:Received: evaluate message e9161c30-f6c3-4f74-a034-36de21ffa184

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 08:25:59:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:26:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:26:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4ca59167-3589-43a8-9d6f-2abbafdc8bf1
01/31/2025 08:26:46:INFO:Received: train message 4ca59167-3589-43a8-9d6f-2abbafdc8bf1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:27:06:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:27:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:27:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 19741c36-d71b-4708-94ee-834eb749135b
01/31/2025 08:27:30:INFO:Received: evaluate message 19741c36-d71b-4708-94ee-834eb749135b
[92mINFO [0m:      Sent reply
01/31/2025 08:27:33:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:28:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:28:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 54252b48-9e75-40cd-ad7c-2131a8bd8040
01/31/2025 08:28:07:INFO:Received: train message 54252b48-9e75-40cd-ad7c-2131a8bd8040
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:28:29:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:29:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:29:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b63aec84-145a-4f6f-a0c4-bb98efc7e2ab
01/31/2025 08:29:25:INFO:Received: evaluate message b63aec84-145a-4f6f-a0c4-bb98efc7e2ab
[92mINFO [0m:      Sent reply
01/31/2025 08:29:28:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:30:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:30:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ccfa78e-4c8d-43ab-978d-275510708bcb
01/31/2025 08:30:01:INFO:Received: train message 8ccfa78e-4c8d-43ab-978d-275510708bcb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:30:21:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:30:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:30:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c3a5acf5-d13f-4cd3-92ee-0d2ffaf42769
01/31/2025 08:30:50:INFO:Received: evaluate message c3a5acf5-d13f-4cd3-92ee-0d2ffaf42769
[92mINFO [0m:      Sent reply
01/31/2025 08:30:52:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:31:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:31:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c37e37be-6477-4352-9e9f-3999235561b8
01/31/2025 08:31:36:INFO:Received: train message c37e37be-6477-4352-9e9f-3999235561b8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:31:57:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:32:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:32:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1e179787-e736-41b7-80dc-d607c674ef55
01/31/2025 08:32:20:INFO:Received: evaluate message 1e179787-e736-41b7-80dc-d607c674ef55
[92mINFO [0m:      Sent reply
01/31/2025 08:32:22:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:33:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:33:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4f227609-fca4-423b-bde3-68953a90aeb6
01/31/2025 08:33:15:INFO:Received: train message 4f227609-fca4-423b-bde3-68953a90aeb6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:33:42:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:34:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:34:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e49d444e-cee3-44f1-989b-fd5296e706ee
01/31/2025 08:34:20:INFO:Received: evaluate message e49d444e-cee3-44f1-989b-fd5296e706ee

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602, 1.0375629482649564], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5903049257232212], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412, 0.8001856626611777]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 08:34:24:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:35:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:35:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 146ad5bd-486e-4df6-8267-0ad56d218002
01/31/2025 08:35:30:INFO:Received: train message 146ad5bd-486e-4df6-8267-0ad56d218002
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:35:57:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:36:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:36:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8cf7c1cc-0876-493c-b22f-8c767e22f4b8
01/31/2025 08:36:57:INFO:Received: evaluate message 8cf7c1cc-0876-493c-b22f-8c767e22f4b8
[92mINFO [0m:      Sent reply
01/31/2025 08:37:01:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:37:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:37:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 68e6ca7a-3c1d-45ea-9f05-dc83247e9324
01/31/2025 08:37:31:INFO:Received: train message 68e6ca7a-3c1d-45ea-9f05-dc83247e9324
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:37:49:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:38:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:38:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 70c8a666-e86d-40c4-b903-c68a4aadb344
01/31/2025 08:38:41:INFO:Received: evaluate message 70c8a666-e86d-40c4-b903-c68a4aadb344
[92mINFO [0m:      Sent reply
01/31/2025 08:38:44:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:39:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:39:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1b00774e-6570-44b8-bd39-a475c71bec7d
01/31/2025 08:39:03:INFO:Received: train message 1b00774e-6570-44b8-bd39-a475c71bec7d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:39:22:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:40:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:40:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9f7cc3ca-9dd9-40e7-a004-45ab4b538d3d
01/31/2025 08:40:04:INFO:Received: evaluate message 9f7cc3ca-9dd9-40e7-a004-45ab4b538d3d
[92mINFO [0m:      Sent reply
01/31/2025 08:40:06:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:40:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:40:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ff778fe9-93a7-448e-9505-12c4da062ef2
01/31/2025 08:40:51:INFO:Received: train message ff778fe9-93a7-448e-9505-12c4da062ef2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:41:13:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:41:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:41:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6ededd52-c7d4-40b9-886a-e4f642995576
01/31/2025 08:41:53:INFO:Received: evaluate message 6ededd52-c7d4-40b9-886a-e4f642995576

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602, 1.0375629482649564, 1.1057447312025468], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5903049257232212, 0.5777951524628616], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412, 0.8001856626611777, 0.8009040786877405]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602, 1.0375629482649564, 1.1057447312025468, 1.0368838819607429], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5903049257232212, 0.5777951524628616, 0.584831899921814], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412, 0.8001856626611777, 0.8009040786877405, 0.8038256400927525]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602, 1.0375629482649564, 1.1057447312025468, 1.0368838819607429, 1.0403738920347498], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5903049257232212, 0.5777951524628616, 0.584831899921814, 0.584831899921814], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412, 0.8001856626611777, 0.8009040786877405, 0.8038256400927525, 0.8044059903580408]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602, 1.0375629482649564, 1.1057447312025468, 1.0368838819607429, 1.0403738920347498, 1.0356549616900155], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5903049257232212, 0.5777951524628616, 0.584831899921814, 0.584831899921814, 0.5910867865519938], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412, 0.8001856626611777, 0.8009040786877405, 0.8038256400927525, 0.8044059903580408, 0.8047467449463152]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 08:41:55:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:42:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:42:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7a4f4764-312f-4db7-841b-f6b06830a74d
01/31/2025 08:42:28:INFO:Received: train message 7a4f4764-312f-4db7-841b-f6b06830a74d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:42:49:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:43:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:43:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d2fafe81-4a2d-4049-a76e-079850ab2baa
01/31/2025 08:43:24:INFO:Received: evaluate message d2fafe81-4a2d-4049-a76e-079850ab2baa
[92mINFO [0m:      Sent reply
01/31/2025 08:43:26:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:44:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:44:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f260979f-4c36-4795-a719-99f199397857
01/31/2025 08:44:03:INFO:Received: train message f260979f-4c36-4795-a719-99f199397857
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:44:23:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:45:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:45:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 91c04df8-2826-4291-a41e-bf85b7e8b5a6
01/31/2025 08:45:02:INFO:Received: evaluate message 91c04df8-2826-4291-a41e-bf85b7e8b5a6
[92mINFO [0m:      Sent reply
01/31/2025 08:45:05:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:45:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:45:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6b7b27f7-eff8-4740-a8b8-9c47788b2ae9
01/31/2025 08:45:35:INFO:Received: train message 6b7b27f7-eff8-4740-a8b8-9c47788b2ae9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:45:56:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:46:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:46:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ca6c14ba-71fc-4a62-8666-8ef7812b5bfd
01/31/2025 08:46:44:INFO:Received: evaluate message ca6c14ba-71fc-4a62-8666-8ef7812b5bfd
[92mINFO [0m:      Sent reply
01/31/2025 08:46:46:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:47:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:47:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3c54ec6d-fb37-4f6d-bde4-537d3a388277
01/31/2025 08:47:26:INFO:Received: train message 3c54ec6d-fb37-4f6d-bde4-537d3a388277

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602, 1.0375629482649564, 1.1057447312025468, 1.0368838819607429, 1.0403738920347498, 1.0356549616900155, 1.035876474686206], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5903049257232212, 0.5777951524628616, 0.584831899921814, 0.584831899921814, 0.5910867865519938, 0.5903049257232212], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412, 0.8001856626611777, 0.8009040786877405, 0.8038256400927525, 0.8044059903580408, 0.8047467449463152, 0.8058870981715842]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602, 1.0375629482649564, 1.1057447312025468, 1.0368838819607429, 1.0403738920347498, 1.0356549616900155, 1.035876474686206, 0.9869848173330127], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5903049257232212, 0.5777951524628616, 0.584831899921814, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412, 0.8001856626611777, 0.8009040786877405, 0.8038256400927525, 0.8044059903580408, 0.8047467449463152, 0.8058870981715842, 0.8094334655940406]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602, 1.0375629482649564, 1.1057447312025468, 1.0368838819607429, 1.0403738920347498, 1.0356549616900155, 1.035876474686206, 0.9869848173330127, 1.0871801342788947], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5903049257232212, 0.5777951524628616, 0.584831899921814, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461, 0.5863956215793589], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412, 0.8001856626611777, 0.8009040786877405, 0.8038256400927525, 0.8044059903580408, 0.8047467449463152, 0.8058870981715842, 0.8094334655940406, 0.8096226415787721]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602, 1.0375629482649564, 1.1057447312025468, 1.0368838819607429, 1.0403738920347498, 1.0356549616900155, 1.035876474686206, 0.9869848173330127, 1.0871801342788947, 1.0335426636279048], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5903049257232212, 0.5777951524628616, 0.584831899921814, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461, 0.5863956215793589, 0.5942142298670836], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412, 0.8001856626611777, 0.8009040786877405, 0.8038256400927525, 0.8044059903580408, 0.8047467449463152, 0.8058870981715842, 0.8094334655940406, 0.8096226415787721, 0.8109623802384914]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:47:54:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:49:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:49:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3cb42357-1a9a-4b93-b9a0-c0367f94efff
01/31/2025 08:49:04:INFO:Received: evaluate message 3cb42357-1a9a-4b93-b9a0-c0367f94efff
[92mINFO [0m:      Sent reply
01/31/2025 08:49:09:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:49:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:49:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 297e3c8e-5e81-44f7-8d6e-94df37565a45
01/31/2025 08:49:44:INFO:Received: train message 297e3c8e-5e81-44f7-8d6e-94df37565a45
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:50:11:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:51:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:51:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6016ef8e-0789-45e5-809f-99944af1f5a8
01/31/2025 08:51:25:INFO:Received: evaluate message 6016ef8e-0789-45e5-809f-99944af1f5a8
[92mINFO [0m:      Sent reply
01/31/2025 08:51:28:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:51:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:51:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e8e5c5e6-2dcf-496c-8f19-93e4dc8fdcff
01/31/2025 08:51:46:INFO:Received: train message e8e5c5e6-2dcf-496c-8f19-93e4dc8fdcff
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 08:52:05:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:53:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:53:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ec89de04-e838-45aa-b572-be35d073fd81
01/31/2025 08:53:04:INFO:Received: evaluate message ec89de04-e838-45aa-b572-be35d073fd81
[92mINFO [0m:      Sent reply
01/31/2025 08:53:07:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 08:53:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 08:53:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 531b6da2-9061-4cdd-b69e-168b2e63f7a1
01/31/2025 08:53:07:INFO:Received: reconnect message 531b6da2-9061-4cdd-b69e-168b2e63f7a1
01/31/2025 08:53:07:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/31/2025 08:53:07:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602, 1.0375629482649564, 1.1057447312025468, 1.0368838819607429, 1.0403738920347498, 1.0356549616900155, 1.035876474686206, 0.9869848173330127, 1.0871801342788947, 1.0335426636279048, 1.0066990948357184], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5903049257232212, 0.5777951524628616, 0.584831899921814, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461, 0.5863956215793589, 0.5942142298670836, 0.596559812353401], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412, 0.8001856626611777, 0.8009040786877405, 0.8038256400927525, 0.8044059903580408, 0.8047467449463152, 0.8058870981715842, 0.8094334655940406, 0.8096226415787721, 0.8109623802384914, 0.8120829695942571]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602, 1.0375629482649564, 1.1057447312025468, 1.0368838819607429, 1.0403738920347498, 1.0356549616900155, 1.035876474686206, 0.9869848173330127, 1.0871801342788947, 1.0335426636279048, 1.0066990948357184, 1.0072884780993399], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5903049257232212, 0.5777951524628616, 0.584831899921814, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461, 0.5863956215793589, 0.5942142298670836, 0.596559812353401, 0.5989053948397185], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412, 0.8001856626611777, 0.8009040786877405, 0.8038256400927525, 0.8044059903580408, 0.8047467449463152, 0.8058870981715842, 0.8094334655940406, 0.8096226415787721, 0.8109623802384914, 0.8120829695942571, 0.8107866127940817]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602, 1.0375629482649564, 1.1057447312025468, 1.0368838819607429, 1.0403738920347498, 1.0356549616900155, 1.035876474686206, 0.9869848173330127, 1.0871801342788947, 1.0335426636279048, 1.0066990948357184, 1.0072884780993399, 1.0146706239891947], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5903049257232212, 0.5777951524628616, 0.584831899921814, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461, 0.5863956215793589, 0.5942142298670836, 0.596559812353401, 0.5989053948397185, 0.5989053948397185], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412, 0.8001856626611777, 0.8009040786877405, 0.8038256400927525, 0.8044059903580408, 0.8047467449463152, 0.8058870981715842, 0.8094334655940406, 0.8096226415787721, 0.8109623802384914, 0.8120829695942571, 0.8107866127940817, 0.8129905939058173]}



Final client history:
{'loss': [1.064968405056969, 1.143424023586479, 1.1448232881532598, 1.0869922246012118, 1.057983738915635, 1.085904567338602, 1.07825235923442, 1.0753258308271956, 1.0734578944259179, 1.071774921909359, 1.0084182970033575, 1.0496187529310386, 1.032506156051597, 1.0391733372369905, 1.0435534100032953, 1.0433790931205809, 1.0687411096153825, 1.015168486860602, 1.0375629482649564, 1.1057447312025468, 1.0368838819607429, 1.0403738920347498, 1.0356549616900155, 1.035876474686206, 0.9869848173330127, 1.0871801342788947, 1.0335426636279048, 1.0066990948357184, 1.0072884780993399, 1.0146706239891947], 'accuracy': [0.5183737294761532, 0.5222830336200156, 0.5340109460516028, 0.544175136825645, 0.5551211884284597, 0.5590304925723222, 0.5598123534010946, 0.5590304925723222, 0.565285379202502, 0.562157935887412, 0.5691946833463644, 0.5770132916340891, 0.5793588741204065, 0.5731039874902267, 0.5863956215793589, 0.584831899921814, 0.5770132916340891, 0.5824863174354965, 0.5903049257232212, 0.5777951524628616, 0.584831899921814, 0.584831899921814, 0.5910867865519938, 0.5903049257232212, 0.5981235340109461, 0.5863956215793589, 0.5942142298670836, 0.596559812353401, 0.5989053948397185, 0.5989053948397185], 'auc': [0.7379489183581498, 0.7546630873005361, 0.7631130250551742, 0.7719102168620879, 0.7776408559894529, 0.7804386268726486, 0.7829072761962275, 0.784628409367552, 0.7860178232000494, 0.7885159101710704, 0.7927181606166256, 0.7949546377789054, 0.7950435856227291, 0.7938107331116697, 0.7958251580437732, 0.7975262536933632, 0.7990436347815142, 0.8006268880018412, 0.8001856626611777, 0.8009040786877405, 0.8038256400927525, 0.8044059903580408, 0.8047467449463152, 0.8058870981715842, 0.8094334655940406, 0.8096226415787721, 0.8109623802384914, 0.8120829695942571, 0.8107866127940817, 0.8129905939058173]}

