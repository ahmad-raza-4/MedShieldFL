nohup: ignoring input
01/31/2025 07:00:11:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/31/2025 07:00:11:DEBUG:ChannelConnectivity.IDLE
01/31/2025 07:00:11:DEBUG:ChannelConnectivity.CONNECTING
01/31/2025 07:00:11:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/31/2025 07:00:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:00:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message 8737a177-e184-4e6e-ab1c-cff35e01bab5
01/31/2025 07:00:11:INFO:Received: get_parameters message 8737a177-e184-4e6e-ab1c-cff35e01bab5
[92mINFO [0m:      Sent reply
01/31/2025 07:00:16:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:00:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:00:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6fd836ce-d435-4953-a066-cc3e0f02c996
01/31/2025 07:00:47:INFO:Received: train message 6fd836ce-d435-4953-a066-cc3e0f02c996
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:01:11:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:02:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:02:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 59317c59-803d-4af4-88d0-c5db792655c3
01/31/2025 07:02:03:INFO:Received: evaluate message 59317c59-803d-4af4-88d0-c5db792655c3
[92mINFO [0m:      Sent reply
01/31/2025 07:02:09:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:02:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:02:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0f832c46-3fba-435d-adfb-e8bbd19cdbae
01/31/2025 07:02:52:INFO:Received: train message 0f832c46-3fba-435d-adfb-e8bbd19cdbae
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:03:12:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:03:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:03:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 13bf048e-8643-4d51-928b-2cf18768eef7
01/31/2025 07:03:58:INFO:Received: evaluate message 13bf048e-8643-4d51-928b-2cf18768eef7
[92mINFO [0m:      Sent reply
01/31/2025 07:04:02:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:04:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:04:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a781c34f-b1c0-46e8-920c-cbac21905c52
01/31/2025 07:04:34:INFO:Received: train message a781c34f-b1c0-46e8-920c-cbac21905c52
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:04:55:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:05:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:05:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a30b0c04-5a68-4738-8067-c7f9840e4054
01/31/2025 07:05:27:INFO:Received: evaluate message a30b0c04-5a68-4738-8067-c7f9840e4054
[92mINFO [0m:      Sent reply
01/31/2025 07:05:29:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:06:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:06:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1f30059e-d8bd-4230-8d10-1d0704a17bf3
01/31/2025 07:06:23:INFO:Received: train message 1f30059e-d8bd-4230-8d10-1d0704a17bf3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:06:40:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:07:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:07:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 66bc1127-502b-4317-a938-75677d0fc369
01/31/2025 07:07:18:INFO:Received: evaluate message 66bc1127-502b-4317-a938-75677d0fc369
[92mINFO [0m:      Sent reply
01/31/2025 07:07:20:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:08:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:08:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d639e006-e0a6-4edd-a489-1e410d1220f0
01/31/2025 07:08:03:INFO:Received: train message d639e006-e0a6-4edd-a489-1e410d1220f0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:08:21:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:09:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:09:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4ca7db30-0c0a-43ad-9b88-a367ed1b59d4
01/31/2025 07:09:06:INFO:Received: evaluate message 4ca7db30-0c0a-43ad-9b88-a367ed1b59d4
[92mINFO [0m:      Sent reply
01/31/2025 07:09:09:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:09:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:09:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2793438c-d6cb-4dd4-865a-1d34fe73fbaa
01/31/2025 07:09:38:INFO:Received: train message 2793438c-d6cb-4dd4-865a-1d34fe73fbaa
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:10:01:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:11:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:11:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6f14924c-2bb7-4bfd-ad70-5c3021b339a7
01/31/2025 07:11:07:INFO:Received: evaluate message 6f14924c-2bb7-4bfd-ad70-5c3021b339a7
[92mINFO [0m:      Sent reply
01/31/2025 07:11:10:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:12:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:12:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1d3b98f0-c03d-4a0b-9d5d-e7b1f835cb81
01/31/2025 07:12:07:INFO:Received: train message 1d3b98f0-c03d-4a0b-9d5d-e7b1f835cb81
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:12:29:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:13:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:13:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message be34452e-82dd-4b86-90af-1010e8c42714
01/31/2025 07:13:14:INFO:Received: evaluate message be34452e-82dd-4b86-90af-1010e8c42714
[92mINFO [0m:      Sent reply
01/31/2025 07:13:17:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:14:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:14:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 671e9a73-3202-4738-968b-387e287930d8
01/31/2025 07:14:21:INFO:Received: train message 671e9a73-3202-4738-968b-387e287930d8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:14:44:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:15:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:15:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 987993c6-e142-4712-951e-391662bc3110
01/31/2025 07:15:24:INFO:Received: evaluate message 987993c6-e142-4712-951e-391662bc3110
[92mINFO [0m:      Sent reply
01/31/2025 07:15:27:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:15:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:15:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 971d1290-0544-4b37-a5be-f5a66d282a6f
01/31/2025 07:15:59:INFO:Received: train message 971d1290-0544-4b37-a5be-f5a66d282a6f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:16:17:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:17:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:17:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7f2c5e69-1a43-4813-a973-ef2846f6934e
01/31/2025 07:17:02:INFO:Received: evaluate message 7f2c5e69-1a43-4813-a973-ef2846f6934e
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148], 'accuracy': [0.5207193119624707], 'auc': [0.7293562379614749]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048], 'accuracy': [0.5207193119624707, 0.5254104769351056], 'auc': [0.7293562379614749, 0.7490635742179705]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 07:17:05:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:17:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:17:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2b24bdb6-23ac-4d1b-abe5-b0053b3941a5
01/31/2025 07:17:56:INFO:Received: train message 2b24bdb6-23ac-4d1b-abe5-b0053b3941a5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:18:15:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:18:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:18:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 76de8b95-1611-4ccc-b151-ecf85f8384f6
01/31/2025 07:18:58:INFO:Received: evaluate message 76de8b95-1611-4ccc-b151-ecf85f8384f6
[92mINFO [0m:      Sent reply
01/31/2025 07:19:03:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:19:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:19:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 10f54db6-58b1-4e68-be80-a3877920471c
01/31/2025 07:19:32:INFO:Received: train message 10f54db6-58b1-4e68-be80-a3877920471c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:19:53:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:20:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:20:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b7c27464-6592-4121-8128-2bfb628a8aee
01/31/2025 07:20:34:INFO:Received: evaluate message b7c27464-6592-4121-8128-2bfb628a8aee
[92mINFO [0m:      Sent reply
01/31/2025 07:20:36:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:21:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:21:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e2f6e8b5-85cf-4cdc-859c-d9d2a34f9282
01/31/2025 07:21:06:INFO:Received: train message e2f6e8b5-85cf-4cdc-859c-d9d2a34f9282
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:21:23:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:22:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:22:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2191fe73-888e-499b-8a9d-c78ababfc222
01/31/2025 07:22:18:INFO:Received: evaluate message 2191fe73-888e-499b-8a9d-c78ababfc222
[92mINFO [0m:      Sent reply
01/31/2025 07:22:20:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:22:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:22:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 07a10a79-6c9f-4d5e-9ad4-b14b9c25320c
01/31/2025 07:22:40:INFO:Received: train message 07a10a79-6c9f-4d5e-9ad4-b14b9c25320c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:22:57:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:24:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:24:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 61b0d966-522d-4bc6-993f-bb7daca1c21f
01/31/2025 07:24:08:INFO:Received: evaluate message 61b0d966-522d-4bc6-993f-bb7daca1c21f
[92mINFO [0m:      Sent reply
01/31/2025 07:24:14:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:24:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:24:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 728bbb83-decf-45b6-8f9c-b18899e7098b
01/31/2025 07:24:44:INFO:Received: train message 728bbb83-decf-45b6-8f9c-b18899e7098b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:25:08:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:26:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:26:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c3803283-07cc-4914-a41d-ae288b433ea9
01/31/2025 07:26:12:INFO:Received: evaluate message c3803283-07cc-4914-a41d-ae288b433ea9
[92mINFO [0m:      Sent reply
01/31/2025 07:26:15:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:26:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:26:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5ba8779b-3a29-4cd0-8a07-c730351fced2
01/31/2025 07:26:49:INFO:Received: train message 5ba8779b-3a29-4cd0-8a07-c730351fced2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:27:12:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:27:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:27:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 86b9cfb5-4ba8-4fff-a118-b01693359b71
01/31/2025 07:27:48:INFO:Received: evaluate message 86b9cfb5-4ba8-4fff-a118-b01693359b71

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 07:27:50:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:28:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:28:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9d2fa0d9-7546-4c1d-88c1-3af7054eabd1
01/31/2025 07:28:49:INFO:Received: train message 9d2fa0d9-7546-4c1d-88c1-3af7054eabd1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:29:14:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:30:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:30:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ba33d634-f9e3-49ea-a91b-90d1eaa797b8
01/31/2025 07:30:08:INFO:Received: evaluate message ba33d634-f9e3-49ea-a91b-90d1eaa797b8
[92mINFO [0m:      Sent reply
01/31/2025 07:30:11:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:30:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:30:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d3f70508-7bc3-460e-ae5d-061741609a6a
01/31/2025 07:30:39:INFO:Received: train message d3f70508-7bc3-460e-ae5d-061741609a6a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:31:02:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:32:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:32:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3416a74a-e0ed-499a-bb49-3093f32e818d
01/31/2025 07:32:02:INFO:Received: evaluate message 3416a74a-e0ed-499a-bb49-3093f32e818d
[92mINFO [0m:      Sent reply
01/31/2025 07:32:04:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:32:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:32:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 600e8faa-bde7-4a77-89fc-e7c2e1f47a0e
01/31/2025 07:32:34:INFO:Received: train message 600e8faa-bde7-4a77-89fc-e7c2e1f47a0e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:32:54:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:33:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:33:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 49ac0de3-4484-4e7a-83eb-83e4bf9dc12a
01/31/2025 07:33:42:INFO:Received: evaluate message 49ac0de3-4484-4e7a-83eb-83e4bf9dc12a
[92mINFO [0m:      Sent reply
01/31/2025 07:33:45:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:34:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:34:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b3e94217-2680-4606-b01a-24ae6348c105
01/31/2025 07:34:18:INFO:Received: train message b3e94217-2680-4606-b01a-24ae6348c105
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:34:39:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:35:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:35:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9c1b75a5-b55d-41a5-9aef-cf58b6f6ca0f
01/31/2025 07:35:21:INFO:Received: evaluate message 9c1b75a5-b55d-41a5-9aef-cf58b6f6ca0f
[92mINFO [0m:      Sent reply
01/31/2025 07:35:23:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:35:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:35:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7add59f1-ad0a-49bb-a40a-c78f39061472
01/31/2025 07:35:55:INFO:Received: train message 7add59f1-ad0a-49bb-a40a-c78f39061472
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:36:14:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:36:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:36:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 892c9fa5-6e5a-4a7a-80c7-a8bb4b53facb
01/31/2025 07:36:52:INFO:Received: evaluate message 892c9fa5-6e5a-4a7a-80c7-a8bb4b53facb

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183, 1.0405050258882536], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724, 0.5887412040656763], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205, 0.7990243112220722]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 07:36:54:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:37:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:37:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4d9b041e-e97d-45d7-b881-38984d4bbee4
01/31/2025 07:37:24:INFO:Received: train message 4d9b041e-e97d-45d7-b881-38984d4bbee4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:37:40:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:38:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:38:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 40c26d9b-63cc-4c8b-9f9a-4a7bf322d68c
01/31/2025 07:38:32:INFO:Received: evaluate message 40c26d9b-63cc-4c8b-9f9a-4a7bf322d68c
[92mINFO [0m:      Sent reply
01/31/2025 07:38:35:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:39:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:39:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0d58a53a-64dd-46c2-aa00-13899f18a308
01/31/2025 07:39:00:INFO:Received: train message 0d58a53a-64dd-46c2-aa00-13899f18a308
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:39:16:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:40:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:40:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 99cd2bc2-a7d5-44fb-b707-3c9a1c70d45d
01/31/2025 07:40:23:INFO:Received: evaluate message 99cd2bc2-a7d5-44fb-b707-3c9a1c70d45d
[92mINFO [0m:      Sent reply
01/31/2025 07:40:26:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:40:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:40:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8c7125cd-50f3-4da9-bac8-975c0f2ae324
01/31/2025 07:40:39:INFO:Received: train message 8c7125cd-50f3-4da9-bac8-975c0f2ae324
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:40:56:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:42:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:42:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6820e644-6515-4fb7-be34-7822cef44de5
01/31/2025 07:42:01:INFO:Received: evaluate message 6820e644-6515-4fb7-be34-7822cef44de5
[92mINFO [0m:      Sent reply
01/31/2025 07:42:03:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:42:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:42:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f34a46d3-c313-4cc0-8110-932ff8ca5d65
01/31/2025 07:42:26:INFO:Received: train message f34a46d3-c313-4cc0-8110-932ff8ca5d65
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:42:44:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:43:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:43:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e3e47bbd-c5af-43aa-96e9-73ebb943bd39
01/31/2025 07:43:49:INFO:Received: evaluate message e3e47bbd-c5af-43aa-96e9-73ebb943bd39

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183, 1.0405050258882536, 1.106826137433861], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724, 0.5887412040656763, 0.5746677091477717], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205, 0.7990243112220722, 0.7997743321755959]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183, 1.0405050258882536, 1.106826137433861, 1.0391728530366315], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724, 0.5887412040656763, 0.5746677091477717, 0.5770132916340891], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205, 0.7990243112220722, 0.7997743321755959, 0.8030130072849428]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183, 1.0405050258882536, 1.106826137433861, 1.0391728530366315, 1.0422610729536663], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724, 0.5887412040656763, 0.5746677091477717, 0.5770132916340891, 0.5809225957779516], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205, 0.7990243112220722, 0.7997743321755959, 0.8030130072849428, 0.8035551513188014]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183, 1.0405050258882536, 1.106826137433861, 1.0391728530366315, 1.0422610729536663, 1.0371296816826612], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724, 0.5887412040656763, 0.5746677091477717, 0.5770132916340891, 0.5809225957779516, 0.5863956215793589], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205, 0.7990243112220722, 0.7997743321755959, 0.8030130072849428, 0.8035551513188014, 0.8039593552097188]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 07:43:52:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:44:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:44:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c44b2a7a-d08d-49a4-a8d8-6b70d23bbd6e
01/31/2025 07:44:32:INFO:Received: train message c44b2a7a-d08d-49a4-a8d8-6b70d23bbd6e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:44:56:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:45:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:45:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 44ddbfbb-3857-40ce-a2e4-5a77a2e619fd
01/31/2025 07:45:55:INFO:Received: evaluate message 44ddbfbb-3857-40ce-a2e4-5a77a2e619fd
[92mINFO [0m:      Sent reply
01/31/2025 07:45:58:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:46:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:46:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 74d06408-4c3d-4dfe-94ba-39bf19a99497
01/31/2025 07:46:29:INFO:Received: train message 74d06408-4c3d-4dfe-94ba-39bf19a99497
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:46:52:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:48:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:48:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2a66f69a-ba5e-430e-a33c-ca934c8a2a90
01/31/2025 07:48:15:INFO:Received: evaluate message 2a66f69a-ba5e-430e-a33c-ca934c8a2a90
[92mINFO [0m:      Sent reply
01/31/2025 07:48:18:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:48:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:48:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3059f2db-c0fc-41eb-bf92-0f9a72e5a74d
01/31/2025 07:48:46:INFO:Received: train message 3059f2db-c0fc-41eb-bf92-0f9a72e5a74d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:49:10:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:50:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:50:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 477d1423-9a69-457c-9cfc-245ed4ec67da
01/31/2025 07:50:14:INFO:Received: evaluate message 477d1423-9a69-457c-9cfc-245ed4ec67da
[92mINFO [0m:      Sent reply
01/31/2025 07:50:17:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:50:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:50:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message af528d66-d85a-4068-bbc1-ceae108d7a1b
01/31/2025 07:50:48:INFO:Received: train message af528d66-d85a-4068-bbc1-ceae108d7a1b

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183, 1.0405050258882536, 1.106826137433861, 1.0391728530366315, 1.0422610729536663, 1.0371296816826612, 1.0382036509879218], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724, 0.5887412040656763, 0.5746677091477717, 0.5770132916340891, 0.5809225957779516, 0.5863956215793589, 0.5879593432369038], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205, 0.7990243112220722, 0.7997743321755959, 0.8030130072849428, 0.8035551513188014, 0.8039593552097188, 0.8049911841168098]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183, 1.0405050258882536, 1.106826137433861, 1.0391728530366315, 1.0422610729536663, 1.0371296816826612, 1.0382036509879218, 0.9888424875682178], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724, 0.5887412040656763, 0.5746677091477717, 0.5770132916340891, 0.5809225957779516, 0.5863956215793589, 0.5879593432369038, 0.599687255668491], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205, 0.7990243112220722, 0.7997743321755959, 0.8030130072849428, 0.8035551513188014, 0.8039593552097188, 0.8049911841168098, 0.8087692278686957]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183, 1.0405050258882536, 1.106826137433861, 1.0391728530366315, 1.0422610729536663, 1.0371296816826612, 1.0382036509879218, 0.9888424875682178, 1.0889605326611516], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724, 0.5887412040656763, 0.5746677091477717, 0.5770132916340891, 0.5809225957779516, 0.5863956215793589, 0.5879593432369038, 0.599687255668491, 0.5793588741204065], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205, 0.7990243112220722, 0.7997743321755959, 0.8030130072849428, 0.8035551513188014, 0.8039593552097188, 0.8049911841168098, 0.8087692278686957, 0.8087532481085512]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183, 1.0405050258882536, 1.106826137433861, 1.0391728530366315, 1.0422610729536663, 1.0371296816826612, 1.0382036509879218, 0.9888424875682178, 1.0889605326611516, 1.0357900751671631], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724, 0.5887412040656763, 0.5746677091477717, 0.5770132916340891, 0.5809225957779516, 0.5863956215793589, 0.5879593432369038, 0.599687255668491, 0.5793588741204065, 0.5879593432369038], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205, 0.7990243112220722, 0.7997743321755959, 0.8030130072849428, 0.8035551513188014, 0.8039593552097188, 0.8049911841168098, 0.8087692278686957, 0.8087532481085512, 0.8101110881156719]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:51:17:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:52:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:52:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e531097f-250b-4d9a-855a-4f9d55fbc0bd
01/31/2025 07:52:15:INFO:Received: evaluate message e531097f-250b-4d9a-855a-4f9d55fbc0bd
[92mINFO [0m:      Sent reply
01/31/2025 07:52:17:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:52:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:52:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 993cbf04-2ed5-4745-867c-11bd1ff542e5
01/31/2025 07:52:39:INFO:Received: train message 993cbf04-2ed5-4745-867c-11bd1ff542e5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:52:57:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:53:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:53:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9ec6e098-bea0-4bdc-b659-6ada29e76998
01/31/2025 07:53:55:INFO:Received: evaluate message 9ec6e098-bea0-4bdc-b659-6ada29e76998
[92mINFO [0m:      Sent reply
01/31/2025 07:53:59:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:54:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:54:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7a21b054-79c1-4a89-a027-f6f0ff342f01
01/31/2025 07:54:29:INFO:Received: train message 7a21b054-79c1-4a89-a027-f6f0ff342f01
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 07:54:49:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:55:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:55:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c606ad31-0caa-4466-8da2-e7d904afc52e
01/31/2025 07:55:35:INFO:Received: evaluate message c606ad31-0caa-4466-8da2-e7d904afc52e
[92mINFO [0m:      Sent reply
01/31/2025 07:55:39:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 07:55:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 07:55:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 32896f26-292d-465e-9fa2-260d604ac0bd
01/31/2025 07:55:41:INFO:Received: reconnect message 32896f26-292d-465e-9fa2-260d604ac0bd
01/31/2025 07:55:41:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/31/2025 07:55:41:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183, 1.0405050258882536, 1.106826137433861, 1.0391728530366315, 1.0422610729536663, 1.0371296816826612, 1.0382036509879218, 0.9888424875682178, 1.0889605326611516, 1.0357900751671631, 1.0085574072538828], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724, 0.5887412040656763, 0.5746677091477717, 0.5770132916340891, 0.5809225957779516, 0.5863956215793589, 0.5879593432369038, 0.599687255668491, 0.5793588741204065, 0.5879593432369038, 0.5957779515246286], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205, 0.7990243112220722, 0.7997743321755959, 0.8030130072849428, 0.8035551513188014, 0.8039593552097188, 0.8049911841168098, 0.8087692278686957, 0.8087532481085512, 0.8101110881156719, 0.8111429830985284]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183, 1.0405050258882536, 1.106826137433861, 1.0391728530366315, 1.0422610729536663, 1.0371296816826612, 1.0382036509879218, 0.9888424875682178, 1.0889605326611516, 1.0357900751671631, 1.0085574072538828, 1.009393268539348], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724, 0.5887412040656763, 0.5746677091477717, 0.5770132916340891, 0.5809225957779516, 0.5863956215793589, 0.5879593432369038, 0.599687255668491, 0.5793588741204065, 0.5879593432369038, 0.5957779515246286, 0.5957779515246286], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205, 0.7990243112220722, 0.7997743321755959, 0.8030130072849428, 0.8035551513188014, 0.8039593552097188, 0.8049911841168098, 0.8087692278686957, 0.8087532481085512, 0.8101110881156719, 0.8111429830985284, 0.8100623570190871]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183, 1.0405050258882536, 1.106826137433861, 1.0391728530366315, 1.0422610729536663, 1.0371296816826612, 1.0382036509879218, 0.9888424875682178, 1.0889605326611516, 1.0357900751671631, 1.0085574072538828, 1.009393268539348, 1.0159809038543999], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724, 0.5887412040656763, 0.5746677091477717, 0.5770132916340891, 0.5809225957779516, 0.5863956215793589, 0.5879593432369038, 0.599687255668491, 0.5793588741204065, 0.5879593432369038, 0.5957779515246286, 0.5957779515246286, 0.5989053948397185], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205, 0.7990243112220722, 0.7997743321755959, 0.8030130072849428, 0.8035551513188014, 0.8039593552097188, 0.8049911841168098, 0.8087692278686957, 0.8087532481085512, 0.8101110881156719, 0.8111429830985284, 0.8100623570190871, 0.8120961426383593]}



Final client history:
{'loss': [1.0714133925229148, 1.1433138056972048, 1.1443595897415078, 1.0891743268325424, 1.0625010216208153, 1.0859563886606665, 1.081820030478969, 1.077057950192825, 1.0769549944234882, 1.0751863186763915, 1.0108165281931807, 1.0523598829780918, 1.0347849803571127, 1.0421926367553163, 1.0468270918463616, 1.0455649126964774, 1.0702908814559233, 1.01790773174183, 1.0405050258882536, 1.106826137433861, 1.0391728530366315, 1.0422610729536663, 1.0371296816826612, 1.0382036509879218, 0.9888424875682178, 1.0889605326611516, 1.0357900751671631, 1.0085574072538828, 1.009393268539348, 1.0159809038543999], 'accuracy': [0.5207193119624707, 0.5254104769351056, 0.5340109460516028, 0.544175136825645, 0.5488663017982799, 0.5559030492572322, 0.5559030492572322, 0.5527756059421423, 0.565285379202502, 0.563721657544957, 0.5676309616888194, 0.5699765441751369, 0.5801407349491791, 0.5707584050039093, 0.5840500390930414, 0.581704456606724, 0.5762314308053167, 0.581704456606724, 0.5887412040656763, 0.5746677091477717, 0.5770132916340891, 0.5809225957779516, 0.5863956215793589, 0.5879593432369038, 0.599687255668491, 0.5793588741204065, 0.5879593432369038, 0.5957779515246286, 0.5957779515246286, 0.5989053948397185], 'auc': [0.7293562379614749, 0.7490635742179705, 0.7592713324992499, 0.7689369432900592, 0.774452520617057, 0.7778252102704238, 0.7807736641494487, 0.782997701960592, 0.7842480690701639, 0.7868229864307842, 0.7910555379974404, 0.7933015065657656, 0.7935060442707331, 0.792812276596959, 0.794619576919031, 0.7962525666236506, 0.7979231343435254, 0.7994336167044205, 0.7990243112220722, 0.7997743321755959, 0.8030130072849428, 0.8035551513188014, 0.8039593552097188, 0.8049911841168098, 0.8087692278686957, 0.8087532481085512, 0.8101110881156719, 0.8111429830985284, 0.8100623570190871, 0.8120961426383593]}

