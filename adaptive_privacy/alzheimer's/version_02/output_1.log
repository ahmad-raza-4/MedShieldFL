nohup: ignoring input
01/30/2025 05:28:22:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/30/2025 05:28:22:DEBUG:ChannelConnectivity.IDLE
01/30/2025 05:28:23:DEBUG:ChannelConnectivity.CONNECTING
01/30/2025 05:28:23:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/30/2025 05:28:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:28:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5395a8c2-1ccc-498b-9136-41a66db55280
01/30/2025 05:28:54:INFO:Received: train message 5395a8c2-1ccc-498b-9136-41a66db55280
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:29:37:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:30:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:30:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6c351632-e1c7-4d4f-9ac9-d59a5d44b503
01/30/2025 05:30:31:INFO:Received: evaluate message 6c351632-e1c7-4d4f-9ac9-d59a5d44b503
[92mINFO [0m:      Sent reply
01/30/2025 05:30:36:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:31:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:31:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 09396c9d-bb5c-4079-ae0d-d814cec4435a
01/30/2025 05:31:54:INFO:Received: train message 09396c9d-bb5c-4079-ae0d-d814cec4435a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:32:42:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:33:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:33:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8ff2dd64-a433-4d27-b228-8f46c5c6efe9
01/30/2025 05:33:53:INFO:Received: evaluate message 8ff2dd64-a433-4d27-b228-8f46c5c6efe9
[92mINFO [0m:      Sent reply
01/30/2025 05:33:59:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:34:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:34:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8738ba55-60ab-4fc7-8235-41a6f3713036
01/30/2025 05:34:25:INFO:Received: train message 8738ba55-60ab-4fc7-8235-41a6f3713036
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:35:17:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:36:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:36:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e77a933c-f07b-41b6-90c4-5497b1d5aaa1
01/30/2025 05:36:37:INFO:Received: evaluate message e77a933c-f07b-41b6-90c4-5497b1d5aaa1
[92mINFO [0m:      Sent reply
01/30/2025 05:36:42:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:37:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:37:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0c5c6d17-158f-4a74-b2f0-6a91eb28ca54
01/30/2025 05:37:40:INFO:Received: train message 0c5c6d17-158f-4a74-b2f0-6a91eb28ca54
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:38:31:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:39:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:39:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 509a16a9-6cfe-4e02-b8d3-c0ff6a12193f
01/30/2025 05:39:17:INFO:Received: evaluate message 509a16a9-6cfe-4e02-b8d3-c0ff6a12193f
[92mINFO [0m:      Sent reply
01/30/2025 05:39:22:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:40:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:40:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f23f9b75-03cb-4a27-816f-c3e21c2b9cbe
01/30/2025 05:40:30:INFO:Received: train message f23f9b75-03cb-4a27-816f-c3e21c2b9cbe
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:41:17:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:41:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:41:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ff5abdce-fcc4-428c-bab3-b8a1e96a15cc
01/30/2025 05:41:49:INFO:Received: evaluate message ff5abdce-fcc4-428c-bab3-b8a1e96a15cc
[92mINFO [0m:      Sent reply
01/30/2025 05:41:54:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:43:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:43:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 520fb1c8-afe1-4590-aabd-f5c9d302f7c4
01/30/2025 05:43:06:INFO:Received: train message 520fb1c8-afe1-4590-aabd-f5c9d302f7c4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:43:54:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:45:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:45:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f58c9d5b-4778-4aa7-bc84-720127231cb1
01/30/2025 05:45:05:INFO:Received: evaluate message f58c9d5b-4778-4aa7-bc84-720127231cb1
[92mINFO [0m:      Sent reply
01/30/2025 05:45:10:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:45:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:45:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b8a6195f-2a61-481b-8914-3263b8616df9
01/30/2025 05:45:56:INFO:Received: train message b8a6195f-2a61-481b-8914-3263b8616df9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:46:42:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:47:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:47:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 28f88da1-4881-44a1-bca7-6a146f7185c1
01/30/2025 05:47:40:INFO:Received: evaluate message 28f88da1-4881-44a1-bca7-6a146f7185c1
[92mINFO [0m:      Sent reply
01/30/2025 05:47:45:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:48:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:48:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3305079c-750d-4eae-ac86-9265add00565
01/30/2025 05:48:31:INFO:Received: train message 3305079c-750d-4eae-ac86-9265add00565
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:49:21:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:50:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:50:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4aabd1e1-a708-43ba-a689-525598afe8f2
01/30/2025 05:50:05:INFO:Received: evaluate message 4aabd1e1-a708-43ba-a689-525598afe8f2
[92mINFO [0m:      Sent reply
01/30/2025 05:50:10:INFO:Sent reply
