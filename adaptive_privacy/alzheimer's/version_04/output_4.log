nohup: ignoring input
02/05/2025 01:40:37:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 01:40:37:DEBUG:ChannelConnectivity.IDLE
02/05/2025 01:40:37:DEBUG:ChannelConnectivity.CONNECTING
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738748437.463850 3631009 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
02/05/2025 01:40:37:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
02/05/2025 01:41:08:INFO:
[92mINFO [0m:      Received: train message 834e86ac-2a27-4407-90a4-3afeed5f99df
02/05/2025 01:41:08:INFO:Received: train message 834e86ac-2a27-4407-90a4-3afeed5f99df
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:42:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:42:34:INFO:
[92mINFO [0m:      Received: evaluate message 5db217a0-c71f-4f44-8857-38588808dadd
02/05/2025 01:42:34:INFO:Received: evaluate message 5db217a0-c71f-4f44-8857-38588808dadd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:42:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:43:30:INFO:
[92mINFO [0m:      Received: train message 4a3e7c8d-a3db-471b-9d8d-d6e498520bbc
02/05/2025 01:43:30:INFO:Received: train message 4a3e7c8d-a3db-471b-9d8d-d6e498520bbc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:44:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:44:58:INFO:
[92mINFO [0m:      Received: evaluate message 43f337f5-1e59-4bda-b83f-73ababe96c08
02/05/2025 01:44:58:INFO:Received: evaluate message 43f337f5-1e59-4bda-b83f-73ababe96c08
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:45:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:45:46:INFO:
[92mINFO [0m:      Received: train message 6092aaec-4802-4c47-8078-a18916d37ad6
02/05/2025 01:45:46:INFO:Received: train message 6092aaec-4802-4c47-8078-a18916d37ad6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:46:32:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:47:01:INFO:
[92mINFO [0m:      Received: evaluate message 63c747c7-b529-4656-8bef-9395c4f8aa8d
02/05/2025 01:47:01:INFO:Received: evaluate message 63c747c7-b529-4656-8bef-9395c4f8aa8d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:47:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:48:02:INFO:
[92mINFO [0m:      Received: train message 17a6a892-92c5-4176-ae5e-e221d1829442
02/05/2025 01:48:02:INFO:Received: train message 17a6a892-92c5-4176-ae5e-e221d1829442
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:48:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:49:35:INFO:
[92mINFO [0m:      Received: evaluate message 4899f0cd-caec-4a38-80ef-ea48bbc18df7
02/05/2025 01:49:35:INFO:Received: evaluate message 4899f0cd-caec-4a38-80ef-ea48bbc18df7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:49:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:50:02:INFO:
[92mINFO [0m:      Received: train message e4f185f1-3251-4cba-accd-57344f1219ed
02/05/2025 01:50:02:INFO:Received: train message e4f185f1-3251-4cba-accd-57344f1219ed
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:50:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:51:49:INFO:
[92mINFO [0m:      Received: evaluate message bc5432fd-391f-4216-8f0e-fd4d69b1aa0d
02/05/2025 01:51:49:INFO:Received: evaluate message bc5432fd-391f-4216-8f0e-fd4d69b1aa0d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:51:52:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:52:30:INFO:
[92mINFO [0m:      Received: train message 33a10a00-5e8c-4fe6-aa60-b758b61ff7ca
02/05/2025 01:52:30:INFO:Received: train message 33a10a00-5e8c-4fe6-aa60-b758b61ff7ca
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:53:16:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:53:52:INFO:
[92mINFO [0m:      Received: evaluate message d313f149-a52c-49a3-9e0e-1453173e351e
02/05/2025 01:53:52:INFO:Received: evaluate message d313f149-a52c-49a3-9e0e-1453173e351e
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30, target_epsilon: 30, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.010018758475780487, 0.0021720053628087044, 0.0489850714802742, 0.015914535149931908]
Noise Multiplier after list and tensor:  0.019272592617198825
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987], 'accuracy': [0.5136825645035183], 'auc': [0.7274635541023244], 'precision': [0.4064689433467124], 'recall': [0.5136825645035183], 'f1': [0.4310635695286084]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.008312656544148922, 0.0011826149420812726, 0.012461948208510876, 0.008477281779050827]
Noise Multiplier after list and tensor:  0.007608625368447974
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356], 'accuracy': [0.5136825645035183, 0.5465207193119624], 'auc': [0.7274635541023244, 0.7524049152502355], 'precision': [0.4064689433467124, 0.455850817216089], 'recall': [0.5136825645035183, 0.5465207193119624], 'f1': [0.4310635695286084, 0.495754266150351]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.007731341756880283, 0.0005913623608648777, 0.016228415071964264, 0.012104085646569729]
Noise Multiplier after list and tensor:  0.009163801209069788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.007063133176416159, 0.0005913522327318788, 0.012460161000490189, 0.009080794639885426]
Noise Multiplier after list and tensor:  0.007298860262380913
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.008667870424687862, 0.0007184272981248796, 0.015073907561600208, 0.011870927177369595]
Noise Multiplier after list and tensor:  0.009082783115445636
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.00777313532307744, 0.000720495474524796, 0.01161463838070631, 0.008223854936659336]
Noise Multiplier after list and tensor:  0.007083031028741971
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:53:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:54:23:INFO:
[92mINFO [0m:      Received: train message eb5ab834-7d96-49c7-be15-8da57a65401c
02/05/2025 01:54:23:INFO:Received: train message eb5ab834-7d96-49c7-be15-8da57a65401c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:55:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:55:52:INFO:
[92mINFO [0m:      Received: evaluate message c1e3a226-cc92-4b58-ac1c-cf46f44ae602
02/05/2025 01:55:52:INFO:Received: evaluate message c1e3a226-cc92-4b58-ac1c-cf46f44ae602
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:55:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:56:46:INFO:
[92mINFO [0m:      Received: train message fd068b7f-7b94-4148-9791-869a863a21ff
02/05/2025 01:56:46:INFO:Received: train message fd068b7f-7b94-4148-9791-869a863a21ff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:57:32:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:58:06:INFO:
[92mINFO [0m:      Received: evaluate message afc59c92-2dbe-4a2f-8102-08820cb8c5e3
02/05/2025 01:58:06:INFO:Received: evaluate message afc59c92-2dbe-4a2f-8102-08820cb8c5e3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:58:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:58:59:INFO:
[92mINFO [0m:      Received: train message dd9f5111-1489-452c-a5f6-6915a204f338
02/05/2025 01:58:59:INFO:Received: train message dd9f5111-1489-452c-a5f6-6915a204f338
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:59:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 02:00:11:INFO:
[92mINFO [0m:      Received: evaluate message 3efcaa15-18dd-4c3b-badc-0ca22e9fd88e
02/05/2025 02:00:11:INFO:Received: evaluate message 3efcaa15-18dd-4c3b-badc-0ca22e9fd88e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 02:00:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 02:01:13:INFO:
[92mINFO [0m:      Received: train message 7533b1e9-886e-4658-b8e6-02aa22d7b69b
02/05/2025 02:01:13:INFO:Received: train message 7533b1e9-886e-4658-b8e6-02aa22d7b69b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 02:01:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 02:02:21:INFO:
[92mINFO [0m:      Received: evaluate message 93dc1da1-7447-45b9-982c-b3217b03f77b
02/05/2025 02:02:21:INFO:Received: evaluate message 93dc1da1-7447-45b9-982c-b3217b03f77b

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.00805746205151081, 0.0007849218090996146, 0.015218080952763557, 0.012683894485235214]
Noise Multiplier after list and tensor:  0.0091860898246523
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354, 1.0712851984040452], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624, 0.7798679677102436], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496, 0.6004973881084149], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949, 0.5012879884895282]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.00717487558722496, 0.00038988576852716506, 0.01297752745449543, 0.008963363245129585]
Noise Multiplier after list and tensor:  0.007376413013844285
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354, 1.0712851984040452, 1.0680485646458135], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624, 0.7798679677102436, 0.7827551087716259], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496, 0.6004973881084149, 0.6035753268698753], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949, 0.5012879884895282, 0.5081731034639204]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.00758205633610487, 0.0010353369871154428, 0.014375998638570309, 0.00959818996489048]
Noise Multiplier after list and tensor:  0.008147895481670275
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354, 1.0712851984040452, 1.0680485646458135, 1.0900122628163509], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624, 0.7798679677102436, 0.7827551087716259, 0.7848791137769334], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496, 0.6004973881084149, 0.6035753268698753, 0.575679808448772], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949, 0.5012879884895282, 0.5081731034639204, 0.5062270369311814]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.42510986328125
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.007233780808746815, 0.00038828811375424266, 0.013193229213356972, 0.009778398089110851]
Noise Multiplier after list and tensor:  0.00764842405624222
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 02:02:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 02:02:47:INFO:
[92mINFO [0m:      Received: reconnect message 022ef3d4-6b4e-4878-a4a2-1205ae57e8c7
02/05/2025 02:02:47:INFO:Received: reconnect message 022ef3d4-6b4e-4878-a4a2-1205ae57e8c7
02/05/2025 02:02:47:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 02:02:47:INFO:Disconnect and shut down
[mutex.cc : 2481] RAW: thread should hold at least a read lock on Mutex 0x7fcb540423b0 
