nohup: ignoring input
02/05/2025 01:40:34:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 01:40:34:DEBUG:ChannelConnectivity.IDLE
02/05/2025 01:40:34:DEBUG:ChannelConnectivity.CONNECTING
02/05/2025 01:40:34:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738748434.375150 3627687 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 01:40:57:INFO:
[92mINFO [0m:      Received: train message 290a725d-962f-4618-a369-a9039f05e779
02/05/2025 01:40:57:INFO:Received: train message 290a725d-962f-4618-a369-a9039f05e779
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:41:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:42:43:INFO:
[92mINFO [0m:      Received: evaluate message 2b8fe4d3-1e15-43e5-90cf-193d83a60216
02/05/2025 01:42:43:INFO:Received: evaluate message 2b8fe4d3-1e15-43e5-90cf-193d83a60216
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:42:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:43:33:INFO:
[92mINFO [0m:      Received: train message 7c112a43-c995-4ef7-8293-403b17e1650c
02/05/2025 01:43:33:INFO:Received: train message 7c112a43-c995-4ef7-8293-403b17e1650c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:44:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:45:06:INFO:
[92mINFO [0m:      Received: evaluate message 93992b61-503f-4f1e-bf6c-9c95f4cf26b1
02/05/2025 01:45:06:INFO:Received: evaluate message 93992b61-503f-4f1e-bf6c-9c95f4cf26b1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:45:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:45:38:INFO:
[92mINFO [0m:      Received: train message 2e028f6b-2b55-468e-9be4-b67a9dd67ddd
02/05/2025 01:45:38:INFO:Received: train message 2e028f6b-2b55-468e-9be4-b67a9dd67ddd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:46:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:47:14:INFO:
[92mINFO [0m:      Received: evaluate message f188ef15-966d-4108-b544-65b9dae3d83d
02/05/2025 01:47:14:INFO:Received: evaluate message f188ef15-966d-4108-b544-65b9dae3d83d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:47:17:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:48:04:INFO:
[92mINFO [0m:      Received: train message c548ebd0-c836-46a5-8a9f-ba0dfc5fa43f
02/05/2025 01:48:04:INFO:Received: train message c548ebd0-c836-46a5-8a9f-ba0dfc5fa43f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:48:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:49:31:INFO:
[92mINFO [0m:      Received: evaluate message 1ce5d5ae-1894-47fb-9086-77085eba5b9a
02/05/2025 01:49:31:INFO:Received: evaluate message 1ce5d5ae-1894-47fb-9086-77085eba5b9a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:49:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:50:02:INFO:
[92mINFO [0m:      Received: train message 2b2d0af4-c6f1-497f-8174-b4cbc504d091
02/05/2025 01:50:02:INFO:Received: train message 2b2d0af4-c6f1-497f-8174-b4cbc504d091
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:50:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:51:42:INFO:
[92mINFO [0m:      Received: evaluate message 8664e7ee-a661-4311-8465-5892df88fff2
02/05/2025 01:51:42:INFO:Received: evaluate message 8664e7ee-a661-4311-8465-5892df88fff2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:51:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:52:22:INFO:
[92mINFO [0m:      Received: train message 90658b92-736f-4f41-9419-c1ed2da4b2f5
02/05/2025 01:52:22:INFO:Received: train message 90658b92-736f-4f41-9419-c1ed2da4b2f5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:53:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:54:04:INFO:
[92mINFO [0m:      Received: evaluate message 0a30270d-3b15-4af4-8981-add236d5a31c
02/05/2025 01:54:04:INFO:Received: evaluate message 0a30270d-3b15-4af4-8981-add236d5a31c
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30, target_epsilon: 30, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0038994105998426676, 0.0016077168984338641, 0.01156093180179596, 0.02309047244489193]
Noise Multiplier after list and tensor:  0.010039632936241105
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987], 'accuracy': [0.5136825645035183], 'auc': [0.7274635541023244], 'precision': [0.4064689433467124], 'recall': [0.5136825645035183], 'f1': [0.4310635695286084]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0018341928953304887, 0.0008983984007500112, 0.02515486814081669, 0.028419263660907745]
Noise Multiplier after list and tensor:  0.014076680774451233
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356], 'accuracy': [0.5136825645035183, 0.5465207193119624], 'auc': [0.7274635541023244, 0.7524049152502355], 'precision': [0.4064689433467124, 0.455850817216089], 'recall': [0.5136825645035183, 0.5465207193119624], 'f1': [0.4310635695286084, 0.495754266150351]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0018445864552631974, 0.0008280992042273283, 0.022603482007980347, 0.02623780257999897]
Noise Multiplier after list and tensor:  0.01287849256186746
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0016431794501841068, 0.0005096428212709725, 0.02584843337535858, 0.029122861102223396]
Noise Multiplier after list and tensor:  0.014281029187259264
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0014405548572540283, 0.0005774290766566992, 0.024199705570936203, 0.026974961161613464]
Noise Multiplier after list and tensor:  0.013298162666615099
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.001393966726027429, 0.0005114673404023051, 0.02526833489537239, 0.027986794710159302]
Noise Multiplier after list and tensor:  0.013790140917990357
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:54:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:54:39:INFO:
[92mINFO [0m:      Received: train message c8f304f0-378d-4d4b-b435-289b571b5120
02/05/2025 01:54:39:INFO:Received: train message c8f304f0-378d-4d4b-b435-289b571b5120
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:55:21:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:56:07:INFO:
[92mINFO [0m:      Received: evaluate message adc01220-1269-46a3-a19e-30605fcb002e
02/05/2025 01:56:07:INFO:Received: evaluate message adc01220-1269-46a3-a19e-30605fcb002e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:56:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:56:31:INFO:
[92mINFO [0m:      Received: train message b3844c76-3be4-45e2-894d-2756835b0c44
02/05/2025 01:56:31:INFO:Received: train message b3844c76-3be4-45e2-894d-2756835b0c44
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:57:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:58:22:INFO:
[92mINFO [0m:      Received: evaluate message f13dd55a-e9b3-49fe-bf4b-d0ee94f2cb8a
02/05/2025 01:58:22:INFO:Received: evaluate message f13dd55a-e9b3-49fe-bf4b-d0ee94f2cb8a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:58:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:59:03:INFO:
[92mINFO [0m:      Received: train message 306a5eaa-7630-4824-8e48-7218b166e3e8
02/05/2025 01:59:03:INFO:Received: train message 306a5eaa-7630-4824-8e48-7218b166e3e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:59:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 02:00:34:INFO:
[92mINFO [0m:      Received: evaluate message 2184743a-2545-4f03-bf50-3cc87fdcab7b
02/05/2025 02:00:34:INFO:Received: evaluate message 2184743a-2545-4f03-bf50-3cc87fdcab7b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 02:00:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 02:00:55:INFO:
[92mINFO [0m:      Received: train message dacc3f05-972b-4e60-b5e3-ea0b54d225e2
02/05/2025 02:00:55:INFO:Received: train message dacc3f05-972b-4e60-b5e3-ea0b54d225e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 02:01:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 02:02:43:INFO:
[92mINFO [0m:      Received: evaluate message a971d8cf-3d56-4e58-bb83-b95c79dfb9f3
02/05/2025 02:02:43:INFO:Received: evaluate message a971d8cf-3d56-4e58-bb83-b95c79dfb9f3

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0016322454903274775, 0.0004462650103960186, 0.020578159019351006, 0.023351414129137993]
Noise Multiplier after list and tensor:  0.011502020912303124
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354, 1.0712851984040452], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624, 0.7798679677102436], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496, 0.6004973881084149], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949, 0.5012879884895282]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0023507708683609962, 0.0008683654596097767, 0.023881344124674797, 0.028019506484270096]
Noise Multiplier after list and tensor:  0.013779996734228916
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354, 1.0712851984040452, 1.0680485646458135], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624, 0.7798679677102436, 0.7827551087716259], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496, 0.6004973881084149, 0.6035753268698753], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949, 0.5012879884895282, 0.5081731034639204]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0020916648209095, 0.0005642799660563469, 0.024617554619908333, 0.02869410440325737]
Noise Multiplier after list and tensor:  0.013991900952532887
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354, 1.0712851984040452, 1.0680485646458135, 1.0900122628163509], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624, 0.7798679677102436, 0.7827551087716259, 0.7848791137769334], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496, 0.6004973881084149, 0.6035753268698753, 0.575679808448772], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949, 0.5012879884895282, 0.5081731034639204, 0.5062270369311814]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0016766031039878726, 0.0008858423680067062, 0.023206720128655434, 0.026395801454782486]
Noise Multiplier after list and tensor:  0.013041241763858125
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 02:02:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 02:02:47:INFO:
[92mINFO [0m:      Received: reconnect message f682750e-506b-44c5-b90a-5eb563acdce8
02/05/2025 02:02:47:INFO:Received: reconnect message f682750e-506b-44c5-b90a-5eb563acdce8
02/05/2025 02:02:47:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 02:02:47:INFO:Disconnect and shut down

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354, 1.0712851984040452, 1.0680485646458135, 1.0900122628163509, 1.0673061738823242], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957, 0.5668491008600469], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624, 0.7798679677102436, 0.7827551087716259, 0.7848791137769334, 0.7859333762694669], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496, 0.6004973881084149, 0.6035753268698753, 0.575679808448772, 0.5826625097258644], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957, 0.5668491008600469], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949, 0.5012879884895282, 0.5081731034639204, 0.5062270369311814, 0.5163980087281406]}



Final client history:
{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354, 1.0712851984040452, 1.0680485646458135, 1.0900122628163509, 1.0673061738823242], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957, 0.5668491008600469], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624, 0.7798679677102436, 0.7827551087716259, 0.7848791137769334, 0.7859333762694669], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496, 0.6004973881084149, 0.6035753268698753, 0.575679808448772, 0.5826625097258644], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957, 0.5668491008600469], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949, 0.5012879884895282, 0.5081731034639204, 0.5062270369311814, 0.5163980087281406]}

