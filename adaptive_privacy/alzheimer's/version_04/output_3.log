nohup: ignoring input
02/05/2025 01:40:31:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 01:40:31:DEBUG:ChannelConnectivity.IDLE
02/05/2025 01:40:31:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738748431.521624 3624662 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 01:41:15:INFO:
[92mINFO [0m:      Received: train message b1b5a76f-9e8b-4fb6-a153-7a340af23abf
02/05/2025 01:41:15:INFO:Received: train message b1b5a76f-9e8b-4fb6-a153-7a340af23abf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:41:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:42:50:INFO:
[92mINFO [0m:      Received: evaluate message c12db8c3-ac71-44e9-afc0-a01beefa039e
02/05/2025 01:42:50:INFO:Received: evaluate message c12db8c3-ac71-44e9-afc0-a01beefa039e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:42:53:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:43:32:INFO:
[92mINFO [0m:      Received: train message ea46fa31-82f9-4f12-90ec-7c0cd4169fa6
02/05/2025 01:43:32:INFO:Received: train message ea46fa31-82f9-4f12-90ec-7c0cd4169fa6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:44:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:44:54:INFO:
[92mINFO [0m:      Received: evaluate message 667f9cd7-9aea-4708-bf6a-a7afcdceb24a
02/05/2025 01:44:54:INFO:Received: evaluate message 667f9cd7-9aea-4708-bf6a-a7afcdceb24a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:44:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:45:40:INFO:
[92mINFO [0m:      Received: train message 2eeac8d8-a1ca-467c-814a-14663212c8eb
02/05/2025 01:45:40:INFO:Received: train message 2eeac8d8-a1ca-467c-814a-14663212c8eb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:46:16:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:47:21:INFO:
[92mINFO [0m:      Received: evaluate message 1f6686aa-8a2f-4a9f-a6c0-d6a832514c7a
02/05/2025 01:47:21:INFO:Received: evaluate message 1f6686aa-8a2f-4a9f-a6c0-d6a832514c7a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:47:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:47:45:INFO:
[92mINFO [0m:      Received: train message 341088e0-2bdf-4cae-bc53-3043288ecac2
02/05/2025 01:47:45:INFO:Received: train message 341088e0-2bdf-4cae-bc53-3043288ecac2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:48:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:49:31:INFO:
[92mINFO [0m:      Received: evaluate message bc369b3a-c7d6-4a12-b955-80070eb08ecd
02/05/2025 01:49:31:INFO:Received: evaluate message bc369b3a-c7d6-4a12-b955-80070eb08ecd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:49:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:50:16:INFO:
[92mINFO [0m:      Received: train message 79651dd9-5e0f-4823-952f-451a96f74bf6
02/05/2025 01:50:16:INFO:Received: train message 79651dd9-5e0f-4823-952f-451a96f74bf6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:50:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:51:40:INFO:
[92mINFO [0m:      Received: evaluate message 032a40dd-6688-4d38-b76e-a757974ca635
02/05/2025 01:51:40:INFO:Received: evaluate message 032a40dd-6688-4d38-b76e-a757974ca635
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:51:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:52:30:INFO:
[92mINFO [0m:      Received: train message 4be5d4c8-5e75-425b-a961-946c623e4cf1
02/05/2025 01:52:30:INFO:Received: train message 4be5d4c8-5e75-425b-a961-946c623e4cf1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:53:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:54:04:INFO:
[92mINFO [0m:      Received: evaluate message ce9c13bf-f557-4af9-a70d-05edaad311a0
02/05/2025 01:54:04:INFO:Received: evaluate message ce9c13bf-f557-4af9-a70d-05edaad311a0
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30, target_epsilon: 30, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008927934803068638, 0.0010508899576961994, 0.012938286177814007, 0.005829792004078627]
Noise Multiplier after list and tensor:  0.007186725735664368
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987], 'accuracy': [0.5136825645035183], 'auc': [0.7274635541023244], 'precision': [0.4064689433467124], 'recall': [0.5136825645035183], 'f1': [0.4310635695286084]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.00995331909507513, 0.0004889030242338777, 0.006334828678518534, 0.004199322313070297]
Noise Multiplier after list and tensor:  0.00524409327772446
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356], 'accuracy': [0.5136825645035183, 0.5465207193119624], 'auc': [0.7274635541023244, 0.7524049152502355], 'precision': [0.4064689433467124, 0.455850817216089], 'recall': [0.5136825645035183, 0.5465207193119624], 'f1': [0.4310635695286084, 0.495754266150351]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.009363869205117226, 0.0005455435602925718, 0.005987063981592655, 0.006304125767201185]
Noise Multiplier after list and tensor:  0.0055501506285509095
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.01141078770160675, 0.0006693055620416999, 0.007482336834073067, 0.005179876461625099]
Noise Multiplier after list and tensor:  0.006185576639836654
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.010365212336182594, 0.0007278509438037872, 0.006428808439522982, 0.005808671470731497]
Noise Multiplier after list and tensor:  0.005832635797560215
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.00944545865058899, 0.0007282717269845307, 0.006596132647246122, 0.0050768498331308365]
Noise Multiplier after list and tensor:  0.00546167821448762
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:54:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:54:44:INFO:
[92mINFO [0m:      Received: train message d2880b9b-3492-4e8d-a8a3-85e46f8315ea
02/05/2025 01:54:44:INFO:Received: train message d2880b9b-3492-4e8d-a8a3-85e46f8315ea
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:55:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:56:09:INFO:
[92mINFO [0m:      Received: evaluate message 3fc57df3-d7ab-48c7-82d1-79aac813b4c3
02/05/2025 01:56:09:INFO:Received: evaluate message 3fc57df3-d7ab-48c7-82d1-79aac813b4c3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:56:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:56:35:INFO:
[92mINFO [0m:      Received: train message e211deb0-3617-4e86-ab85-0c44591b5f24
02/05/2025 01:56:35:INFO:Received: train message e211deb0-3617-4e86-ab85-0c44591b5f24
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:57:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:58:20:INFO:
[92mINFO [0m:      Received: evaluate message 67ac5a4d-2453-4254-88ac-4378d89c8c93
02/05/2025 01:58:20:INFO:Received: evaluate message 67ac5a4d-2453-4254-88ac-4378d89c8c93
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 01:58:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 01:58:53:INFO:
[92mINFO [0m:      Received: train message ac1d3a2a-3547-4b12-8a58-efb3ba19a989
02/05/2025 01:58:53:INFO:Received: train message ac1d3a2a-3547-4b12-8a58-efb3ba19a989
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 01:59:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 02:00:28:INFO:
[92mINFO [0m:      Received: evaluate message 3fcbf83a-2006-4901-96b2-40ff96c39c52
02/05/2025 02:00:28:INFO:Received: evaluate message 3fcbf83a-2006-4901-96b2-40ff96c39c52
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 02:00:31:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 02:01:13:INFO:
[92mINFO [0m:      Received: train message a4b254e9-2ab9-4ca5-aed3-ae58cda145fa
02/05/2025 02:01:13:INFO:Received: train message a4b254e9-2ab9-4ca5-aed3-ae58cda145fa
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 02:01:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 02:02:43:INFO:
[92mINFO [0m:      Received: evaluate message bdba73c6-ffe5-412f-b9c2-7682a42a8ad7
02/05/2025 02:02:43:INFO:Received: evaluate message bdba73c6-ffe5-412f-b9c2-7682a42a8ad7

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.009379252791404724, 0.0005482698907144368, 0.005891628563404083, 0.00659205112606287]
Noise Multiplier after list and tensor:  0.0056028005928965285
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354, 1.0712851984040452], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624, 0.7798679677102436], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496, 0.6004973881084149], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949, 0.5012879884895282]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.00970026757568121, 0.00048161757877096534, 0.00569260586053133, 0.005637931637465954]
Noise Multiplier after list and tensor:  0.005378105663112365
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354, 1.0712851984040452, 1.0680485646458135], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624, 0.7798679677102436, 0.7827551087716259], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496, 0.6004973881084149, 0.6035753268698753], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949, 0.5012879884895282, 0.5081731034639204]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.009491982869803905, 0.0006035512196831405, 0.005674060899764299, 0.005375193897634745]
Noise Multiplier after list and tensor:  0.0052861972217215225
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354, 1.0712851984040452, 1.0680485646458135, 1.0900122628163509], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624, 0.7798679677102436, 0.7827551087716259, 0.7848791137769334], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496, 0.6004973881084149, 0.6035753268698753, 0.575679808448772], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949, 0.5012879884895282, 0.5081731034639204, 0.5062270369311814]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.009133344516158104, 0.00042545254109427333, 0.0063967639580369, 0.0052412040531635284]
Noise Multiplier after list and tensor:  0.005299191267113201
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 02:02:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 02:02:46:INFO:
[92mINFO [0m:      Received: reconnect message 9b75d12d-32e9-425a-b89e-5b68968e32f1
02/05/2025 02:02:46:INFO:Received: reconnect message 9b75d12d-32e9-425a-b89e-5b68968e32f1
02/05/2025 02:02:47:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 02:02:47:INFO:Disconnect and shut down

{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354, 1.0712851984040452, 1.0680485646458135, 1.0900122628163509, 1.0673061738823242], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957, 0.5668491008600469], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624, 0.7798679677102436, 0.7827551087716259, 0.7848791137769334, 0.7859333762694669], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496, 0.6004973881084149, 0.6035753268698753, 0.575679808448772, 0.5826625097258644], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957, 0.5668491008600469], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949, 0.5012879884895282, 0.5081731034639204, 0.5062270369311814, 0.5163980087281406]}



Final client history:
{'loss': [1.065360100218987, 1.0266984310627356, 1.1080930096922301, 1.0847538999042257, 1.1355590179061592, 1.0567495710128354, 1.0712851984040452, 1.0680485646458135, 1.0900122628163509, 1.0673061738823242], 'accuracy': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957, 0.5668491008600469], 'auc': [0.7274635541023244, 0.7524049152502355, 0.7615749362556182, 0.7686694979515858, 0.7721474586838541, 0.7779635591367624, 0.7798679677102436, 0.7827551087716259, 0.7848791137769334, 0.7859333762694669], 'precision': [0.4064689433467124, 0.455850817216089, 0.43515488441095684, 0.46009651953826447, 0.4348040419071978, 0.4694083658782496, 0.6004973881084149, 0.6035753268698753, 0.575679808448772, 0.5826625097258644], 'recall': [0.5136825645035183, 0.5465207193119624, 0.5355746677091477, 0.5551211884284597, 0.5363565285379203, 0.5590304925723222, 0.5590304925723222, 0.562157935887412, 0.563721657544957, 0.5668491008600469], 'f1': [0.4310635695286084, 0.495754266150351, 0.46915548608713314, 0.49994255764141227, 0.46622079529892746, 0.5096821297595949, 0.5012879884895282, 0.5081731034639204, 0.5062270369311814, 0.5163980087281406]}

