nohup: ignoring input
02/05/2025 04:39:23:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 04:39:23:DEBUG:ChannelConnectivity.IDLE
02/05/2025 04:39:23:DEBUG:ChannelConnectivity.CONNECTING
[92mINFO [0m:      
02/05/2025 04:39:23:DEBUG:ChannelConnectivity.READY
02/05/2025 04:39:23:INFO:
[92mINFO [0m:      Received: get_parameters message ea6871f8-be91-4f83-b5da-fb15d87506b3
02/05/2025 04:39:23:INFO:Received: get_parameters message ea6871f8-be91-4f83-b5da-fb15d87506b3
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738759163.652347 2185396 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      Sent reply
02/05/2025 04:39:29:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:39:39:INFO:
[92mINFO [0m:      Received: train message 12c9ea59-75f4-44ef-848d-50baa90c6ab1
02/05/2025 04:39:39:INFO:Received: train message 12c9ea59-75f4-44ef-848d-50baa90c6ab1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:40:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:40:49:INFO:
[92mINFO [0m:      Received: evaluate message 0de3dc6f-34aa-4948-9a37-55ae16912622
02/05/2025 04:40:49:INFO:Received: evaluate message 0de3dc6f-34aa-4948-9a37-55ae16912622
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:40:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:41:03:INFO:
[92mINFO [0m:      Received: train message 396a209b-8d69-4dd4-9ed5-ab6664d3fd6d
02/05/2025 04:41:03:INFO:Received: train message 396a209b-8d69-4dd4-9ed5-ab6664d3fd6d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:41:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:42:08:INFO:
[92mINFO [0m:      Received: evaluate message 6dbd146e-9b2f-46fc-a6aa-5d8ee6a52c9a
02/05/2025 04:42:08:INFO:Received: evaluate message 6dbd146e-9b2f-46fc-a6aa-5d8ee6a52c9a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:42:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:42:21:INFO:
[92mINFO [0m:      Received: train message 8a6e1bef-d18d-42cf-8a6c-5c4b150283e6
02/05/2025 04:42:21:INFO:Received: train message 8a6e1bef-d18d-42cf-8a6c-5c4b150283e6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:43:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:43:30:INFO:
[92mINFO [0m:      Received: evaluate message ca41469b-090f-454d-9923-b6b37c03d593
02/05/2025 04:43:30:INFO:Received: evaluate message ca41469b-090f-454d-9923-b6b37c03d593
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:43:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:43:44:INFO:
[92mINFO [0m:      Received: train message 2cf2793b-318d-4917-b087-285a9506ddc3
02/05/2025 04:43:44:INFO:Received: train message 2cf2793b-318d-4917-b087-285a9506ddc3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:44:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:44:46:INFO:
[92mINFO [0m:      Received: evaluate message c130b2e8-280f-4ef5-b9db-b37b4671320d
02/05/2025 04:44:46:INFO:Received: evaluate message c130b2e8-280f-4ef5-b9db-b37b4671320d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:44:52:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:44:58:INFO:
[92mINFO [0m:      Received: train message c330b1a7-f101-4617-9c41-a1ffc53a916c
02/05/2025 04:44:58:INFO:Received: train message c330b1a7-f101-4617-9c41-a1ffc53a916c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:45:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:46:03:INFO:
[92mINFO [0m:      Received: evaluate message b3c51403-0256-4011-9604-2e462667ef21
02/05/2025 04:46:03:INFO:Received: evaluate message b3c51403-0256-4011-9604-2e462667ef21
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:46:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:46:16:INFO:
[92mINFO [0m:      Received: train message 567af950-33ef-4268-be82-986f698511e0
02/05/2025 04:46:16:INFO:Received: train message 567af950-33ef-4268-be82-986f698511e0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:47:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:47:21:INFO:
[92mINFO [0m:      Received: evaluate message 04981d89-1999-4227-9f8c-96714e1a3e13
02/05/2025 04:47:21:INFO:Received: evaluate message 04981d89-1999-4227-9f8c-96714e1a3e13
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:47:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:47:34:INFO:
[92mINFO [0m:      Received: train message 4f8b4440-b95c-4589-83ea-5cf48c7d932f
02/05/2025 04:47:34:INFO:Received: train message 4f8b4440-b95c-4589-83ea-5cf48c7d932f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:48:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:48:40:INFO:
[92mINFO [0m:      Received: evaluate message 0644f745-8187-4d15-abc7-59a3925ea104
02/05/2025 04:48:40:INFO:Received: evaluate message 0644f745-8187-4d15-abc7-59a3925ea104
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954], 'accuracy': [0.35105551211884284], 'auc': [0.49385709274345885], 'precision': [0.6231785640434563], 'recall': [0.35105551211884284], 'f1': [0.1833951127195922]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646], 'accuracy': [0.35105551211884284, 0.43471462079749806], 'auc': [0.49385709274345885, 0.4926623554484254], 'precision': [0.6231785640434563, 0.56764227853351], 'recall': [0.35105551211884284, 0.43471462079749806], 'f1': [0.1833951127195922, 0.3510492981299551]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126, 1.8211777950749908], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582, 0.5721949151975435], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837, 0.527881748466725], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713, 0.47150202919337764]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:48:44:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:48:51:INFO:
[92mINFO [0m:      Received: train message d0bf6383-ce5b-45e7-9013-516c6dfdb11a
02/05/2025 04:48:51:INFO:Received: train message d0bf6383-ce5b-45e7-9013-516c6dfdb11a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:49:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:49:55:INFO:
[92mINFO [0m:      Received: evaluate message 1749a928-850f-41c4-867d-fe27b59a575e
02/05/2025 04:49:55:INFO:Received: evaluate message 1749a928-850f-41c4-867d-fe27b59a575e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:50:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:50:07:INFO:
[92mINFO [0m:      Received: train message c7861fa3-0fad-4960-b2ed-bb5c40791f24
02/05/2025 04:50:07:INFO:Received: train message c7861fa3-0fad-4960-b2ed-bb5c40791f24
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:50:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:51:14:INFO:
[92mINFO [0m:      Received: evaluate message 130dc6e4-aaf6-4c0a-90a5-fd91996f383e
02/05/2025 04:51:14:INFO:Received: evaluate message 130dc6e4-aaf6-4c0a-90a5-fd91996f383e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:51:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:51:29:INFO:
[92mINFO [0m:      Received: train message 13e77aee-db14-4585-90e4-e711e3ab4a83
02/05/2025 04:51:29:INFO:Received: train message 13e77aee-db14-4585-90e4-e711e3ab4a83
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:52:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:52:36:INFO:
[92mINFO [0m:      Received: evaluate message b4e5cce0-e3bd-46a1-b4e7-49208e082d66
02/05/2025 04:52:36:INFO:Received: evaluate message b4e5cce0-e3bd-46a1-b4e7-49208e082d66
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:52:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:52:42:INFO:
[92mINFO [0m:      Received: reconnect message 3993e754-5db8-4a74-bb23-c03f80ae6bb9
02/05/2025 04:52:42:INFO:Received: reconnect message 3993e754-5db8-4a74-bb23-c03f80ae6bb9
02/05/2025 04:52:42:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 04:52:42:INFO:Disconnect and shut down

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126, 1.8211777950749908, 1.9451654829183904], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582, 0.5721949151975435, 0.5678565402090624], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837, 0.527881748466725, 0.534388193394705], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713, 0.47150202919337764, 0.4680520134848537]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126, 1.8211777950749908, 1.9451654829183904, 1.910481638041346], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582, 0.5721949151975435, 0.5678565402090624, 0.5816478990426055], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837, 0.527881748466725, 0.534388193394705, 0.5317417441177757], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713, 0.47150202919337764, 0.4680520134848537, 0.4718829233473253]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126, 1.8211777950749908, 1.9451654829183904, 1.910481638041346, 1.8484912142914847], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009, 0.508209538702111], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582, 0.5721949151975435, 0.5678565402090624, 0.5816478990426055, 0.6061525975384168], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837, 0.527881748466725, 0.534388193394705, 0.5317417441177757, 0.5248556730657858], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009, 0.508209538702111], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713, 0.47150202919337764, 0.4680520134848537, 0.4718829233473253, 0.4693398156061687]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126, 1.8211777950749908, 1.9451654829183904, 1.910481638041346, 1.8484912142914847, 2.0151613874359464], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009, 0.508209538702111, 0.5058639562157936], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582, 0.5721949151975435, 0.5678565402090624, 0.5816478990426055, 0.6061525975384168, 0.5724589607873449], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837, 0.527881748466725, 0.534388193394705, 0.5317417441177757, 0.5248556730657858, 0.5497248083323522], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009, 0.508209538702111, 0.5058639562157936], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713, 0.47150202919337764, 0.4680520134848537, 0.4718829233473253, 0.4693398156061687, 0.4615842965514985]}



Final client history:
{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126, 1.8211777950749908, 1.9451654829183904, 1.910481638041346, 1.8484912142914847, 2.0151613874359464], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009, 0.508209538702111, 0.5058639562157936], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582, 0.5721949151975435, 0.5678565402090624, 0.5816478990426055, 0.6061525975384168, 0.5724589607873449], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837, 0.527881748466725, 0.534388193394705, 0.5317417441177757, 0.5248556730657858, 0.5497248083323522], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009, 0.508209538702111, 0.5058639562157936], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713, 0.47150202919337764, 0.4680520134848537, 0.4718829233473253, 0.4693398156061687, 0.4615842965514985]}

