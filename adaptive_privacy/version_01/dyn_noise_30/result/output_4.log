nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_30/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/18/2025 11:02:25:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/18/2025 11:02:25:DEBUG:ChannelConnectivity.IDLE
01/18/2025 11:02:25:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/18/2025 11:09:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:09:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1021a75d-6bc5-4a9e-aa76-1da99e6b1fb7
01/18/2025 11:09:04:INFO:Received: train message 1021a75d-6bc5-4a9e-aa76-1da99e6b1fb7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:18:20:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:37:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:37:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d902e1a9-9e8e-4574-8285-fe5de2678b06
01/18/2025 11:37:08:INFO:Received: evaluate message d902e1a9-9e8e-4574-8285-fe5de2678b06
[92mINFO [0m:      Sent reply
01/18/2025 11:41:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:41:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:41:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cf7b9a75-022b-42c5-8c02-f8fcb4e7e06c
01/18/2025 11:41:54:INFO:Received: train message cf7b9a75-022b-42c5-8c02-f8fcb4e7e06c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:50:56:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:06:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:06:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 066a27d4-5103-49a4-9a7e-e19220735f88
01/18/2025 12:06:03:INFO:Received: evaluate message 066a27d4-5103-49a4-9a7e-e19220735f88
[92mINFO [0m:      Sent reply
01/18/2025 12:10:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:10:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:10:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 688be037-e3ca-49d9-8862-68ad7092ec73
01/18/2025 12:10:51:INFO:Received: train message 688be037-e3ca-49d9-8862-68ad7092ec73
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:20:22:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:33:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:33:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 80e8fa9b-e9e1-4e37-b3ea-6f5fd24554c6
01/18/2025 12:33:10:INFO:Received: evaluate message 80e8fa9b-e9e1-4e37-b3ea-6f5fd24554c6
[92mINFO [0m:      Sent reply
01/18/2025 12:37:51:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:38:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:38:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4e286261-0025-4d08-9d10-3e633f6200ed
01/18/2025 12:38:48:INFO:Received: train message 4e286261-0025-4d08-9d10-3e633f6200ed
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:48:43:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:02:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:02:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 85a2a065-a794-4094-8e82-4a1f285d4680
01/18/2025 13:02:11:INFO:Received: evaluate message 85a2a065-a794-4094-8e82-4a1f285d4680
[92mINFO [0m:      Sent reply
01/18/2025 13:06:37:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:07:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:07:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 30759e3a-ea00-487c-ad83-b3be1fabc6e7
01/18/2025 13:07:15:INFO:Received: train message 30759e3a-ea00-487c-ad83-b3be1fabc6e7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:17:20:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:31:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:31:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4bae94e8-2a6e-4cab-a205-b7f1192bdfe8
01/18/2025 13:31:17:INFO:Received: evaluate message 4bae94e8-2a6e-4cab-a205-b7f1192bdfe8
[92mINFO [0m:      Sent reply
01/18/2025 13:35:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:36:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:36:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5f25399c-0478-4e42-9070-84021d795760
01/18/2025 13:36:54:INFO:Received: train message 5f25399c-0478-4e42-9070-84021d795760
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:46:21:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:59:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:59:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f68eb335-86b3-4207-8f70-702c6aad2a2d
01/18/2025 13:59:20:INFO:Received: evaluate message f68eb335-86b3-4207-8f70-702c6aad2a2d
[92mINFO [0m:      Sent reply
01/18/2025 14:03:34:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:04:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:04:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 61d7f11b-a163-4c9b-9316-147ad001ef8b
01/18/2025 14:04:35:INFO:Received: train message 61d7f11b-a163-4c9b-9316-147ad001ef8b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:13:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:28:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:28:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9f9541e6-51d2-40da-9243-aed8820b5d5f
01/18/2025 14:28:18:INFO:Received: evaluate message 9f9541e6-51d2-40da-9243-aed8820b5d5f
[92mINFO [0m:      Sent reply
01/18/2025 14:32:46:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:33:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:33:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e22c4b55-3f72-4f5b-93cf-71ae8a09846a
01/18/2025 14:33:17:INFO:Received: train message e22c4b55-3f72-4f5b-93cf-71ae8a09846a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:42:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:57:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:57:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1df9817a-a8ba-4011-a91b-43a10bd26311
01/18/2025 14:57:23:INFO:Received: evaluate message 1df9817a-a8ba-4011-a91b-43a10bd26311
[92mINFO [0m:      Sent reply
01/18/2025 15:01:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:02:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:02:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message db3d9804-b789-46fd-8f33-79ef532d8c8f
01/18/2025 15:02:38:INFO:Received: train message db3d9804-b789-46fd-8f33-79ef532d8c8f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:11:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:28:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:28:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f39e9aae-4354-4c36-81f1-36bc7a5a0947
01/18/2025 15:28:23:INFO:Received: evaluate message f39e9aae-4354-4c36-81f1-36bc7a5a0947
[92mINFO [0m:      Sent reply
01/18/2025 15:33:06:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:33:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:33:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 733e2f69-7bb8-43b8-b67d-e47d1af8300e
01/18/2025 15:33:44:INFO:Received: train message 733e2f69-7bb8-43b8-b67d-e47d1af8300e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:43:06:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:01:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:01:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7c8b5607-da3f-41b8-9093-60f861d8eac4
01/18/2025 16:01:59:INFO:Received: evaluate message 7c8b5607-da3f-41b8-9093-60f861d8eac4
[92mINFO [0m:      Sent reply
01/18/2025 16:05:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:06:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:06:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e244e8ad-666d-4689-ad80-bd95ffbd79de
01/18/2025 16:06:49:INFO:Received: train message e244e8ad-666d-4689-ad80-bd95ffbd79de
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:15:32:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:34:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:34:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 107486d6-0299-428c-97df-eba17397456e
01/18/2025 16:34:21:INFO:Received: evaluate message 107486d6-0299-428c-97df-eba17397456e
[92mINFO [0m:      Sent reply
01/18/2025 16:38:32:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:39:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:39:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 070f3a7d-6cf3-49a8-b2c7-26c260c34fdb
01/18/2025 16:39:05:INFO:Received: train message 070f3a7d-6cf3-49a8-b2c7-26c260c34fdb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:48:51:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:05:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:05:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5a04b48f-bca4-4362-bfb3-50e0184981e6
01/18/2025 17:05:00:INFO:Received: evaluate message 5a04b48f-bca4-4362-bfb3-50e0184981e6
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_30', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_30']
BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535], 'accuracy': [0.3419250906161901], 'auc': [0.5460484530250033]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187], 'accuracy': [0.3419250906161901, 0.3403141361256545], 'auc': [0.5460484530250033, 0.5873079809432222]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00
[92mINFO [0m:      Sent reply
01/18/2025 17:09:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:09:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:09:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8d6b6871-227d-441b-8742-c0a072fbaffc
01/18/2025 17:09:45:INFO:Received: train message 8d6b6871-227d-441b-8742-c0a072fbaffc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:19:15:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:34:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:34:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fabba1cd-1d58-4042-8810-b0e23e8b742e
01/18/2025 17:34:23:INFO:Received: evaluate message fabba1cd-1d58-4042-8810-b0e23e8b742e
[92mINFO [0m:      Sent reply
01/18/2025 17:38:51:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:39:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:39:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d5ad2b04-b730-4753-9391-679f1506e5b4
01/18/2025 17:39:23:INFO:Received: train message d5ad2b04-b730-4753-9391-679f1506e5b4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:48:44:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:02:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:02:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e4826d8c-670a-4722-86f3-7ae710f03a5e
01/18/2025 18:02:48:INFO:Received: evaluate message e4826d8c-670a-4722-86f3-7ae710f03a5e
[92mINFO [0m:      Sent reply
01/18/2025 18:07:16:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:07:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:07:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b68eba50-412a-403c-b44e-3540e75a6957
01/18/2025 18:07:42:INFO:Received: train message b68eba50-412a-403c-b44e-3540e75a6957
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:17:20:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:30:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:30:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 386cd48d-6ab9-4648-87dd-707617cd54cd
01/18/2025 18:30:08:INFO:Received: evaluate message 386cd48d-6ab9-4648-87dd-707617cd54cd
[92mINFO [0m:      Sent reply
01/18/2025 18:34:31:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:35:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:35:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8773593e-c9a5-4fcc-9a15-71ac4d5f8f91
01/18/2025 18:35:15:INFO:Received: train message 8773593e-c9a5-4fcc-9a15-71ac4d5f8f91
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:44:36:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:57:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:57:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 98a32c00-6531-4ff5-95f7-caddbc44df51
01/18/2025 18:57:35:INFO:Received: evaluate message 98a32c00-6531-4ff5-95f7-caddbc44df51
[92mINFO [0m:      Sent reply
01/18/2025 19:02:08:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:02:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:02:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 87a15672-e3d5-416c-9c3e-50a7728eefdc
01/18/2025 19:02:39:INFO:Received: train message 87a15672-e3d5-416c-9c3e-50a7728eefdc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:12:01:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:25:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:25:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f66a182a-5374-463f-9b8b-a9a7b8e373a9
01/18/2025 19:25:17:INFO:Received: evaluate message f66a182a-5374-463f-9b8b-a9a7b8e373a9
[92mINFO [0m:      Sent reply
01/18/2025 19:30:03:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:30:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:30:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dadf453f-b9dc-41be-bac4-1c0424acc6c0
01/18/2025 19:30:55:INFO:Received: train message dadf453f-b9dc-41be-bac4-1c0424acc6c0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:40:31:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:53:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:53:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2688e72c-9671-4210-a8c1-ade893a8950d
01/18/2025 19:53:26:INFO:Received: evaluate message 2688e72c-9671-4210-a8c1-ade893a8950d
[92mINFO [0m:      Sent reply
01/18/2025 19:58:23:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:58:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:58:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6709ad5f-964e-4c76-8951-e887285102b6
01/18/2025 19:58:51:INFO:Received: train message 6709ad5f-964e-4c76-8951-e887285102b6

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.3122419733554125
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.291425841798385
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.2706097102413575
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.24979357868433
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031]}

BaseNM 0.41748046875
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:08:56:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:21:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:21:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1d4faa00-72ae-4338-83cb-0164316b8892
01/18/2025 20:21:14:INFO:Received: evaluate message 1d4faa00-72ae-4338-83cb-0164316b8892
[92mINFO [0m:      Sent reply
01/18/2025 20:25:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:26:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:26:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5271ee0e-e7ce-4849-a97d-a5078d45277e
01/18/2025 20:26:28:INFO:Received: train message 5271ee0e-e7ce-4849-a97d-a5078d45277e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:35:20:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:45:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:45:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5790d0ac-6647-4c4b-8032-aba0fd982474
01/18/2025 20:45:18:INFO:Received: evaluate message 5790d0ac-6647-4c4b-8032-aba0fd982474
[92mINFO [0m:      Sent reply
01/18/2025 20:48:08:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:49:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:49:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2b249951-b21a-44f7-8900-ddd74e7b4cc5
01/18/2025 20:49:54:INFO:Received: train message 2b249951-b21a-44f7-8900-ddd74e7b4cc5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:58:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:09:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:09:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 47ab8a4e-29d4-428f-ad79-042fce1fd1b7
01/18/2025 21:09:15:INFO:Received: evaluate message 47ab8a4e-29d4-428f-ad79-042fce1fd1b7
[92mINFO [0m:      Sent reply
01/18/2025 21:13:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:13:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:13:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 35c53756-df3d-4db4-8538-9c7c6241deb4
01/18/2025 21:13:44:INFO:Received: train message 35c53756-df3d-4db4-8538-9c7c6241deb4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 21:22:15:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:33:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:33:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2c883b19-b8e6-4aee-aa3f-60f0274fb3b6
01/18/2025 21:33:16:INFO:Received: evaluate message 2c883b19-b8e6-4aee-aa3f-60f0274fb3b6
[92mINFO [0m:      Sent reply
01/18/2025 21:37:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:37:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:37:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2b87fd6f-8ab9-4f09-80c7-62ea9beea988
01/18/2025 21:37:58:INFO:Received: train message 2b87fd6f-8ab9-4f09-80c7-62ea9beea988
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 21:46:46:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:56:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:56:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6f8ea275-748e-4e63-bd13-f90ef1674f71
01/18/2025 21:56:47:INFO:Received: evaluate message 6f8ea275-748e-4e63-bd13-f90ef1674f71
[92mINFO [0m:      Sent reply
01/18/2025 22:00:42:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:01:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:01:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3800cc9c-6f4f-453e-b399-4631b17c66a9
01/18/2025 22:01:26:INFO:Received: train message 3800cc9c-6f4f-453e-b399-4631b17c66a9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 22:10:22:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:20:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:20:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6eafc15f-83fe-4fbb-b379-b7773a517e22
01/18/2025 22:20:49:INFO:Received: evaluate message 6eafc15f-83fe-4fbb-b379-b7773a517e22
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.2289774471273025
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.208161315570275
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.18734518401324748
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.16652905245622
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.1457129208991925
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.124896789342165
Epsilon = 30.00
[92mINFO [0m:      Sent reply
01/18/2025 22:24:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:25:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:25:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message de42cffe-c354-4cd2-8562-b1dcf3c089be
01/18/2025 22:25:29:INFO:Received: train message de42cffe-c354-4cd2-8562-b1dcf3c089be
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 22:34:21:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:44:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:44:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eb72ad43-21f4-441e-b006-ebae177c5d83
01/18/2025 22:44:43:INFO:Received: evaluate message eb72ad43-21f4-441e-b006-ebae177c5d83
[92mINFO [0m:      Sent reply
01/18/2025 22:48:49:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:49:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:49:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 39b6bb2f-2f0f-4e00-a50e-db7b749c79dd
01/18/2025 22:49:14:INFO:Received: train message 39b6bb2f-2f0f-4e00-a50e-db7b749c79dd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 22:58:19:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:08:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:08:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3bfc18fa-d2f5-440c-ad08-290087090500
01/18/2025 23:08:48:INFO:Received: evaluate message 3bfc18fa-d2f5-440c-ad08-290087090500
[92mINFO [0m:      Sent reply
01/18/2025 23:12:56:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:13:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:13:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0e4c623d-6dee-4f31-886f-35d9a4ffeaf3
01/18/2025 23:13:24:INFO:Received: train message 0e4c623d-6dee-4f31-886f-35d9a4ffeaf3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 23:22:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:32:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:32:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6408d7db-97ca-44db-b498-f94bb5cd7000
01/18/2025 23:32:38:INFO:Received: evaluate message 6408d7db-97ca-44db-b498-f94bb5cd7000
[92mINFO [0m:      Sent reply
01/18/2025 23:36:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:37:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:37:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9b1cd5ee-2774-4a7d-99ae-b65ef706af2b
01/18/2025 23:37:16:INFO:Received: train message 9b1cd5ee-2774-4a7d-99ae-b65ef706af2b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 23:46:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:56:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:56:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dbdb657d-ffdb-475f-9555-edbda9af235b
01/18/2025 23:56:18:INFO:Received: evaluate message dbdb657d-ffdb-475f-9555-edbda9af235b

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.1040806577851375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.08326452622811001
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.062448394671082486
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.04163226311405499
Epsilon = 30.00
[92mINFO [0m:      Sent reply
01/19/2025 00:00:22:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:00:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:00:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bdc2b2ec-1b9f-4b8d-8409-88c3f09d00e1
01/19/2025 00:00:44:INFO:Received: train message bdc2b2ec-1b9f-4b8d-8409-88c3f09d00e1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/19/2025 00:09:41:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:19:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:19:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 197a2afa-58a0-4705-a10b-9ff3b302e85f
01/19/2025 00:19:53:INFO:Received: evaluate message 197a2afa-58a0-4705-a10b-9ff3b302e85f
[92mINFO [0m:      Sent reply
01/19/2025 00:24:02:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:24:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:24:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 24c14619-ff77-40b3-a3df-d07067b63708
01/19/2025 00:24:26:INFO:Received: train message 24c14619-ff77-40b3-a3df-d07067b63708
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/19/2025 00:33:26:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:43:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:43:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ecd4372c-65eb-43cf-9d89-2e246d1f57ff
01/19/2025 00:43:35:INFO:Received: evaluate message ecd4372c-65eb-43cf-9d89-2e246d1f57ff
[92mINFO [0m:      Sent reply
01/19/2025 00:47:31:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:47:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:47:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 775c217d-9032-4b2f-939b-772e7590a4db
01/19/2025 00:47:39:INFO:Received: reconnect message 775c217d-9032-4b2f-939b-772e7590a4db
01/19/2025 00:47:39:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/19/2025 00:47:39:INFO:Disconnect and shut down

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966, 117.42482489347458], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925, 0.5296012887635925], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297, 0.7193555349179758]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.020816131557027495
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966, 117.42482489347458, 117.25908535718918], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925, 0.5296012887635925, 0.5328231977446637], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297, 0.7193555349179758, 0.7212846690688237]}

BaseNM 0.41748046875
noise multiplier 0.3122419733554125
Noise multiplier before  adjustment: 0.3122419733554125
Noise multiplier before convergence adjustment: 0.3122419733554125
Updated noise multiplier after convergence adjustment: 0.0
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966, 117.42482489347458, 117.25908535718918, 117.31337916851044], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925, 0.5296012887635925, 0.5328231977446637, 0.5332259363672976], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297, 0.7193555349179758, 0.7212846690688237, 0.7229776848061777]}



Final client history:
{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966, 117.42482489347458, 117.25908535718918, 117.31337916851044], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925, 0.5296012887635925, 0.5328231977446637, 0.5332259363672976], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297, 0.7193555349179758, 0.7212846690688237, 0.7229776848061777]}

