nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/new_dynamic_noise_clients_1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/12/2025 09:56:18:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/12/2025 09:56:18:DEBUG:ChannelConnectivity.IDLE
01/12/2025 09:56:18:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/12/2025 09:56:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 09:56:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 52183a7e-10fa-4c1b-8aa2-15cc401ef1ec
01/12/2025 09:56:57:INFO:Received: train message 52183a7e-10fa-4c1b-8aa2-15cc401ef1ec
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 09:59:26:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 10:16:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:16:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ece79aec-f2c2-4703-9645-5d298c09c1a8
01/12/2025 10:16:48:INFO:Received: evaluate message ece79aec-f2c2-4703-9645-5d298c09c1a8
[92mINFO [0m:      Sent reply
01/12/2025 10:21:55:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 10:22:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:22:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3d77df35-c07c-491e-9356-4be51602e945
01/12/2025 10:22:40:INFO:Received: train message 3d77df35-c07c-491e-9356-4be51602e945
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 10:25:17:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 10:44:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:44:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ec1624da-43d2-4c0f-b4f5-057d495a7399
01/12/2025 10:44:17:INFO:Received: evaluate message ec1624da-43d2-4c0f-b4f5-057d495a7399
[92mINFO [0m:      Sent reply
01/12/2025 10:49:08:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 10:50:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:50:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d885a6dc-92d3-4aff-af02-795f43cc67ba
01/12/2025 10:50:17:INFO:Received: train message d885a6dc-92d3-4aff-af02-795f43cc67ba
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 10:52:52:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:12:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:12:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9b2f62ce-8e09-4961-9f82-1d1c672c775c
01/12/2025 11:12:07:INFO:Received: evaluate message 9b2f62ce-8e09-4961-9f82-1d1c672c775c
[92mINFO [0m:      Sent reply
01/12/2025 11:16:51:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:17:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:17:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c3efe851-308f-4d19-bed6-78d8c59ceb86
01/12/2025 11:17:54:INFO:Received: train message c3efe851-308f-4d19-bed6-78d8c59ceb86
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 11:20:38:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:39:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:39:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0ff03cd5-f87d-408d-b55c-658e95556fd7
01/12/2025 11:39:49:INFO:Received: evaluate message 0ff03cd5-f87d-408d-b55c-658e95556fd7
[92mINFO [0m:      Sent reply
01/12/2025 11:45:13:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:46:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:46:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 341c6124-6f9e-422c-a3aa-c598b641ce12
01/12/2025 11:46:10:INFO:Received: train message 341c6124-6f9e-422c-a3aa-c598b641ce12
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 11:49:36:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:09:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:09:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 87e6eb66-f092-41f9-98da-49ccdb7a75d5
01/12/2025 12:09:29:INFO:Received: evaluate message 87e6eb66-f092-41f9-98da-49ccdb7a75d5
[92mINFO [0m:      Sent reply
01/12/2025 12:13:42:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:14:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:14:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 14f878e3-0166-4d1c-b5b0-14653068ff7f
01/12/2025 12:14:00:INFO:Received: train message 14f878e3-0166-4d1c-b5b0-14653068ff7f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 12:16:02:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:37:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:37:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ffd00dc6-7887-42f4-9b85-aa4a15681b55
01/12/2025 12:37:56:INFO:Received: evaluate message ffd00dc6-7887-42f4-9b85-aa4a15681b55
[92mINFO [0m:      Sent reply
01/12/2025 12:42:31:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:43:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:43:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ccc2b680-242c-4cb7-a0d3-e3c26785d9bc
01/12/2025 12:43:06:INFO:Received: train message ccc2b680-242c-4cb7-a0d3-e3c26785d9bc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 12:45:37:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 13:08:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 13:08:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 45eedf33-93ff-47fa-8dc7-a4d3ef58a662
01/12/2025 13:08:22:INFO:Received: evaluate message 45eedf33-93ff-47fa-8dc7-a4d3ef58a662
[92mINFO [0m:      Sent reply
01/12/2025 13:12:41:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 13:12:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 13:12:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f89261b1-cf11-42e3-b593-00b19ec7e4c8
01/12/2025 13:12:53:INFO:Received: train message f89261b1-cf11-42e3-b593-00b19ec7e4c8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 13:14:38:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 13:45:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 13:45:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 905092a5-24ec-423f-867f-b0d3e1ead7b0
01/12/2025 13:45:16:INFO:Received: evaluate message 905092a5-24ec-423f-867f-b0d3e1ead7b0
[92mINFO [0m:      Sent reply
01/12/2025 13:49:34:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 13:49:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 13:49:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cf89aa91-754a-4608-b8cf-7853d2f1c173
01/12/2025 13:49:59:INFO:Received: train message cf89aa91-754a-4608-b8cf-7853d2f1c173
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 13:52:18:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 14:24:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 14:24:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b738d90c-2f33-4ca1-97ba-53f9a3790340
01/12/2025 14:24:45:INFO:Received: evaluate message b738d90c-2f33-4ca1-97ba-53f9a3790340
[92mINFO [0m:      Sent reply
01/12/2025 14:29:12:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 14:29:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 14:29:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6944b0a6-ac57-4a8b-ab19-0246c58c0c10
01/12/2025 14:29:43:INFO:Received: train message 6944b0a6-ac57-4a8b-ab19-0246c58c0c10
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 14:31:51:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:01:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:01:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ab848df5-aa41-4460-b9bb-5f8b202f1349
01/12/2025 15:01:30:INFO:Received: evaluate message ab848df5-aa41-4460-b9bb-5f8b202f1349
[92mINFO [0m:      Sent reply
01/12/2025 15:05:44:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:06:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:06:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1d9d20fe-d748-4592-99a2-51f6e279a2c5
01/12/2025 15:06:14:INFO:Received: train message 1d9d20fe-d748-4592-99a2-51f6e279a2c5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 15:08:32:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:36:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:36:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c12265d8-be0e-4915-93aa-f24643e6f56a
01/12/2025 15:36:26:INFO:Received: evaluate message c12265d8-be0e-4915-93aa-f24643e6f56a
[92mINFO [0m:      Sent reply
01/12/2025 15:41:06:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:41:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:41:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fc6829ec-173d-4f6e-a5c3-2e2bc7a4e89a
01/12/2025 15:41:51:INFO:Received: train message fc6829ec-173d-4f6e-a5c3-2e2bc7a4e89a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 15:43:56:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:08:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:08:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f33c3c24-9568-4dba-9a4a-39b6d422f65b
01/12/2025 16:08:37:INFO:Received: evaluate message f33c3c24-9568-4dba-9a4a-39b6d422f65b
[92mINFO [0m:      Sent reply
01/12/2025 16:13:31:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:15:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:15:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd64206b-47ca-4c28-a123-7bc4d64563f6
01/12/2025 16:15:03:INFO:Received: train message cd64206b-47ca-4c28-a123-7bc4d64563f6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 16:17:16:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:39:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:39:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c7afca6d-6ae8-49e4-bd43-eabf68febd01
01/12/2025 16:39:42:INFO:Received: evaluate message c7afca6d-6ae8-49e4-bd43-eabf68febd01
[92mINFO [0m:      Sent reply
01/12/2025 16:45:11:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:46:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:46:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 90895012-e429-4533-9301-4f225b022746
01/12/2025 16:46:22:INFO:Received: train message 90895012-e429-4533-9301-4f225b022746
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 16:48:52:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:11:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:11:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a9e862b7-cde5-4b90-9842-d89c6a95614d
01/12/2025 17:11:52:INFO:Received: evaluate message a9e862b7-cde5-4b90-9842-d89c6a95614d
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/new_dynamic_noise_clients_1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/new_dynamic_noise_clients_1.0']
Epoch 1 - Adjusted noise multipliers: [6.5625]
Epsilon = 0.29

{'loss': [142.32107663154602], 'accuracy': [0.3383004430124849], 'auc': [0.5426145523098456]}

Epoch 2 - Adjusted noise multipliers: [6.516566283255163]
Epsilon = 0.29

{'loss': [142.32107663154602, 134.23665523529053], 'accuracy': [0.3383004430124849, 0.3395086588803866], 'auc': [0.5426145523098456, 0.5873936166804948]}

Epoch 3 - Adjusted noise multipliers: [6.493720131978282]
Epsilon = 0.29

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956]}

Epoch 4 - Adjusted noise multipliers: [6.470954076046937]
Epsilon = 0.29

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795]}

Epoch 5 - Adjusted noise multipliers: [6.448267834658277]
Epsilon = 0.29

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267]}

Epoch 6 - Adjusted noise multipliers: [6.42566112799391]
Epsilon = 0.30

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334]}

Epoch 7 - Adjusted noise multipliers: [6.403133677216444]
Epsilon = 0.30

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264]}

Epoch 8 - Adjusted noise multipliers: [6.380685204466051]
Epsilon = 0.30

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147]}

Epoch 9 - Adjusted noise multipliers: [6.358315432857041]
Epsilon = 0.30

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062]}

Epoch 10 - Adjusted noise multipliers: [6.336024086474445]
Epsilon = 0.30

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218]}

Epoch 11 - Adjusted noise multipliers: [6.313810890370614]
Epsilon = 0.30

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423]}

Epoch 12 - Adjusted noise multipliers: [6.291675570561824]
Epsilon = 0.30

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502]}

Epoch 13 - Adjusted noise multipliers: [6.269617854024901]
Epsilon = 0.30

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675]}

Epoch 14 - Adjusted noise multipliers: [6.247637468693848]
Epsilon = 0.31
[92mINFO [0m:      Sent reply
01/12/2025 17:16:36:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:17:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:17:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a5f10ae1-6aeb-4be7-a138-597d5defb53a
01/12/2025 17:17:34:INFO:Received: train message a5f10ae1-6aeb-4be7-a138-597d5defb53a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 17:19:50:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:44:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:44:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f52a2747-b061-43c9-8ef0-5121900f1378
01/12/2025 17:44:36:INFO:Received: evaluate message f52a2747-b061-43c9-8ef0-5121900f1378
[92mINFO [0m:      Sent reply
01/12/2025 17:49:19:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:50:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:50:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 70fba043-847f-4bb5-8552-9490e8be9f21
01/12/2025 17:50:16:INFO:Received: train message 70fba043-847f-4bb5-8552-9490e8be9f21
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 17:52:28:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:16:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:16:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5fc7bddb-4ff5-46db-b30c-c5f4c2ba8f20
01/12/2025 18:16:09:INFO:Received: evaluate message 5fc7bddb-4ff5-46db-b30c-c5f4c2ba8f20
[92mINFO [0m:      Sent reply
01/12/2025 18:21:33:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:22:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:22:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c0c982f5-4883-4e80-93fb-c077814b8178
01/12/2025 18:22:30:INFO:Received: train message c0c982f5-4883-4e80-93fb-c077814b8178
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 18:24:36:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:46:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:46:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 91a38454-005e-4263-8d1b-b633dbc2c5ff
01/12/2025 18:46:36:INFO:Received: evaluate message 91a38454-005e-4263-8d1b-b633dbc2c5ff
[92mINFO [0m:      Sent reply
01/12/2025 18:52:04:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:53:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:53:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 65c68bc1-973e-4fe4-bad7-5eb485b46590
01/12/2025 18:53:16:INFO:Received: train message 65c68bc1-973e-4fe4-bad7-5eb485b46590
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 18:55:37:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:18:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:18:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f23a2c36-2b60-42c4-93b1-f80693266d5b
01/12/2025 19:18:44:INFO:Received: evaluate message f23a2c36-2b60-42c4-93b1-f80693266d5b
[92mINFO [0m:      Sent reply
01/12/2025 19:24:03:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:24:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:24:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c4d6f7b2-5b0d-4402-869e-4340012a48fd
01/12/2025 19:24:50:INFO:Received: train message c4d6f7b2-5b0d-4402-869e-4340012a48fd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 19:26:21:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:51:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:51:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a0d460f4-bf9d-4638-aae6-5a0ed99d8452
01/12/2025 19:51:36:INFO:Received: evaluate message a0d460f4-bf9d-4638-aae6-5a0ed99d8452
[92mINFO [0m:      Sent reply
01/12/2025 19:56:29:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:56:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:56:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0dcc89a5-f2bf-4e00-abb8-8134a6c20fc2
01/12/2025 19:56:54:INFO:Received: train message 0dcc89a5-f2bf-4e00-abb8-8134a6c20fc2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 19:58:13:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 20:24:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 20:24:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ac45e569-baff-4d38-97a4-1e6b51591fd3
01/12/2025 20:24:21:INFO:Received: evaluate message ac45e569-baff-4d38-97a4-1e6b51591fd3
[92mINFO [0m:      Sent reply
01/12/2025 20:29:12:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 20:30:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 20:30:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1795596e-6722-4159-9fe6-bf8c263729b2
01/12/2025 20:30:05:INFO:Received: train message 1795596e-6722-4159-9fe6-bf8c263729b2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 20:32:23:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 20:55:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 20:55:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message df4723f3-7b3e-4e4d-9865-fc3d37253ce7
01/12/2025 20:55:57:INFO:Received: evaluate message df4723f3-7b3e-4e4d-9865-fc3d37253ce7

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922]}

Epoch 15 - Adjusted noise multipliers: [6.225734143456497]
Epsilon = 0.31

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373]}

Epoch 16 - Adjusted noise multipliers: [6.203907608151158]
Epsilon = 0.31

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182]}

Epoch 17 - Adjusted noise multipliers: [6.1821575935632875]
Epsilon = 0.31

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125]}

Epoch 18 - Adjusted noise multipliers: [6.160483831422174]
Epsilon = 0.31

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858]}

Epoch 19 - Adjusted noise multipliers: [6.138886054397624]
Epsilon = 0.31

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044]}

Epoch 20 - Adjusted noise multipliers: [6.1173639960966595]
Epsilon = 0.31

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349]}

Epoch 21 - Adjusted noise multipliers: [6.095917391060247]
Epsilon = 0.31
[92mINFO [0m:      Sent reply
01/12/2025 21:01:46:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 21:02:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 21:02:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ac0618d9-7506-41f3-9d50-3cc3cf00d069
01/12/2025 21:02:50:INFO:Received: train message ac0618d9-7506-41f3-9d50-3cc3cf00d069
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 21:04:59:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 21:54:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 21:54:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c4ed04ec-3975-4633-90e6-48d29e59d29a
01/12/2025 21:54:13:INFO:Received: evaluate message c4ed04ec-3975-4633-90e6-48d29e59d29a
[92mINFO [0m:      Sent reply
01/12/2025 22:00:30:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 22:02:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 22:02:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 494bdd62-d48d-4f05-8e40-4570b3bd0cba
01/12/2025 22:02:35:INFO:Received: train message 494bdd62-d48d-4f05-8e40-4570b3bd0cba
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 22:05:38:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 22:53:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 22:53:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 646e0782-a887-4977-9ecf-d690021733e4
01/12/2025 22:53:13:INFO:Received: evaluate message 646e0782-a887-4977-9ecf-d690021733e4
[92mINFO [0m:      Sent reply
01/12/2025 22:59:30:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 23:01:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 23:01:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ec5a084-1717-43fd-a787-bbf07872d874
01/12/2025 23:01:12:INFO:Received: train message 8ec5a084-1717-43fd-a787-bbf07872d874
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 23:03:34:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 23:58:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 23:58:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4a89af0e-4680-4963-a97d-9c79ffee2e5d
01/12/2025 23:58:33:INFO:Received: evaluate message 4a89af0e-4680-4963-a97d-9c79ffee2e5d
[92mINFO [0m:      Sent reply
01/13/2025 00:02:59:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 00:07:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 00:07:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f5330845-f5af-4c01-8659-fbf9b78c165e
01/13/2025 00:07:59:INFO:Received: train message f5330845-f5af-4c01-8659-fbf9b78c165e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 00:10:16:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:15:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:15:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 07b8d880-de1f-461c-8997-b954473c85ca
01/13/2025 01:15:00:INFO:Received: evaluate message 07b8d880-de1f-461c-8997-b954473c85ca
[92mINFO [0m:      Sent reply
01/13/2025 01:19:42:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:23:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:23:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 917b269e-c7a6-4282-8923-aba9f6f727b1
01/13/2025 01:23:57:INFO:Received: train message 917b269e-c7a6-4282-8923-aba9f6f727b1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 01:24:51:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:54:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:54:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 44056256-6ef5-4c2d-812a-51203bcd8fdf
01/13/2025 01:54:35:INFO:Received: evaluate message 44056256-6ef5-4c2d-812a-51203bcd8fdf

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199]}

Epoch 22 - Adjusted noise multipliers: [6.07454597476001]
Epsilon = 0.31

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075]}

Epoch 23 - Adjusted noise multipliers: [6.05324948359497]
Epsilon = 0.32

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279]}

Epoch 24 - Adjusted noise multipliers: [6.032027654888298]
Epsilon = 0.32

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798]}

Epoch 25 - Adjusted noise multipliers: [6.010880226884071]
Epsilon = 0.32

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997]}

Epoch 26 - Adjusted noise multipliers: [5.989806938744045]
Epsilon = 0.32
[92mINFO [0m:      Sent reply
01/13/2025 01:58:21:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:01:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:01:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1b216c04-4b31-4a45-af28-f7a6afd2c6b3
01/13/2025 02:01:50:INFO:Received: train message 1b216c04-4b31-4a45-af28-f7a6afd2c6b3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 02:04:01:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:32:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:32:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 73b2680d-5fa3-48fe-87f2-c61b24aa94dd
01/13/2025 02:32:02:INFO:Received: evaluate message 73b2680d-5fa3-48fe-87f2-c61b24aa94dd
[92mINFO [0m:      Sent reply
01/13/2025 02:37:08:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:37:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:37:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a750b07c-7155-410d-8b29-675aa92adbb1
01/13/2025 02:37:58:INFO:Received: train message a750b07c-7155-410d-8b29-675aa92adbb1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 02:40:25:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:00:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:00:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 00961294-f1fb-4be5-ad7e-67d52d9bc1aa
01/13/2025 03:00:23:INFO:Received: evaluate message 00961294-f1fb-4be5-ad7e-67d52d9bc1aa
[92mINFO [0m:      Sent reply
01/13/2025 03:05:19:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:05:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:05:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 86a0ec6b-1f28-4a68-9e32-d736ef478199
01/13/2025 03:05:36:INFO:Received: train message 86a0ec6b-1f28-4a68-9e32-d736ef478199
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 03:07:18:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:31:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:31:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ab1b50c4-42e6-4473-8503-b4fbba6a6ba0
01/13/2025 03:31:13:INFO:Received: evaluate message ab1b50c4-42e6-4473-8503-b4fbba6a6ba0
[92mINFO [0m:      Sent reply
01/13/2025 03:36:47:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:37:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:37:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 48e2b96c-2b8e-458a-a9db-dff5a55168cf
01/13/2025 03:37:31:INFO:Received: train message 48e2b96c-2b8e-458a-a9db-dff5a55168cf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 03:40:05:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 04:00:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 04:00:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d744424e-bfb0-4da0-95bc-83079efa0ffd
01/13/2025 04:00:33:INFO:Received: evaluate message d744424e-bfb0-4da0-95bc-83079efa0ffd

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927, 120.14814364910126], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804, 0.5259766411598872], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997, 0.7108235099425215]}

Epoch 27 - Adjusted noise multipliers: [5.9688075305444395]
Epsilon = 0.32

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927, 120.14814364910126, 120.49823808670044], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804, 0.5259766411598872, 0.5267821184051551], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997, 0.7108235099425215, 0.712988051186769]}

Epoch 28 - Adjusted noise multipliers: [5.947881743272728]
Epsilon = 0.32

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927, 120.14814364910126, 120.49823808670044, 120.97020924091339], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804, 0.5259766411598872, 0.5267821184051551, 0.527184857027789], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997, 0.7108235099425215, 0.712988051186769, 0.7152528953486761]}

Epoch 29 - Adjusted noise multipliers: [5.927029318824443]
Epsilon = 0.32

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927, 120.14814364910126, 120.49823808670044, 120.97020924091339, 120.67132663726807], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804, 0.5259766411598872, 0.5267821184051551, 0.527184857027789, 0.5296012887635925], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997, 0.7108235099425215, 0.712988051186769, 0.7152528953486761, 0.7179602061623762]}

Epoch 30 - Adjusted noise multipliers: [5.90625]
Epsilon = 0.33
[92mINFO [0m:      Sent reply
01/13/2025 04:04:59:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 04:05:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 04:05:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 9dc90915-f987-4495-9548-e6baa0340077
01/13/2025 04:05:18:INFO:Received: reconnect message 9dc90915-f987-4495-9548-e6baa0340077
01/13/2025 04:05:18:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/13/2025 04:05:18:INFO:Disconnect and shut down

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927, 120.14814364910126, 120.49823808670044, 120.97020924091339, 120.67132663726807, 120.4772458076477], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804, 0.5259766411598872, 0.5267821184051551, 0.527184857027789, 0.5296012887635925, 0.5291985501409585], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997, 0.7108235099425215, 0.712988051186769, 0.7152528953486761, 0.7179602061623762, 0.7176607396735059]}



Final client history:
{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927, 120.14814364910126, 120.49823808670044, 120.97020924091339, 120.67132663726807, 120.4772458076477], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804, 0.5259766411598872, 0.5267821184051551, 0.527184857027789, 0.5296012887635925, 0.5291985501409585], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997, 0.7108235099425215, 0.712988051186769, 0.7152528953486761, 0.7179602061623762, 0.7176607396735059]}

