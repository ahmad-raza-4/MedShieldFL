nohup: ignoring input
01/26/2025 22:40:08:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/26/2025 22:40:08:DEBUG:ChannelConnectivity.IDLE
01/26/2025 22:40:08:DEBUG:ChannelConnectivity.CONNECTING
01/26/2025 22:40:08:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/26/2025 22:40:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:40:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message bca82dbc-db35-4858-8b30-5e6140db2618
01/26/2025 22:40:08:INFO:Received: get_parameters message bca82dbc-db35-4858-8b30-5e6140db2618
[92mINFO [0m:      Sent reply
01/26/2025 22:40:13:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:40:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:40:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0801150e-bd15-4c77-bd0b-63b009f32482
01/26/2025 22:40:45:INFO:Received: train message 0801150e-bd15-4c77-bd0b-63b009f32482
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:41:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:41:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:41:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 03092c20-a47d-4be2-a742-686f9a5b902f
01/26/2025 22:41:46:INFO:Received: evaluate message 03092c20-a47d-4be2-a742-686f9a5b902f
[92mINFO [0m:      Sent reply
01/26/2025 22:41:48:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:42:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:42:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d5c4933a-ecd7-422d-b43e-9d65e7c79fd6
01/26/2025 22:42:32:INFO:Received: train message d5c4933a-ecd7-422d-b43e-9d65e7c79fd6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:42:55:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:43:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:43:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6647f9f7-b860-4452-9b69-cd4e1a637e12
01/26/2025 22:43:58:INFO:Received: evaluate message 6647f9f7-b860-4452-9b69-cd4e1a637e12
[92mINFO [0m:      Sent reply
01/26/2025 22:44:02:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:44:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:44:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c1f71b3d-b652-4617-9097-57ec8148dd42
01/26/2025 22:44:50:INFO:Received: train message c1f71b3d-b652-4617-9097-57ec8148dd42
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:45:13:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:46:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:46:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 513a9b6c-c0f7-436b-95a0-ef59c6319f41
01/26/2025 22:46:01:INFO:Received: evaluate message 513a9b6c-c0f7-436b-95a0-ef59c6319f41
[92mINFO [0m:      Sent reply
01/26/2025 22:46:09:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:46:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:46:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 31cd0114-7f0f-4956-a202-aeb45014c2d7
01/26/2025 22:46:53:INFO:Received: train message 31cd0114-7f0f-4956-a202-aeb45014c2d7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:47:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:48:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:48:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5d9949b-59e8-4af8-9350-6fca5135e4c6
01/26/2025 22:48:09:INFO:Received: evaluate message b5d9949b-59e8-4af8-9350-6fca5135e4c6
[92mINFO [0m:      Sent reply
01/26/2025 22:48:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:48:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:48:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 095bd7ed-1c11-49c9-92f7-11c857fea4b0
01/26/2025 22:48:48:INFO:Received: train message 095bd7ed-1c11-49c9-92f7-11c857fea4b0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:49:17:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:49:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:49:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 513f1564-e9b7-4c21-b01e-287fb520bb6c
01/26/2025 22:49:58:INFO:Received: evaluate message 513f1564-e9b7-4c21-b01e-287fb520bb6c
[92mINFO [0m:      Sent reply
01/26/2025 22:50:02:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:50:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:50:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ca44bd6f-5e7c-4e97-a7be-db5333f558c2
01/26/2025 22:50:46:INFO:Received: train message ca44bd6f-5e7c-4e97-a7be-db5333f558c2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:51:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:52:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:52:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9eaec376-8038-40f3-9349-8b60ef737635
01/26/2025 22:52:10:INFO:Received: evaluate message 9eaec376-8038-40f3-9349-8b60ef737635
[92mINFO [0m:      Sent reply
01/26/2025 22:52:13:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:52:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:52:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b9a5b8f9-18b6-43b9-a596-14cb5c0595cd
01/26/2025 22:52:54:INFO:Received: train message b9a5b8f9-18b6-43b9-a596-14cb5c0595cd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:53:25:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:54:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:54:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9f5f1cc5-7e28-4f2e-9a27-d6273a9443d7
01/26/2025 22:54:17:INFO:Received: evaluate message 9f5f1cc5-7e28-4f2e-9a27-d6273a9443d7
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30, target_epsilon: 30, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544], 'accuracy': [0.5199374511336982], 'auc': [0.7254494785945842]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378], 'accuracy': [0.5199374511336982, 0.5238467552775606], 'auc': [0.7254494785945842, 0.7453794430136471]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 22:54:21:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:55:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:55:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 487e47b7-d906-4d17-a41b-95481aab1837
01/26/2025 22:55:10:INFO:Received: train message 487e47b7-d906-4d17-a41b-95481aab1837
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:55:41:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:56:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:56:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4ee3e676-79a7-4fe0-9543-b275e6b49855
01/26/2025 22:56:34:INFO:Received: evaluate message 4ee3e676-79a7-4fe0-9543-b275e6b49855
[92mINFO [0m:      Sent reply
01/26/2025 22:56:40:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:57:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:57:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c8135b58-1fbf-41fd-aa42-f539db4dfb9b
01/26/2025 22:57:16:INFO:Received: train message c8135b58-1fbf-41fd-aa42-f539db4dfb9b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:57:49:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:58:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:58:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1ce93dbb-c4dd-46ab-8c2d-f7aabe22f38b
01/26/2025 22:58:43:INFO:Received: evaluate message 1ce93dbb-c4dd-46ab-8c2d-f7aabe22f38b
[92mINFO [0m:      Sent reply
01/26/2025 22:58:47:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:59:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:59:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8e30f312-257e-4ba7-b4ba-d0be31b0db47
01/26/2025 22:59:28:INFO:Received: train message 8e30f312-257e-4ba7-b4ba-d0be31b0db47
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:59:58:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:00:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:00:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f42da965-fe68-4fad-8994-8e268d3e5600
01/26/2025 23:00:55:INFO:Received: evaluate message f42da965-fe68-4fad-8994-8e268d3e5600
[92mINFO [0m:      Sent reply
01/26/2025 23:00:59:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:01:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:01:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8f930e46-df9a-4e5f-b130-b38c12b4b26e
01/26/2025 23:01:44:INFO:Received: train message 8f930e46-df9a-4e5f-b130-b38c12b4b26e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:02:17:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:03:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:03:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 51d0ac00-057f-45d8-b915-47589c6466d5
01/26/2025 23:03:20:INFO:Received: evaluate message 51d0ac00-057f-45d8-b915-47589c6466d5
[92mINFO [0m:      Sent reply
01/26/2025 23:03:24:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:03:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:03:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 06e47a75-2998-41c2-bb23-065ba91ac527
01/26/2025 23:03:44:INFO:Received: train message 06e47a75-2998-41c2-bb23-065ba91ac527
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:04:10:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:05:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:05:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a48b04bc-b4ad-471d-bff3-df781b499ada
01/26/2025 23:05:03:INFO:Received: evaluate message a48b04bc-b4ad-471d-bff3-df781b499ada
[92mINFO [0m:      Sent reply
01/26/2025 23:05:06:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:05:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:05:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9561a845-ec73-42b8-8c3f-440c6ebf389e
01/26/2025 23:05:45:INFO:Received: train message 9561a845-ec73-42b8-8c3f-440c6ebf389e

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:06:12:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:07:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:07:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ca5686aa-cb49-4932-80e1-adc13391ec2d
01/26/2025 23:07:04:INFO:Received: evaluate message ca5686aa-cb49-4932-80e1-adc13391ec2d
[92mINFO [0m:      Sent reply
01/26/2025 23:07:08:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:07:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:07:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8857f866-2e47-4043-810a-07b3e73cce82
01/26/2025 23:07:40:INFO:Received: train message 8857f866-2e47-4043-810a-07b3e73cce82
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:08:07:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:08:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:08:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 813bb59e-303f-4bd0-be7b-adcee5dff428
01/26/2025 23:08:49:INFO:Received: evaluate message 813bb59e-303f-4bd0-be7b-adcee5dff428
[92mINFO [0m:      Sent reply
01/26/2025 23:08:52:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:09:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:09:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4b8c85d3-7d6b-44aa-aad8-567269d22ee5
01/26/2025 23:09:36:INFO:Received: train message 4b8c85d3-7d6b-44aa-aad8-567269d22ee5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:09:58:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:11:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:11:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9e4c39e1-8e40-4fa0-8ee5-99d2315cb4bb
01/26/2025 23:11:04:INFO:Received: evaluate message 9e4c39e1-8e40-4fa0-8ee5-99d2315cb4bb
[92mINFO [0m:      Sent reply
01/26/2025 23:11:09:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:11:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:11:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c69fe32c-8386-486d-89c9-feba653382c0
01/26/2025 23:11:59:INFO:Received: train message c69fe32c-8386-486d-89c9-feba653382c0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:12:25:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:13:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:13:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8f625a67-2043-476a-b4a0-01c24e4c031a
01/26/2025 23:13:19:INFO:Received: evaluate message 8f625a67-2043-476a-b4a0-01c24e4c031a
[92mINFO [0m:      Sent reply
01/26/2025 23:13:26:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:14:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:14:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 78fabd9d-1505-45c7-90de-fe5578882ac1
01/26/2025 23:14:09:INFO:Received: train message 78fabd9d-1505-45c7-90de-fe5578882ac1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:14:36:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:15:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:15:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7f44df4f-919b-4dd3-9c36-1974a1ef6943
01/26/2025 23:15:33:INFO:Received: evaluate message 7f44df4f-919b-4dd3-9c36-1974a1ef6943
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:15:37:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:16:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:16:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f4eb5699-6514-4091-8e1d-7d7a20ba9947
01/26/2025 23:16:23:INFO:Received: train message f4eb5699-6514-4091-8e1d-7d7a20ba9947
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:17:01:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:17:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:17:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b7efb863-6238-46e9-b0f8-6ce057d9bba6
01/26/2025 23:17:42:INFO:Received: evaluate message b7efb863-6238-46e9-b0f8-6ce057d9bba6
[92mINFO [0m:      Sent reply
01/26/2025 23:17:46:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:18:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:18:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d6ae31e7-a83a-49c8-82ac-a7b950ac5907
01/26/2025 23:18:52:INFO:Received: train message d6ae31e7-a83a-49c8-82ac-a7b950ac5907
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:19:20:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:20:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:20:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3f68859f-a4e0-4e48-8ab4-a4127e1390d2
01/26/2025 23:20:21:INFO:Received: evaluate message 3f68859f-a4e0-4e48-8ab4-a4127e1390d2
[92mINFO [0m:      Sent reply
01/26/2025 23:20:26:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:21:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:21:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 99e8d04d-05d8-4c7e-9cca-1c13ab76ace0
01/26/2025 23:21:17:INFO:Received: train message 99e8d04d-05d8-4c7e-9cca-1c13ab76ace0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:21:46:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:22:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:22:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7584a59c-4e88-45eb-bf61-34cb18c3891a
01/26/2025 23:22:49:INFO:Received: evaluate message 7584a59c-4e88-45eb-bf61-34cb18c3891a
[92mINFO [0m:      Sent reply
01/26/2025 23:22:55:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:23:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:23:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ff33d8c7-4714-42dc-a62a-dacc72215395
01/26/2025 23:23:08:INFO:Received: train message ff33d8c7-4714-42dc-a62a-dacc72215395
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:23:45:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:25:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:25:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 727a929d-dc91-4d80-8dfa-a3aa40eb70b9
01/26/2025 23:25:13:INFO:Received: evaluate message 727a929d-dc91-4d80-8dfa-a3aa40eb70b9

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:25:18:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:25:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:25:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6af9639d-d22a-4f6b-ba2e-5dfdeb911757
01/26/2025 23:25:50:INFO:Received: train message 6af9639d-d22a-4f6b-ba2e-5dfdeb911757
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:26:18:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:26:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:26:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 22271738-3372-4c84-8b40-bae1cb657288
01/26/2025 23:26:43:INFO:Received: evaluate message 22271738-3372-4c84-8b40-bae1cb657288
[92mINFO [0m:      Sent reply
01/26/2025 23:26:47:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:27:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:27:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6be984cb-a28d-44ae-afdf-58c90c51e95c
01/26/2025 23:27:27:INFO:Received: train message 6be984cb-a28d-44ae-afdf-58c90c51e95c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:27:54:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:28:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:28:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 386319b3-3102-4095-8a0d-f865004e27e3
01/26/2025 23:28:44:INFO:Received: evaluate message 386319b3-3102-4095-8a0d-f865004e27e3
[92mINFO [0m:      Sent reply
01/26/2025 23:28:49:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:29:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:29:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9a271a2d-dbfe-497c-a48f-867ef811b3e6
01/26/2025 23:29:33:INFO:Received: train message 9a271a2d-dbfe-497c-a48f-867ef811b3e6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:30:02:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:30:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:30:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 68e2d32d-f0b7-49fd-82c7-3100fe8387dc
01/26/2025 23:30:46:INFO:Received: evaluate message 68e2d32d-f0b7-49fd-82c7-3100fe8387dc
[92mINFO [0m:      Sent reply
01/26/2025 23:30:50:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:31:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:31:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 00e0b0c4-8cfc-4fa6-8472-4d8ec3a40907
01/26/2025 23:31:10:INFO:Received: train message 00e0b0c4-8cfc-4fa6-8472-4d8ec3a40907

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:31:35:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:32:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:32:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8e9894bb-1206-47c4-a136-489f437378f6
01/26/2025 23:32:22:INFO:Received: evaluate message 8e9894bb-1206-47c4-a136-489f437378f6
[92mINFO [0m:      Sent reply
01/26/2025 23:32:26:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:33:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:33:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b4e72e07-d0e8-4b48-b579-a94cf6dd6177
01/26/2025 23:33:06:INFO:Received: train message b4e72e07-d0e8-4b48-b579-a94cf6dd6177
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:33:49:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:34:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:34:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 50fe9413-c72b-4f1b-bedf-f79a6490dd22
01/26/2025 23:34:38:INFO:Received: evaluate message 50fe9413-c72b-4f1b-bedf-f79a6490dd22
[92mINFO [0m:      Sent reply
01/26/2025 23:34:44:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:35:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:35:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2dcc89d3-1e4e-483d-b2fd-6346c392f35b
01/26/2025 23:35:19:INFO:Received: train message 2dcc89d3-1e4e-483d-b2fd-6346c392f35b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:35:45:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:36:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:36:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 53a02148-2336-41f9-962a-d59c9a12bf5b
01/26/2025 23:36:26:INFO:Received: evaluate message 53a02148-2336-41f9-962a-d59c9a12bf5b
[92mINFO [0m:      Sent reply
01/26/2025 23:36:29:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:36:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:36:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e1eb1457-82ad-42f7-acab-25440096840d
01/26/2025 23:36:58:INFO:Received: train message e1eb1457-82ad-42f7-acab-25440096840d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:37:27:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:38:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:38:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fdf0261f-c950-4055-b55a-fd5caa5db3ea
01/26/2025 23:38:12:INFO:Received: evaluate message fdf0261f-c950-4055-b55a-fd5caa5db3ea
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
[92mINFO [0m:      Sent reply
01/26/2025 23:38:16:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:38:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:38:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 158dca40-32d9-4bbd-9cbc-668cc84e0e6f
01/26/2025 23:38:43:INFO:Received: train message 158dca40-32d9-4bbd-9cbc-668cc84e0e6f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:39:09:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:39:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:39:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7a6fec4c-4b25-47f5-81ce-9af697c45a92
01/26/2025 23:39:59:INFO:Received: evaluate message 7a6fec4c-4b25-47f5-81ce-9af697c45a92
[92mINFO [0m:      Sent reply
01/26/2025 23:40:03:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:41:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:41:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 30adc124-7230-4b24-8c62-58312fe8ed5d
01/26/2025 23:41:02:INFO:Received: train message 30adc124-7230-4b24-8c62-58312fe8ed5d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:41:36:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:42:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:42:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 45990ac3-d17b-4aea-977e-5de7788de609
01/26/2025 23:42:36:INFO:Received: evaluate message 45990ac3-d17b-4aea-977e-5de7788de609
[92mINFO [0m:      Sent reply
01/26/2025 23:42:41:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:42:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:42:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message f080cee6-1c2e-45da-895d-6fa0002eae56
01/26/2025 23:42:41:INFO:Received: reconnect message f080cee6-1c2e-45da-895d-6fa0002eae56
01/26/2025 23:42:41:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/26/2025 23:42:41:INFO:Disconnect and shut down
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3957366943359375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.008361379615962505, 0.0025497872848063707, 0.015941936522722244, 0.003049680031836033]
Noise Multiplier after list and tensor:  0.007475695863831788
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504, 1.0184915054580772], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836, 0.5918686473807663], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188, 0.8098659294281917]}



Final client history:
{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504, 1.0184915054580772], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836, 0.5918686473807663], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188, 0.8098659294281917]}

