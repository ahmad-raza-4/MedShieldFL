nohup: ignoring input
01/25/2025 11:18:42:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/25/2025 11:18:42:DEBUG:ChannelConnectivity.IDLE
01/25/2025 11:18:42:DEBUG:ChannelConnectivity.CONNECTING
01/25/2025 11:18:42:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/25/2025 11:18:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:18:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message 9902eb22-846d-4c1e-877c-c1c29f45c078
01/25/2025 11:18:43:INFO:Received: get_parameters message 9902eb22-846d-4c1e-877c-c1c29f45c078
[92mINFO [0m:      Sent reply
01/25/2025 11:18:50:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:19:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:19:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7372afcd-6fe2-4042-9f32-d7c7ae52468d
01/25/2025 11:19:01:INFO:Received: train message 7372afcd-6fe2-4042-9f32-d7c7ae52468d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
01/25/2025 11:19:08:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.
BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:19:18:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:19:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:19:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9edd65eb-abd1-4fb2-8f5f-c229428aa6c5
01/25/2025 11:19:37:INFO:Received: evaluate message 9edd65eb-abd1-4fb2-8f5f-c229428aa6c5
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:19:42:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:19:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:19:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 44e9e054-9d24-4131-b12d-ac822530e5d1
01/25/2025 11:19:51:INFO:Received: train message 44e9e054-9d24-4131-b12d-ac822530e5d1

{'loss': [48.86307740211487], 'accuracy': [0.35105551211884284], 'auc': [0.3711869239492664]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:20:09:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:20:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:20:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 23fedcbd-2a79-4830-b780-79847f79b91d
01/25/2025 11:20:30:INFO:Received: evaluate message 23fedcbd-2a79-4830-b780-79847f79b91d
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:20:33:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:20:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:20:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dd542ddd-119d-46c4-a17a-dfd3e06c2cf8
01/25/2025 11:20:41:INFO:Received: train message dd542ddd-119d-46c4-a17a-dfd3e06c2cf8

{'loss': [48.86307740211487, 53.19741877913475], 'accuracy': [0.35105551211884284, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:21:01:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:21:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:21:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 563cc757-e58d-45ac-b071-66964a295db9
01/25/2025 11:21:20:INFO:Received: evaluate message 563cc757-e58d-45ac-b071-66964a295db9
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:21:23:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:21:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:21:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fc37b4ea-ac6f-4b2d-be95-7b8649257193
01/25/2025 11:21:30:INFO:Received: train message fc37b4ea-ac6f-4b2d-be95-7b8649257193

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:21:50:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:22:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:22:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3ccc3a7a-b0ee-4351-8f6a-9376a241e3df
01/25/2025 11:22:03:INFO:Received: evaluate message 3ccc3a7a-b0ee-4351-8f6a-9376a241e3df
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:22:05:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:22:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:22:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 10a5e743-b67b-4e11-980e-b926bff5bbe5
01/25/2025 11:22:12:INFO:Received: train message 10a5e743-b67b-4e11-980e-b926bff5bbe5

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:22:24:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:22:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:22:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 606ee9d8-db2f-499f-bfe4-ea6376172235
01/25/2025 11:22:38:INFO:Received: evaluate message 606ee9d8-db2f-499f-bfe4-ea6376172235
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:22:40:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:22:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:22:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 78babdd0-2797-47d4-bc4a-7912f893226d
01/25/2025 11:22:47:INFO:Received: train message 78babdd0-2797-47d4-bc4a-7912f893226d

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:23:01:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:23:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:23:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dc616967-39d3-40d5-a794-e46a307cfcec
01/25/2025 11:23:14:INFO:Received: evaluate message dc616967-39d3-40d5-a794-e46a307cfcec
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:23:16:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:23:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:23:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6813276f-f4ed-415a-9221-04b61bccf9b9
01/25/2025 11:23:24:INFO:Received: train message 6813276f-f4ed-415a-9221-04b61bccf9b9

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:23:38:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:23:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:23:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message caafdcde-f517-440a-9ffa-fb0b6ba9b8c0
01/25/2025 11:23:54:INFO:Received: evaluate message caafdcde-f517-440a-9ffa-fb0b6ba9b8c0
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:23:56:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:24:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:24:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message acf87768-074b-4750-98cd-9d6e000bb81b
01/25/2025 11:24:02:INFO:Received: train message acf87768-074b-4750-98cd-9d6e000bb81b

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:24:17:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:24:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:24:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0425e6d9-debf-4903-a183-ec9d1be96fad
01/25/2025 11:24:30:INFO:Received: evaluate message 0425e6d9-debf-4903-a183-ec9d1be96fad
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:24:32:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:24:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:24:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6cc54316-217b-4788-a73c-1671ccf7f76c
01/25/2025 11:24:38:INFO:Received: train message 6cc54316-217b-4788-a73c-1671ccf7f76c

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:24:55:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:25:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:25:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8a4bbc1c-3c77-4d34-8d07-b7507569834e
01/25/2025 11:25:09:INFO:Received: evaluate message 8a4bbc1c-3c77-4d34-8d07-b7507569834e
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:25:11:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:25:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:25:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5e75c571-3b1f-48d4-87fe-71ce0fd5ed30
01/25/2025 11:25:18:INFO:Received: train message 5e75c571-3b1f-48d4-87fe-71ce0fd5ed30

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:25:32:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:25:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:25:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2cb4b80d-ddad-4ca8-b649-fbed425f5a53
01/25/2025 11:25:46:INFO:Received: evaluate message 2cb4b80d-ddad-4ca8-b649-fbed425f5a53
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:25:48:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:25:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:25:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2881581e-d26e-4288-ad86-f1b9f7d91899
01/25/2025 11:25:55:INFO:Received: train message 2881581e-d26e-4288-ad86-f1b9f7d91899

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:26:11:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:26:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:26:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 412774ce-fe49-4687-9f6b-b0a9230fbc62
01/25/2025 11:26:24:INFO:Received: evaluate message 412774ce-fe49-4687-9f6b-b0a9230fbc62
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:26:26:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:26:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:26:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a6524b2f-ceff-455f-9177-b3fceecc6477
01/25/2025 11:26:33:INFO:Received: train message a6524b2f-ceff-455f-9177-b3fceecc6477

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:26:48:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:27:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:27:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d1f81899-b0a1-4287-aefc-03e2c583ae3f
01/25/2025 11:27:01:INFO:Received: evaluate message d1f81899-b0a1-4287-aefc-03e2c583ae3f
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:27:03:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:27:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:27:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d43c4635-b8c7-4e54-a983-44a95f6d00c1
01/25/2025 11:27:10:INFO:Received: train message d43c4635-b8c7-4e54-a983-44a95f6d00c1

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:27:25:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:27:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:27:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d9f9ace1-eff8-436a-a1a5-72c32cf2eacd
01/25/2025 11:27:38:INFO:Received: evaluate message d9f9ace1-eff8-436a-a1a5-72c32cf2eacd
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:27:40:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:27:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:27:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a8d37ee9-69ec-44d7-9b2c-08ee6bd62616
01/25/2025 11:27:46:INFO:Received: train message a8d37ee9-69ec-44d7-9b2c-08ee6bd62616

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:28:01:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:28:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:28:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 008ac8c7-677c-4901-9da7-d5e5b8391f9f
01/25/2025 11:28:15:INFO:Received: evaluate message 008ac8c7-677c-4901-9da7-d5e5b8391f9f
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:28:17:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:28:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:28:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5c45f4e5-6047-469b-be3d-bd3b9aa5ca3e
01/25/2025 11:28:24:INFO:Received: train message 5c45f4e5-6047-469b-be3d-bd3b9aa5ca3e

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:28:40:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:28:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:28:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ce80fc29-4105-4047-b6fd-c2bc96f27421
01/25/2025 11:28:53:INFO:Received: evaluate message ce80fc29-4105-4047-b6fd-c2bc96f27421
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:28:55:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:29:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:29:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bc5799c5-5a3c-495a-8700-01aa31ebd645
01/25/2025 11:29:02:INFO:Received: train message bc5799c5-5a3c-495a-8700-01aa31ebd645

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6388829191525778
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:29:16:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:29:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:29:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f2144647-15b5-441b-a6e5-a6fe1ca7e864
01/25/2025 11:29:29:INFO:Received: evaluate message f2144647-15b5-441b-a6e5-a6fe1ca7e864
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:29:32:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:29:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:29:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3357f02a-2c84-44d4-9560-9368da406c8e
01/25/2025 11:29:39:INFO:Received: train message 3357f02a-2c84-44d4-9560-9368da406c8e

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.5932484249273936
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:29:54:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:30:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:30:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 463e2690-8783-41be-bc65-a1ab782263f4
01/25/2025 11:30:07:INFO:Received: evaluate message 463e2690-8783-41be-bc65-a1ab782263f4
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:30:09:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:30:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:30:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e5b318cc-6cdc-414a-87a3-9973901c04ca
01/25/2025 11:30:16:INFO:Received: train message e5b318cc-6cdc-414a-87a3-9973901c04ca

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.5476139307022095
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:30:31:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:30:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:30:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 457a8263-669e-478e-a902-e86d1f8c6cae
01/25/2025 11:30:45:INFO:Received: evaluate message 457a8263-669e-478e-a902-e86d1f8c6cae
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:30:47:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:30:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:30:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5b1476ed-e08e-4bbc-be20-020de511f674
01/25/2025 11:30:54:INFO:Received: train message 5b1476ed-e08e-4bbc-be20-020de511f674

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.5019794364770254
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:31:10:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:31:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:31:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 619ccd7c-a3e4-4224-a5b6-07303b68a1fc
01/25/2025 11:31:24:INFO:Received: evaluate message 619ccd7c-a3e4-4224-a5b6-07303b68a1fc
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:31:26:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:31:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:31:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 448ed597-61f2-48fe-b528-f02201a80097
01/25/2025 11:31:33:INFO:Received: train message 448ed597-61f2-48fe-b528-f02201a80097

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.4563449422518413
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:31:47:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:32:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:32:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7f67b6de-f402-4e54-8434-f4c32fd2902d
01/25/2025 11:32:00:INFO:Received: evaluate message 7f67b6de-f402-4e54-8434-f4c32fd2902d
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:32:02:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:32:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:32:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d37bc27e-9526-4b48-a078-e0033acb8baa
01/25/2025 11:32:08:INFO:Received: train message d37bc27e-9526-4b48-a078-e0033acb8baa

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.41071044802665707
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:32:21:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:32:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:32:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 36591389-ddd4-4d22-9906-b6fa89913992
01/25/2025 11:32:33:INFO:Received: evaluate message 36591389-ddd4-4d22-9906-b6fa89913992
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:32:36:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:32:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:32:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 33c75fc5-8984-4a75-8fcf-8ecbac656946
01/25/2025 11:32:43:INFO:Received: train message 33c75fc5-8984-4a75-8fcf-8ecbac656946

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.36507595380147295
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:32:57:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:33:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:33:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0c1b3719-e25a-4ce6-a670-c81ea04995f6
01/25/2025 11:33:12:INFO:Received: evaluate message 0c1b3719-e25a-4ce6-a670-c81ea04995f6
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:33:14:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:33:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:33:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 70c094ea-71c3-4e47-97a8-39fa58a16efb
01/25/2025 11:33:21:INFO:Received: train message 70c094ea-71c3-4e47-97a8-39fa58a16efb

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935, 99.60209187492728], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621, 0.5270964478588234]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.3194414595762889
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:33:36:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:33:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:33:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d866f386-39c2-406b-8d96-3d61ef61b318
01/25/2025 11:33:50:INFO:Received: evaluate message d866f386-39c2-406b-8d96-3d61ef61b318
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:33:52:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:34:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:34:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ce23f2ac-0bed-41c4-8d2d-dc4b262730c7
01/25/2025 11:34:00:INFO:Received: train message ce23f2ac-0bed-41c4-8d2d-dc4b262730c7

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935, 99.60209187492728, 98.74537834525108], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621, 0.5270964478588234, 0.5274184925990982]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.27380696535110477
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:34:14:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:34:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:34:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8974b2a9-15b6-468c-82b9-f37a97c3e670
01/25/2025 11:34:28:INFO:Received: evaluate message 8974b2a9-15b6-468c-82b9-f37a97c3e670
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 11:34:30:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:34:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:34:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4f1ffd89-e17f-44ee-9a6c-07da64bf9e2a
01/25/2025 11:34:37:INFO:Received: train message 4f1ffd89-e17f-44ee-9a6c-07da64bf9e2a

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935, 99.60209187492728, 98.74537834525108, 99.45555519685149], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621, 0.5270964478588234, 0.5274184925990982, 0.5272973516513328]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.22817247112592065
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 11:34:50:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 11:35:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 11:35:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 25cf633a-0051-49a7-a444-2954fbab87fd
01/25/2025 11:35:02:INFO:Received: evaluate message 25cf633a-0051-49a7-a444-2954fbab87fd
Epsilon = 1.00
ERROR: Unexpected segmentation fault encountered in worker.
 [91mERROR [0m:     Client raised an exception.
Traceback (most recent call last):
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1132, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/multiprocessing/queues.py", line 107, in get
    if not self._poll(timeout):
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/multiprocessing/connection.py", line 424, in _poll
    r = wait([self], timeout)
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 522688) is killed by signal: Segmentation fault. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 496, in _start_client_internal
    reply_message = client_app(message=message, context=context)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client_app.py", line 98, in __call__
    return self._call(message, context)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client_app.py", line 81, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/message_handler/message_handler.py", line 137, in handle_legacy_message_from_msgtype
    evaluate_res = maybe_call_evaluate(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "client_1.py", line 328, in evaluate
    loss, accuracy, auc_score = test(self.model, self.testloader)
  File "client_1.py", line 116, in test
    for data in testloader:
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1328, in _next_data
    idx, data = self._get_data()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1294, in _get_data
    success, data = self._try_get_data()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1145, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 522688) exited unexpectedly
01/25/2025 11:35:03:ERROR:Client raised an exception.
Traceback (most recent call last):
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1132, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/multiprocessing/queues.py", line 107, in get
    if not self._poll(timeout):
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/multiprocessing/connection.py", line 424, in _poll
    r = wait([self], timeout)
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 522688) is killed by signal: Segmentation fault. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 496, in _start_client_internal
    reply_message = client_app(message=message, context=context)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client_app.py", line 98, in __call__
    return self._call(message, context)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client_app.py", line 81, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/message_handler/message_handler.py", line 137, in handle_legacy_message_from_msgtype
    evaluate_res = maybe_call_evaluate(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "client_1.py", line 328, in evaluate
    loss, accuracy, auc_score = test(self.model, self.testloader)
  File "client_1.py", line 116, in test
    for data in testloader:
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1328, in _next_data
    idx, data = self._get_data()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1294, in _get_data
    success, data = self._try_get_data()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1145, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 522688) exited unexpectedly
01/25/2025 11:35:04:DEBUG:gRPC channel closed
Traceback (most recent call last):
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1132, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/multiprocessing/queues.py", line 107, in get
    if not self._poll(timeout):
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/multiprocessing/connection.py", line 424, in _poll
    r = wait([self], timeout)
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 522688) is killed by signal: Segmentation fault. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "client_1.py", line 353, in <module>
    fl.client.start_client(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 291, in start_client
    _start_client_internal(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 503, in _start_client_internal
    raise ex
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 496, in _start_client_internal
    reply_message = client_app(message=message, context=context)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client_app.py", line 98, in __call__
    return self._call(message, context)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client_app.py", line 81, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/message_handler/message_handler.py", line 137, in handle_legacy_message_from_msgtype
    evaluate_res = maybe_call_evaluate(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "client_1.py", line 328, in evaluate
    loss, accuracy, auc_score = test(self.model, self.testloader)
  File "client_1.py", line 116, in test
    for data in testloader:
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1328, in _next_data
    idx, data = self._get_data()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1294, in _get_data
    success, data = self._try_get_data()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1145, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 522688) exited unexpectedly
