nohup: ignoring input
01/26/2025 22:40:08:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/26/2025 22:40:08:DEBUG:ChannelConnectivity.IDLE
01/26/2025 22:40:08:DEBUG:ChannelConnectivity.CONNECTING
01/26/2025 22:40:08:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/26/2025 22:40:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:40:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message bca82dbc-db35-4858-8b30-5e6140db2618
01/26/2025 22:40:08:INFO:Received: get_parameters message bca82dbc-db35-4858-8b30-5e6140db2618
[92mINFO [0m:      Sent reply
01/26/2025 22:40:13:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:40:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:40:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0801150e-bd15-4c77-bd0b-63b009f32482
01/26/2025 22:40:45:INFO:Received: train message 0801150e-bd15-4c77-bd0b-63b009f32482
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:41:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:41:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:41:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 03092c20-a47d-4be2-a742-686f9a5b902f
01/26/2025 22:41:46:INFO:Received: evaluate message 03092c20-a47d-4be2-a742-686f9a5b902f
[92mINFO [0m:      Sent reply
01/26/2025 22:41:48:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:42:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:42:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d5c4933a-ecd7-422d-b43e-9d65e7c79fd6
01/26/2025 22:42:32:INFO:Received: train message d5c4933a-ecd7-422d-b43e-9d65e7c79fd6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:42:55:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:43:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:43:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6647f9f7-b860-4452-9b69-cd4e1a637e12
01/26/2025 22:43:58:INFO:Received: evaluate message 6647f9f7-b860-4452-9b69-cd4e1a637e12
