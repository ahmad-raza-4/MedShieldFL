nohup: ignoring input
01/25/2025 13:12:34:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/25/2025 13:12:34:DEBUG:ChannelConnectivity.IDLE
01/25/2025 13:12:34:DEBUG:ChannelConnectivity.CONNECTING
01/25/2025 13:12:34:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/25/2025 13:12:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:12:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message c85fff64-6221-4c95-baeb-e7ffb8df187b
01/25/2025 13:12:34:INFO:Received: get_parameters message c85fff64-6221-4c95-baeb-e7ffb8df187b
[92mINFO [0m:      Sent reply
01/25/2025 13:12:38:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:12:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:12:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b4d5a8e4-8884-498e-a8e0-c3e968a0292e
01/25/2025 13:12:47:INFO:Received: train message b4d5a8e4-8884-498e-a8e0-c3e968a0292e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
01/25/2025 13:12:52:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:13:04:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:13:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:13:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 36958dbd-4c48-4841-8e99-d6674c5cfa48
01/25/2025 13:13:17:INFO:Received: evaluate message 36958dbd-4c48-4841-8e99-d6674c5cfa48
[92mINFO [0m:      Sent reply
01/25/2025 13:13:20:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:13:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:13:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7f7ca6c2-4047-4d4f-aaa7-930eda701a9a
01/25/2025 13:13:26:INFO:Received: train message 7f7ca6c2-4047-4d4f-aaa7-930eda701a9a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:13:43:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:13:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:13:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 60781d2f-994b-4362-a2fb-7255a73a4699
01/25/2025 13:13:55:INFO:Received: evaluate message 60781d2f-994b-4362-a2fb-7255a73a4699
[92mINFO [0m:      Sent reply
01/25/2025 13:13:57:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:14:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:14:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b50c2152-bf44-4fb5-bed2-1b16241eb837
01/25/2025 13:14:03:INFO:Received: train message b50c2152-bf44-4fb5-bed2-1b16241eb837
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:14:18:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:14:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:14:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 20ad58db-fa5b-4439-b3ae-0ee254c95c6d
01/25/2025 13:14:32:INFO:Received: evaluate message 20ad58db-fa5b-4439-b3ae-0ee254c95c6d
[92mINFO [0m:      Sent reply
01/25/2025 13:14:35:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:14:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:14:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 53f74bfd-0c7c-4453-b23d-5f1ec44608c4
01/25/2025 13:14:41:INFO:Received: train message 53f74bfd-0c7c-4453-b23d-5f1ec44608c4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:14:59:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:15:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:15:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6cc65c85-b0c9-46bb-a030-34e614bd420a
01/25/2025 13:15:12:INFO:Received: evaluate message 6cc65c85-b0c9-46bb-a030-34e614bd420a
[92mINFO [0m:      Sent reply
01/25/2025 13:15:14:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:15:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:15:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message acc854fa-08a6-4460-8bc7-0e3bc73ef191
01/25/2025 13:15:20:INFO:Received: train message acc854fa-08a6-4460-8bc7-0e3bc73ef191
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:15:35:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:15:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:15:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d39b11f0-26c9-4f7a-8d50-371d3ea001d3
01/25/2025 13:15:49:INFO:Received: evaluate message d39b11f0-26c9-4f7a-8d50-371d3ea001d3
[92mINFO [0m:      Sent reply
01/25/2025 13:15:52:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:15:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:15:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c75e8826-195a-44a0-bb42-7bb89d6c591a
01/25/2025 13:15:57:INFO:Received: train message c75e8826-195a-44a0-bb42-7bb89d6c591a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:16:11:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:16:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:16:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7a05dcf5-0164-4862-98ef-9ce3dc450849
01/25/2025 13:16:23:INFO:Received: evaluate message 7a05dcf5-0164-4862-98ef-9ce3dc450849
[92mINFO [0m:      Sent reply
01/25/2025 13:16:25:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:16:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:16:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b9495b09-8e9c-4a13-b6cb-2922a3b08ed2
01/25/2025 13:16:31:INFO:Received: train message b9495b09-8e9c-4a13-b6cb-2922a3b08ed2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:16:48:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:17:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:17:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 499102db-bdc0-4ec9-8ef1-05efdfa7402e
01/25/2025 13:17:00:INFO:Received: evaluate message 499102db-bdc0-4ec9-8ef1-05efdfa7402e
[92mINFO [0m:      Sent reply
01/25/2025 13:17:04:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:17:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:17:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6ed2ab32-39e0-4bc4-a14b-8e90dc7d8e8b
01/25/2025 13:17:10:INFO:Received: train message 6ed2ab32-39e0-4bc4-a14b-8e90dc7d8e8b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:17:25:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:17:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:17:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a91699a5-41ca-4cfd-9c89-5cc6318642a2
01/25/2025 13:17:37:INFO:Received: evaluate message a91699a5-41ca-4cfd-9c89-5cc6318642a2
[92mINFO [0m:      Sent reply
01/25/2025 13:17:39:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:17:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:17:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 686497b5-9006-466a-a26c-967a2975de96
01/25/2025 13:17:45:INFO:Received: train message 686497b5-9006-466a-a26c-967a2975de96
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:18:17:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:18:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:18:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 21dd0a18-e920-438c-8ac5-1ff8ea97fd07
01/25/2025 13:18:32:INFO:Received: evaluate message 21dd0a18-e920-438c-8ac5-1ff8ea97fd07
[92mINFO [0m:      Sent reply
01/25/2025 13:18:34:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:18:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:18:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fc88272a-b0f7-4a8e-9eac-b9b551b853b2
01/25/2025 13:18:41:INFO:Received: train message fc88272a-b0f7-4a8e-9eac-b9b551b853b2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:18:55:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:19:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:19:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 16fa7aca-88ad-4f56-bad5-7c511a5dea13
01/25/2025 13:19:09:INFO:Received: evaluate message 16fa7aca-88ad-4f56-bad5-7c511a5dea13
[92mINFO [0m:      Sent reply
01/25/2025 13:19:11:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:19:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:19:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 77dd01a6-a5c5-4764-aecf-5c63c0b28970
01/25/2025 13:19:18:INFO:Received: train message 77dd01a6-a5c5-4764-aecf-5c63c0b28970
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:19:35:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:19:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:19:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ae922e80-09ac-412b-96d9-f5eec67ced5c
01/25/2025 13:19:46:INFO:Received: evaluate message ae922e80-09ac-412b-96d9-f5eec67ced5c
[92mINFO [0m:      Sent reply
01/25/2025 13:19:48:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:19:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:19:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a33990ce-6b32-4c28-90f9-947554769a61
01/25/2025 13:19:54:INFO:Received: train message a33990ce-6b32-4c28-90f9-947554769a61
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:20:11:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:20:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:20:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6a9ff5cf-670e-4c9d-9b45-3b11a388a2f3
01/25/2025 13:20:22:INFO:Received: evaluate message 6a9ff5cf-670e-4c9d-9b45-3b11a388a2f3
BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487], 'accuracy': [0.35105551211884284], 'auc': [0.3711869239492664]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475], 'accuracy': [0.35105551211884284, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 13:20:25:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:20:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:20:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 65314fe6-5a60-4911-8a46-42b6bad617cc
01/25/2025 13:20:31:INFO:Received: train message 65314fe6-5a60-4911-8a46-42b6bad617cc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:20:46:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:20:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:20:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0e1b6835-a6bc-4097-a609-9d98482e2098
01/25/2025 13:20:58:INFO:Received: evaluate message 0e1b6835-a6bc-4097-a609-9d98482e2098
[92mINFO [0m:      Sent reply
01/25/2025 13:21:00:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:21:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:21:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 093e9321-43e5-4583-8660-afb76e3e5f5a
01/25/2025 13:21:06:INFO:Received: train message 093e9321-43e5-4583-8660-afb76e3e5f5a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:21:22:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:21:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:21:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 05a5cee9-3d37-4bda-abb2-b0c2e778e1fa
01/25/2025 13:21:37:INFO:Received: evaluate message 05a5cee9-3d37-4bda-abb2-b0c2e778e1fa
[92mINFO [0m:      Sent reply
01/25/2025 13:21:39:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:21:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:21:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4fc639f8-1b06-4ac0-8eae-ac5ef2efa82b
01/25/2025 13:21:45:INFO:Received: train message 4fc639f8-1b06-4ac0-8eae-ac5ef2efa82b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:22:00:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:22:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:22:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 08e468c6-536e-4812-9680-2cc7dfd37a33
01/25/2025 13:22:12:INFO:Received: evaluate message 08e468c6-536e-4812-9680-2cc7dfd37a33
[92mINFO [0m:      Sent reply
01/25/2025 13:22:15:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:22:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:22:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 15531005-93ea-425b-9cc6-a1beb4c9f225
01/25/2025 13:22:21:INFO:Received: train message 15531005-93ea-425b-9cc6-a1beb4c9f225
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:22:35:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:22:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:22:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message be4eb4d6-cd49-431e-8ad0-ebe8c61e0c47
01/25/2025 13:22:47:INFO:Received: evaluate message be4eb4d6-cd49-431e-8ad0-ebe8c61e0c47
[92mINFO [0m:      Sent reply
01/25/2025 13:22:48:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:22:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:22:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5fac99d1-7f4f-4739-a457-5ec5007569ba
01/25/2025 13:22:54:INFO:Received: train message 5fac99d1-7f4f-4739-a457-5ec5007569ba
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:23:08:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:23:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:23:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9fd14c76-02d8-485b-a7a4-c900020692be
01/25/2025 13:23:20:INFO:Received: evaluate message 9fd14c76-02d8-485b-a7a4-c900020692be
[92mINFO [0m:      Sent reply
01/25/2025 13:23:22:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:23:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:23:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3cf96f32-c5e1-4875-883c-8604c59816a5
01/25/2025 13:23:29:INFO:Received: train message 3cf96f32-c5e1-4875-883c-8604c59816a5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:23:43:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:23:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:23:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 505f6fbe-f72e-41c5-8d0f-87f3b78ad9fa
01/25/2025 13:23:57:INFO:Received: evaluate message 505f6fbe-f72e-41c5-8d0f-87f3b78ad9fa
[92mINFO [0m:      Sent reply
01/25/2025 13:23:59:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:24:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:24:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8e610326-beed-4d7c-87f2-bf28310af902
01/25/2025 13:24:06:INFO:Received: train message 8e610326-beed-4d7c-87f2-bf28310af902

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6845174133777618
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.6388829191525778
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.5932484249273936
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.5476139307022095
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:24:21:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:24:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:24:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2cce3c92-07bd-4638-bcab-fbd1298309cd
01/25/2025 13:24:35:INFO:Received: evaluate message 2cce3c92-07bd-4638-bcab-fbd1298309cd
[92mINFO [0m:      Sent reply
01/25/2025 13:24:39:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:24:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:24:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message da2e45a6-3d61-47e2-8f94-f6b8524ad1ff
01/25/2025 13:24:46:INFO:Received: train message da2e45a6-3d61-47e2-8f94-f6b8524ad1ff
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:25:07:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:25:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:25:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 18b6f2ed-eb5b-467c-9fc8-7e1874854b9d
01/25/2025 13:25:23:INFO:Received: evaluate message 18b6f2ed-eb5b-467c-9fc8-7e1874854b9d
[92mINFO [0m:      Sent reply
01/25/2025 13:25:26:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:25:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:25:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ec5fd5d4-46d3-4431-b96c-5589d288592a
01/25/2025 13:25:33:INFO:Received: train message ec5fd5d4-46d3-4431-b96c-5589d288592a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:25:58:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:26:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:26:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ebc84132-083c-4bcc-a2ea-e6f92c918aa2
01/25/2025 13:26:15:INFO:Received: evaluate message ebc84132-083c-4bcc-a2ea-e6f92c918aa2
[92mINFO [0m:      Sent reply
01/25/2025 13:26:18:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:26:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:26:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 23402992-325b-4c35-9981-a34ae073cc27
01/25/2025 13:26:25:INFO:Received: train message 23402992-325b-4c35-9981-a34ae073cc27
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:26:50:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:27:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:27:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 28ec8c43-713c-4b58-b88f-2c13681c1ffc
01/25/2025 13:27:05:INFO:Received: evaluate message 28ec8c43-713c-4b58-b88f-2c13681c1ffc
[92mINFO [0m:      Sent reply
01/25/2025 13:27:08:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:27:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:27:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 83b64f05-6770-48a5-82d0-8f42b859aa09
01/25/2025 13:27:16:INFO:Received: train message 83b64f05-6770-48a5-82d0-8f42b859aa09
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:27:36:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:27:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:27:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1066d709-6fd7-445f-8ecf-da731c39812f
01/25/2025 13:27:54:INFO:Received: evaluate message 1066d709-6fd7-445f-8ecf-da731c39812f
[92mINFO [0m:      Sent reply
01/25/2025 13:27:57:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:28:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:28:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ef24d837-715d-46d6-9fde-67f4dfbd0c28
01/25/2025 13:28:04:INFO:Received: train message ef24d837-715d-46d6-9fde-67f4dfbd0c28
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:28:27:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:28:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:28:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f5b2717a-9b65-436c-a89b-b5737075f810
01/25/2025 13:28:43:INFO:Received: evaluate message f5b2717a-9b65-436c-a89b-b5737075f810
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.5019794364770254
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.4563449422518413
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.41071044802665707
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.36507595380147295
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935, 99.60209187492728], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621, 0.5270964478588234]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.3194414595762889
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935, 99.60209187492728, 98.74537834525108], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621, 0.5270964478588234, 0.5274184925990982]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.27380696535110477
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 13:28:46:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:28:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:28:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2e994913-78e6-4202-bd8c-69f3a8bbed50
01/25/2025 13:28:53:INFO:Received: train message 2e994913-78e6-4202-bd8c-69f3a8bbed50
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:29:18:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:29:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:29:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f2ed53ab-5a5b-4b94-adb6-38dcae4cda1c
01/25/2025 13:29:33:INFO:Received: evaluate message f2ed53ab-5a5b-4b94-adb6-38dcae4cda1c
[92mINFO [0m:      Sent reply
01/25/2025 13:29:36:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:29:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:29:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 52599c02-6531-4764-b411-90bba47487df
01/25/2025 13:29:43:INFO:Received: train message 52599c02-6531-4764-b411-90bba47487df
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:30:03:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:30:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:30:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 931e5acc-9914-40bd-99c8-85a3efc35c04
01/25/2025 13:30:19:INFO:Received: evaluate message 931e5acc-9914-40bd-99c8-85a3efc35c04
[92mINFO [0m:      Sent reply
01/25/2025 13:30:22:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:30:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:30:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0bac93a5-c7a2-47b4-bf37-cea348a191ab
01/25/2025 13:30:30:INFO:Received: train message 0bac93a5-c7a2-47b4-bf37-cea348a191ab
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:30:52:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:31:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:31:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 22d91ac4-beb8-470e-8c6a-a2ef86adb967
01/25/2025 13:31:08:INFO:Received: evaluate message 22d91ac4-beb8-470e-8c6a-a2ef86adb967
[92mINFO [0m:      Sent reply
01/25/2025 13:31:11:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:31:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:31:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 49c7faf5-f24b-46c5-883b-bef6d23d7a38
01/25/2025 13:31:18:INFO:Received: train message 49c7faf5-f24b-46c5-883b-bef6d23d7a38
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:31:39:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:31:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:31:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3bfcac1e-174e-4a6b-92de-b95b9b66c08a
01/25/2025 13:31:53:INFO:Received: evaluate message 3bfcac1e-174e-4a6b-92de-b95b9b66c08a

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935, 99.60209187492728, 98.74537834525108, 99.45555519685149], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621, 0.5270964478588234, 0.5274184925990982, 0.5272973516513328]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.22817247112592065
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935, 99.60209187492728, 98.74537834525108, 99.45555519685149, 100.67745199427009], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621, 0.5270964478588234, 0.5274184925990982, 0.5272973516513328, 0.5276980208185909]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.18253797690073653
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935, 99.60209187492728, 98.74537834525108, 99.45555519685149, 100.67745199427009, 101.04630618914962], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621, 0.5270964478588234, 0.5274184925990982, 0.5272973516513328, 0.5276980208185909, 0.5279907541259226]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.13690348267555233
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935, 99.60209187492728, 98.74537834525108, 99.45555519685149, 100.67745199427009, 101.04630618914962, 99.82436210289598], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621, 0.5270964478588234, 0.5274184925990982, 0.5272973516513328, 0.5276980208185909, 0.5279907541259226, 0.5281151756388444]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.09126898845036822
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/25/2025 13:31:56:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:32:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:32:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ea906648-4a16-4698-b6ea-dc27497c02ef
01/25/2025 13:32:03:INFO:Received: train message ea906648-4a16-4698-b6ea-dc27497c02ef
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:32:19:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:32:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:32:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 60507628-db0d-4c40-a19f-6c57c521181d
01/25/2025 13:32:32:INFO:Received: evaluate message 60507628-db0d-4c40-a19f-6c57c521181d
[92mINFO [0m:      Sent reply
01/25/2025 13:32:34:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:32:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:32:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message da0570aa-5cc2-4997-918f-b8c7de87383a
01/25/2025 13:32:40:INFO:Received: train message da0570aa-5cc2-4997-918f-b8c7de87383a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/25/2025 13:32:55:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:33:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:33:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8ddfe3f2-828e-4866-8e5f-2d59f6a37094
01/25/2025 13:33:07:INFO:Received: evaluate message 8ddfe3f2-828e-4866-8e5f-2d59f6a37094
[92mINFO [0m:      Sent reply
01/25/2025 13:33:09:INFO:Sent reply
[92mINFO [0m:      
01/25/2025 13:33:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/25/2025 13:33:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 42bb0fd5-1507-4330-aaa4-f02db51ebbe2
01/25/2025 13:33:09:INFO:Received: reconnect message 42bb0fd5-1507-4330-aaa4-f02db51ebbe2
01/25/2025 13:33:09:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/25/2025 13:33:09:INFO:Disconnect and shut down

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935, 99.60209187492728, 98.74537834525108, 99.45555519685149, 100.67745199427009, 101.04630618914962, 99.82436210289598, 101.32120484858751], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621, 0.5270964478588234, 0.5274184925990982, 0.5272973516513328, 0.5276980208185909, 0.5279907541259226, 0.5281151756388444, 0.5278462228905152]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.04563449422518411
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935, 99.60209187492728, 98.74537834525108, 99.45555519685149, 100.67745199427009, 101.04630618914962, 99.82436210289598, 101.32120484858751, 100.1617233492434], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621, 0.5270964478588234, 0.5274184925990982, 0.5272973516513328, 0.5276980208185909, 0.5279907541259226, 0.5281151756388444, 0.5278462228905152, 0.5277413153245984]}

BaseNM 2.6171875
noise multiplier 0.6845174133777618
Noise multiplier before  adjustment: 0.6845174133777618
Noise multiplier before convergence adjustment: 0.6845174133777618
Updated noise multiplier after convergence adjustment: 0.0
Epsilon = 1.00

{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935, 99.60209187492728, 98.74537834525108, 99.45555519685149, 100.67745199427009, 101.04630618914962, 99.82436210289598, 101.32120484858751, 100.1617233492434, 98.76229656487703], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621, 0.5270964478588234, 0.5274184925990982, 0.5272973516513328, 0.5276980208185909, 0.5279907541259226, 0.5281151756388444, 0.5278462228905152, 0.5277413153245984, 0.5272898770744897]}



Final client history:
{'loss': [48.86307740211487, 53.19741877913475, 57.57680770754814, 62.8864831328392, 68.9459213912487, 75.2104541733861, 82.94934429228306, 90.96185195073485, 96.54766833782196, 98.4664455242455, 100.66125182434916, 100.93961292132735, 101.71645893156528, 101.06062316894531, 102.0853727273643, 100.62840722501278, 101.41830759495497, 100.66189385578036, 100.25517142191529, 101.01447511091828, 100.6949011310935, 99.60209187492728, 98.74537834525108, 99.45555519685149, 100.67745199427009, 101.04630618914962, 99.82436210289598, 101.32120484858751, 100.1617233492434, 98.76229656487703], 'accuracy': [0.35105551211884284, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704, 0.3502736512900704], 'auc': [0.3711869239492664, 0.4070704572109096, 0.43224481024610084, 0.450453279683937, 0.4639483552259732, 0.47313090197464247, 0.4810740877971388, 0.48768028327716273, 0.4942152847136674, 0.498634475928011, 0.5035032550076648, 0.5076166505771671, 0.5114729182018243, 0.5155248659189147, 0.5173230747289129, 0.5199598996149398, 0.5213191171846571, 0.5230823147715922, 0.5243544032583543, 0.5253412722805645, 0.5264397929744621, 0.5270964478588234, 0.5274184925990982, 0.5272973516513328, 0.5276980208185909, 0.5279907541259226, 0.5281151756388444, 0.5278462228905152, 0.5277413153245984, 0.5272898770744897]}

