nohup: ignoring input
Error importing huggingface_hub.hf_api: No module named 'tqdm'
Error importing huggingface_hub.hf_api: No module named 'tqdm'
01/21/2025 06:56:52:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/21/2025 06:56:52:DEBUG:ChannelConnectivity.IDLE
01/21/2025 06:56:52:DEBUG:ChannelConnectivity.CONNECTING
01/21/2025 06:56:52:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/21/2025 06:56:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 06:56:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message 9044fcac-c652-4e58-b1d1-f0311444a75b
01/21/2025 06:56:52:INFO:Received: get_parameters message 9044fcac-c652-4e58-b1d1-f0311444a75b
[92mINFO [0m:      Sent reply
01/21/2025 06:56:56:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 06:57:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 06:57:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 63b5d7aa-d466-4f84-8c77-86eae37738d4
01/21/2025 06:57:05:INFO:Received: train message 63b5d7aa-d466-4f84-8c77-86eae37738d4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
01/21/2025 06:57:08:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.
BaseNM 1.23046875
noise multiplier 0.17096698796376586
Noise multiplier before  adjustment: 0.17096698796376586
Noise multiplier before convergence adjustment: 0.17096698796376586
Updated noise multiplier after convergence adjustment: 0.17096698796376586
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 06:57:16:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 06:57:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 06:57:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 36ffa359-9abd-4881-ac96-9c93c59fb515
01/21/2025 06:57:28:INFO:Received: evaluate message 36ffa359-9abd-4881-ac96-9c93c59fb515
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/21/2025 06:57:30:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 06:57:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 06:57:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c33d3a28-73a6-4225-bc7b-2a9add8d0312
01/21/2025 06:57:36:INFO:Received: train message c33d3a28-73a6-4225-bc7b-2a9add8d0312

{'loss': [52.544208347797394], 'accuracy': [0.34792806880375293], 'auc': [0.3519498898221043]}

BaseNM 1.23046875
noise multiplier 0.17096698796376586
Noise multiplier before  adjustment: 0.17096698796376586
Noise multiplier before convergence adjustment: 0.17096698796376586
Updated noise multiplier after convergence adjustment: 0.17096698796376586
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 06:57:47:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 06:57:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 06:57:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c57951d1-10ea-4618-b752-ae2151fd7dd7
01/21/2025 06:57:58:INFO:Received: evaluate message c57951d1-10ea-4618-b752-ae2151fd7dd7
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/21/2025 06:58:03:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 06:58:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 06:58:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4dc093fa-3817-4466-95b4-b39cb3df2592
01/21/2025 06:58:09:INFO:Received: train message 4dc093fa-3817-4466-95b4-b39cb3df2592

{'loss': [52.544208347797394, 56.372040152549744], 'accuracy': [0.34792806880375293, 0.22830336200156373], 'auc': [0.3519498898221043, 0.3657147568088748]}

BaseNM 1.23046875
noise multiplier 0.17096698796376586
Noise multiplier before  adjustment: 0.17096698796376586
Noise multiplier before convergence adjustment: 0.17096698796376586
Updated noise multiplier after convergence adjustment: 0.17096698796376586
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 06:58:18:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 06:58:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 06:58:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 13696c81-e4f0-4cbc-9a6c-3a86d2693b8c
01/21/2025 06:58:33:INFO:Received: evaluate message 13696c81-e4f0-4cbc-9a6c-3a86d2693b8c
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/21/2025 06:58:35:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 06:58:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 06:58:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f7f04f42-ba76-40e5-96ea-c536afb35a45
01/21/2025 06:58:41:INFO:Received: train message f7f04f42-ba76-40e5-96ea-c536afb35a45

{'loss': [52.544208347797394, 56.372040152549744, 60.752862989902496], 'accuracy': [0.34792806880375293, 0.22830336200156373, 0.1415168100078186], 'auc': [0.3519498898221043, 0.3657147568088748, 0.3833557905735372]}

BaseNM 1.23046875
noise multiplier 0.17096698796376586
Noise multiplier before  adjustment: 0.17096698796376586
Noise multiplier before convergence adjustment: 0.17096698796376586
Updated noise multiplier after convergence adjustment: 0.17096698796376586
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 06:58:49:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 06:59:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 06:59:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 809269c0-4e5f-43bf-8da9-195e9854dca1
01/21/2025 06:59:00:INFO:Received: evaluate message 809269c0-4e5f-43bf-8da9-195e9854dca1
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/21/2025 06:59:05:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 06:59:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 06:59:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6669b5b9-bfad-411c-bc71-63df0a42f943
01/21/2025 06:59:11:INFO:Received: train message 6669b5b9-bfad-411c-bc71-63df0a42f943

{'loss': [52.544208347797394, 56.372040152549744, 60.752862989902496, 65.47998541593552], 'accuracy': [0.34792806880375293, 0.22830336200156373, 0.1415168100078186, 0.13995308835027365], 'auc': [0.3519498898221043, 0.3657147568088748, 0.3833557905735372, 0.40009237087193295]}

BaseNM 1.23046875
noise multiplier 0.17096698796376586
Noise multiplier before  adjustment: 0.17096698796376586
Noise multiplier before convergence adjustment: 0.17096698796376586
Updated noise multiplier after convergence adjustment: 0.17096698796376586
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 06:59:20:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 06:59:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 06:59:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c2dcc358-5ecf-402b-8504-2508ee65b11a
01/21/2025 06:59:32:INFO:Received: evaluate message c2dcc358-5ecf-402b-8504-2508ee65b11a
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/21/2025 06:59:34:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 06:59:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 06:59:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9a7e21d8-6912-452d-bdca-3b15e6414a8f
01/21/2025 06:59:40:INFO:Received: train message 9a7e21d8-6912-452d-bdca-3b15e6414a8f

{'loss': [52.544208347797394, 56.372040152549744, 60.752862989902496, 65.47998541593552, 71.01385760307312], 'accuracy': [0.34792806880375293, 0.22830336200156373, 0.1415168100078186, 0.13995308835027365, 0.13995308835027365], 'auc': [0.3519498898221043, 0.3657147568088748, 0.3833557905735372, 0.40009237087193295, 0.4171125008513183]}

BaseNM 1.23046875
noise multiplier 0.17096698796376586
Noise multiplier before  adjustment: 0.17096698796376586
Noise multiplier before convergence adjustment: 0.17096698796376586
Updated noise multiplier after convergence adjustment: 0.17096698796376586
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 06:59:48:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 07:00:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 07:00:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2f755916-fdf7-44e6-98ca-b08a4e8774e4
01/21/2025 07:00:00:INFO:Received: evaluate message 2f755916-fdf7-44e6-98ca-b08a4e8774e4
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/21/2025 07:00:02:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 07:00:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 07:00:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f4cb0cdc-2d6a-4567-bde4-f7a1d91c2111
01/21/2025 07:00:07:INFO:Received: train message f4cb0cdc-2d6a-4567-bde4-f7a1d91c2111

{'loss': [52.544208347797394, 56.372040152549744, 60.752862989902496, 65.47998541593552, 71.01385760307312, 75.74117603898048], 'accuracy': [0.34792806880375293, 0.22830336200156373, 0.1415168100078186, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365], 'auc': [0.3519498898221043, 0.3657147568088748, 0.3833557905735372, 0.40009237087193295, 0.4171125008513183, 0.4295464676895764]}

BaseNM 1.23046875
noise multiplier 0.17096698796376586
Noise multiplier before  adjustment: 0.17096698796376586
Noise multiplier before convergence adjustment: 0.17096698796376586
Updated noise multiplier after convergence adjustment: 0.17096698796376586
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 07:00:16:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 07:00:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 07:00:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d0b58985-290b-43a4-a5f2-a250debd0a5d
01/21/2025 07:00:28:INFO:Received: evaluate message d0b58985-290b-43a4-a5f2-a250debd0a5d
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/21/2025 07:00:29:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 07:00:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 07:00:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ed07341-2c10-4441-97b1-978fc0aa7fac
01/21/2025 07:00:35:INFO:Received: train message 8ed07341-2c10-4441-97b1-978fc0aa7fac

{'loss': [52.544208347797394, 56.372040152549744, 60.752862989902496, 65.47998541593552, 71.01385760307312, 75.74117603898048, 81.31963819265366], 'accuracy': [0.34792806880375293, 0.22830336200156373, 0.1415168100078186, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365], 'auc': [0.3519498898221043, 0.3657147568088748, 0.3833557905735372, 0.40009237087193295, 0.4171125008513183, 0.4295464676895764, 0.4404275859153555]}

BaseNM 1.23046875
noise multiplier 0.17096698796376586
Noise multiplier before  adjustment: 0.17096698796376586
Noise multiplier before convergence adjustment: 0.17096698796376586
Updated noise multiplier after convergence adjustment: 0.17096698796376586
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 07:00:45:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 07:00:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 07:00:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 29299e2d-4f9c-4626-b172-4a25ae48ee8a
01/21/2025 07:00:56:INFO:Received: evaluate message 29299e2d-4f9c-4626-b172-4a25ae48ee8a
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/21/2025 07:00:58:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 07:01:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 07:01:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c464ad8d-3c2d-4c60-afd6-05fc61d65aea
01/21/2025 07:01:04:INFO:Received: train message c464ad8d-3c2d-4c60-afd6-05fc61d65aea

{'loss': [52.544208347797394, 56.372040152549744, 60.752862989902496, 65.47998541593552, 71.01385760307312, 75.74117603898048, 81.31963819265366, 86.70735937356949], 'accuracy': [0.34792806880375293, 0.22830336200156373, 0.1415168100078186, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365], 'auc': [0.3519498898221043, 0.3657147568088748, 0.3833557905735372, 0.40009237087193295, 0.4171125008513183, 0.4295464676895764, 0.4404275859153555, 0.44895956886422705]}

BaseNM 1.23046875
noise multiplier 0.17096698796376586
Noise multiplier before  adjustment: 0.17096698796376586
Noise multiplier before convergence adjustment: 0.17096698796376586
Updated noise multiplier after convergence adjustment: 0.17096698796376586
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 07:01:13:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 07:01:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 07:01:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8fa97543-a13b-452d-a73a-5b4246b07c31
01/21/2025 07:01:25:INFO:Received: evaluate message 8fa97543-a13b-452d-a73a-5b4246b07c31
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/21/2025 07:01:27:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 07:01:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 07:01:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 706b6076-79d5-47d1-949d-410e32732d63
01/21/2025 07:01:33:INFO:Received: train message 706b6076-79d5-47d1-949d-410e32732d63

{'loss': [52.544208347797394, 56.372040152549744, 60.752862989902496, 65.47998541593552, 71.01385760307312, 75.74117603898048, 81.31963819265366, 86.70735937356949, 93.19147183001041], 'accuracy': [0.34792806880375293, 0.22830336200156373, 0.1415168100078186, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365], 'auc': [0.3519498898221043, 0.3657147568088748, 0.3833557905735372, 0.40009237087193295, 0.4171125008513183, 0.4295464676895764, 0.4404275859153555, 0.44895956886422705, 0.4575459207833301]}

BaseNM 1.23046875
noise multiplier 0.17096698796376586
Noise multiplier before  adjustment: 0.17096698796376586
Noise multiplier before convergence adjustment: 0.17096698796376586
Updated noise multiplier after convergence adjustment: 0.17096698796376586
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/21/2025 07:01:43:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 07:01:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 07:01:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1fd82f9c-2447-483d-8240-a530a3160b9c
01/21/2025 07:01:54:INFO:Received: evaluate message 1fd82f9c-2447-483d-8240-a530a3160b9c
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/21/2025 07:01:56:INFO:Sent reply
[92mINFO [0m:      
01/21/2025 07:01:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/21/2025 07:01:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message f5e1f667-871c-4adc-b161-3e368363a773
01/21/2025 07:01:56:INFO:Received: reconnect message f5e1f667-871c-4adc-b161-3e368363a773
01/21/2025 07:01:56:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/21/2025 07:01:56:INFO:Disconnect and shut down

{'loss': [52.544208347797394, 56.372040152549744, 60.752862989902496, 65.47998541593552, 71.01385760307312, 75.74117603898048, 81.31963819265366, 86.70735937356949, 93.19147183001041, 99.81376619637012], 'accuracy': [0.34792806880375293, 0.22830336200156373, 0.1415168100078186, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365], 'auc': [0.3519498898221043, 0.3657147568088748, 0.3833557905735372, 0.40009237087193295, 0.4171125008513183, 0.4295464676895764, 0.4404275859153555, 0.44895956886422705, 0.4575459207833301, 0.464966181078634]}



Final client history:
{'loss': [52.544208347797394, 56.372040152549744, 60.752862989902496, 65.47998541593552, 71.01385760307312, 75.74117603898048, 81.31963819265366, 86.70735937356949, 93.19147183001041, 99.81376619637012], 'accuracy': [0.34792806880375293, 0.22830336200156373, 0.1415168100078186, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365, 0.13995308835027365], 'auc': [0.3519498898221043, 0.3657147568088748, 0.3833557905735372, 0.40009237087193295, 0.4171125008513183, 0.4295464676895764, 0.4404275859153555, 0.44895956886422705, 0.4575459207833301, 0.464966181078634]}

