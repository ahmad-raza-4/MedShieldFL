nohup: ignoring input
01/27/2025 00:41:21:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 00:41:21:DEBUG:ChannelConnectivity.IDLE
01/27/2025 00:41:21:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 00:41:21:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 00:41:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:41:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bbea8972-7d97-4e60-9133-7cd223d9d4d1
01/27/2025 00:41:43:INFO:Received: train message bbea8972-7d97-4e60-9133-7cd223d9d4d1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:42:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:43:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:43:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e98cfd94-f508-4a41-af79-8f7d0a2e62f8
01/27/2025 00:43:29:INFO:Received: evaluate message e98cfd94-f508-4a41-af79-8f7d0a2e62f8
[92mINFO [0m:      Sent reply
01/27/2025 00:43:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:44:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:44:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b9f922c0-cef3-449e-8fcd-6201414e328e
01/27/2025 00:44:36:INFO:Received: train message b9f922c0-cef3-449e-8fcd-6201414e328e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:45:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:45:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:45:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3322b01c-5b8b-47ba-86f6-414e1386b454
01/27/2025 00:45:38:INFO:Received: evaluate message 3322b01c-5b8b-47ba-86f6-414e1386b454
[92mINFO [0m:      Sent reply
01/27/2025 00:45:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:46:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:46:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8724790b-4262-4a47-b339-0f73a4bc1ad5
01/27/2025 00:46:04:INFO:Received: train message 8724790b-4262-4a47-b339-0f73a4bc1ad5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:46:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:47:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:47:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bf955f40-4cc3-4a8d-9a07-a856d9867578
01/27/2025 00:47:18:INFO:Received: evaluate message bf955f40-4cc3-4a8d-9a07-a856d9867578
[92mINFO [0m:      Sent reply
01/27/2025 00:47:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:47:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:47:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message af1d358b-ed24-4c33-9f41-cec316c57500
01/27/2025 00:47:55:INFO:Received: train message af1d358b-ed24-4c33-9f41-cec316c57500
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:48:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:48:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:48:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c7ff76df-6d98-456d-a0ed-41024c289b84
01/27/2025 00:48:43:INFO:Received: evaluate message c7ff76df-6d98-456d-a0ed-41024c289b84
[92mINFO [0m:      Sent reply
01/27/2025 00:48:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:49:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:49:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 88fb1dd3-ecfc-4173-a7b1-ab07710abbfe
01/27/2025 00:49:36:INFO:Received: train message 88fb1dd3-ecfc-4173-a7b1-ab07710abbfe
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:50:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:50:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:50:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 365be380-02f9-498b-8c51-a6045d7984cf
01/27/2025 00:50:37:INFO:Received: evaluate message 365be380-02f9-498b-8c51-a6045d7984cf
[92mINFO [0m:      Sent reply
01/27/2025 00:50:41:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:51:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:51:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ebf8b6f7-7e29-448a-8569-bdb94bcfb5f4
01/27/2025 00:51:19:INFO:Received: train message ebf8b6f7-7e29-448a-8569-bdb94bcfb5f4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:51:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:52:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:52:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 98925eb9-934f-4c69-9fb5-cadba87a4052
01/27/2025 00:52:21:INFO:Received: evaluate message 98925eb9-934f-4c69-9fb5-cadba87a4052
[92mINFO [0m:      Sent reply
01/27/2025 00:52:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:52:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:52:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b8c9d7d2-36c4-4218-875a-d4cb809aab7f
01/27/2025 00:52:51:INFO:Received: train message b8c9d7d2-36c4-4218-875a-d4cb809aab7f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:53:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:53:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:53:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 15ece5bb-3239-46bb-b8c1-7089ba7a0a5d
01/27/2025 00:53:49:INFO:Received: evaluate message 15ece5bb-3239-46bb-b8c1-7089ba7a0a5d
[92mINFO [0m:      Sent reply
01/27/2025 00:53:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:54:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:54:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 160238d7-5113-44a3-b117-4d9708f9ed60
01/27/2025 00:54:33:INFO:Received: train message 160238d7-5113-44a3-b117-4d9708f9ed60
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406], 'accuracy': [0.5160281469898358], 'auc': [0.7097259125281268]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157], 'accuracy': [0.5160281469898358, 0.506645817044566], 'auc': [0.7097259125281268, 0.7279838714938965]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:55:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:55:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:55:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a642d366-e155-4f77-b4a7-05a2556f964e
01/27/2025 00:55:29:INFO:Received: evaluate message a642d366-e155-4f77-b4a7-05a2556f964e
[92mINFO [0m:      Sent reply
01/27/2025 00:55:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:56:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:56:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c22a785c-8ab8-48b1-b0e9-2c74d6b893da
01/27/2025 00:56:14:INFO:Received: train message c22a785c-8ab8-48b1-b0e9-2c74d6b893da
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:56:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:56:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:56:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d8006a30-f2da-40d3-a57c-1c36f4079fc6
01/27/2025 00:56:57:INFO:Received: evaluate message d8006a30-f2da-40d3-a57c-1c36f4079fc6
[92mINFO [0m:      Sent reply
01/27/2025 00:57:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:57:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:57:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7c2b01de-6459-4a99-8920-63ca3956244e
01/27/2025 00:57:45:INFO:Received: train message 7c2b01de-6459-4a99-8920-63ca3956244e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:58:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:58:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:58:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 55058f65-d001-4609-b7bd-0fe005060807
01/27/2025 00:58:48:INFO:Received: evaluate message 55058f65-d001-4609-b7bd-0fe005060807
[92mINFO [0m:      Sent reply
01/27/2025 00:58:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:59:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:59:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 64f13fff-bfd3-4f67-95af-a7e11fc09d94
01/27/2025 00:59:23:INFO:Received: train message 64f13fff-bfd3-4f67-95af-a7e11fc09d94
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:59:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:00:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:00:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6595bb94-3c27-4abf-83f9-a2ab7ec6e953
01/27/2025 01:00:15:INFO:Received: evaluate message 6595bb94-3c27-4abf-83f9-a2ab7ec6e953
[92mINFO [0m:      Sent reply
01/27/2025 01:00:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:00:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:00:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8fc983c2-c651-4371-9c3c-59f43fe0a217
01/27/2025 01:00:51:INFO:Received: train message 8fc983c2-c651-4371-9c3c-59f43fe0a217
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:01:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:01:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:01:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message de0e6823-157c-4322-b424-370a012cee83
01/27/2025 01:01:49:INFO:Received: evaluate message de0e6823-157c-4322-b424-370a012cee83
[92mINFO [0m:      Sent reply
01/27/2025 01:01:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:02:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:02:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 42602c51-d6e3-4f10-b0f3-a5fdafcf0145
01/27/2025 01:02:24:INFO:Received: train message 42602c51-d6e3-4f10-b0f3-a5fdafcf0145
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:02:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:03:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:03:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c0573170-98ab-4aa7-8e16-a96c1a166c89
01/27/2025 01:03:00:INFO:Received: evaluate message c0573170-98ab-4aa7-8e16-a96c1a166c89
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
[92mINFO [0m:      Sent reply
01/27/2025 01:03:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:03:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:03:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 88ffd276-7d7d-4f5a-b69c-beef6c34e454
01/27/2025 01:03:38:INFO:Received: train message 88ffd276-7d7d-4f5a-b69c-beef6c34e454
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:03:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:04:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:04:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7d6a5f69-469c-4501-ac49-5989cb0f074b
01/27/2025 01:04:26:INFO:Received: evaluate message 7d6a5f69-469c-4501-ac49-5989cb0f074b
[92mINFO [0m:      Sent reply
01/27/2025 01:04:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:05:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:05:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b5154cbe-ca8c-4844-9f7c-80e9dece79fb
01/27/2025 01:05:01:INFO:Received: train message b5154cbe-ca8c-4844-9f7c-80e9dece79fb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:05:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:05:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:05:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c0c357cc-cb1f-4d83-8dd0-06da1ba59ec4
01/27/2025 01:05:55:INFO:Received: evaluate message c0c357cc-cb1f-4d83-8dd0-06da1ba59ec4
[92mINFO [0m:      Sent reply
01/27/2025 01:05:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:06:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:06:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 28aee310-501f-4c9a-b606-fe5144dc0507
01/27/2025 01:06:24:INFO:Received: train message 28aee310-501f-4c9a-b606-fe5144dc0507
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:06:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:07:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:07:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2e578265-d4e6-4c72-ac38-0dcd15cd13aa
01/27/2025 01:07:05:INFO:Received: evaluate message 2e578265-d4e6-4c72-ac38-0dcd15cd13aa
[92mINFO [0m:      Sent reply
01/27/2025 01:07:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:08:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:08:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 58c0bc64-98ae-4e31-9f54-4ee20701857d
01/27/2025 01:08:01:INFO:Received: train message 58c0bc64-98ae-4e31-9f54-4ee20701857d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:08:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:08:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:08:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d8c06e34-d76c-4949-abe3-fadd1418a821
01/27/2025 01:08:53:INFO:Received: evaluate message d8c06e34-d76c-4949-abe3-fadd1418a821
[92mINFO [0m:      Sent reply
01/27/2025 01:08:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:09:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:09:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bce9bbae-a615-4b13-a57b-6b66e3558aab
01/27/2025 01:09:08:INFO:Received: train message bce9bbae-a615-4b13-a57b-6b66e3558aab
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:09:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:09:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:09:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9d68c8f8-2234-44af-b91f-f54c3aa0444a
01/27/2025 01:09:59:INFO:Received: evaluate message 9d68c8f8-2234-44af-b91f-f54c3aa0444a
[92mINFO [0m:      Sent reply
01/27/2025 01:10:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:10:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:10:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 26920f9b-0597-4497-8ac5-fb117e5bde84
01/27/2025 01:10:37:INFO:Received: train message 26920f9b-0597-4497-8ac5-fb117e5bde84
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:10:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:11:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:11:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 11a3abfd-c68a-48d7-91a5-9b54acbdd35c
01/27/2025 01:11:34:INFO:Received: evaluate message 11a3abfd-c68a-48d7-91a5-9b54acbdd35c
[92mINFO [0m:      Sent reply
01/27/2025 01:11:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:12:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:12:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 39e71b6b-2169-45fb-9666-5a93586e1e0e
01/27/2025 01:12:09:INFO:Received: train message 39e71b6b-2169-45fb-9666-5a93586e1e0e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:12:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:13:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:13:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2e2d4728-f136-44a7-8125-0da8fbb63821
01/27/2025 01:13:33:INFO:Received: evaluate message 2e2d4728-f136-44a7-8125-0da8fbb63821
[92mINFO [0m:      Sent reply
01/27/2025 01:13:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:14:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:14:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a71c4613-4853-4faf-83cb-adb082f79980
01/27/2025 01:14:22:INFO:Received: train message a71c4613-4853-4faf-83cb-adb082f79980
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:14:41:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:15:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:15:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 67ff2740-1331-4db0-a231-502cc83dea6d
01/27/2025 01:15:16:INFO:Received: evaluate message 67ff2740-1331-4db0-a231-502cc83dea6d
[92mINFO [0m:      Sent reply
01/27/2025 01:15:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:15:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:15:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b3bf61fb-73da-43fe-a966-1ba1fa89bdfa
01/27/2025 01:15:54:INFO:Received: train message b3bf61fb-73da-43fe-a966-1ba1fa89bdfa
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:16:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:16:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:16:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 06272941-9fb4-4852-bf0d-34f4ac392418
01/27/2025 01:16:46:INFO:Received: evaluate message 06272941-9fb4-4852-bf0d-34f4ac392418
[92mINFO [0m:      Sent reply
01/27/2025 01:16:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:17:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:17:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 15134b13-ea1a-4824-83a8-499acbab7cea
01/27/2025 01:17:07:INFO:Received: train message 15134b13-ea1a-4824-83a8-499acbab7cea
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:17:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:18:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:18:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 47ac8e8d-61b0-4503-8ad9-648e06c6714f
01/27/2025 01:18:05:INFO:Received: evaluate message 47ac8e8d-61b0-4503-8ad9-648e06c6714f
[92mINFO [0m:      Sent reply
01/27/2025 01:18:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:18:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:18:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e8bb7974-f74f-44fa-a434-6cd7c9fdee18
01/27/2025 01:18:17:INFO:Received: train message e8bb7974-f74f-44fa-a434-6cd7c9fdee18
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:18:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:19:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:19:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4b63f906-863d-4ae6-a406-4f41216841ed
01/27/2025 01:19:24:INFO:Received: evaluate message 4b63f906-863d-4ae6-a406-4f41216841ed
[92mINFO [0m:      Sent reply
01/27/2025 01:19:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:19:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:19:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 32b203d4-806a-4428-af44-fcef0c9f07aa
01/27/2025 01:19:53:INFO:Received: train message 32b203d4-806a-4428-af44-fcef0c9f07aa
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:20:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:20:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:20:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dc1f46fe-aefd-40c0-820e-93aef0736a41
01/27/2025 01:20:36:INFO:Received: evaluate message dc1f46fe-aefd-40c0-820e-93aef0736a41
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:20:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:21:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:21:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ca4f0e60-b4bd-47d5-9c67-fcb33dabad1a
01/27/2025 01:21:23:INFO:Received: train message ca4f0e60-b4bd-47d5-9c67-fcb33dabad1a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:21:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:22:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:22:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1210f44e-736f-4cef-88b9-df34891b2377
01/27/2025 01:22:10:INFO:Received: evaluate message 1210f44e-736f-4cef-88b9-df34891b2377
[92mINFO [0m:      Sent reply
01/27/2025 01:22:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:22:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:22:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6b61e45a-a55e-4018-ac32-c7a129077696
01/27/2025 01:22:36:INFO:Received: train message 6b61e45a-a55e-4018-ac32-c7a129077696
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:22:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:23:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:23:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8be503f0-38bf-45be-8c85-895ea9638799
01/27/2025 01:23:20:INFO:Received: evaluate message 8be503f0-38bf-45be-8c85-895ea9638799
[92mINFO [0m:      Sent reply
01/27/2025 01:23:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:23:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:23:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9253c015-aab2-4371-b382-b7a48d597d71
01/27/2025 01:23:52:INFO:Received: train message 9253c015-aab2-4371-b382-b7a48d597d71
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:24:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:24:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:24:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e0f6054a-e057-41f7-a13a-13c94e3d6683
01/27/2025 01:24:50:INFO:Received: evaluate message e0f6054a-e057-41f7-a13a-13c94e3d6683

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:24:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:25:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:25:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ae52b607-a4ea-447a-86fa-78435ec27e44
01/27/2025 01:25:36:INFO:Received: train message ae52b607-a4ea-447a-86fa-78435ec27e44
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:25:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:26:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:26:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5206a579-6a75-4405-8e1e-7053c3dfec1d
01/27/2025 01:26:23:INFO:Received: evaluate message 5206a579-6a75-4405-8e1e-7053c3dfec1d
[92mINFO [0m:      Sent reply
01/27/2025 01:26:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:26:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:26:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ddb07dea-cf8a-4151-a0f2-1a52aed352d2
01/27/2025 01:26:58:INFO:Received: train message ddb07dea-cf8a-4151-a0f2-1a52aed352d2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:27:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:27:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:27:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0fea663d-0469-47a0-b43b-4d8d30c51e51
01/27/2025 01:27:32:INFO:Received: evaluate message 0fea663d-0469-47a0-b43b-4d8d30c51e51
[92mINFO [0m:      Sent reply
01/27/2025 01:27:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:27:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:27:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message f11caf7f-f555-4e09-b1dc-775a744d9182
01/27/2025 01:27:53:INFO:Received: reconnect message f11caf7f-f555-4e09-b1dc-775a744d9182
01/27/2025 01:27:53:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 01:27:53:INFO:Disconnect and shut down

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.7890625
Data Scaling Factor: 0.2821875 where Client Data Size: 1806
Noise Multiplier after Fisher Scaling:  [0.07040143013000488, 0.021280726417899132, 0.2842015027999878, 0.2896442413330078]
Noise Multiplier after list and tensor:  0.1663819751702249
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695, 1.0624548865072982], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791, 0.5809225957779516], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168, 0.7911837794938563]}



Final client history:
{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695, 1.0624548865072982], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791, 0.5809225957779516], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168, 0.7911837794938563]}

