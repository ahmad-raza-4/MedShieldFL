nohup: ignoring input
01/27/2025 00:41:19:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 00:41:19:DEBUG:ChannelConnectivity.IDLE
01/27/2025 00:41:19:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 00:41:19:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 00:42:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:42:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bbe32eeb-135d-45f2-9ac9-4c506a4cf732
01/27/2025 00:42:09:INFO:Received: train message bbe32eeb-135d-45f2-9ac9-4c506a4cf732
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:42:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:43:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:43:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c25c87b4-cdda-4cfa-90fc-7df13249c4c4
01/27/2025 00:43:34:INFO:Received: evaluate message c25c87b4-cdda-4cfa-90fc-7df13249c4c4
[92mINFO [0m:      Sent reply
01/27/2025 00:43:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:44:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:44:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f2a798c6-c7ed-47ac-be09-fb9d977e0f8c
01/27/2025 00:44:37:INFO:Received: train message f2a798c6-c7ed-47ac-be09-fb9d977e0f8c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:45:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:45:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:45:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 65009249-3e20-4049-a99a-5a92d81dce20
01/27/2025 00:45:42:INFO:Received: evaluate message 65009249-3e20-4049-a99a-5a92d81dce20
[92mINFO [0m:      Sent reply
01/27/2025 00:45:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:46:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:46:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1f5d24e3-5e87-49d7-817d-b256433b8c04
01/27/2025 00:46:23:INFO:Received: train message 1f5d24e3-5e87-49d7-817d-b256433b8c04
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:46:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:47:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:47:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2a9aca65-0ed1-4bb2-ad5f-63ad79c6ea8e
01/27/2025 00:47:11:INFO:Received: evaluate message 2a9aca65-0ed1-4bb2-ad5f-63ad79c6ea8e
[92mINFO [0m:      Sent reply
01/27/2025 00:47:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:47:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:47:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6096ab62-bb61-4509-b2bd-45de04546fc4
01/27/2025 00:47:55:INFO:Received: train message 6096ab62-bb61-4509-b2bd-45de04546fc4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:48:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:48:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:48:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bc2bc07a-d3ce-43c4-b2c2-2bdc13b260f5
01/27/2025 00:48:57:INFO:Received: evaluate message bc2bc07a-d3ce-43c4-b2c2-2bdc13b260f5
[92mINFO [0m:      Sent reply
01/27/2025 00:49:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:49:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:49:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 11132ca9-1305-4215-98ae-88295ab2791c
01/27/2025 00:49:40:INFO:Received: train message 11132ca9-1305-4215-98ae-88295ab2791c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:50:04:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:50:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:50:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 34211b6e-1a11-4d53-9f93-8ce745c09683
01/27/2025 00:50:41:INFO:Received: evaluate message 34211b6e-1a11-4d53-9f93-8ce745c09683
[92mINFO [0m:      Sent reply
01/27/2025 00:50:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:51:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:51:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3d901e28-3bdf-4204-87bf-3452f71af5dc
01/27/2025 00:51:04:INFO:Received: train message 3d901e28-3bdf-4204-87bf-3452f71af5dc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:51:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:52:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:52:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 40180987-e338-42fb-9cc1-544dafe0d309
01/27/2025 00:52:04:INFO:Received: evaluate message 40180987-e338-42fb-9cc1-544dafe0d309
[92mINFO [0m:      Sent reply
01/27/2025 00:52:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:52:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:52:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 76211b45-ef94-4125-acf1-fb4f293bab82
01/27/2025 00:52:50:INFO:Received: train message 76211b45-ef94-4125-acf1-fb4f293bab82
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:53:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:53:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:53:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 97d6b2ed-c7c7-40ee-bdd4-024ba16a2859
01/27/2025 00:53:48:INFO:Received: evaluate message 97d6b2ed-c7c7-40ee-bdd4-024ba16a2859
[92mINFO [0m:      Sent reply
01/27/2025 00:53:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:54:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:54:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 06566880-99ad-4801-92bc-c5c51011eec7
01/27/2025 00:54:35:INFO:Received: train message 06566880-99ad-4801-92bc-c5c51011eec7
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406], 'accuracy': [0.5160281469898358], 'auc': [0.7097259125281268]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157], 'accuracy': [0.5160281469898358, 0.506645817044566], 'auc': [0.7097259125281268, 0.7279838714938965]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:55:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:55:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:55:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 05af6a70-929e-481a-b3db-722bb1a7e9f8
01/27/2025 00:55:38:INFO:Received: evaluate message 05af6a70-929e-481a-b3db-722bb1a7e9f8
[92mINFO [0m:      Sent reply
01/27/2025 00:55:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:56:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:56:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9540c06d-fcb7-4867-a46b-f522105a504b
01/27/2025 00:56:16:INFO:Received: train message 9540c06d-fcb7-4867-a46b-f522105a504b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:56:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:57:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:57:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9889e47a-9315-4846-9ad1-8d903b790210
01/27/2025 00:57:15:INFO:Received: evaluate message 9889e47a-9315-4846-9ad1-8d903b790210
[92mINFO [0m:      Sent reply
01/27/2025 00:57:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:57:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:57:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 68f211bd-5445-48dc-8b13-96a20e137782
01/27/2025 00:57:53:INFO:Received: train message 68f211bd-5445-48dc-8b13-96a20e137782
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:58:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:58:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:58:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9eec1e3f-4808-42e7-b5d4-7f2a555a63d1
01/27/2025 00:58:49:INFO:Received: evaluate message 9eec1e3f-4808-42e7-b5d4-7f2a555a63d1
[92mINFO [0m:      Sent reply
01/27/2025 00:58:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:59:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:59:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dfb952a1-fea1-447a-a337-f259bb82c884
01/27/2025 00:59:10:INFO:Received: train message dfb952a1-fea1-447a-a337-f259bb82c884
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:59:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:00:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:00:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cb2e650d-7afe-416e-aaba-bb14c96cff9a
01/27/2025 01:00:23:INFO:Received: evaluate message cb2e650d-7afe-416e-aaba-bb14c96cff9a
[92mINFO [0m:      Sent reply
01/27/2025 01:00:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:00:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:00:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 720ba8b6-abda-4338-8dd5-a61be90ee3d0
01/27/2025 01:00:50:INFO:Received: train message 720ba8b6-abda-4338-8dd5-a61be90ee3d0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:01:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:01:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:01:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7df363e3-79c8-4896-9167-2d240f447cd2
01/27/2025 01:01:47:INFO:Received: evaluate message 7df363e3-79c8-4896-9167-2d240f447cd2
[92mINFO [0m:      Sent reply
01/27/2025 01:01:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:02:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:02:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e77d8b6d-8dc9-43ab-a128-25fdca6539c6
01/27/2025 01:02:03:INFO:Received: train message e77d8b6d-8dc9-43ab-a128-25fdca6539c6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:02:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:03:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:03:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b1e81f7d-e541-4219-96ca-ab04939c9bd9
01/27/2025 01:03:03:INFO:Received: evaluate message b1e81f7d-e541-4219-96ca-ab04939c9bd9
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
[92mINFO [0m:      Sent reply
01/27/2025 01:03:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:03:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:03:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d6fb25d7-7671-4198-836f-e73f5ba2d379
01/27/2025 01:03:35:INFO:Received: train message d6fb25d7-7671-4198-836f-e73f5ba2d379
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:03:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:04:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:04:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0f2a02b5-1c22-4ead-94dc-aabd3152c955
01/27/2025 01:04:32:INFO:Received: evaluate message 0f2a02b5-1c22-4ead-94dc-aabd3152c955
[92mINFO [0m:      Sent reply
01/27/2025 01:04:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:05:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:05:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 481863d1-9a20-45de-8a42-9dd680597591
01/27/2025 01:05:02:INFO:Received: train message 481863d1-9a20-45de-8a42-9dd680597591
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:05:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:05:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:05:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b59900e6-9d19-467e-93e7-3dca9391fe67
01/27/2025 01:05:55:INFO:Received: evaluate message b59900e6-9d19-467e-93e7-3dca9391fe67
[92mINFO [0m:      Sent reply
01/27/2025 01:05:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:06:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:06:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c90cbf55-8f85-4e81-acb6-eb04e4fed88e
01/27/2025 01:06:17:INFO:Received: train message c90cbf55-8f85-4e81-acb6-eb04e4fed88e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:06:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:06:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:06:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e8349fe1-c08c-42af-bc53-315c36751091
01/27/2025 01:06:57:INFO:Received: evaluate message e8349fe1-c08c-42af-bc53-315c36751091
[92mINFO [0m:      Sent reply
01/27/2025 01:06:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:08:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:08:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a66b2ff3-2a74-468e-9508-f992afc7aa24
01/27/2025 01:08:01:INFO:Received: train message a66b2ff3-2a74-468e-9508-f992afc7aa24
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:08:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:08:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:08:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cdba249d-a629-4a45-8bfd-8c87682608f9
01/27/2025 01:08:53:INFO:Received: evaluate message cdba249d-a629-4a45-8bfd-8c87682608f9
[92mINFO [0m:      Sent reply
01/27/2025 01:08:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:09:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:09:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 773b03b8-b410-41ed-b071-f9d65a09cd47
01/27/2025 01:09:19:INFO:Received: train message 773b03b8-b410-41ed-b071-f9d65a09cd47
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:09:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:10:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:10:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 592675d5-8a4f-4300-ae28-81de8ab6b0fe
01/27/2025 01:10:13:INFO:Received: evaluate message 592675d5-8a4f-4300-ae28-81de8ab6b0fe
[92mINFO [0m:      Sent reply
01/27/2025 01:10:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:10:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:10:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 536bbcc1-dfaf-4c50-8939-1bd1da2782a8
01/27/2025 01:10:50:INFO:Received: train message 536bbcc1-dfaf-4c50-8939-1bd1da2782a8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:11:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:11:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:11:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 176dbb09-b169-4a75-af4c-b7aaaa40783c
01/27/2025 01:11:39:INFO:Received: evaluate message 176dbb09-b169-4a75-af4c-b7aaaa40783c
[92mINFO [0m:      Sent reply
01/27/2025 01:11:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:12:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:12:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9d62c278-9f85-4fdf-a5cf-b8314aef55fd
01/27/2025 01:12:20:INFO:Received: train message 9d62c278-9f85-4fdf-a5cf-b8314aef55fd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:12:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:13:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:13:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c31c4dbe-83c8-4637-b43c-ab67056ac73e
01/27/2025 01:13:26:INFO:Received: evaluate message c31c4dbe-83c8-4637-b43c-ab67056ac73e
[92mINFO [0m:      Sent reply
01/27/2025 01:13:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:14:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:14:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 048711ae-dcf9-46ee-b6f2-4c5a01cb69d9
01/27/2025 01:14:22:INFO:Received: train message 048711ae-dcf9-46ee-b6f2-4c5a01cb69d9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:14:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:15:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:15:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4c543f65-c825-42af-b686-b405d551fce8
01/27/2025 01:15:25:INFO:Received: evaluate message 4c543f65-c825-42af-b686-b405d551fce8
[92mINFO [0m:      Sent reply
01/27/2025 01:15:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:15:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:15:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd9d1a42-ef8a-40b9-8f7f-0b847e4ac763
01/27/2025 01:15:47:INFO:Received: train message cd9d1a42-ef8a-40b9-8f7f-0b847e4ac763
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:16:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:16:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:16:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0fd48ce2-e4c7-436e-82a1-23d22e6eeb9a
01/27/2025 01:16:37:INFO:Received: evaluate message 0fd48ce2-e4c7-436e-82a1-23d22e6eeb9a
[92mINFO [0m:      Sent reply
01/27/2025 01:16:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:17:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:17:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 96e46da2-a6ee-4b35-af86-3c719f5b5314
01/27/2025 01:17:09:INFO:Received: train message 96e46da2-a6ee-4b35-af86-3c719f5b5314
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:17:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:18:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:18:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ad952774-6f6b-47d0-8655-a77d71f602e8
01/27/2025 01:18:04:INFO:Received: evaluate message ad952774-6f6b-47d0-8655-a77d71f602e8
[92mINFO [0m:      Sent reply
01/27/2025 01:18:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:18:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:18:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 34dcf13f-5785-406f-8f40-d6e109392dd7
01/27/2025 01:18:39:INFO:Received: train message 34dcf13f-5785-406f-8f40-d6e109392dd7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:18:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:19:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:19:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9225b8dd-7f5c-4a57-9424-5a4672dd739a
01/27/2025 01:19:19:INFO:Received: evaluate message 9225b8dd-7f5c-4a57-9424-5a4672dd739a
[92mINFO [0m:      Sent reply
01/27/2025 01:19:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:20:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:20:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2d74b421-12d2-4106-b5b8-6d1dd4161316
01/27/2025 01:20:00:INFO:Received: train message 2d74b421-12d2-4106-b5b8-6d1dd4161316
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:20:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:20:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:20:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c3cfae9d-f90a-4149-8941-97a47565cfac
01/27/2025 01:20:49:INFO:Received: evaluate message c3cfae9d-f90a-4149-8941-97a47565cfac
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:20:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:21:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:21:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ba76f100-e9b3-4408-a7bc-c2115630bef6
01/27/2025 01:21:19:INFO:Received: train message ba76f100-e9b3-4408-a7bc-c2115630bef6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:21:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:22:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:22:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 40f47d63-6f00-4872-b7e9-23df648ac88b
01/27/2025 01:22:13:INFO:Received: evaluate message 40f47d63-6f00-4872-b7e9-23df648ac88b
[92mINFO [0m:      Sent reply
01/27/2025 01:22:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:22:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:22:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8eafff30-4b65-4cd7-83f5-66f15f1c4fca
01/27/2025 01:22:48:INFO:Received: train message 8eafff30-4b65-4cd7-83f5-66f15f1c4fca
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:23:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:23:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:23:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fab85ced-11e4-42ed-8d22-176e48a6bd21
01/27/2025 01:23:36:INFO:Received: evaluate message fab85ced-11e4-42ed-8d22-176e48a6bd21
[92mINFO [0m:      Sent reply
01/27/2025 01:23:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:23:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:23:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f3179ba5-9057-4de6-a4fe-536a7d14a8b1
01/27/2025 01:23:57:INFO:Received: train message f3179ba5-9057-4de6-a4fe-536a7d14a8b1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:24:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:25:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:25:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f0300903-813e-4d0d-9d1b-6a701a1f4891
01/27/2025 01:25:03:INFO:Received: evaluate message f0300903-813e-4d0d-9d1b-6a701a1f4891

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:25:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:25:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:25:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f1152193-cbdb-4419-a6d5-6cbde7d6535f
01/27/2025 01:25:20:INFO:Received: train message f1152193-cbdb-4419-a6d5-6cbde7d6535f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:25:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:26:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:26:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 835fc1bd-5368-422b-bcc4-e311de0fa4b2
01/27/2025 01:26:09:INFO:Received: evaluate message 835fc1bd-5368-422b-bcc4-e311de0fa4b2
[92mINFO [0m:      Sent reply
01/27/2025 01:26:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:27:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:27:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 927f551b-3445-4f60-a77a-3b099a4c8106
01/27/2025 01:27:00:INFO:Received: train message 927f551b-3445-4f60-a77a-3b099a4c8106
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:27:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:27:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:27:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e923141f-d569-4878-9cbe-1e60e1391103
01/27/2025 01:27:50:INFO:Received: evaluate message e923141f-d569-4878-9cbe-1e60e1391103
[92mINFO [0m:      Sent reply
01/27/2025 01:27:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:27:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:27:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 236bce90-0adb-4a0a-a39a-db0af3df2bb9
01/27/2025 01:27:53:INFO:Received: reconnect message 236bce90-0adb-4a0a-a39a-db0af3df2bb9
01/27/2025 01:27:53:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 01:27:53:INFO:Disconnect and shut down

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.57421875
Data Scaling Factor: 0.24171875 where Client Data Size: 1547
Noise Multiplier after Fisher Scaling:  [0.08683837205171585, 0.05789743736386299, 0.4116928279399872, 0.05876486748456955]
Noise Multiplier after list and tensor:  0.1537983762100339
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1547, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695, 1.0624548865072982], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791, 0.5809225957779516], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168, 0.7911837794938563]}



Final client history:
{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695, 1.0624548865072982], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791, 0.5809225957779516], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168, 0.7911837794938563]}

