nohup: ignoring input
01/27/2025 00:41:12:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 00:41:12:DEBUG:ChannelConnectivity.IDLE
01/27/2025 00:41:12:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 00:41:12:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 00:42:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:42:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b78af3c4-c6fe-4cb0-9c56-06f611ca31c0
01/27/2025 00:42:08:INFO:Received: train message b78af3c4-c6fe-4cb0-9c56-06f611ca31c0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:42:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:43:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:43:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ffab76f7-2349-4b59-bf6d-400938b4d2a0
01/27/2025 00:43:40:INFO:Received: evaluate message ffab76f7-2349-4b59-bf6d-400938b4d2a0
[92mINFO [0m:      Sent reply
01/27/2025 00:43:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:44:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:44:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0aacf952-fca0-401a-818c-f7e2bbac2348
01/27/2025 00:44:19:INFO:Received: train message 0aacf952-fca0-401a-818c-f7e2bbac2348
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:44:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:45:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:45:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0fdcd462-a71e-4f06-be34-341bd88d53f8
01/27/2025 00:45:44:INFO:Received: evaluate message 0fdcd462-a71e-4f06-be34-341bd88d53f8
[92mINFO [0m:      Sent reply
01/27/2025 00:45:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:46:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:46:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f7b2b095-4d30-4e33-817a-5626d45083b6
01/27/2025 00:46:09:INFO:Received: train message f7b2b095-4d30-4e33-817a-5626d45083b6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:46:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:47:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:47:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 602e0240-cd6b-463e-9695-b6c6e70c184b
01/27/2025 00:47:15:INFO:Received: evaluate message 602e0240-cd6b-463e-9695-b6c6e70c184b
[92mINFO [0m:      Sent reply
01/27/2025 00:47:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:48:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:48:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dfc7551f-f6d8-400c-a983-6bf4655cf444
01/27/2025 00:48:01:INFO:Received: train message dfc7551f-f6d8-400c-a983-6bf4655cf444
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:48:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:48:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:48:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bba93406-3fef-4153-b4a1-c248974862b3
01/27/2025 00:48:58:INFO:Received: evaluate message bba93406-3fef-4153-b4a1-c248974862b3
[92mINFO [0m:      Sent reply
01/27/2025 00:49:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:49:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:49:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a470847b-d0c3-4949-bb6f-ddc0f1ff5d6d
01/27/2025 00:49:20:INFO:Received: train message a470847b-d0c3-4949-bb6f-ddc0f1ff5d6d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:49:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:50:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:50:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message af98d39a-05b7-448b-895b-dc077d1cd726
01/27/2025 00:50:31:INFO:Received: evaluate message af98d39a-05b7-448b-895b-dc077d1cd726
[92mINFO [0m:      Sent reply
01/27/2025 00:50:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:51:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:51:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 913dbb30-cbd2-4cd1-ac93-233f9dfe84c5
01/27/2025 00:51:16:INFO:Received: train message 913dbb30-cbd2-4cd1-ac93-233f9dfe84c5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:51:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:52:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:52:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ef723be7-c8a6-4643-8061-8ba355e2f4d7
01/27/2025 00:52:11:INFO:Received: evaluate message ef723be7-c8a6-4643-8061-8ba355e2f4d7
[92mINFO [0m:      Sent reply
01/27/2025 00:52:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:52:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:52:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0d7d5797-2729-49d5-954c-ece001839a8f
01/27/2025 00:52:57:INFO:Received: train message 0d7d5797-2729-49d5-954c-ece001839a8f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:53:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:53:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:53:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 473bf8ab-7185-4b4f-a163-71269c6f987d
01/27/2025 00:53:57:INFO:Received: evaluate message 473bf8ab-7185-4b4f-a163-71269c6f987d
[92mINFO [0m:      Sent reply
01/27/2025 00:54:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:54:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:54:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5a2a96a7-2cfc-4a53-849b-c58da9d2be76
01/27/2025 00:54:14:INFO:Received: train message 5a2a96a7-2cfc-4a53-849b-c58da9d2be76
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406], 'accuracy': [0.5160281469898358], 'auc': [0.7097259125281268]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157], 'accuracy': [0.5160281469898358, 0.506645817044566], 'auc': [0.7097259125281268, 0.7279838714938965]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:54:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:55:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:55:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3dcf62f2-b70e-4fe1-8b00-c13c50e85e0e
01/27/2025 00:55:42:INFO:Received: evaluate message 3dcf62f2-b70e-4fe1-8b00-c13c50e85e0e
[92mINFO [0m:      Sent reply
01/27/2025 00:55:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:56:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:56:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 991bb07f-a076-4d9e-9827-9381790fe459
01/27/2025 00:56:05:INFO:Received: train message 991bb07f-a076-4d9e-9827-9381790fe459
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:56:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:57:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:57:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c7d78d60-cdb2-4b1a-a591-0828bbb29a44
01/27/2025 00:57:15:INFO:Received: evaluate message c7d78d60-cdb2-4b1a-a591-0828bbb29a44
[92mINFO [0m:      Sent reply
01/27/2025 00:57:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:57:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:57:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1257df7c-e8db-4c14-b6ce-7302b926552c
01/27/2025 00:57:39:INFO:Received: train message 1257df7c-e8db-4c14-b6ce-7302b926552c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:57:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:58:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:58:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 358dd90f-1e9b-42f1-91f7-3538f28f0757
01/27/2025 00:58:41:INFO:Received: evaluate message 358dd90f-1e9b-42f1-91f7-3538f28f0757
[92mINFO [0m:      Sent reply
01/27/2025 00:58:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:59:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:59:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 10155c7d-6977-481d-969f-bd8c92c2d00e
01/27/2025 00:59:20:INFO:Received: train message 10155c7d-6977-481d-969f-bd8c92c2d00e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:59:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:00:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:00:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 859efce4-f121-4c56-a3b8-aca9eca9fec0
01/27/2025 01:00:25:INFO:Received: evaluate message 859efce4-f121-4c56-a3b8-aca9eca9fec0
[92mINFO [0m:      Sent reply
01/27/2025 01:00:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:00:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:00:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 914d11ee-9136-4b24-b769-b4532fca8c62
01/27/2025 01:00:49:INFO:Received: train message 914d11ee-9136-4b24-b769-b4532fca8c62
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:00:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:01:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:01:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9dac42f2-8864-4bb1-a85d-3e8d6f91aabc
01/27/2025 01:01:43:INFO:Received: evaluate message 9dac42f2-8864-4bb1-a85d-3e8d6f91aabc
[92mINFO [0m:      Sent reply
01/27/2025 01:01:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:02:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:02:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f6d4115d-438b-4706-87e0-e33638eeca27
01/27/2025 01:02:11:INFO:Received: train message f6d4115d-438b-4706-87e0-e33638eeca27
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:02:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:03:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:03:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 18621081-5fdc-49bc-80e1-2e64c91cda98
01/27/2025 01:03:11:INFO:Received: evaluate message 18621081-5fdc-49bc-80e1-2e64c91cda98
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
[92mINFO [0m:      Sent reply
01/27/2025 01:03:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:03:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:03:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9e03a06e-1ef3-4377-b5b0-26705becbd0e
01/27/2025 01:03:45:INFO:Received: train message 9e03a06e-1ef3-4377-b5b0-26705becbd0e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:03:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:04:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:04:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 34042cc2-0d95-4543-9026-925b600aea20
01/27/2025 01:04:13:INFO:Received: evaluate message 34042cc2-0d95-4543-9026-925b600aea20
[92mINFO [0m:      Sent reply
01/27/2025 01:04:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:04:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:04:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ea70a4bb-09ab-43a1-a229-6624525ff04a
01/27/2025 01:04:57:INFO:Received: train message ea70a4bb-09ab-43a1-a229-6624525ff04a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:05:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:05:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:05:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c57a372f-b005-49c8-8ba4-725998576cd0
01/27/2025 01:05:55:INFO:Received: evaluate message c57a372f-b005-49c8-8ba4-725998576cd0
[92mINFO [0m:      Sent reply
01/27/2025 01:05:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:06:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:06:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a269d0ac-a5ef-4a4a-92d2-08f72acb8ac3
01/27/2025 01:06:27:INFO:Received: train message a269d0ac-a5ef-4a4a-92d2-08f72acb8ac3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:06:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:07:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:07:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9c6a7c40-d7c3-4bab-82cc-ce576cd5dc32
01/27/2025 01:07:18:INFO:Received: evaluate message 9c6a7c40-d7c3-4bab-82cc-ce576cd5dc32
[92mINFO [0m:      Sent reply
01/27/2025 01:07:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:07:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:07:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 22bd907e-2790-4846-a366-be2626808e07
01/27/2025 01:07:52:INFO:Received: train message 22bd907e-2790-4846-a366-be2626808e07
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:08:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:08:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:08:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1e8dce5c-8b92-4fd5-9b4c-6fe98c7cef3b
01/27/2025 01:08:50:INFO:Received: evaluate message 1e8dce5c-8b92-4fd5-9b4c-6fe98c7cef3b
[92mINFO [0m:      Sent reply
01/27/2025 01:08:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:09:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:09:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6411f388-997f-47af-8d2e-a53ade825bfe
01/27/2025 01:09:25:INFO:Received: train message 6411f388-997f-47af-8d2e-a53ade825bfe
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:09:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:10:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:10:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6f1d3877-1439-419c-931e-d1b48aa286a4
01/27/2025 01:10:13:INFO:Received: evaluate message 6f1d3877-1439-419c-931e-d1b48aa286a4
[92mINFO [0m:      Sent reply
01/27/2025 01:10:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:10:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:10:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 17dd417b-496d-4131-8a6c-5ae7bc407cf8
01/27/2025 01:10:27:INFO:Received: train message 17dd417b-496d-4131-8a6c-5ae7bc407cf8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:10:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:11:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:11:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bdff9505-652f-4a5b-aa85-219feacee722
01/27/2025 01:11:38:INFO:Received: evaluate message bdff9505-652f-4a5b-aa85-219feacee722
[92mINFO [0m:      Sent reply
01/27/2025 01:11:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:12:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:12:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9d6181a9-8bc5-4b80-b35a-f3385997a04d
01/27/2025 01:12:15:INFO:Received: train message 9d6181a9-8bc5-4b80-b35a-f3385997a04d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:12:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:13:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:13:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fda39d41-a26e-4048-a0b2-e221eddae71a
01/27/2025 01:13:40:INFO:Received: evaluate message fda39d41-a26e-4048-a0b2-e221eddae71a
[92mINFO [0m:      Sent reply
01/27/2025 01:13:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:14:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:14:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 82eef3c3-6a5f-4f47-9b5f-3c06307d4e9b
01/27/2025 01:14:28:INFO:Received: train message 82eef3c3-6a5f-4f47-9b5f-3c06307d4e9b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:14:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:15:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:15:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b79f919b-7bd1-483e-bc55-99ae342533ad
01/27/2025 01:15:26:INFO:Received: evaluate message b79f919b-7bd1-483e-bc55-99ae342533ad
[92mINFO [0m:      Sent reply
01/27/2025 01:15:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:15:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:15:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 518dc324-38a9-4177-8308-5ff406d57064
01/27/2025 01:15:52:INFO:Received: train message 518dc324-38a9-4177-8308-5ff406d57064
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:16:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:16:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:16:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f613b26e-14dd-4cc7-8b06-cbeadc605e1e
01/27/2025 01:16:44:INFO:Received: evaluate message f613b26e-14dd-4cc7-8b06-cbeadc605e1e
[92mINFO [0m:      Sent reply
01/27/2025 01:16:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:17:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:17:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8094549c-d2cc-4efe-b2a3-680f2d0d0aa4
01/27/2025 01:17:16:INFO:Received: train message 8094549c-d2cc-4efe-b2a3-680f2d0d0aa4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:17:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:17:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:17:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b2a08073-d5ff-4246-9934-2e4c1e5555c4
01/27/2025 01:17:57:INFO:Received: evaluate message b2a08073-d5ff-4246-9934-2e4c1e5555c4
[92mINFO [0m:      Sent reply
01/27/2025 01:17:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:18:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:18:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6da8e54d-0bb4-4b96-a736-387095bce31d
01/27/2025 01:18:36:INFO:Received: train message 6da8e54d-0bb4-4b96-a736-387095bce31d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:18:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:19:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:19:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 66c47de7-93a5-4283-b9a1-ec03c8ca374a
01/27/2025 01:19:26:INFO:Received: evaluate message 66c47de7-93a5-4283-b9a1-ec03c8ca374a
[92mINFO [0m:      Sent reply
01/27/2025 01:19:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:19:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:19:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ad9d0e37-9688-40ae-9e79-857c7e1b9891
01/27/2025 01:19:53:INFO:Received: train message ad9d0e37-9688-40ae-9e79-857c7e1b9891
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:20:04:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:20:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:20:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9af15bfd-1532-4ab3-bc8f-e91fa6aa0dce
01/27/2025 01:20:46:INFO:Received: evaluate message 9af15bfd-1532-4ab3-bc8f-e91fa6aa0dce
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:20:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:21:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:21:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9dda923d-0f94-40ff-941c-020f6a709b9f
01/27/2025 01:21:13:INFO:Received: train message 9dda923d-0f94-40ff-941c-020f6a709b9f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:21:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:21:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:21:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 40f3eeb6-9175-4fc5-bf6b-a8912b3a602c
01/27/2025 01:21:58:INFO:Received: evaluate message 40f3eeb6-9175-4fc5-bf6b-a8912b3a602c
[92mINFO [0m:      Sent reply
01/27/2025 01:22:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:22:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:22:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1c3629cf-4507-4299-9e4e-06e4e1f455e2
01/27/2025 01:22:31:INFO:Received: train message 1c3629cf-4507-4299-9e4e-06e4e1f455e2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:22:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:23:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:23:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 12f3ffbd-1952-4dec-82fd-f28f56509f68
01/27/2025 01:23:36:INFO:Received: evaluate message 12f3ffbd-1952-4dec-82fd-f28f56509f68
[92mINFO [0m:      Sent reply
01/27/2025 01:23:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:24:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:24:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 72dac05c-b38b-4249-99d6-6bc03e658dc5
01/27/2025 01:24:06:INFO:Received: train message 72dac05c-b38b-4249-99d6-6bc03e658dc5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:24:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:24:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:24:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0a45d8a7-ae0e-4f29-ac0a-7e72e2d0e37d
01/27/2025 01:24:56:INFO:Received: evaluate message 0a45d8a7-ae0e-4f29-ac0a-7e72e2d0e37d

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:24:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:25:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:25:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 754e2ca4-9f4a-4046-886c-9cb84bd7f2e6
01/27/2025 01:25:32:INFO:Received: train message 754e2ca4-9f4a-4046-886c-9cb84bd7f2e6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:25:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:26:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:26:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0331d316-e462-4e89-b2f1-0cd72cbf11a1
01/27/2025 01:26:16:INFO:Received: evaluate message 0331d316-e462-4e89-b2f1-0cd72cbf11a1
[92mINFO [0m:      Sent reply
01/27/2025 01:26:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:26:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:26:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4247b6dd-11ba-4717-8d7d-efe776849830
01/27/2025 01:26:39:INFO:Received: train message 4247b6dd-11ba-4717-8d7d-efe776849830
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:26:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:27:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:27:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1ec4aef9-aafe-4dd9-ba69-b1acd399b6c2
01/27/2025 01:27:50:INFO:Received: evaluate message 1ec4aef9-aafe-4dd9-ba69-b1acd399b6c2
[92mINFO [0m:      Sent reply
01/27/2025 01:27:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:27:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:27:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 55536073-8c3c-4a6b-9b5a-bde0baba4231
01/27/2025 01:27:53:INFO:Received: reconnect message 55536073-8c3c-4a6b-9b5a-bde0baba4231
01/27/2025 01:27:53:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 01:27:53:INFO:Disconnect and shut down

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.34375
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.016693340614438057, 0.014036275446414948, 0.09161395579576492, 0.03210073709487915]
Noise Multiplier after list and tensor:  0.03861107723787427
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695, 1.0624548865072982], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791, 0.5809225957779516], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168, 0.7911837794938563]}



Final client history:
{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695, 1.0624548865072982], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791, 0.5809225957779516], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168, 0.7911837794938563]}

