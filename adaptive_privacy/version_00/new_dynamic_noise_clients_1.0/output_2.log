nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/new_dynamic_noise_clients_1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/12/2025 09:56:19:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/12/2025 09:56:19:DEBUG:ChannelConnectivity.IDLE
01/12/2025 09:56:19:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/12/2025 09:56:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 09:56:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5c709768-e165-4b55-92eb-96091eb56714
01/12/2025 09:56:40:INFO:Received: train message 5c709768-e165-4b55-92eb-96091eb56714
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 10:09:29:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 10:16:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:16:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5cbdd832-cdcf-4837-a82a-313ad112c4e9
01/12/2025 10:16:48:INFO:Received: evaluate message 5cbdd832-cdcf-4837-a82a-313ad112c4e9
[92mINFO [0m:      Sent reply
01/12/2025 10:22:01:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 10:22:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:22:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message aa076290-5616-400d-a81c-635e743cc6f3
01/12/2025 10:22:26:INFO:Received: train message aa076290-5616-400d-a81c-635e743cc6f3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 10:35:24:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 10:44:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:44:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ffae6ed5-8e9d-443d-9164-f8da4455935b
01/12/2025 10:44:39:INFO:Received: evaluate message ffae6ed5-8e9d-443d-9164-f8da4455935b
[92mINFO [0m:      Sent reply
01/12/2025 10:49:40:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 10:49:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:49:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 784b9e35-0645-4070-8b3f-6b6458bd0b9e
01/12/2025 10:49:52:INFO:Received: train message 784b9e35-0645-4070-8b3f-6b6458bd0b9e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 11:02:45:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:12:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:12:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 06f82914-16bd-47d4-afd2-021d46727428
01/12/2025 11:12:26:INFO:Received: evaluate message 06f82914-16bd-47d4-afd2-021d46727428
[92mINFO [0m:      Sent reply
01/12/2025 11:17:18:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:18:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:18:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 38515d64-37df-4c19-98b5-e91a6753c21f
01/12/2025 11:18:00:INFO:Received: train message 38515d64-37df-4c19-98b5-e91a6753c21f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 11:31:17:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:40:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:40:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9b6973a9-c107-4134-b16d-fc803ec06311
01/12/2025 11:40:03:INFO:Received: evaluate message 9b6973a9-c107-4134-b16d-fc803ec06311
[92mINFO [0m:      Sent reply
01/12/2025 11:45:26:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:45:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:45:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e1c9bc1d-a979-46cb-b959-4ef1e2f8d951
01/12/2025 11:45:55:INFO:Received: train message e1c9bc1d-a979-46cb-b959-4ef1e2f8d951
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 12:02:33:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:09:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:09:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 71d69a6c-521e-47a4-b116-362edda476c7
01/12/2025 12:09:34:INFO:Received: evaluate message 71d69a6c-521e-47a4-b116-362edda476c7
[92mINFO [0m:      Sent reply
01/12/2025 12:13:44:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:14:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:14:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ed9fdf74-d695-425f-9c35-445c438dc4b5
01/12/2025 12:14:04:INFO:Received: train message ed9fdf74-d695-425f-9c35-445c438dc4b5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 12:30:31:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:37:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:37:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fcdc25d3-3e7a-4130-90d2-0529a17b74f6
01/12/2025 12:37:52:INFO:Received: evaluate message fcdc25d3-3e7a-4130-90d2-0529a17b74f6
[92mINFO [0m:      Sent reply
01/12/2025 12:42:28:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:42:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:42:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 753f11ff-cc1c-457c-a3ca-527b38075bf6
01/12/2025 12:42:56:INFO:Received: train message 753f11ff-cc1c-457c-a3ca-527b38075bf6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 12:59:42:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 13:08:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 13:08:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9b72af1c-cc74-44ac-b9e3-b5ef844e0e9b
01/12/2025 13:08:17:INFO:Received: evaluate message 9b72af1c-cc74-44ac-b9e3-b5ef844e0e9b
[92mINFO [0m:      Sent reply
01/12/2025 13:12:39:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 13:13:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 13:13:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3dbb0fc0-55e4-41d8-bdee-e67775cfc57e
01/12/2025 13:13:16:INFO:Received: train message 3dbb0fc0-55e4-41d8-bdee-e67775cfc57e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 13:28:11:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 13:45:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 13:45:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 77402db7-b577-4074-b0b6-eef2a1e59b25
01/12/2025 13:45:17:INFO:Received: evaluate message 77402db7-b577-4074-b0b6-eef2a1e59b25
[92mINFO [0m:      Sent reply
01/12/2025 13:49:34:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 13:50:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 13:50:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ce0f1bc-b5fa-49dc-ae29-c2a0051191bc
01/12/2025 13:50:06:INFO:Received: train message 8ce0f1bc-b5fa-49dc-ae29-c2a0051191bc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 14:04:23:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 14:24:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 14:24:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0c468173-d9fb-490d-b785-c1f78efbf33d
01/12/2025 14:24:54:INFO:Received: evaluate message 0c468173-d9fb-490d-b785-c1f78efbf33d
[92mINFO [0m:      Sent reply
01/12/2025 14:29:24:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 14:29:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 14:29:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 07e9dddf-b0a9-4b9b-bf42-2889cc94838e
01/12/2025 14:29:51:INFO:Received: train message 07e9dddf-b0a9-4b9b-bf42-2889cc94838e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 14:42:44:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:01:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:01:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3a4fdd90-7b81-4152-b4bd-0808fa01f2b8
01/12/2025 15:01:16:INFO:Received: evaluate message 3a4fdd90-7b81-4152-b4bd-0808fa01f2b8
[92mINFO [0m:      Sent reply
01/12/2025 15:05:30:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:06:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:06:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 582f1115-8515-4018-8181-bea8ff9cbc7f
01/12/2025 15:06:21:INFO:Received: train message 582f1115-8515-4018-8181-bea8ff9cbc7f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 15:23:00:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:36:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:36:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 46eade1e-2f4d-4cb8-99ff-16b87357ce95
01/12/2025 15:36:38:INFO:Received: evaluate message 46eade1e-2f4d-4cb8-99ff-16b87357ce95
[92mINFO [0m:      Sent reply
01/12/2025 15:41:14:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:42:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:42:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0da53d0d-aab2-41da-beed-ac9bd2a8d436
01/12/2025 15:42:14:INFO:Received: train message 0da53d0d-aab2-41da-beed-ac9bd2a8d436
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 15:57:03:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:08:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:08:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f580a51b-0fbc-4871-be79-a4ad7875dd20
01/12/2025 16:08:57:INFO:Received: evaluate message f580a51b-0fbc-4871-be79-a4ad7875dd20
[92mINFO [0m:      Sent reply
01/12/2025 16:14:16:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:15:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:15:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eb384601-f0da-4370-b999-23bd9e6df2dc
01/12/2025 16:15:15:INFO:Received: train message eb384601-f0da-4370-b999-23bd9e6df2dc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 16:29:38:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:39:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:39:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 87888ef9-ac45-475a-bd78-598fe9b861d4
01/12/2025 16:39:43:INFO:Received: evaluate message 87888ef9-ac45-475a-bd78-598fe9b861d4
[92mINFO [0m:      Sent reply
01/12/2025 16:45:35:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:46:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:46:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 53dbec9e-4b88-442b-9fbc-8165304cc9c2
01/12/2025 16:46:11:INFO:Received: train message 53dbec9e-4b88-442b-9fbc-8165304cc9c2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 17:02:05:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:11:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:11:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4948fcc0-d313-4669-8a1b-620f000a6df8
01/12/2025 17:11:52:INFO:Received: evaluate message 4948fcc0-d313-4669-8a1b-620f000a6df8
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/new_dynamic_noise_clients_1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/new_dynamic_noise_clients_1.0']
Epoch 1 - Adjusted noise multipliers: [6.5625]
Epsilon = 0.09

{'loss': [142.32107663154602], 'accuracy': [0.3383004430124849], 'auc': [0.5426145523098456]}

Epoch 2 - Adjusted noise multipliers: [6.516566283255163]
Epsilon = 0.09

{'loss': [142.32107663154602, 134.23665523529053], 'accuracy': [0.3383004430124849, 0.3395086588803866], 'auc': [0.5426145523098456, 0.5873936166804948]}

Epoch 3 - Adjusted noise multipliers: [6.493720131978282]
Epsilon = 0.09

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956]}

Epoch 4 - Adjusted noise multipliers: [6.470954076046937]
Epsilon = 0.09

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795]}

Epoch 5 - Adjusted noise multipliers: [6.448267834658277]
Epsilon = 0.09

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267]}

Epoch 6 - Adjusted noise multipliers: [6.42566112799391]
Epsilon = 0.09

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334]}

Epoch 7 - Adjusted noise multipliers: [6.403133677216444]
Epsilon = 0.09

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264]}

Epoch 8 - Adjusted noise multipliers: [6.380685204466051]
Epsilon = 0.09

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147]}

Epoch 9 - Adjusted noise multipliers: [6.358315432857041]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062]}

Epoch 10 - Adjusted noise multipliers: [6.336024086474445]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218]}

Epoch 11 - Adjusted noise multipliers: [6.313810890370614]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423]}

Epoch 12 - Adjusted noise multipliers: [6.291675570561824]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502]}

Epoch 13 - Adjusted noise multipliers: [6.269617854024901]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675]}

Epoch 14 - Adjusted noise multipliers: [6.247637468693848]
Epsilon = 0.10
[92mINFO [0m:      Sent reply
01/12/2025 17:16:43:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:17:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:17:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bd6f8829-86e8-4d9b-8366-9b53cc643651
01/12/2025 17:17:34:INFO:Received: train message bd6f8829-86e8-4d9b-8366-9b53cc643651
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 17:33:58:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:44:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:44:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ede85c60-d29c-48ea-a109-e98f40b30411
01/12/2025 17:44:41:INFO:Received: evaluate message ede85c60-d29c-48ea-a109-e98f40b30411
[92mINFO [0m:      Sent reply
01/12/2025 17:49:43:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:50:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:50:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fb962f4e-f4cd-49bf-84ce-49e2c882b3b6
01/12/2025 17:50:20:INFO:Received: train message fb962f4e-f4cd-49bf-84ce-49e2c882b3b6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 18:04:06:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:15:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:15:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1afc86ce-bf7e-4eaf-b4f9-ecb21797599c
01/12/2025 18:15:55:INFO:Received: evaluate message 1afc86ce-bf7e-4eaf-b4f9-ecb21797599c
[92mINFO [0m:      Sent reply
01/12/2025 18:21:28:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:22:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:22:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ba84a469-b6dd-4724-b4d5-65bf9b8d20f1
01/12/2025 18:22:20:INFO:Received: train message ba84a469-b6dd-4724-b4d5-65bf9b8d20f1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 18:35:48:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:46:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:46:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9db29f9c-8b81-4209-abe3-a9d0ff1f090a
01/12/2025 18:46:16:INFO:Received: evaluate message 9db29f9c-8b81-4209-abe3-a9d0ff1f090a
[92mINFO [0m:      Sent reply
01/12/2025 18:51:20:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:53:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:53:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b402d530-4ac1-4df5-b1f1-fa62a8996751
01/12/2025 18:53:05:INFO:Received: train message b402d530-4ac1-4df5-b1f1-fa62a8996751
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 19:08:31:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:18:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:18:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b7d423a6-a2fa-49c3-a9ef-7c7f05198d4d
01/12/2025 19:18:31:INFO:Received: evaluate message b7d423a6-a2fa-49c3-a9ef-7c7f05198d4d
[92mINFO [0m:      Sent reply
01/12/2025 19:24:24:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:25:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:25:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6f42b145-d6e5-447a-ad49-d9b1627ee3cd
01/12/2025 19:25:08:INFO:Received: train message 6f42b145-d6e5-447a-ad49-d9b1627ee3cd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 19:40:14:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:51:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:51:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 532966b7-3c5f-421d-9148-8c46d83929bc
01/12/2025 19:51:43:INFO:Received: evaluate message 532966b7-3c5f-421d-9148-8c46d83929bc
[92mINFO [0m:      Sent reply
01/12/2025 19:56:38:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:57:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:57:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8d03a5fd-e57f-435f-96a3-20302f070a6d
01/12/2025 19:57:10:INFO:Received: train message 8d03a5fd-e57f-435f-96a3-20302f070a6d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 20:12:46:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 20:24:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 20:24:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8db74e85-b3a9-49df-b00b-00d360a9c9c0
01/12/2025 20:24:01:INFO:Received: evaluate message 8db74e85-b3a9-49df-b00b-00d360a9c9c0
[92mINFO [0m:      Sent reply
01/12/2025 20:28:17:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 20:29:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 20:29:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6b23a5c5-b17d-45f3-9fa4-d447ca9c07d1
01/12/2025 20:29:47:INFO:Received: train message 6b23a5c5-b17d-45f3-9fa4-d447ca9c07d1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 20:43:10:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 20:55:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 20:55:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5982cd6-bbdc-4912-9380-9f9b52499015
01/12/2025 20:55:54:INFO:Received: evaluate message b5982cd6-bbdc-4912-9380-9f9b52499015

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922]}

Epoch 15 - Adjusted noise multipliers: [6.225734143456497]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373]}

Epoch 16 - Adjusted noise multipliers: [6.203907608151158]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182]}

Epoch 17 - Adjusted noise multipliers: [6.1821575935632875]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125]}

Epoch 18 - Adjusted noise multipliers: [6.160483831422174]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858]}

Epoch 19 - Adjusted noise multipliers: [6.138886054397624]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044]}

Epoch 20 - Adjusted noise multipliers: [6.1173639960966595]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349]}

Epoch 21 - Adjusted noise multipliers: [6.095917391060247]
Epsilon = 0.10
[92mINFO [0m:      Sent reply
01/12/2025 21:01:49:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 21:03:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 21:03:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2ec374b9-9ada-40f3-a9d7-342da3ab997a
01/12/2025 21:03:25:INFO:Received: train message 2ec374b9-9ada-40f3-a9d7-342da3ab997a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 21:21:22:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 21:54:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 21:54:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b6c4548e-de7a-4197-826e-e6e5d6621d0b
01/12/2025 21:54:35:INFO:Received: evaluate message b6c4548e-de7a-4197-826e-e6e5d6621d0b
[92mINFO [0m:      Sent reply
01/12/2025 22:00:45:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 22:02:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 22:02:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cecd4e09-7c81-4069-8981-b1eeeaa51584
01/12/2025 22:02:00:INFO:Received: train message cecd4e09-7c81-4069-8981-b1eeeaa51584
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 22:19:58:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 22:53:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 22:53:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4c214257-d8d6-48c9-86bf-21125cd79b13
01/12/2025 22:53:02:INFO:Received: evaluate message 4c214257-d8d6-48c9-86bf-21125cd79b13
[92mINFO [0m:      Sent reply
01/12/2025 22:59:13:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 23:01:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 23:01:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5e3a0d8a-3bad-424c-8ed5-7ce36419e1ac
01/12/2025 23:01:03:INFO:Received: train message 5e3a0d8a-3bad-424c-8ed5-7ce36419e1ac
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 23:19:08:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 23:59:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 23:59:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e7a447cf-f308-47fb-9be4-fba415e0566a
01/12/2025 23:59:10:INFO:Received: evaluate message e7a447cf-f308-47fb-9be4-fba415e0566a
[92mINFO [0m:      Sent reply
01/13/2025 00:04:09:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 00:07:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 00:07:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6439e45e-bf7a-4761-af46-054411ad4b02
01/13/2025 00:07:59:INFO:Received: train message 6439e45e-bf7a-4761-af46-054411ad4b02
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 00:22:24:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:15:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:15:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message be1d13be-2232-4fa3-afe6-f926254501de
01/13/2025 01:15:15:INFO:Received: evaluate message be1d13be-2232-4fa3-afe6-f926254501de
[92mINFO [0m:      Sent reply
01/13/2025 01:19:58:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:24:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:24:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 76d8f660-76aa-4c38-8ef1-5af8df01108e
01/13/2025 01:24:41:INFO:Received: train message 76d8f660-76aa-4c38-8ef1-5af8df01108e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 01:39:18:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:54:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:54:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9d616d96-d04d-4a56-8600-b99c5c4237fc
01/13/2025 01:54:00:INFO:Received: evaluate message 9d616d96-d04d-4a56-8600-b99c5c4237fc

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199]}

Epoch 22 - Adjusted noise multipliers: [6.07454597476001]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075]}

Epoch 23 - Adjusted noise multipliers: [6.05324948359497]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279]}

Epoch 24 - Adjusted noise multipliers: [6.032027654888298]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798]}

Epoch 25 - Adjusted noise multipliers: [6.010880226884071]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997]}

Epoch 26 - Adjusted noise multipliers: [5.989806938744045]
Epsilon = 0.10
[92mINFO [0m:      Sent reply
01/13/2025 02:00:14:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:01:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:01:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d715b320-e607-492a-a46c-35908847c6ca
01/13/2025 02:01:17:INFO:Received: train message d715b320-e607-492a-a46c-35908847c6ca
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 02:15:47:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:32:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:32:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6f852ea5-2c9a-4638-a74e-cf81d3cf0360
01/13/2025 02:32:06:INFO:Received: evaluate message 6f852ea5-2c9a-4638-a74e-cf81d3cf0360
[92mINFO [0m:      Sent reply
01/13/2025 02:37:20:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:37:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:37:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 16117d9f-a837-418a-8d49-6631e44c6e94
01/13/2025 02:37:57:INFO:Received: train message 16117d9f-a837-418a-8d49-6631e44c6e94
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 02:50:36:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:00:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:00:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e6875ddf-d07d-48bd-9275-3f353f7511fb
01/13/2025 03:00:15:INFO:Received: evaluate message e6875ddf-d07d-48bd-9275-3f353f7511fb
[92mINFO [0m:      Sent reply
01/13/2025 03:05:08:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:05:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:05:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 752789b5-6d71-4c5b-bfff-1e4777127af5
01/13/2025 03:05:57:INFO:Received: train message 752789b5-6d71-4c5b-bfff-1e4777127af5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 03:18:27:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:31:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:31:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7f3fff4a-18e9-404e-83b1-7c94ec115491
01/13/2025 03:31:09:INFO:Received: evaluate message 7f3fff4a-18e9-404e-83b1-7c94ec115491
[92mINFO [0m:      Sent reply
01/13/2025 03:36:27:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:37:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:37:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d40609df-8a50-48df-8017-407fed27046a
01/13/2025 03:37:23:INFO:Received: train message d40609df-8a50-48df-8017-407fed27046a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 03:50:19:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 04:00:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 04:00:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8605b568-6517-4fdd-b788-9d1deb968f29
01/13/2025 04:00:38:INFO:Received: evaluate message 8605b568-6517-4fdd-b788-9d1deb968f29

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927, 120.14814364910126], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804, 0.5259766411598872], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997, 0.7108235099425215]}

Epoch 27 - Adjusted noise multipliers: [5.9688075305444395]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927, 120.14814364910126, 120.49823808670044], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804, 0.5259766411598872, 0.5267821184051551], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997, 0.7108235099425215, 0.712988051186769]}

Epoch 28 - Adjusted noise multipliers: [5.947881743272728]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927, 120.14814364910126, 120.49823808670044, 120.97020924091339], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804, 0.5259766411598872, 0.5267821184051551, 0.527184857027789], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997, 0.7108235099425215, 0.712988051186769, 0.7152528953486761]}

Epoch 29 - Adjusted noise multipliers: [5.927029318824443]
Epsilon = 0.10

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927, 120.14814364910126, 120.49823808670044, 120.97020924091339, 120.67132663726807], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804, 0.5259766411598872, 0.5267821184051551, 0.527184857027789, 0.5296012887635925], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997, 0.7108235099425215, 0.712988051186769, 0.7152528953486761, 0.7179602061623762]}

Epoch 30 - Adjusted noise multipliers: [5.90625]
Epsilon = 0.10
[92mINFO [0m:      Sent reply
01/13/2025 04:04:55:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 04:05:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 04:05:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message d5bca603-f78e-465a-8333-1e2f72158cbc
01/13/2025 04:05:18:INFO:Received: reconnect message d5bca603-f78e-465a-8333-1e2f72158cbc
01/13/2025 04:05:18:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/13/2025 04:05:18:INFO:Disconnect and shut down

{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927, 120.14814364910126, 120.49823808670044, 120.97020924091339, 120.67132663726807, 120.4772458076477], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804, 0.5259766411598872, 0.5267821184051551, 0.527184857027789, 0.5296012887635925, 0.5291985501409585], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997, 0.7108235099425215, 0.712988051186769, 0.7152528953486761, 0.7179602061623762, 0.7176607396735059]}



Final client history:
{'loss': [142.32107663154602, 134.23665523529053, 136.12105679512024, 140.07609140872955, 141.6434142589569, 141.5808573961258, 138.84101843833923, 136.98838675022125, 134.7799082994461, 133.17851054668427, 131.3119169473648, 131.25493013858795, 129.37895095348358, 129.03271234035492, 128.96149384975433, 127.91069281101227, 126.60621798038483, 126.43137884140015, 125.31101906299591, 123.96355712413788, 122.9172990322113, 122.52266871929169, 122.14226818084717, 120.95663583278656, 121.52689921855927, 120.14814364910126, 120.49823808670044, 120.97020924091339, 120.67132663726807, 120.4772458076477], 'accuracy': [0.3383004430124849, 0.3395086588803866, 0.3411196133709223, 0.3407168747482884, 0.34474426097462746, 0.35118807893677, 0.3689085783326621, 0.38542086186065244, 0.40515505436971405, 0.4192509061619009, 0.4329440193314539, 0.43576318968989125, 0.44663713250100684, 0.45388642770841725, 0.4607329842931937, 0.4695932339911397, 0.4784534836890858, 0.4812726540475232, 0.490938380990737, 0.49456302859444223, 0.4993958920660491, 0.5098670962545309, 0.5171163914619412, 0.5219492549335482, 0.5211437776882804, 0.5259766411598872, 0.5267821184051551, 0.527184857027789, 0.5296012887635925, 0.5291985501409585], 'auc': [0.5426145523098456, 0.5873936166804948, 0.609386764389956, 0.6214781059488795, 0.6323423469000267, 0.6365424751393334, 0.6456415990489264, 0.6518130850934147, 0.6562647543351062, 0.6573787860388218, 0.6629578368084423, 0.6672990970988502, 0.6701897226275675, 0.6756767206689922, 0.6768641059034373, 0.6815306121900182, 0.684562207016125, 0.6868794836107858, 0.6888305126364044, 0.6947256716904349, 0.6962851966772199, 0.6987630197681075, 0.7005204776365279, 0.7041978795930798, 0.7070246852260997, 0.7108235099425215, 0.712988051186769, 0.7152528953486761, 0.7179602061623762, 0.7176607396735059]}

