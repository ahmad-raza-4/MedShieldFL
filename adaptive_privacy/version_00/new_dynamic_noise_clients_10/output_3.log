nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/new_dynamic_noise_clients_10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/12/2025 10:16:11:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/12/2025 10:16:11:DEBUG:ChannelConnectivity.IDLE
01/12/2025 10:16:11:DEBUG:ChannelConnectivity.CONNECTING
01/12/2025 10:16:11:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/12/2025 10:16:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:16:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message db075e75-5154-46b4-a8f9-da15630c2c4f
01/12/2025 10:16:11:INFO:Received: get_parameters message db075e75-5154-46b4-a8f9-da15630c2c4f
[92mINFO [0m:      Sent reply
01/12/2025 10:16:15:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 10:16:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:16:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 99752cc1-12eb-495b-ab4b-a1be61f408ca
01/12/2025 10:16:47:INFO:Received: train message 99752cc1-12eb-495b-ab4b-a1be61f408ca
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 10:28:18:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 10:38:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:38:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eb8433ea-d71d-4db6-9c31-fed54217c89c
01/12/2025 10:38:06:INFO:Received: evaluate message eb8433ea-d71d-4db6-9c31-fed54217c89c
[92mINFO [0m:      Sent reply
01/12/2025 10:41:29:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 10:43:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:43:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f0e9aafd-e5f9-4f09-bba7-5c64e507c4cf
01/12/2025 10:43:46:INFO:Received: train message f0e9aafd-e5f9-4f09-bba7-5c64e507c4cf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 10:54:55:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:05:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:05:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0a111d61-00f8-41a8-a300-7a74d86e314b
01/12/2025 11:05:54:INFO:Received: evaluate message 0a111d61-00f8-41a8-a300-7a74d86e314b
[92mINFO [0m:      Sent reply
01/12/2025 11:09:57:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:11:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:11:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7f80eb4e-07c3-497a-b61f-f6dc4a6539c8
01/12/2025 11:11:10:INFO:Received: train message 7f80eb4e-07c3-497a-b61f-f6dc4a6539c8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 11:24:14:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:39:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:39:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 45ed0d9d-387b-496c-b935-73c7e4de75c1
01/12/2025 11:39:05:INFO:Received: evaluate message 45ed0d9d-387b-496c-b935-73c7e4de75c1
[92mINFO [0m:      Sent reply
01/12/2025 11:43:33:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:44:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:44:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1e757383-d2f6-4ec0-9650-ccd09506865f
01/12/2025 11:44:49:INFO:Received: train message 1e757383-d2f6-4ec0-9650-ccd09506865f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 11:58:28:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:14:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:14:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c1cda880-f809-45d5-a529-5f95a5b4600e
01/12/2025 12:14:21:INFO:Received: evaluate message c1cda880-f809-45d5-a529-5f95a5b4600e
[92mINFO [0m:      Sent reply
01/12/2025 12:18:55:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:21:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:21:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e3665ab8-ceae-4c07-98c4-edbd54942fa5
01/12/2025 12:21:02:INFO:Received: train message e3665ab8-ceae-4c07-98c4-edbd54942fa5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 12:32:29:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:49:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:49:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8d2ecd01-fe36-47e8-9b10-8d35141e9c33
01/12/2025 12:49:49:INFO:Received: evaluate message 8d2ecd01-fe36-47e8-9b10-8d35141e9c33
[92mINFO [0m:      Sent reply
01/12/2025 12:53:57:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:56:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:56:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0d5b1580-420f-436e-87d5-93619edb4b88
01/12/2025 12:56:22:INFO:Received: train message 0d5b1580-420f-436e-87d5-93619edb4b88
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 13:07:47:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 13:24:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 13:24:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c22f14f6-e0b2-48f4-a186-63c79bcf0d18
01/12/2025 13:24:14:INFO:Received: evaluate message c22f14f6-e0b2-48f4-a186-63c79bcf0d18
[92mINFO [0m:      Sent reply
01/12/2025 13:28:30:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 13:29:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 13:29:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 038c8019-860f-4432-a729-3a9b1ec655e1
01/12/2025 13:29:59:INFO:Received: train message 038c8019-860f-4432-a729-3a9b1ec655e1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 13:41:04:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 14:02:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 14:02:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 39b5e30a-f66d-4d52-abe9-9e7d6df7b0c7
01/12/2025 14:02:52:INFO:Received: evaluate message 39b5e30a-f66d-4d52-abe9-9e7d6df7b0c7
[92mINFO [0m:      Sent reply
01/12/2025 14:07:45:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 14:10:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 14:10:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 567bebbe-4266-4830-89b1-8ade4687d563
01/12/2025 14:10:03:INFO:Received: train message 567bebbe-4266-4830-89b1-8ade4687d563
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 14:21:25:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 14:37:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 14:37:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 89a832a7-07c3-4ce8-9520-22a55c83c68d
01/12/2025 14:37:15:INFO:Received: evaluate message 89a832a7-07c3-4ce8-9520-22a55c83c68d
[92mINFO [0m:      Sent reply
01/12/2025 14:40:47:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 14:43:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 14:43:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 45b1085d-4062-4183-b617-c4b395ac8539
01/12/2025 14:43:45:INFO:Received: train message 45b1085d-4062-4183-b617-c4b395ac8539
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 14:56:24:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:08:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:08:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a1c8271f-e3f0-4c61-bfa2-62e7f4568d88
01/12/2025 15:08:20:INFO:Received: evaluate message a1c8271f-e3f0-4c61-bfa2-62e7f4568d88
[92mINFO [0m:      Sent reply
01/12/2025 15:13:30:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:14:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:14:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4740297e-e689-4b22-a3c9-25ebd5751b6b
01/12/2025 15:14:17:INFO:Received: train message 4740297e-e689-4b22-a3c9-25ebd5751b6b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 15:27:28:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:39:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:39:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4f19dbe8-300a-4fc2-8ec2-3fc87da8363a
01/12/2025 15:39:41:INFO:Received: evaluate message 4f19dbe8-300a-4fc2-8ec2-3fc87da8363a
[92mINFO [0m:      Sent reply
01/12/2025 15:44:41:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:45:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:45:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0ef9dc92-b760-45f8-92a4-62e1c4f36da9
01/12/2025 15:45:32:INFO:Received: train message 0ef9dc92-b760-45f8-92a4-62e1c4f36da9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 15:57:31:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:10:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:10:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 81ce7133-1585-4939-9714-d59d38d733c0
01/12/2025 16:10:45:INFO:Received: evaluate message 81ce7133-1585-4939-9714-d59d38d733c0
[92mINFO [0m:      Sent reply
01/12/2025 16:16:11:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:16:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:16:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b2b6fb3a-ef34-4a26-9199-6785a5aa6d05
01/12/2025 16:16:56:INFO:Received: train message b2b6fb3a-ef34-4a26-9199-6785a5aa6d05
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 16:29:33:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:40:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:40:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 744eeb58-2c5d-4dee-ba7f-a784992ff270
01/12/2025 16:40:34:INFO:Received: evaluate message 744eeb58-2c5d-4dee-ba7f-a784992ff270
[92mINFO [0m:      Sent reply
01/12/2025 16:46:14:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:47:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:47:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a590e400-289e-4578-a820-c17c459e262b
01/12/2025 16:47:01:INFO:Received: train message a590e400-289e-4578-a820-c17c459e262b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 17:01:42:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:12:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:12:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ccc17936-2a89-415d-bdd2-ff1458731eb5
01/12/2025 17:12:13:INFO:Received: evaluate message ccc17936-2a89-415d-bdd2-ff1458731eb5
[92mINFO [0m:      Sent reply
01/12/2025 17:17:25:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:18:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:18:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 964a7740-01bb-4671-9510-f3d6cea3c51e
01/12/2025 17:18:31:INFO:Received: train message 964a7740-01bb-4671-9510-f3d6cea3c51e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 17:33:08:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:44:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:44:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b26a9b89-4a50-4e76-b2fe-0d5fd36d4466
01/12/2025 17:44:38:INFO:Received: evaluate message b26a9b89-4a50-4e76-b2fe-0d5fd36d4466
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/new_dynamic_noise_clients_10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/new_dynamic_noise_clients_10']
Epoch 1 - Adjusted noise multipliers: [0.86669921875]
Epsilon = 1.76

{'loss': [141.91264295578003], 'accuracy': [0.342327829238824], 'auc': [0.5461435952188229]}

Epoch 2 - Adjusted noise multipliers: [0.8606328238674044]
Epsilon = 1.80

{'loss': [141.91264295578003, 134.78684961795807], 'accuracy': [0.342327829238824, 0.3411196133709223], 'auc': [0.5461435952188229, 0.5873603541092904]}

Epoch 3 - Adjusted noise multipliers: [0.8576155680254056]
Epsilon = 1.81

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998]}

Epoch 4 - Adjusted noise multipliers: [0.8546088902517346]
Epsilon = 1.83

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606]}

Epoch 5 - Adjusted noise multipliers: [0.8516127534611936]
Epsilon = 1.85

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792]}

Epoch 6 - Adjusted noise multipliers: [0.8486271206986005]
Epsilon = 1.87

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705]}

Epoch 7 - Adjusted noise multipliers: [0.8456519551383325]
Epsilon = 1.89

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158]}

Epoch 8 - Adjusted noise multipliers: [0.842687220083872]
Epsilon = 1.91

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806]}

Epoch 9 - Adjusted noise multipliers: [0.8397328789673547]
Epsilon = 1.93

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214]}

Epoch 10 - Adjusted noise multipliers: [0.8367888953491176]
Epsilon = 1.95

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554]}

Epoch 11 - Adjusted noise multipliers: [0.8338552329172499]
Epsilon = 1.97

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869]}

Epoch 12 - Adjusted noise multipliers: [0.8309318554871457]
Epsilon = 1.99

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278]}

Epoch 13 - Adjusted noise multipliers: [0.8280187270010564]
Epsilon = 2.01

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744]}

Epoch 14 - Adjusted noise multipliers: [0.8251158115276473]
Epsilon = 2.03
[92mINFO [0m:      Sent reply
01/12/2025 17:49:41:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:50:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:50:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 98cb4e7a-ef68-445b-be1b-3f65196af313
01/12/2025 17:50:21:INFO:Received: train message 98cb4e7a-ef68-445b-be1b-3f65196af313
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 18:02:44:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:16:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:16:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4a4c24b6-a500-48b2-ba6d-fb8badf81589
01/12/2025 18:16:14:INFO:Received: evaluate message 4a4c24b6-a500-48b2-ba6d-fb8badf81589
[92mINFO [0m:      Sent reply
01/12/2025 18:21:53:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:22:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:22:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 661d1828-0d6a-4ab7-b165-88fa9b6f4b88
01/12/2025 18:22:35:INFO:Received: train message 661d1828-0d6a-4ab7-b165-88fa9b6f4b88
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 18:35:24:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:46:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:46:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4c5a19b5-f4bf-4e38-ba01-8b2382756f22
01/12/2025 18:46:49:INFO:Received: evaluate message 4c5a19b5-f4bf-4e38-ba01-8b2382756f22
[92mINFO [0m:      Sent reply
01/12/2025 18:52:51:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:53:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:53:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 102d2280-e8d6-4b17-b235-f2bc21b207cc
01/12/2025 18:53:01:INFO:Received: train message 102d2280-e8d6-4b17-b235-f2bc21b207cc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 19:07:10:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:18:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:18:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 992c749b-9a16-4e8f-a234-b43f1592657a
01/12/2025 19:18:17:INFO:Received: evaluate message 992c749b-9a16-4e8f-a234-b43f1592657a
[92mINFO [0m:      Sent reply
01/12/2025 19:24:04:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:25:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:25:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bbd1f992-cd18-4f9e-aac5-d82d04d42ff9
01/12/2025 19:25:02:INFO:Received: train message bbd1f992-cd18-4f9e-aac5-d82d04d42ff9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 19:40:28:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:51:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:51:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 76907ef8-b952-4a0f-ab42-161027636b19
01/12/2025 19:51:43:INFO:Received: evaluate message 76907ef8-b952-4a0f-ab42-161027636b19
[92mINFO [0m:      Sent reply
01/12/2025 19:57:00:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:57:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:57:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a4203e19-7eac-4b3a-b4ec-59fee3036315
01/12/2025 19:57:47:INFO:Received: train message a4203e19-7eac-4b3a-b4ec-59fee3036315
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 20:13:01:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 20:24:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 20:24:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7006f3bc-781f-434e-a7f2-c23e127ab873
01/12/2025 20:24:29:INFO:Received: evaluate message 7006f3bc-781f-434e-a7f2-c23e127ab873
[92mINFO [0m:      Sent reply
01/12/2025 20:29:48:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 20:30:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 20:30:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 30ccdb8a-f293-451f-a70a-7c29ed0f6b5b
01/12/2025 20:30:31:INFO:Received: train message 30ccdb8a-f293-451f-a70a-7c29ed0f6b5b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 20:43:51:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 20:56:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 20:56:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b4b7b751-a1c4-41ec-be4d-699ffd2e7be8
01/12/2025 20:56:29:INFO:Received: evaluate message b4b7b751-a1c4-41ec-be4d-699ffd2e7be8
[92mINFO [0m:      Sent reply
01/12/2025 21:02:49:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 21:03:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 21:03:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6f81f802-915d-47ca-a3fc-4216583c1508
01/12/2025 21:03:27:INFO:Received: train message 6f81f802-915d-47ca-a3fc-4216583c1508
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 21:22:18:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 21:53:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 21:53:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 43b5c078-c4e4-452e-bc91-702d07c8a495
01/12/2025 21:53:21:INFO:Received: evaluate message 43b5c078-c4e4-452e-bc91-702d07c8a495

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799]}

Epoch 15 - Adjusted noise multipliers: [0.8222230732615536]
Epsilon = 2.05

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952]}

Epoch 16 - Adjusted noise multipliers: [0.8193404765229393]
Epsilon = 2.08

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585]}

Epoch 17 - Adjusted noise multipliers: [0.8164679857570563]
Epsilon = 2.10

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001]}

Epoch 18 - Adjusted noise multipliers: [0.8136055655338065]
Epsilon = 2.12

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081]}

Epoch 19 - Adjusted noise multipliers: [0.8107531805473052]
Epsilon = 2.14

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972]}

Epoch 20 - Adjusted noise multipliers: [0.8079107956154443]
Epsilon = 2.17

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029]}

Epoch 21 - Adjusted noise multipliers: [0.8050783756794597]
Epsilon = 2.19
[92mINFO [0m:      Sent reply
01/12/2025 22:00:31:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 22:01:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 22:01:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b1eaf3e2-3f08-4346-a27f-e9013ff1552f
01/12/2025 22:01:24:INFO:Received: train message b1eaf3e2-3f08-4346-a27f-e9013ff1552f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 22:20:24:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 22:52:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 22:52:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9ec6c0a4-d511-4005-b7e6-861c00232f59
01/12/2025 22:52:19:INFO:Received: evaluate message 9ec6c0a4-d511-4005-b7e6-861c00232f59
[92mINFO [0m:      Sent reply
01/12/2025 22:59:29:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 23:00:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 23:00:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 704e7599-2ab6-4667-8150-37acabe06c3a
01/12/2025 23:00:32:INFO:Received: train message 704e7599-2ab6-4667-8150-37acabe06c3a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 23:19:58:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 23:42:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 23:42:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2d695f40-18a5-4df2-9996-b6fe95bdf721
01/12/2025 23:42:01:INFO:Received: evaluate message 2d695f40-18a5-4df2-9996-b6fe95bdf721
[92mINFO [0m:      Sent reply
01/12/2025 23:48:02:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 23:48:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 23:48:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 53233c4c-3b89-42fd-94eb-8fdaf3c3e058
01/12/2025 23:48:44:INFO:Received: train message 53233c4c-3b89-42fd-94eb-8fdaf3c3e058
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 00:08:24:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 00:25:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 00:25:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cbaca8eb-3bd8-4656-800a-8dd886d612a9
01/13/2025 00:25:26:INFO:Received: evaluate message cbaca8eb-3bd8-4656-800a-8dd886d612a9
[92mINFO [0m:      Sent reply
01/13/2025 00:32:10:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 00:32:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 00:32:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b7de6581-3d51-47f3-981d-40e20d668d89
01/13/2025 00:32:55:INFO:Received: train message b7de6581-3d51-47f3-981d-40e20d668d89
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 00:51:11:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:02:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:02:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 16eb16dc-7ef5-443f-aa65-e3d59e0f39ff
01/13/2025 01:02:08:INFO:Received: evaluate message 16eb16dc-7ef5-443f-aa65-e3d59e0f39ff
[92mINFO [0m:      Sent reply
01/13/2025 01:08:30:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:09:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:09:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f63365ad-8d38-4567-9c1e-5879e4ba5809
01/13/2025 01:09:24:INFO:Received: train message f63365ad-8d38-4567-9c1e-5879e4ba5809
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 01:29:40:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:41:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:41:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d9005d12-5607-412a-91eb-d83064557ce4
01/13/2025 01:41:55:INFO:Received: evaluate message d9005d12-5607-412a-91eb-d83064557ce4

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546]}

Epoch 22 - Adjusted noise multipliers: [0.8022558858034984]
Epsilon = 2.21

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975]}

Epoch 23 - Adjusted noise multipliers: [0.7994432911741869]
Epsilon = 2.24

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261]}

Epoch 24 - Adjusted noise multipliers: [0.796640557100203]
Epsilon = 2.26

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982]}

Epoch 25 - Adjusted noise multipliers: [0.7938476490118471]
Epsilon = 2.28

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484]}

Epoch 26 - Adjusted noise multipliers: [0.791064532460616]
Epsilon = 2.31
[92mINFO [0m:      Sent reply
01/13/2025 01:46:30:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:47:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:47:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 337b8a70-190f-476d-985c-6bcef8ff8a52
01/13/2025 01:47:06:INFO:Received: train message 337b8a70-190f-476d-985c-6bcef8ff8a52
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 02:01:54:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:18:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:18:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 82933651-206e-43f7-8a65-6e37b82a7be7
01/13/2025 02:18:00:INFO:Received: evaluate message 82933651-206e-43f7-8a65-6e37b82a7be7
[92mINFO [0m:      Sent reply
01/13/2025 02:22:30:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:22:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:22:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2619d3c0-0ae0-4bb4-acaa-f0ecff46e37b
01/13/2025 02:22:53:INFO:Received: train message 2619d3c0-0ae0-4bb4-acaa-f0ecff46e37b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 02:36:33:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:48:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:48:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 939bad6b-9055-4694-8f4b-22bdde46db3c
01/13/2025 02:48:57:INFO:Received: evaluate message 939bad6b-9055-4694-8f4b-22bdde46db3c
[92mINFO [0m:      Sent reply
01/13/2025 02:53:03:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:53:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:53:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4c7192ad-c4be-4511-9278-5e2783b8355c
01/13/2025 02:53:26:INFO:Received: train message 4c7192ad-c4be-4511-9278-5e2783b8355c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 03:05:42:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:22:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:22:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f19a3b21-d9fd-4154-80d6-05d098ba412d
01/13/2025 03:22:03:INFO:Received: evaluate message f19a3b21-d9fd-4154-80d6-05d098ba412d
[92mINFO [0m:      Sent reply
01/13/2025 03:26:29:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:27:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:27:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cde3c809-9113-45e7-9cd2-602d67a48c3e
01/13/2025 03:27:33:INFO:Received: train message cde3c809-9113-45e7-9cd2-602d67a48c3e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 03:40:30:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:57:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:57:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 34dfef0f-89fd-4865-b572-97d98790c0a2
01/13/2025 03:57:54:INFO:Received: evaluate message 34dfef0f-89fd-4865-b572-97d98790c0a2

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573, 118.86477249860764], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516, 0.5275875956504229], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484, 0.7133914210720078]}

Epoch 27 - Adjusted noise multipliers: [0.7882911731187783]
Epsilon = 2.33

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573, 118.86477249860764, 118.52131652832031], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516, 0.5275875956504229, 0.527184857027789], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484, 0.7133914210720078, 0.7157697370039796]}

Epoch 28 - Adjusted noise multipliers: [0.7855275367789503]
Epsilon = 2.36

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573, 118.86477249860764, 118.52131652832031, 118.55394315719604], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516, 0.5275875956504229, 0.527184857027789, 0.5291985501409585], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484, 0.7133914210720078, 0.7157697370039796, 0.7175741294208162]}

Epoch 29 - Adjusted noise multipliers: [0.7827735893536746]
Epsilon = 2.38

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573, 118.86477249860764, 118.52131652832031, 118.55394315719604, 118.17346894741058], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516, 0.5275875956504229, 0.527184857027789, 0.5291985501409585, 0.5300040273862263], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484, 0.7133914210720078, 0.7157697370039796, 0.7175741294208162, 0.7199986636342428]}

Epoch 30 - Adjusted noise multipliers: [0.780029296875]
Epsilon = 2.41
[92mINFO [0m:      Sent reply
01/13/2025 04:02:33:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 04:02:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 04:02:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 2774b1fa-864c-4c7d-bca9-d3e659f69ee9
01/13/2025 04:02:36:INFO:Received: reconnect message 2774b1fa-864c-4c7d-bca9-d3e659f69ee9
01/13/2025 04:02:36:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/13/2025 04:02:36:INFO:Disconnect and shut down

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573, 118.86477249860764, 118.52131652832031, 118.55394315719604, 118.17346894741058, 117.99289846420288], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516, 0.5275875956504229, 0.527184857027789, 0.5291985501409585, 0.5300040273862263, 0.5316149818767619], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484, 0.7133914210720078, 0.7157697370039796, 0.7175741294208162, 0.7199986636342428, 0.7215043693989601]}



Final client history:
{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573, 118.86477249860764, 118.52131652832031, 118.55394315719604, 118.17346894741058, 117.99289846420288], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516, 0.5275875956504229, 0.527184857027789, 0.5291985501409585, 0.5300040273862263, 0.5316149818767619], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484, 0.7133914210720078, 0.7157697370039796, 0.7175741294208162, 0.7199986636342428, 0.7215043693989601]}

