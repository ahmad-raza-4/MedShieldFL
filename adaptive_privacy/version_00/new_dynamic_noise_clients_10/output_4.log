nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/new_dynamic_noise_clients_10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/12/2025 10:16:12:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/12/2025 10:16:12:DEBUG:ChannelConnectivity.IDLE
01/12/2025 10:16:13:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/12/2025 10:16:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:16:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5b051b2d-028d-4892-a0e4-a7cb856e28c8
01/12/2025 10:16:54:INFO:Received: train message 5b051b2d-028d-4892-a0e4-a7cb856e28c8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 10:26:27:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 10:38:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:38:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 59e4c290-c75a-47fd-914f-a1b9b7ba2ead
01/12/2025 10:38:30:INFO:Received: evaluate message 59e4c290-c75a-47fd-914f-a1b9b7ba2ead
[92mINFO [0m:      Sent reply
01/12/2025 10:43:08:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 10:43:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 10:43:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5d69799e-e3da-49b9-95f1-34e5ddc91655
01/12/2025 10:43:46:INFO:Received: train message 5d69799e-e3da-49b9-95f1-34e5ddc91655
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 10:53:15:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:05:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:05:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bb55f952-03b7-404e-a5d0-5c29938479f0
01/12/2025 11:05:42:INFO:Received: evaluate message bb55f952-03b7-404e-a5d0-5c29938479f0
[92mINFO [0m:      Sent reply
01/12/2025 11:10:26:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:11:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:11:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f2e509e3-ed4c-4208-bbb9-3cb090823d1b
01/12/2025 11:11:03:INFO:Received: train message f2e509e3-ed4c-4208-bbb9-3cb090823d1b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 11:21:13:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:38:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:38:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bbe6169c-fc04-4cbd-963c-64b6d3ff9076
01/12/2025 11:38:49:INFO:Received: evaluate message bbe6169c-fc04-4cbd-963c-64b6d3ff9076
[92mINFO [0m:      Sent reply
01/12/2025 11:43:41:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 11:44:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 11:44:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b66cc347-1f57-411b-84bd-e0b6284af805
01/12/2025 11:44:49:INFO:Received: train message b66cc347-1f57-411b-84bd-e0b6284af805
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 11:56:48:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:14:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:14:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 719dbf92-3370-434e-a318-cf1c8eaafd7a
01/12/2025 12:14:35:INFO:Received: evaluate message 719dbf92-3370-434e-a318-cf1c8eaafd7a
[92mINFO [0m:      Sent reply
01/12/2025 12:20:13:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:20:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:20:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message aeba8de4-3cc6-4fba-bf24-554078566f23
01/12/2025 12:20:51:INFO:Received: train message aeba8de4-3cc6-4fba-bf24-554078566f23
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 12:30:43:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:49:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:49:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8b647aef-5a77-4230-8332-3a4df3dd3c1d
01/12/2025 12:49:28:INFO:Received: evaluate message 8b647aef-5a77-4230-8332-3a4df3dd3c1d
[92mINFO [0m:      Sent reply
01/12/2025 12:55:15:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 12:56:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 12:56:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ca955f93-b06d-4ac8-8aa9-1f3ed02226ab
01/12/2025 12:56:07:INFO:Received: train message ca955f93-b06d-4ac8-8aa9-1f3ed02226ab
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 13:05:05:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 13:24:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 13:24:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 29d6064e-0629-40ff-a557-fc510e3f3a04
01/12/2025 13:24:09:INFO:Received: evaluate message 29d6064e-0629-40ff-a557-fc510e3f3a04
[92mINFO [0m:      Sent reply
01/12/2025 13:29:36:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 13:30:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 13:30:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f59158f9-4764-4abc-9913-556b11829cf2
01/12/2025 13:30:14:INFO:Received: train message f59158f9-4764-4abc-9913-556b11829cf2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 13:40:27:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 14:02:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 14:02:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 03dfd5cc-9466-49ad-9e7d-3383443ea12a
01/12/2025 14:02:54:INFO:Received: evaluate message 03dfd5cc-9466-49ad-9e7d-3383443ea12a
[92mINFO [0m:      Sent reply
01/12/2025 14:09:07:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 14:10:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 14:10:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 37edcd7c-43a0-4096-b57f-cb5784814f07
01/12/2025 14:10:08:INFO:Received: train message 37edcd7c-43a0-4096-b57f-cb5784814f07
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 14:20:39:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 14:37:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 14:37:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ac43a0a7-eeaa-468e-837a-e01a3e9a9491
01/12/2025 14:37:11:INFO:Received: evaluate message ac43a0a7-eeaa-468e-837a-e01a3e9a9491
[92mINFO [0m:      Sent reply
01/12/2025 14:43:01:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 14:43:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 14:43:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3caebdd5-0e65-473c-a5d7-5a8ba5735648
01/12/2025 14:43:18:INFO:Received: train message 3caebdd5-0e65-473c-a5d7-5a8ba5735648
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 14:51:51:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:08:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:08:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9df0f789-7ba5-458b-8648-be6d129726c0
01/12/2025 15:08:22:INFO:Received: evaluate message 9df0f789-7ba5-458b-8648-be6d129726c0
[92mINFO [0m:      Sent reply
01/12/2025 15:13:33:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:14:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:14:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c973880b-8cfd-4eb2-bbf1-bb4b7af0d234
01/12/2025 15:14:12:INFO:Received: train message c973880b-8cfd-4eb2-bbf1-bb4b7af0d234
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 15:24:28:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:39:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:39:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message af4aba79-8208-48b3-b513-85c0f296af83
01/12/2025 15:39:20:INFO:Received: evaluate message af4aba79-8208-48b3-b513-85c0f296af83
[92mINFO [0m:      Sent reply
01/12/2025 15:44:21:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 15:45:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 15:45:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b8f235fd-b822-4b53-93b9-bb62f1481e31
01/12/2025 15:45:20:INFO:Received: train message b8f235fd-b822-4b53-93b9-bb62f1481e31
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 15:54:31:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:10:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:10:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ae665025-2e2b-4d6b-bf76-40d550b97ad0
01/12/2025 16:10:36:INFO:Received: evaluate message ae665025-2e2b-4d6b-bf76-40d550b97ad0
[92mINFO [0m:      Sent reply
01/12/2025 16:16:07:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:16:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:16:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3acbd28b-160b-4acd-a7f1-d455dd50cd2a
01/12/2025 16:16:58:INFO:Received: train message 3acbd28b-160b-4acd-a7f1-d455dd50cd2a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 16:26:31:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:40:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:40:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 877e92d1-87a6-4f55-b3e7-6faebf76293a
01/12/2025 16:40:28:INFO:Received: evaluate message 877e92d1-87a6-4f55-b3e7-6faebf76293a
[92mINFO [0m:      Sent reply
01/12/2025 16:46:16:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 16:47:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 16:47:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd9e4a5e-5c33-4b28-869e-1ed23dfdd3ec
01/12/2025 16:47:04:INFO:Received: train message cd9e4a5e-5c33-4b28-869e-1ed23dfdd3ec
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 16:58:29:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:12:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:12:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4226dde2-d6e2-413c-9964-e3bb4a95ed61
01/12/2025 17:12:30:INFO:Received: evaluate message 4226dde2-d6e2-413c-9964-e3bb4a95ed61
[92mINFO [0m:      Sent reply
01/12/2025 17:17:36:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:18:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:18:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9d5274d8-2a50-4e40-93d4-e1cf6f630663
01/12/2025 17:18:33:INFO:Received: train message 9d5274d8-2a50-4e40-93d4-e1cf6f630663
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 17:29:40:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:44:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:44:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e2862c95-665e-41ed-817e-3b712545c2cf
01/12/2025 17:44:57:INFO:Received: evaluate message e2862c95-665e-41ed-817e-3b712545c2cf
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/new_dynamic_noise_clients_10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/new_dynamic_noise_clients_10']
Epoch 1 - Adjusted noise multipliers: [0.86669921875]
Epsilon = 2.22

{'loss': [141.91264295578003], 'accuracy': [0.342327829238824], 'auc': [0.5461435952188229]}

Epoch 2 - Adjusted noise multipliers: [0.8606328238674044]
Epsilon = 2.27

{'loss': [141.91264295578003, 134.78684961795807], 'accuracy': [0.342327829238824, 0.3411196133709223], 'auc': [0.5461435952188229, 0.5873603541092904]}

Epoch 3 - Adjusted noise multipliers: [0.8576155680254056]
Epsilon = 2.29

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998]}

Epoch 4 - Adjusted noise multipliers: [0.8546088902517346]
Epsilon = 2.31

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606]}

Epoch 5 - Adjusted noise multipliers: [0.8516127534611936]
Epsilon = 2.33

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792]}

Epoch 6 - Adjusted noise multipliers: [0.8486271206986005]
Epsilon = 2.35

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705]}

Epoch 7 - Adjusted noise multipliers: [0.8456519551383325]
Epsilon = 2.38

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158]}

Epoch 8 - Adjusted noise multipliers: [0.842687220083872]
Epsilon = 2.40

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806]}

Epoch 9 - Adjusted noise multipliers: [0.8397328789673547]
Epsilon = 2.42

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214]}

Epoch 10 - Adjusted noise multipliers: [0.8367888953491176]
Epsilon = 2.45

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554]}

Epoch 11 - Adjusted noise multipliers: [0.8338552329172499]
Epsilon = 2.47

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869]}

Epoch 12 - Adjusted noise multipliers: [0.8309318554871457]
Epsilon = 2.50

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278]}

Epoch 13 - Adjusted noise multipliers: [0.8280187270010564]
Epsilon = 2.52

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744]}

Epoch 14 - Adjusted noise multipliers: [0.8251158115276473]
Epsilon = 2.54
[92mINFO [0m:      Sent reply
01/12/2025 17:50:05:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 17:50:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 17:50:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fc93a0d5-89f3-4d62-87de-0cc55ae48e2d
01/12/2025 17:50:41:INFO:Received: train message fc93a0d5-89f3-4d62-87de-0cc55ae48e2d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 18:00:16:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:16:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:16:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1ad747f0-4cec-4a55-9681-088b48a981e0
01/12/2025 18:16:22:INFO:Received: evaluate message 1ad747f0-4cec-4a55-9681-088b48a981e0
[92mINFO [0m:      Sent reply
01/12/2025 18:22:05:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:22:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:22:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 734de527-8879-4b91-b4ce-645d6d3b88a9
01/12/2025 18:22:46:INFO:Received: train message 734de527-8879-4b91-b4ce-645d6d3b88a9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 18:32:06:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:46:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:46:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f34dbdcd-6198-49cd-b0b7-366d6a5bcdd9
01/12/2025 18:46:54:INFO:Received: evaluate message f34dbdcd-6198-49cd-b0b7-366d6a5bcdd9
[92mINFO [0m:      Sent reply
01/12/2025 18:52:53:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 18:53:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 18:53:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 10207fd7-842f-4502-a794-288d11bd3a30
01/12/2025 18:53:29:INFO:Received: train message 10207fd7-842f-4502-a794-288d11bd3a30
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 19:04:37:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:18:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:18:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5e99e0f3-d429-4bde-b7a9-442af3e74406
01/12/2025 19:18:39:INFO:Received: evaluate message 5e99e0f3-d429-4bde-b7a9-442af3e74406
[92mINFO [0m:      Sent reply
01/12/2025 19:24:44:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:25:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:25:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d2ae659d-4a3b-4938-b756-65e4699d665d
01/12/2025 19:25:24:INFO:Received: train message d2ae659d-4a3b-4938-b756-65e4699d665d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 19:37:11:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:51:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:51:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e6352df7-3471-452b-932b-70ab89d66ee5
01/12/2025 19:51:45:INFO:Received: evaluate message e6352df7-3471-452b-932b-70ab89d66ee5
[92mINFO [0m:      Sent reply
01/12/2025 19:56:59:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 19:57:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 19:57:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9fcf9135-d724-484f-9d11-b45b17932e66
01/12/2025 19:57:23:INFO:Received: train message 9fcf9135-d724-484f-9d11-b45b17932e66
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 20:09:19:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 20:24:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 20:24:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4ebcb8a9-7301-467a-b30c-24db017fa84d
01/12/2025 20:24:20:INFO:Received: evaluate message 4ebcb8a9-7301-467a-b30c-24db017fa84d
[92mINFO [0m:      Sent reply
01/12/2025 20:29:42:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 20:30:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 20:30:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d21999be-0d48-4aeb-afd3-075d777d36c3
01/12/2025 20:30:22:INFO:Received: train message d21999be-0d48-4aeb-afd3-075d777d36c3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 20:39:58:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 20:56:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 20:56:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4b81c723-6b90-4b26-bd6e-a6e496801a8b
01/12/2025 20:56:24:INFO:Received: evaluate message 4b81c723-6b90-4b26-bd6e-a6e496801a8b
[92mINFO [0m:      Sent reply
01/12/2025 21:02:44:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 21:03:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 21:03:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6e665d1f-63af-4ad4-82c6-568248e53f18
01/12/2025 21:03:27:INFO:Received: train message 6e665d1f-63af-4ad4-82c6-568248e53f18
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 21:17:40:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 21:53:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 21:53:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 187eab57-3556-4cba-b1fe-1e75d0c608a0
01/12/2025 21:53:40:INFO:Received: evaluate message 187eab57-3556-4cba-b1fe-1e75d0c608a0

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799]}

Epoch 15 - Adjusted noise multipliers: [0.8222230732615536]
Epsilon = 2.57

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952]}

Epoch 16 - Adjusted noise multipliers: [0.8193404765229393]
Epsilon = 2.59

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585]}

Epoch 17 - Adjusted noise multipliers: [0.8164679857570563]
Epsilon = 2.62

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001]}

Epoch 18 - Adjusted noise multipliers: [0.8136055655338065]
Epsilon = 2.65

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081]}

Epoch 19 - Adjusted noise multipliers: [0.8107531805473052]
Epsilon = 2.67

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972]}

Epoch 20 - Adjusted noise multipliers: [0.8079107956154443]
Epsilon = 2.70

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029]}

Epoch 21 - Adjusted noise multipliers: [0.8050783756794597]
Epsilon = 2.72
[92mINFO [0m:      Sent reply
01/12/2025 22:00:57:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 22:01:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 22:01:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a87538c3-28d1-4110-b63d-0a0be6f7dd71
01/12/2025 22:01:45:INFO:Received: train message a87538c3-28d1-4110-b63d-0a0be6f7dd71
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 22:16:00:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 22:52:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 22:52:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 17b55c68-4625-4277-81c5-384fb0de9d7e
01/12/2025 22:52:40:INFO:Received: evaluate message 17b55c68-4625-4277-81c5-384fb0de9d7e
[92mINFO [0m:      Sent reply
01/12/2025 22:59:54:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 23:00:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 23:00:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4efa9898-b077-44b2-a46b-6920f152d756
01/12/2025 23:00:46:INFO:Received: train message 4efa9898-b077-44b2-a46b-6920f152d756
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/12/2025 23:14:47:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 23:42:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 23:42:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b35adede-cce4-42ed-8b94-c240864df768
01/12/2025 23:42:00:INFO:Received: evaluate message b35adede-cce4-42ed-8b94-c240864df768
[92mINFO [0m:      Sent reply
01/12/2025 23:47:55:INFO:Sent reply
[92mINFO [0m:      
01/12/2025 23:48:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/12/2025 23:48:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 53864f88-b822-46c7-b5eb-65aebf7f84ec
01/12/2025 23:48:32:INFO:Received: train message 53864f88-b822-46c7-b5eb-65aebf7f84ec
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 00:00:25:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 00:25:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 00:25:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 55fede48-cb22-4924-a704-f6398ee03617
01/13/2025 00:25:13:INFO:Received: evaluate message 55fede48-cb22-4924-a704-f6398ee03617
[92mINFO [0m:      Sent reply
01/13/2025 00:32:01:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 00:32:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 00:32:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ca2e46c0-4f5f-40e5-ac1d-7148dd5270ad
01/13/2025 00:32:45:INFO:Received: train message ca2e46c0-4f5f-40e5-ac1d-7148dd5270ad
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 00:45:17:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:02:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:02:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c05a800b-c50b-4da3-ba1a-64e127dcb760
01/13/2025 01:02:14:INFO:Received: evaluate message c05a800b-c50b-4da3-ba1a-64e127dcb760
[92mINFO [0m:      Sent reply
01/13/2025 01:08:36:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:09:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:09:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fda99277-cccb-4dc1-8bae-bf7d036a724a
01/13/2025 01:09:07:INFO:Received: train message fda99277-cccb-4dc1-8bae-bf7d036a724a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 01:23:51:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:41:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:41:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9dc84ce9-f7b5-46d3-9a97-dfe68d1cee2f
01/13/2025 01:41:56:INFO:Received: evaluate message 9dc84ce9-f7b5-46d3-9a97-dfe68d1cee2f

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546]}

Epoch 22 - Adjusted noise multipliers: [0.8022558858034984]
Epsilon = 2.75

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975]}

Epoch 23 - Adjusted noise multipliers: [0.7994432911741869]
Epsilon = 2.78

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261]}

Epoch 24 - Adjusted noise multipliers: [0.796640557100203]
Epsilon = 2.81

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982]}

Epoch 25 - Adjusted noise multipliers: [0.7938476490118471]
Epsilon = 2.83

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484]}

Epoch 26 - Adjusted noise multipliers: [0.791064532460616]
Epsilon = 2.86
[92mINFO [0m:      Sent reply
01/13/2025 01:46:30:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 01:46:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 01:46:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 085103fd-8857-449e-9a71-32caca71d3ca
01/13/2025 01:46:45:INFO:Received: train message 085103fd-8857-449e-9a71-32caca71d3ca
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 01:56:54:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:17:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:17:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f675fef0-89f3-4876-8ba3-b5ef8d8cebc0
01/13/2025 02:17:41:INFO:Received: evaluate message f675fef0-89f3-4876-8ba3-b5ef8d8cebc0
[92mINFO [0m:      Sent reply
01/13/2025 02:22:09:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:22:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:22:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 40b0d835-b598-48b7-ae93-e18a1d3a702d
01/13/2025 02:22:57:INFO:Received: train message 40b0d835-b598-48b7-ae93-e18a1d3a702d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 02:32:29:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:48:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:48:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5cf6b10d-93ed-4bb5-90e8-2e5b48a80ae2
01/13/2025 02:48:46:INFO:Received: evaluate message 5cf6b10d-93ed-4bb5-90e8-2e5b48a80ae2
[92mINFO [0m:      Sent reply
01/13/2025 02:52:41:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 02:53:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 02:53:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 706839db-0fa2-4cce-b8cf-dbba31b2d734
01/13/2025 02:53:35:INFO:Received: train message 706839db-0fa2-4cce-b8cf-dbba31b2d734
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 03:02:42:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:22:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:22:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b6c5e180-9f10-4beb-adce-02d0da5a1f39
01/13/2025 03:22:13:INFO:Received: evaluate message b6c5e180-9f10-4beb-adce-02d0da5a1f39
[92mINFO [0m:      Sent reply
01/13/2025 03:26:45:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:27:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:27:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 93d4e0d1-340b-42fd-9bf0-e7f658b0ae41
01/13/2025 03:27:12:INFO:Received: train message 93d4e0d1-340b-42fd-9bf0-e7f658b0ae41
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: divide by zero encountered in log
  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/prv/prvs.py:55: RuntimeWarning: divide by zero encountered in log
  t > np.log(1 - q),
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/13/2025 03:36:14:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 03:57:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 03:57:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f2517795-258a-4d0d-b4e0-5154d38ebf79
01/13/2025 03:57:37:INFO:Received: evaluate message f2517795-258a-4d0d-b4e0-5154d38ebf79

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573, 118.86477249860764], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516, 0.5275875956504229], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484, 0.7133914210720078]}

Epoch 27 - Adjusted noise multipliers: [0.7882911731187783]
Epsilon = 2.89

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573, 118.86477249860764, 118.52131652832031], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516, 0.5275875956504229, 0.527184857027789], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484, 0.7133914210720078, 0.7157697370039796]}

Epoch 28 - Adjusted noise multipliers: [0.7855275367789503]
Epsilon = 2.92

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573, 118.86477249860764, 118.52131652832031, 118.55394315719604], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516, 0.5275875956504229, 0.527184857027789, 0.5291985501409585], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484, 0.7133914210720078, 0.7157697370039796, 0.7175741294208162]}

Epoch 29 - Adjusted noise multipliers: [0.7827735893536746]
Epsilon = 2.95

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573, 118.86477249860764, 118.52131652832031, 118.55394315719604, 118.17346894741058], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516, 0.5275875956504229, 0.527184857027789, 0.5291985501409585, 0.5300040273862263], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484, 0.7133914210720078, 0.7157697370039796, 0.7175741294208162, 0.7199986636342428]}

Epoch 30 - Adjusted noise multipliers: [0.780029296875]
Epsilon = 2.98
[92mINFO [0m:      Sent reply
01/13/2025 04:02:05:INFO:Sent reply
[92mINFO [0m:      
01/13/2025 04:02:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/13/2025 04:02:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 5a5af872-b5ed-4b2b-b3b0-65a7cb0144e0
01/13/2025 04:02:36:INFO:Received: reconnect message 5a5af872-b5ed-4b2b-b3b0-65a7cb0144e0
01/13/2025 04:02:36:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/13/2025 04:02:36:INFO:Disconnect and shut down

{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573, 118.86477249860764, 118.52131652832031, 118.55394315719604, 118.17346894741058, 117.99289846420288], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516, 0.5275875956504229, 0.527184857027789, 0.5291985501409585, 0.5300040273862263, 0.5316149818767619], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484, 0.7133914210720078, 0.7157697370039796, 0.7175741294208162, 0.7199986636342428, 0.7215043693989601]}



Final client history:
{'loss': [141.91264295578003, 134.78684961795807, 136.3272968530655, 138.98673737049103, 141.06381249427795, 141.38739907741547, 139.48290348052979, 137.21858823299408, 135.22687828540802, 133.1402566432953, 130.77949845790863, 129.43034946918488, 128.1956009864807, 126.9017585515976, 126.31660383939743, 125.66316282749176, 124.40782767534256, 123.39414149522781, 122.70182430744171, 121.53864914178848, 121.14356243610382, 120.90208548307419, 120.47297698259354, 120.0247134566307, 119.54902851581573, 118.86477249860764, 118.52131652832031, 118.55394315719604, 118.17346894741058, 117.99289846420288], 'accuracy': [0.342327829238824, 0.3411196133709223, 0.3419250906161901, 0.3427305678614579, 0.3463552154651631, 0.3532017720499396, 0.3685058397100282, 0.38219895287958117, 0.3995167136528393, 0.414418042690294, 0.43133306484091827, 0.44542891663310513, 0.45791381393475633, 0.46435763189689894, 0.4699959726137737, 0.4752315747080145, 0.4800644381796214, 0.4873137333870318, 0.49335481272654047, 0.5014095851792187, 0.50463149416029, 0.5094643576318969, 0.5130890052356021, 0.518324607329843, 0.5243656866693516, 0.5275875956504229, 0.527184857027789, 0.5291985501409585, 0.5300040273862263, 0.5316149818767619], 'auc': [0.5461435952188229, 0.5873603541092904, 0.6123719630478998, 0.6257330774990606, 0.6346371901220792, 0.6406570054229705, 0.6473310431516158, 0.6526018539566806, 0.6577509040881214, 0.6619974649977554, 0.6669782453096869, 0.6710981269396278, 0.6747010236986744, 0.6786860861263799, 0.6816387679273952, 0.6850096065058585, 0.6884339448672001, 0.6917552832288081, 0.694433850368972, 0.6981300521637029, 0.7005104953651546, 0.7029378608918975, 0.7054054742196261, 0.7079810066114982, 0.710588247362484, 0.7133914210720078, 0.7157697370039796, 0.7175741294208162, 0.7199986636342428, 0.7215043693989601]}

