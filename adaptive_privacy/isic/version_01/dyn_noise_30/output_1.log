nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_30/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/18/2025 11:08:39:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/18/2025 11:08:39:DEBUG:ChannelConnectivity.IDLE
01/18/2025 11:08:39:DEBUG:ChannelConnectivity.CONNECTING
01/18/2025 11:08:39:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/18/2025 11:09:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:09:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c3439ffb-bddc-4a79-bfb6-fdcfd13fde4c
01/18/2025 11:09:07:INFO:Received: train message c3439ffb-bddc-4a79-bfb6-fdcfd13fde4c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:36:34:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:37:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:37:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7980d27f-82e7-4f65-9116-c63a1123767b
01/18/2025 11:37:12:INFO:Received: evaluate message 7980d27f-82e7-4f65-9116-c63a1123767b
[92mINFO [0m:      Sent reply
01/18/2025 11:41:22:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:41:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:41:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message da562298-2ae5-4197-ae8e-6cc9c33657d4
01/18/2025 11:41:50:INFO:Received: train message da562298-2ae5-4197-ae8e-6cc9c33657d4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:05:24:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:05:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:05:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message af010dfa-7912-4f6a-a532-cd5833705268
01/18/2025 12:05:42:INFO:Received: evaluate message af010dfa-7912-4f6a-a532-cd5833705268
[92mINFO [0m:      Sent reply
01/18/2025 12:09:34:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:10:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:10:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f8ed452e-85fc-4d35-ac96-fa0889e3f495
01/18/2025 12:10:50:INFO:Received: train message f8ed452e-85fc-4d35-ac96-fa0889e3f495
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:32:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:33:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:33:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3e6442d8-7d63-4c7e-9550-8d1b2017d622
01/18/2025 12:33:06:INFO:Received: evaluate message 3e6442d8-7d63-4c7e-9550-8d1b2017d622
[92mINFO [0m:      Sent reply
01/18/2025 12:37:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:38:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:38:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 953eda2c-5464-42c2-95fa-864742193985
01/18/2025 12:38:44:INFO:Received: train message 953eda2c-5464-42c2-95fa-864742193985
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:01:37:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:02:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:02:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 51839bfe-ffe2-41ae-a7f0-b6cff363160d
01/18/2025 13:02:12:INFO:Received: evaluate message 51839bfe-ffe2-41ae-a7f0-b6cff363160d
[92mINFO [0m:      Sent reply
01/18/2025 13:06:52:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:07:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:07:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c98ff117-1af6-4e0c-b4b8-89713666892b
01/18/2025 13:07:18:INFO:Received: train message c98ff117-1af6-4e0c-b4b8-89713666892b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:30:49:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:31:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:31:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7a1d04fe-a8a0-4413-9a00-f54cf6065ba2
01/18/2025 13:31:21:INFO:Received: evaluate message 7a1d04fe-a8a0-4413-9a00-f54cf6065ba2
[92mINFO [0m:      Sent reply
01/18/2025 13:36:14:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:37:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:37:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6e85e49e-7762-4c15-a3bc-0679198eb5be
01/18/2025 13:37:00:INFO:Received: train message 6e85e49e-7762-4c15-a3bc-0679198eb5be
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:58:41:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:59:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:59:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e70c0a66-0a6b-4a45-ae12-f0987ae276d9
01/18/2025 13:59:19:INFO:Received: evaluate message e70c0a66-0a6b-4a45-ae12-f0987ae276d9
[92mINFO [0m:      Sent reply
01/18/2025 14:03:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:04:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:04:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd230ce9-a58f-48a1-a25e-8199c6de8487
01/18/2025 14:04:35:INFO:Received: train message cd230ce9-a58f-48a1-a25e-8199c6de8487
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:27:28:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:28:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:28:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5098d83f-cd94-49d8-bc0c-78737114a820
01/18/2025 14:28:07:INFO:Received: evaluate message 5098d83f-cd94-49d8-bc0c-78737114a820
[92mINFO [0m:      Sent reply
01/18/2025 14:32:57:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:33:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:33:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 96a26b32-cf7d-4c11-a8a8-10f29957c146
01/18/2025 14:33:43:INFO:Received: train message 96a26b32-cf7d-4c11-a8a8-10f29957c146
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:56:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:57:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:57:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5348882-53fa-48a8-ac36-9e1debb8bf02
01/18/2025 14:57:24:INFO:Received: evaluate message b5348882-53fa-48a8-ac36-9e1debb8bf02
[92mINFO [0m:      Sent reply
01/18/2025 15:02:13:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:02:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:02:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d9958975-7db6-49e3-b0ef-88a97986ced6
01/18/2025 15:02:51:INFO:Received: train message d9958975-7db6-49e3-b0ef-88a97986ced6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:27:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:28:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:28:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e24ed2d5-5586-4baa-aada-993d6a380b1e
01/18/2025 15:28:26:INFO:Received: evaluate message e24ed2d5-5586-4baa-aada-993d6a380b1e
[92mINFO [0m:      Sent reply
01/18/2025 15:33:12:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:33:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:33:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 26ee414c-9a4c-4188-891a-ffd381ab5c0e
01/18/2025 15:33:44:INFO:Received: train message 26ee414c-9a4c-4188-891a-ffd381ab5c0e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:01:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:02:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:02:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8ce5c3cf-37d0-4990-be72-a70ec45fb93a
01/18/2025 16:02:24:INFO:Received: evaluate message 8ce5c3cf-37d0-4990-be72-a70ec45fb93a
[92mINFO [0m:      Sent reply
01/18/2025 16:06:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:06:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:06:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cdde6855-11aa-4cb7-a70d-38b6082491ab
01/18/2025 16:06:49:INFO:Received: train message cdde6855-11aa-4cb7-a70d-38b6082491ab
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:33:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:34:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:34:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ed8f2093-438d-4e10-afe0-be3f1aec89aa
01/18/2025 16:34:18:INFO:Received: evaluate message ed8f2093-438d-4e10-afe0-be3f1aec89aa
[92mINFO [0m:      Sent reply
01/18/2025 16:38:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:39:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:39:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 06fa9915-4971-4fde-b51e-e3a042dfdbdc
01/18/2025 16:39:11:INFO:Received: train message 06fa9915-4971-4fde-b51e-e3a042dfdbdc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:04:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:04:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:04:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5d28906e-ca12-42fb-ba86-8034184f70b2
01/18/2025 17:04:59:INFO:Received: evaluate message 5d28906e-ca12-42fb-ba86-8034184f70b2
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_30', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_30']
BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535], 'accuracy': [0.3419250906161901], 'auc': [0.5460484530250033]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187], 'accuracy': [0.3419250906161901, 0.3403141361256545], 'auc': [0.5460484530250033, 0.5873079809432222]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00
[92mINFO [0m:      Sent reply
01/18/2025 17:09:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:09:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:09:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a054bf75-4c82-4605-846e-5439467b02bb
01/18/2025 17:09:45:INFO:Received: train message a054bf75-4c82-4605-846e-5439467b02bb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:33:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:34:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:34:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 651d54a1-001a-4aa3-a75e-5256a055bb5c
01/18/2025 17:34:30:INFO:Received: evaluate message 651d54a1-001a-4aa3-a75e-5256a055bb5c
[92mINFO [0m:      Sent reply
01/18/2025 17:38:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:39:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:39:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 03d9c4a8-8d52-4b5e-886d-3ee9e1829e38
01/18/2025 17:39:09:INFO:Received: train message 03d9c4a8-8d52-4b5e-886d-3ee9e1829e38
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:02:06:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:02:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:02:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 10053f95-d3e1-49a4-9b97-2c55827b9db1
01/18/2025 18:02:36:INFO:Received: evaluate message 10053f95-d3e1-49a4-9b97-2c55827b9db1
[92mINFO [0m:      Sent reply
01/18/2025 18:06:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:07:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:07:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a7214e40-e64a-4fa3-8cd4-b69f2c2a2c00
01/18/2025 18:07:59:INFO:Received: train message a7214e40-e64a-4fa3-8cd4-b69f2c2a2c00
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:29:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:30:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:30:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fa829a0a-d45e-4e7f-947e-eb69c22bc8d8
01/18/2025 18:30:19:INFO:Received: evaluate message fa829a0a-d45e-4e7f-947e-eb69c22bc8d8
[92mINFO [0m:      Sent reply
01/18/2025 18:34:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:35:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:35:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fc2e484a-e45f-49f1-b1d3-265cd7b70fe6
01/18/2025 18:35:08:INFO:Received: train message fc2e484a-e45f-49f1-b1d3-265cd7b70fe6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:56:48:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:57:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:57:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dc6d355c-6359-48e9-b1ab-18e62b11770c
01/18/2025 18:57:31:INFO:Received: evaluate message dc6d355c-6359-48e9-b1ab-18e62b11770c
[92mINFO [0m:      Sent reply
01/18/2025 19:01:40:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:02:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:02:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 194aecee-e9ed-4756-9787-b52c21f6bf06
01/18/2025 19:02:50:INFO:Received: train message 194aecee-e9ed-4756-9787-b52c21f6bf06
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:24:36:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:25:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:25:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 82745386-8634-4c0c-bb04-e69dc74e7fbb
01/18/2025 19:25:23:INFO:Received: evaluate message 82745386-8634-4c0c-bb04-e69dc74e7fbb
[92mINFO [0m:      Sent reply
01/18/2025 19:29:23:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:30:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:30:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd0892a2-fa66-4f5b-9e6d-0946863b50ba
01/18/2025 19:30:49:INFO:Received: train message cd0892a2-fa66-4f5b-9e6d-0946863b50ba
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:52:40:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:53:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:53:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 34513e3f-0c66-4ea1-8b5c-772b1f64dcab
01/18/2025 19:53:27:INFO:Received: evaluate message 34513e3f-0c66-4ea1-8b5c-772b1f64dcab
[92mINFO [0m:      Sent reply
01/18/2025 19:57:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:58:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:58:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4d71f8ed-2ace-41c5-afd0-006343bdd432
01/18/2025 19:58:59:INFO:Received: train message 4d71f8ed-2ace-41c5-afd0-006343bdd432

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.30809821374714375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.2875583328306675
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.26701845191419127
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.246478570997715
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031]}

/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:20:28:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:21:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:21:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message efb252b2-2228-4fc7-a8c1-a17a2119d97e
01/18/2025 20:21:17:INFO:Received: evaluate message efb252b2-2228-4fc7-a8c1-a17a2119d97e
[92mINFO [0m:      Sent reply
01/18/2025 20:25:46:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:26:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:26:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ace57c4f-e444-49c9-a692-54d73ca41290
01/18/2025 20:26:27:INFO:Received: train message ace57c4f-e444-49c9-a692-54d73ca41290
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:45:04:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:45:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:45:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f3726050-8cab-449f-829a-e79f4e4ef4d3
01/18/2025 20:45:38:INFO:Received: evaluate message f3726050-8cab-449f-829a-e79f4e4ef4d3
[92mINFO [0m:      Sent reply
01/18/2025 20:49:31:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:50:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:50:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f7ff34ce-6fc3-4996-b4fd-f1cf623b0d35
01/18/2025 20:50:06:INFO:Received: train message f7ff34ce-6fc3-4996-b4fd-f1cf623b0d35
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 21:08:48:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:09:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:09:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cee0b5a9-d5e9-4416-9f1e-bf5a07028257
01/18/2025 21:09:12:INFO:Received: evaluate message cee0b5a9-d5e9-4416-9f1e-bf5a07028257
[92mINFO [0m:      Sent reply
01/18/2025 21:13:00:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:13:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:13:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b112d1db-837d-4468-ae81-ff5b9a71b5ad
01/18/2025 21:13:50:INFO:Received: train message b112d1db-837d-4468-ae81-ff5b9a71b5ad
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 21:32:40:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:33:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:33:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message babd9a16-c996-4249-97df-10530aa9a560
01/18/2025 21:33:09:INFO:Received: evaluate message babd9a16-c996-4249-97df-10530aa9a560
[92mINFO [0m:      Sent reply
01/18/2025 21:36:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:37:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:37:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0e8b0fa8-3222-401d-8ef6-2c97726a4dcb
01/18/2025 21:37:58:INFO:Received: train message 0e8b0fa8-3222-401d-8ef6-2c97726a4dcb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 21:56:28:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:57:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:57:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8f2a646a-1a92-47bc-a800-7e48b6407088
01/18/2025 21:57:03:INFO:Received: evaluate message 8f2a646a-1a92-47bc-a800-7e48b6407088
[92mINFO [0m:      Sent reply
01/18/2025 22:01:04:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:01:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:01:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dba793a9-9d88-4e51-9787-beb7de75adc0
01/18/2025 22:01:18:INFO:Received: train message dba793a9-9d88-4e51-9787-beb7de75adc0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 22:20:14:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:20:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:20:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7aa3f533-cfef-41d5-b3ca-521822548aa3
01/18/2025 22:20:47:INFO:Received: evaluate message 7aa3f533-cfef-41d5-b3ca-521822548aa3
BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.22593869008123876
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.20539880916476252
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.18485892824828623
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.16431904733181
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.14377916641533375
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146]}

BaseNM 0.41748046875
noise multiplier 0.30809821374714375
Noise multiplier before  adjustment: 0.30809821374714375
Noise multiplier before convergence adjustment: 0.30809821374714375
Updated noise multiplier after convergence adjustment: 0.1232392854988575
Epsilon = 30.00
[92mINFO [0m:      Sent reply
01/18/2025 22:24:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:25:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:25:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bf7ad2b4-0c2f-416c-9c77-a6de891cd7dc
01/18/2025 22:25:31:INFO:Received: train message bf7ad2b4-0c2f-416c-9c77-a6de891cd7dc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 22:44:13:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:44:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:44:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ae3b7e72-ca1d-48cc-8870-f7852dfece93
01/18/2025 22:44:43:INFO:Received: evaluate message ae3b7e72-ca1d-48cc-8870-f7852dfece93
[92mINFO [0m:      Sent reply
01/18/2025 22:48:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:49:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:49:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 17f16c30-16ba-460a-8b7b-58da9118bfc7
01/18/2025 22:49:09:INFO:Received: train message 17f16c30-16ba-460a-8b7b-58da9118bfc7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 23:08:14:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:08:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:08:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cbcf2b82-e47c-4da8-832e-94133b606ef0
01/18/2025 23:08:36:INFO:Received: evaluate message cbcf2b82-e47c-4da8-832e-94133b606ef0
[92mINFO [0m:      Sent reply
01/18/2025 23:12:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:13:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:13:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6ab94d7e-57a4-4d28-ad0a-d073f440447b
01/18/2025 23:13:21:INFO:Received: train message 6ab94d7e-57a4-4d28-ad0a-d073f440447b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 23:32:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:32:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:32:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3f6883c7-6f37-4d11-87f3-09dedcd91468
01/18/2025 23:32:27:INFO:Received: evaluate message 3f6883c7-6f37-4d11-87f3-09dedcd91468
[92mINFO [0m:      Sent reply
01/18/2025 23:35:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:37:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:37:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2b200920-77fc-4fb3-b751-4de9b82845bf
01/18/2025 23:37:06:INFO:Received: train message 2b200920-77fc-4fb3-b751-4de9b82845bf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 23:55:51:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:56:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:56:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5aa05138-02f9-4d7e-b11e-901b9d7bafa3
01/18/2025 23:56:17:INFO:Received: evaluate message 5aa05138-02f9-4d7e-b11e-901b9d7bafa3
