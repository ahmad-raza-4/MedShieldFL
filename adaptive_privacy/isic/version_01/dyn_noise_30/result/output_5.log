nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_30/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/18/2025 10:59:42:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/18/2025 10:59:42:DEBUG:ChannelConnectivity.IDLE
01/18/2025 10:59:43:DEBUG:ChannelConnectivity.CONNECTING
01/18/2025 10:59:43:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/18/2025 11:09:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:09:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 77bc02a0-df85-424e-b7f4-c6659c7f2548
01/18/2025 11:09:10:INFO:Received: train message 77bc02a0-df85-424e-b7f4-c6659c7f2548
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:13:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:37:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:37:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 690da1ff-7056-48a8-b751-fc9a7eebee4d
01/18/2025 11:37:09:INFO:Received: evaluate message 690da1ff-7056-48a8-b751-fc9a7eebee4d
[92mINFO [0m:      Sent reply
01/18/2025 11:41:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:41:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:41:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2e85913f-deca-4c9c-820a-dbf93d851ed9
01/18/2025 11:41:49:INFO:Received: train message 2e85913f-deca-4c9c-820a-dbf93d851ed9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:46:07:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:05:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:05:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6dbc2d60-ea5c-4bbe-b1d4-f69e18bd2866
01/18/2025 12:05:50:INFO:Received: evaluate message 6dbc2d60-ea5c-4bbe-b1d4-f69e18bd2866
[92mINFO [0m:      Sent reply
01/18/2025 12:09:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:10:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:10:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4bfac939-214a-40de-8150-b66e5dc4c4e8
01/18/2025 12:10:45:INFO:Received: train message 4bfac939-214a-40de-8150-b66e5dc4c4e8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:15:19:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:33:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:33:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3b2b98f3-a31f-4f3f-89e3-3ab1b4e2fc52
01/18/2025 12:33:19:INFO:Received: evaluate message 3b2b98f3-a31f-4f3f-89e3-3ab1b4e2fc52
[92mINFO [0m:      Sent reply
01/18/2025 12:38:08:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:38:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:38:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7da23b96-63b7-4852-b31d-a895a1a3a69f
01/18/2025 12:38:39:INFO:Received: train message 7da23b96-63b7-4852-b31d-a895a1a3a69f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:42:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:02:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:02:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 632c3afe-7710-4295-9938-a9ab37615270
01/18/2025 13:02:13:INFO:Received: evaluate message 632c3afe-7710-4295-9938-a9ab37615270
[92mINFO [0m:      Sent reply
01/18/2025 13:06:26:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:07:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:07:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fb921732-0ed2-4ae2-a4b4-57b3797a26b1
01/18/2025 13:07:15:INFO:Received: train message fb921732-0ed2-4ae2-a4b4-57b3797a26b1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:11:32:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:31:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:31:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b2d06dab-d321-4a9d-b78d-a62ab842ca86
01/18/2025 13:31:24:INFO:Received: evaluate message b2d06dab-d321-4a9d-b78d-a62ab842ca86
[92mINFO [0m:      Sent reply
01/18/2025 13:36:11:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:36:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:36:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f5bec08c-e9bf-4524-bd4c-99e92e364d02
01/18/2025 13:36:52:INFO:Received: train message f5bec08c-e9bf-4524-bd4c-99e92e364d02
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:41:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:59:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:59:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ebab31d0-4cde-4338-bcb2-3a761cc10b67
01/18/2025 13:59:13:INFO:Received: evaluate message ebab31d0-4cde-4338-bcb2-3a761cc10b67
[92mINFO [0m:      Sent reply
01/18/2025 14:03:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:04:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:04:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1c7d05ee-c5cb-444c-bac5-e650040ddad5
01/18/2025 14:04:22:INFO:Received: train message 1c7d05ee-c5cb-444c-bac5-e650040ddad5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:08:22:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:28:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:28:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c58f7f22-1893-4f06-9af0-85a26ed7c4b1
01/18/2025 14:28:07:INFO:Received: evaluate message c58f7f22-1893-4f06-9af0-85a26ed7c4b1
[92mINFO [0m:      Sent reply
01/18/2025 14:32:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:33:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:33:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 321ad034-f2bf-46d3-8cac-e38df914d1af
01/18/2025 14:33:23:INFO:Received: train message 321ad034-f2bf-46d3-8cac-e38df914d1af
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:37:25:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:57:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:57:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2ca148e3-5f4b-4069-8932-0341eacb719c
01/18/2025 14:57:21:INFO:Received: evaluate message 2ca148e3-5f4b-4069-8932-0341eacb719c
[92mINFO [0m:      Sent reply
01/18/2025 15:01:56:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:02:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:02:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1a71f8ba-494a-4211-a85c-642d27c785bd
01/18/2025 15:02:32:INFO:Received: train message 1a71f8ba-494a-4211-a85c-642d27c785bd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:06:37:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:28:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:28:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6809c46f-1316-45f2-9222-373aadfad483
01/18/2025 15:28:30:INFO:Received: evaluate message 6809c46f-1316-45f2-9222-373aadfad483
[92mINFO [0m:      Sent reply
01/18/2025 15:33:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:33:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:33:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c2510011-4a32-4daa-8cae-5422524decfd
01/18/2025 15:33:47:INFO:Received: train message c2510011-4a32-4daa-8cae-5422524decfd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:38:08:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:02:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:02:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 09b0a317-53b3-4d8c-b2df-9f04abda3726
01/18/2025 16:02:08:INFO:Received: evaluate message 09b0a317-53b3-4d8c-b2df-9f04abda3726
[92mINFO [0m:      Sent reply
01/18/2025 16:06:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:07:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:07:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message db1f423d-4d0e-4653-badf-8e5655a7d6b0
01/18/2025 16:07:00:INFO:Received: train message db1f423d-4d0e-4653-badf-8e5655a7d6b0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:11:16:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:34:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:34:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 680aa0ed-236e-493a-924c-427ef26d2895
01/18/2025 16:34:26:INFO:Received: evaluate message 680aa0ed-236e-493a-924c-427ef26d2895
[92mINFO [0m:      Sent reply
01/18/2025 16:38:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:39:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:39:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 492971a4-a706-45e5-801f-6627bf97c476
01/18/2025 16:39:01:INFO:Received: train message 492971a4-a706-45e5-801f-6627bf97c476
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:43:12:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:05:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:05:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7d75da96-a6b5-498f-bc38-ba2dc6a986d9
01/18/2025 17:05:00:INFO:Received: evaluate message 7d75da96-a6b5-498f-bc38-ba2dc6a986d9
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_30', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_30']
BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535], 'accuracy': [0.3419250906161901], 'auc': [0.5460484530250033]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187], 'accuracy': [0.3419250906161901, 0.3403141361256545], 'auc': [0.5460484530250033, 0.5873079809432222]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00
[92mINFO [0m:      Sent reply
01/18/2025 17:09:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:09:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:09:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1fccd3a3-5dc7-41a6-963c-3b2b87b29acc
01/18/2025 17:09:52:INFO:Received: train message 1fccd3a3-5dc7-41a6-963c-3b2b87b29acc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:14:11:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:34:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:34:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7550a285-33ae-4688-8639-464261b32827
01/18/2025 17:34:27:INFO:Received: evaluate message 7550a285-33ae-4688-8639-464261b32827
[92mINFO [0m:      Sent reply
01/18/2025 17:38:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:39:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:39:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message aea6ba18-cae3-4808-a905-0d271e5cedf5
01/18/2025 17:39:38:INFO:Received: train message aea6ba18-cae3-4808-a905-0d271e5cedf5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:44:06:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:02:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:02:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 47b2ee02-4617-4174-a900-9cbeb6e4b6db
01/18/2025 18:02:49:INFO:Received: evaluate message 47b2ee02-4617-4174-a900-9cbeb6e4b6db
[92mINFO [0m:      Sent reply
01/18/2025 18:07:16:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:07:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:07:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 928e9c9b-e008-4a85-95e4-c59668b84264
01/18/2025 18:07:32:INFO:Received: train message 928e9c9b-e008-4a85-95e4-c59668b84264
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:12:02:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:30:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:30:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b9326762-08e7-435d-aed2-1114e373b766
01/18/2025 18:30:12:INFO:Received: evaluate message b9326762-08e7-435d-aed2-1114e373b766
[92mINFO [0m:      Sent reply
01/18/2025 18:34:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:35:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:35:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3db2310c-bef0-4894-8737-0399c46157d0
01/18/2025 18:35:06:INFO:Received: train message 3db2310c-bef0-4894-8737-0399c46157d0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:39:24:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:57:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:57:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b3bef8c6-f1f7-46d0-a3cd-ee695659863c
01/18/2025 18:57:16:INFO:Received: evaluate message b3bef8c6-f1f7-46d0-a3cd-ee695659863c
[92mINFO [0m:      Sent reply
01/18/2025 19:01:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:02:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:02:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b879fcd8-a534-4bd0-b4c1-a460c359794a
01/18/2025 19:02:41:INFO:Received: train message b879fcd8-a534-4bd0-b4c1-a460c359794a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:07:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:25:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:25:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e24ac3d8-a9de-498f-91c0-8d0f4ce39b52
01/18/2025 19:25:18:INFO:Received: evaluate message e24ac3d8-a9de-498f-91c0-8d0f4ce39b52
[92mINFO [0m:      Sent reply
01/18/2025 19:30:06:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:30:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:30:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f395674e-1468-40c9-9796-6114bdc3f8e1
01/18/2025 19:30:22:INFO:Received: train message f395674e-1468-40c9-9796-6114bdc3f8e1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:34:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:53:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:53:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b8eb6ba5-7043-49dc-bda9-77f7b0cdf730
01/18/2025 19:53:22:INFO:Received: evaluate message b8eb6ba5-7043-49dc-bda9-77f7b0cdf730
[92mINFO [0m:      Sent reply
01/18/2025 19:58:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:58:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:58:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d6ee1c3f-6892-454f-ad21-ea06d3148925
01/18/2025 19:58:52:INFO:Received: train message d6ee1c3f-6892-454f-ad21-ea06d3148925

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.3054885065648705
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.28512260612721246
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.2647567056895544
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.24439080525189638
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031]}

BaseNM 0.41748046875
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:03:24:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:21:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:21:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b88337d7-284f-4601-851f-61f794cc7640
01/18/2025 20:21:13:INFO:Received: evaluate message b88337d7-284f-4601-851f-61f794cc7640
[92mINFO [0m:      Sent reply
01/18/2025 20:25:52:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:26:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:26:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eaf01e49-ea20-4f19-8e63-940306e44cbf
01/18/2025 20:26:03:INFO:Received: train message eaf01e49-ea20-4f19-8e63-940306e44cbf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:28:43:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:45:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:45:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 841efdf9-771b-42a8-8f40-63bdcd84a73f
01/18/2025 20:45:34:INFO:Received: evaluate message 841efdf9-771b-42a8-8f40-63bdcd84a73f
[92mINFO [0m:      Sent reply
01/18/2025 20:49:29:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:49:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:49:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 33da86a2-c2ef-4a01-8c63-1e6f401519cf
01/18/2025 20:49:54:INFO:Received: train message 33da86a2-c2ef-4a01-8c63-1e6f401519cf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:53:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:09:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:09:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d3e17fdd-d614-4c0a-bb80-17fc73a3d9c6
01/18/2025 21:09:15:INFO:Received: evaluate message d3e17fdd-d614-4c0a-bb80-17fc73a3d9c6
[92mINFO [0m:      Sent reply
01/18/2025 21:13:22:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:13:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:13:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 976af8fc-7c8b-442b-81d7-5e19efd6a470
01/18/2025 21:13:56:INFO:Received: train message 976af8fc-7c8b-442b-81d7-5e19efd6a470
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 21:18:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:33:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:33:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ae5fa476-9145-4ab0-8a7e-8412537fe0a8
01/18/2025 21:33:15:INFO:Received: evaluate message ae5fa476-9145-4ab0-8a7e-8412537fe0a8
[92mINFO [0m:      Sent reply
01/18/2025 21:37:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:37:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:37:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 862babd0-4c0e-40fe-852b-6f1f1ff3dd4d
01/18/2025 21:37:39:INFO:Received: train message 862babd0-4c0e-40fe-852b-6f1f1ff3dd4d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 21:40:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:56:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:56:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 43b26dec-fa86-4ff8-85e1-10ff95131722
01/18/2025 21:56:59:INFO:Received: evaluate message 43b26dec-fa86-4ff8-85e1-10ff95131722
[92mINFO [0m:      Sent reply
01/18/2025 22:01:03:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:01:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:01:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f0e31c2b-95e0-4128-838b-d8d4ef1c2119
01/18/2025 22:01:37:INFO:Received: train message f0e31c2b-95e0-4128-838b-d8d4ef1c2119
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 22:05:42:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:20:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:20:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0d4fc56c-a38e-457f-a413-33cea79d6293
01/18/2025 22:20:41:INFO:Received: evaluate message 0d4fc56c-a38e-457f-a413-33cea79d6293
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.22402490481423837
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.20365900437658033
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.18329310393892229
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.16292720350126425
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.14256130306360623
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.12219540262594819
Epsilon = 30.00
[92mINFO [0m:      Sent reply
01/18/2025 22:24:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:25:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:25:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f1de28c1-01a8-4eca-b579-5edbb7173aae
01/18/2025 22:25:11:INFO:Received: train message f1de28c1-01a8-4eca-b579-5edbb7173aae
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 22:28:37:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:44:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:44:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message de3d7c01-6420-416a-bb14-d9725a134ff6
01/18/2025 22:44:31:INFO:Received: evaluate message de3d7c01-6420-416a-bb14-d9725a134ff6
[92mINFO [0m:      Sent reply
01/18/2025 22:47:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:49:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:49:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0bd8d959-7ab1-4ec3-b407-97fef2396105
01/18/2025 22:49:20:INFO:Received: train message 0bd8d959-7ab1-4ec3-b407-97fef2396105
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 22:53:25:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:08:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:08:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d64337b7-bdaa-4121-81f2-8003dfa70ca6
01/18/2025 23:08:48:INFO:Received: evaluate message d64337b7-bdaa-4121-81f2-8003dfa70ca6
[92mINFO [0m:      Sent reply
01/18/2025 23:12:56:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:13:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:13:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 313ddee3-601a-4d38-bc4f-d1ff1a185c2b
01/18/2025 23:13:07:INFO:Received: train message 313ddee3-601a-4d38-bc4f-d1ff1a185c2b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 23:16:01:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:32:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:32:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e8a17916-16e1-4f70-9783-25b36054df2d
01/18/2025 23:32:40:INFO:Received: evaluate message e8a17916-16e1-4f70-9783-25b36054df2d
[92mINFO [0m:      Sent reply
01/18/2025 23:36:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:37:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:37:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message aff59a32-c26c-4afc-b63d-391b2de3e509
01/18/2025 23:37:16:INFO:Received: train message aff59a32-c26c-4afc-b63d-391b2de3e509
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 23:41:12:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:56:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:56:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4afb30b9-e868-4133-b61b-d9482c8a24fc
01/18/2025 23:56:10:INFO:Received: evaluate message 4afb30b9-e868-4133-b61b-d9482c8a24fc

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.10182950218829016
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.08146360175063214
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.06109770131297408
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.040731800875316054
Epsilon = 30.00
[92mINFO [0m:      Sent reply
01/19/2025 00:00:02:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:00:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:00:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c79c38ed-832e-4974-a46c-6428d8a9579d
01/19/2025 00:00:52:INFO:Received: train message c79c38ed-832e-4974-a46c-6428d8a9579d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/19/2025 00:05:01:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:19:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:19:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 94c71d48-4dab-47f1-918d-22a21dee4eb0
01/19/2025 00:19:53:INFO:Received: evaluate message 94c71d48-4dab-47f1-918d-22a21dee4eb0
[92mINFO [0m:      Sent reply
01/19/2025 00:24:02:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:24:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:24:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c4e412df-5b3a-4dd6-b816-15152db2dba2
01/19/2025 00:24:26:INFO:Received: train message c4e412df-5b3a-4dd6-b816-15152db2dba2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/19/2025 00:28:27:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:43:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:43:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c3ebbb11-2082-47c1-96a8-2f8907ffa71c
01/19/2025 00:43:40:INFO:Received: evaluate message c3ebbb11-2082-47c1-96a8-2f8907ffa71c
[92mINFO [0m:      Sent reply
01/19/2025 00:47:38:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:47:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:47:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message fd3fb696-beca-4da5-b182-670387ce410d
01/19/2025 00:47:39:INFO:Received: reconnect message fd3fb696-beca-4da5-b182-670387ce410d
01/19/2025 00:47:39:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/19/2025 00:47:39:INFO:Disconnect and shut down

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966, 117.42482489347458], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925, 0.5296012887635925], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297, 0.7193555349179758]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.020365900437658027
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966, 117.42482489347458, 117.25908535718918], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925, 0.5296012887635925, 0.5328231977446637], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297, 0.7193555349179758, 0.7212846690688237]}

BaseNM 0.41748046875
noise multiplier 0.3054885065648705
Noise multiplier before  adjustment: 0.3054885065648705
Noise multiplier before convergence adjustment: 0.3054885065648705
Updated noise multiplier after convergence adjustment: 0.0
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966, 117.42482489347458, 117.25908535718918, 117.31337916851044], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925, 0.5296012887635925, 0.5328231977446637, 0.5332259363672976], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297, 0.7193555349179758, 0.7212846690688237, 0.7229776848061777]}



Final client history:
{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966, 117.42482489347458, 117.25908535718918, 117.31337916851044], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925, 0.5296012887635925, 0.5328231977446637, 0.5332259363672976], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297, 0.7193555349179758, 0.7212846690688237, 0.7229776848061777]}

