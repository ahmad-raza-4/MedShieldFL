nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_1/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/18/2025 06:28:40:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/18/2025 06:28:40:DEBUG:ChannelConnectivity.IDLE
01/18/2025 06:28:40:DEBUG:ChannelConnectivity.CONNECTING
01/18/2025 06:28:40:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/18/2025 06:34:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:34:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 149351d2-e82e-4cdc-a42b-4940d1825cbc
01/18/2025 06:34:11:INFO:Received: train message 149351d2-e82e-4cdc-a42b-4940d1825cbc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 06:38:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:53:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:53:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 32747a6e-cc8b-4a47-ad01-8c8195214b29
01/18/2025 06:53:23:INFO:Received: evaluate message 32747a6e-cc8b-4a47-ad01-8c8195214b29
[92mINFO [0m:      Sent reply
01/18/2025 06:57:28:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:57:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:57:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 79e1f7a6-1ebf-418a-949c-57515acb0451
01/18/2025 06:57:53:INFO:Received: train message 79e1f7a6-1ebf-418a-949c-57515acb0451
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:01:44:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:17:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:17:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 890ba1bd-8413-449a-958a-9ac5e5b644f1
01/18/2025 07:17:06:INFO:Received: evaluate message 890ba1bd-8413-449a-958a-9ac5e5b644f1
[92mINFO [0m:      Sent reply
01/18/2025 07:21:07:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:21:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:21:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 672b7651-328e-4152-ac97-182201e2331a
01/18/2025 07:21:27:INFO:Received: train message 672b7651-328e-4152-ac97-182201e2331a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:25:14:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:40:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:40:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fd526570-a658-4925-a9ea-33f6166beb4d
01/18/2025 07:40:11:INFO:Received: evaluate message fd526570-a658-4925-a9ea-33f6166beb4d
[92mINFO [0m:      Sent reply
01/18/2025 07:43:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:44:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:44:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 94c9492f-9f53-4fb6-93c3-8cceea3eae30
01/18/2025 07:44:53:INFO:Received: train message 94c9492f-9f53-4fb6-93c3-8cceea3eae30
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:48:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:03:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:03:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9f9f7e75-7e85-42cb-957f-baa851d86ff3
01/18/2025 08:03:33:INFO:Received: evaluate message 9f9f7e75-7e85-42cb-957f-baa851d86ff3
[92mINFO [0m:      Sent reply
01/18/2025 08:07:29:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:07:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:07:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 147f5996-31ec-417e-af65-705c41d10b2a
01/18/2025 08:07:59:INFO:Received: train message 147f5996-31ec-417e-af65-705c41d10b2a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:11:43:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:30:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:30:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d16325c2-647a-455d-ac8d-80b11d7849f7
01/18/2025 08:30:46:INFO:Received: evaluate message d16325c2-647a-455d-ac8d-80b11d7849f7
[92mINFO [0m:      Sent reply
01/18/2025 08:34:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:35:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:35:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 05d17772-b529-4893-b48d-b8f41a43abfc
01/18/2025 08:35:41:INFO:Received: train message 05d17772-b529-4893-b48d-b8f41a43abfc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:39:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:58:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:58:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 51a629cd-9431-4e55-be2e-b726fc8779c8
01/18/2025 08:58:32:INFO:Received: evaluate message 51a629cd-9431-4e55-be2e-b726fc8779c8
[92mINFO [0m:      Sent reply
01/18/2025 09:03:48:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:04:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:04:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ed2f6de-7dc5-48b2-aa24-c38044082285
01/18/2025 09:04:30:INFO:Received: train message 8ed2f6de-7dc5-48b2-aa24-c38044082285
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:08:30:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:27:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:27:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 94731614-29b1-4dc5-a16d-10bb95cefdef
01/18/2025 09:27:43:INFO:Received: evaluate message 94731614-29b1-4dc5-a16d-10bb95cefdef
[92mINFO [0m:      Sent reply
01/18/2025 09:32:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:33:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:33:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message be697eab-aa6e-43e4-8310-d3bf92881898
01/18/2025 09:33:13:INFO:Received: train message be697eab-aa6e-43e4-8310-d3bf92881898
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:37:08:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:55:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:55:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a30f9089-df9f-4f09-b45d-d6a2e0c2561c
01/18/2025 09:55:06:INFO:Received: evaluate message a30f9089-df9f-4f09-b45d-d6a2e0c2561c
[92mINFO [0m:      Sent reply
01/18/2025 10:00:04:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:00:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:00:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0e115cb6-834d-4e51-9c59-300789ce2b43
01/18/2025 10:00:46:INFO:Received: train message 0e115cb6-834d-4e51-9c59-300789ce2b43
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:04:48:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:23:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:23:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3c3e036b-c779-42b3-b983-5cb62f021b88
01/18/2025 10:23:07:INFO:Received: evaluate message 3c3e036b-c779-42b3-b983-5cb62f021b88
[92mINFO [0m:      Sent reply
01/18/2025 10:27:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:28:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:28:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4a5312f4-6edc-47d0-b3e9-1e6205d095ae
01/18/2025 10:28:12:INFO:Received: train message 4a5312f4-6edc-47d0-b3e9-1e6205d095ae
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:32:07:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:49:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:49:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f723819f-a6c9-4203-992c-b74e9ed1b7e9
01/18/2025 10:49:52:INFO:Received: evaluate message f723819f-a6c9-4203-992c-b74e9ed1b7e9
[92mINFO [0m:      Sent reply
01/18/2025 10:54:32:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:55:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:55:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5ea9ca46-873c-4d73-b436-e75dca9cee14
01/18/2025 10:55:12:INFO:Received: train message 5ea9ca46-873c-4d73-b436-e75dca9cee14
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:59:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:16:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:16:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d990070a-12ce-47c3-86f2-18b9ca533e39
01/18/2025 11:16:20:INFO:Received: evaluate message d990070a-12ce-47c3-86f2-18b9ca533e39
[92mINFO [0m:      Sent reply
01/18/2025 11:20:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:21:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:21:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d96d96b5-ac0f-43cc-9795-af18a0302656
01/18/2025 11:21:20:INFO:Received: train message d96d96b5-ac0f-43cc-9795-af18a0302656
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:25:28:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:42:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:42:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6d69fb16-ecc4-4bd0-9f09-b63501623c95
01/18/2025 11:42:49:INFO:Received: evaluate message 6d69fb16-ecc4-4bd0-9f09-b63501623c95
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_1', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_1']
BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962], 'accuracy': [0.3415223519935562], 'auc': [0.5447577460866678]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633], 'accuracy': [0.3415223519935562, 0.3403141361256545], 'auc': [0.5447577460866678, 0.5869994805277825]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/18/2025 11:47:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:48:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:48:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 477cb04f-aedb-4f19-a3a1-6ddfd6c7dd70
01/18/2025 11:48:08:INFO:Received: train message 477cb04f-aedb-4f19-a3a1-6ddfd6c7dd70
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:52:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:10:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:10:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f4d0c852-82ac-47a1-b971-3270d24093bf
01/18/2025 12:10:05:INFO:Received: evaluate message f4d0c852-82ac-47a1-b971-3270d24093bf
[92mINFO [0m:      Sent reply
01/18/2025 12:15:08:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:15:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:15:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5ad27670-e0bf-4f7e-8e97-85ad48f653f3
01/18/2025 12:15:56:INFO:Received: train message 5ad27670-e0bf-4f7e-8e97-85ad48f653f3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:20:01:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:37:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:37:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ff04a20b-b84a-4dab-b2bc-f49b7acc2dbe
01/18/2025 12:37:26:INFO:Received: evaluate message ff04a20b-b84a-4dab-b2bc-f49b7acc2dbe
[92mINFO [0m:      Sent reply
01/18/2025 12:42:34:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:43:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:43:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a77f9023-4c47-4acd-893c-712aa54c2151
01/18/2025 12:43:03:INFO:Received: train message a77f9023-4c47-4acd-893c-712aa54c2151
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:47:24:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:05:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:05:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 88069e3e-2a2b-4f63-b7bd-cbb4bc079a31
01/18/2025 13:05:07:INFO:Received: evaluate message 88069e3e-2a2b-4f63-b7bd-cbb4bc079a31
[92mINFO [0m:      Sent reply
01/18/2025 13:09:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:10:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:10:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f769a006-b6d2-42db-9ad0-9fd2a991a481
01/18/2025 13:10:26:INFO:Received: train message f769a006-b6d2-42db-9ad0-9fd2a991a481
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:14:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:31:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:31:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5ffa1f0c-b489-49bd-80de-8006625db177
01/18/2025 13:31:05:INFO:Received: evaluate message 5ffa1f0c-b489-49bd-80de-8006625db177
[92mINFO [0m:      Sent reply
01/18/2025 13:34:51:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:36:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:36:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e7b19460-4560-4294-9c49-ec674a26f549
01/18/2025 13:36:26:INFO:Received: train message e7b19460-4560-4294-9c49-ec674a26f549
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:40:22:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:59:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:59:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f4e765e6-8da5-40a9-af07-cfaaa27e7334
01/18/2025 13:59:00:INFO:Received: evaluate message f4e765e6-8da5-40a9-af07-cfaaa27e7334
[92mINFO [0m:      Sent reply
01/18/2025 14:03:31:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:03:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:03:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1e391549-752d-4662-9c5c-b865c4ff785b
01/18/2025 14:03:48:INFO:Received: train message 1e391549-752d-4662-9c5c-b865c4ff785b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:07:19:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:26:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:26:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3f8718b3-c7b6-42dd-9a19-b1f0bb074d80
01/18/2025 14:26:25:INFO:Received: evaluate message 3f8718b3-c7b6-42dd-9a19-b1f0bb074d80
[92mINFO [0m:      Sent reply
01/18/2025 14:30:53:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:31:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:31:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 61107bb1-abff-4344-910f-7ef1c9455add
01/18/2025 14:31:21:INFO:Received: train message 61107bb1-abff-4344-910f-7ef1c9455add

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.244406985118985
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 2.0947798527777195
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 1.9451527204364538
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 1.7955255880951881
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:35:23:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:53:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:53:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 07f7b8de-a4b7-4eb7-95da-6d874c24e588
01/18/2025 14:53:34:INFO:Received: evaluate message 07f7b8de-a4b7-4eb7-95da-6d874c24e588
[92mINFO [0m:      Sent reply
01/18/2025 14:57:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:58:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:58:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 91d6cc5a-b5c0-4178-b837-7ce8aa81a521
01/18/2025 14:58:29:INFO:Received: train message 91d6cc5a-b5c0-4178-b837-7ce8aa81a521
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:02:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:21:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:21:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f30ac031-2935-4eff-a63e-27e87ec0bbf8
01/18/2025 15:21:08:INFO:Received: evaluate message f30ac031-2935-4eff-a63e-27e87ec0bbf8
[92mINFO [0m:      Sent reply
01/18/2025 15:25:31:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:26:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:26:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0fc4e188-0728-44f1-a6f1-d45b21e9ade3
01/18/2025 15:26:10:INFO:Received: train message 0fc4e188-0728-44f1-a6f1-d45b21e9ade3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:30:14:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:48:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:48:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5ca09ed0-cd4e-4444-a6f5-d669d287692e
01/18/2025 15:48:40:INFO:Received: evaluate message 5ca09ed0-cd4e-4444-a6f5-d669d287692e
[92mINFO [0m:      Sent reply
01/18/2025 15:52:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:53:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:53:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 00512570-16e1-42ae-b2af-c11fe98184fe
01/18/2025 15:53:46:INFO:Received: train message 00512570-16e1-42ae-b2af-c11fe98184fe
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:57:48:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:17:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:17:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9691e1b9-bc01-4349-a301-c03a0619ab9f
01/18/2025 16:17:17:INFO:Received: evaluate message 9691e1b9-bc01-4349-a301-c03a0619ab9f
[92mINFO [0m:      Sent reply
01/18/2025 16:21:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:22:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:22:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9ea8ee54-4bdc-4fa5-bc5d-2251a77ff922
01/18/2025 16:22:24:INFO:Received: train message 9ea8ee54-4bdc-4fa5-bc5d-2251a77ff922
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:26:06:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:46:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:46:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3b497692-c448-4144-a495-8cc469fb6c04
01/18/2025 16:46:56:INFO:Received: evaluate message 3b497692-c448-4144-a495-8cc469fb6c04
[92mINFO [0m:      Sent reply
01/18/2025 16:52:14:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:52:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:52:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7b578eb7-4561-4747-8aeb-6b747189d843
01/18/2025 16:52:36:INFO:Received: train message 7b578eb7-4561-4747-8aeb-6b747189d843
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:56:28:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:18:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:18:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1732c541-7fa7-4870-8b9e-300dc2d14a8d
01/18/2025 17:18:01:INFO:Received: evaluate message 1732c541-7fa7-4870-8b9e-300dc2d14a8d
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 1.6458984557539227
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 1.496271323412657
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 1.346644191071391
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 1.1970170587301254
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 1.0473899263888597
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 0.8977627940475941
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/18/2025 17:22:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:24:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:24:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0083b491-6455-4af4-9e2a-080af8828543
01/18/2025 17:24:15:INFO:Received: train message 0083b491-6455-4af4-9e2a-080af8828543
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:28:19:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:52:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:52:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9630805e-3df6-4a8c-be40-ebff4a22810c
01/18/2025 17:52:34:INFO:Received: evaluate message 9630805e-3df6-4a8c-be40-ebff4a22810c
[92mINFO [0m:      Sent reply
01/18/2025 17:56:28:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:57:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:57:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7ac9eca9-de0a-447f-9976-0e09066d7f6d
01/18/2025 17:57:10:INFO:Received: train message 7ac9eca9-de0a-447f-9976-0e09066d7f6d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:00:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:24:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:24:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 36619973-a44e-4d1b-aa62-bb30595cad90
01/18/2025 18:24:48:INFO:Received: evaluate message 36619973-a44e-4d1b-aa62-bb30595cad90
[92mINFO [0m:      Sent reply
01/18/2025 18:28:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:29:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:29:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3d7ca44d-4051-402f-ac0d-fe2d07de1bde
01/18/2025 18:29:29:INFO:Received: train message 3d7ca44d-4051-402f-ac0d-fe2d07de1bde
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:33:32:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:54:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:54:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 26cc7eaf-1cc0-4a5e-b630-d12baa8d2113
01/18/2025 18:54:17:INFO:Received: evaluate message 26cc7eaf-1cc0-4a5e-b630-d12baa8d2113
[92mINFO [0m:      Sent reply
01/18/2025 18:57:24:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:59:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:59:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f8f9f871-dafe-42c1-8a71-c2ce8cb0bbdb
01/18/2025 18:59:21:INFO:Received: train message f8f9f871-dafe-42c1-8a71-c2ce8cb0bbdb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:03:13:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:22:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:22:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 63661612-332b-43f2-af09-5160062d9e13
01/18/2025 19:22:17:INFO:Received: evaluate message 63661612-332b-43f2-af09-5160062d9e13

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 0.7481356617063285
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 0.5985085293650628
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349, 119.47592175006866], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838, 0.5251711639146194], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765, 0.7122401309258455]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 0.4488813970237969
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349, 119.47592175006866, 119.1336715221405], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838, 0.5251711639146194, 0.527184857027789], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765, 0.7122401309258455, 0.7145001845596337]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 0.2992542646825313
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/18/2025 19:26:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:27:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:27:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e20d254e-5528-40f3-84d0-58529874c7f3
01/18/2025 19:27:24:INFO:Received: train message e20d254e-5528-40f3-84d0-58529874c7f3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:31:25:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:49:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:49:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f067ae18-912e-49a6-b76a-1db60011bde2
01/18/2025 19:49:44:INFO:Received: evaluate message f067ae18-912e-49a6-b76a-1db60011bde2
[92mINFO [0m:      Sent reply
01/18/2025 19:54:03:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:54:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:54:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5e7e04d3-afb1-4094-9974-91a4b71d39a8
01/18/2025 19:54:45:INFO:Received: train message 5e7e04d3-afb1-4094-9974-91a4b71d39a8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:58:44:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:18:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:18:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2946c8bb-c20c-4de9-9499-5fea92a38cdc
01/18/2025 20:18:59:INFO:Received: evaluate message 2946c8bb-c20c-4de9-9499-5fea92a38cdc
[92mINFO [0m:      Sent reply
01/18/2025 20:23:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:23:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:23:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message b58864e2-ef8d-4f5e-a9de-84c24f4c5995
01/18/2025 20:23:46:INFO:Received: reconnect message b58864e2-ef8d-4f5e-a9de-84c24f4c5995
01/18/2025 20:23:46:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/18/2025 20:23:46:INFO:Disconnect and shut down

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349, 119.47592175006866, 119.1336715221405, 118.44663709402084], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838, 0.5251711639146194, 0.527184857027789, 0.5283930728956907], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765, 0.7122401309258455, 0.7145001845596337, 0.7172991897557845]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 0.14962713234126565
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349, 119.47592175006866, 119.1336715221405, 118.44663709402084, 118.22421109676361], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838, 0.5251711639146194, 0.527184857027789, 0.5283930728956907, 0.5308095046314941], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765, 0.7122401309258455, 0.7145001845596337, 0.7172991897557845, 0.7193194506724596]}

BaseNM 3.06640625
noise multiplier 2.244406985118985
Noise multiplier before  adjustment: 2.244406985118985
Noise multiplier before convergence adjustment: 2.244406985118985
Updated noise multiplier after convergence adjustment: 0.0
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349, 119.47592175006866, 119.1336715221405, 118.44663709402084, 118.22421109676361, 118.26175034046173], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838, 0.5251711639146194, 0.527184857027789, 0.5283930728956907, 0.5308095046314941, 0.5300040273862263], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765, 0.7122401309258455, 0.7145001845596337, 0.7172991897557845, 0.7193194506724596, 0.7209046546349261]}



Final client history:
{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349, 119.47592175006866, 119.1336715221405, 118.44663709402084, 118.22421109676361, 118.26175034046173], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838, 0.5251711639146194, 0.527184857027789, 0.5283930728956907, 0.5308095046314941, 0.5300040273862263], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765, 0.7122401309258455, 0.7145001845596337, 0.7172991897557845, 0.7193194506724596, 0.7209046546349261]}

