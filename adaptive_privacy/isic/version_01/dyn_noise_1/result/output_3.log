nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_1/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/18/2025 06:31:01:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/18/2025 06:31:01:DEBUG:ChannelConnectivity.IDLE
01/18/2025 06:31:01:DEBUG:ChannelConnectivity.CONNECTING
01/18/2025 06:31:01:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/18/2025 06:34:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:34:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5566af97-271e-4866-ae19-e520fb9cedc2
01/18/2025 06:34:01:INFO:Received: train message 5566af97-271e-4866-ae19-e520fb9cedc2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 06:45:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:53:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:53:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 345521d2-f435-443c-bf7e-0a5b669dbb68
01/18/2025 06:53:16:INFO:Received: evaluate message 345521d2-f435-443c-bf7e-0a5b669dbb68
[92mINFO [0m:      Sent reply
01/18/2025 06:57:12:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:58:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:58:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5077f477-07dc-4074-9df1-c0019de9bea8
01/18/2025 06:58:03:INFO:Received: train message 5077f477-07dc-4074-9df1-c0019de9bea8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:09:43:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:17:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:17:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b360d4ce-26f7-4bb7-a42a-4ad635158d40
01/18/2025 07:17:03:INFO:Received: evaluate message b360d4ce-26f7-4bb7-a42a-4ad635158d40
[92mINFO [0m:      Sent reply
01/18/2025 07:21:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:21:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:21:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 554bef01-9ce7-4fc4-90df-545e5110ad09
01/18/2025 07:21:18:INFO:Received: train message 554bef01-9ce7-4fc4-90df-545e5110ad09
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:32:28:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:40:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:40:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4b32a36f-152a-4a8e-851c-24e9c21f550e
01/18/2025 07:40:30:INFO:Received: evaluate message 4b32a36f-152a-4a8e-851c-24e9c21f550e
[92mINFO [0m:      Sent reply
01/18/2025 07:44:22:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:44:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:44:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f40c9237-70ac-4dc1-a129-63997409e663
01/18/2025 07:44:48:INFO:Received: train message f40c9237-70ac-4dc1-a129-63997409e663
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:56:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:03:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:03:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0031ae97-78b6-4599-ac74-1a91a25c6294
01/18/2025 08:03:34:INFO:Received: evaluate message 0031ae97-78b6-4599-ac74-1a91a25c6294
[92mINFO [0m:      Sent reply
01/18/2025 08:07:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:08:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:08:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 00324348-0b5f-4a47-88cb-dbceda0a42bb
01/18/2025 08:08:19:INFO:Received: train message 00324348-0b5f-4a47-88cb-dbceda0a42bb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:19:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:30:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:30:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 78cef274-e166-468d-aace-7b141ccc44f2
01/18/2025 08:30:47:INFO:Received: evaluate message 78cef274-e166-468d-aace-7b141ccc44f2
[92mINFO [0m:      Sent reply
01/18/2025 08:35:08:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:35:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:35:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5a68ea89-5b9d-46f5-a1db-a822b3ae3b1b
01/18/2025 08:35:50:INFO:Received: train message 5a68ea89-5b9d-46f5-a1db-a822b3ae3b1b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:47:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:58:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:58:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c8753e4c-e601-40a5-937e-f991e0c27842
01/18/2025 08:58:52:INFO:Received: evaluate message c8753e4c-e601-40a5-937e-f991e0c27842
[92mINFO [0m:      Sent reply
01/18/2025 09:04:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:04:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:04:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6f8652b7-c076-4c78-bc49-55fc86266c6a
01/18/2025 09:04:55:INFO:Received: train message 6f8652b7-c076-4c78-bc49-55fc86266c6a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:16:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:27:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:27:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9617853e-7c11-4fdb-a1b1-1e947588c67f
01/18/2025 09:27:24:INFO:Received: evaluate message 9617853e-7c11-4fdb-a1b1-1e947588c67f
[92mINFO [0m:      Sent reply
01/18/2025 09:32:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:33:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:33:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d4505291-6984-4525-82e5-5930e396e0a7
01/18/2025 09:33:24:INFO:Received: train message d4505291-6984-4525-82e5-5930e396e0a7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:45:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:55:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:55:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ee4a1e85-fd56-4286-b83b-87111947110c
01/18/2025 09:55:07:INFO:Received: evaluate message ee4a1e85-fd56-4286-b83b-87111947110c
[92mINFO [0m:      Sent reply
01/18/2025 10:00:28:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:01:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:01:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b8d15a59-62bc-485c-b221-2fc4e2a3226f
01/18/2025 10:01:06:INFO:Received: train message b8d15a59-62bc-485c-b221-2fc4e2a3226f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:13:29:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:23:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:23:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4b7873ee-666e-4a91-97fd-bd073fcffe84
01/18/2025 10:23:06:INFO:Received: evaluate message 4b7873ee-666e-4a91-97fd-bd073fcffe84
[92mINFO [0m:      Sent reply
01/18/2025 10:27:53:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:28:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:28:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 749afc15-dadd-4f0f-a311-b7de7074959d
01/18/2025 10:28:31:INFO:Received: train message 749afc15-dadd-4f0f-a311-b7de7074959d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:41:26:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:49:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:49:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 069cc743-276a-4bf1-b451-b118359f27b9
01/18/2025 10:49:52:INFO:Received: evaluate message 069cc743-276a-4bf1-b451-b118359f27b9
[92mINFO [0m:      Sent reply
01/18/2025 10:54:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:54:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:54:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9241ca99-f5f0-4d9a-954b-55d2293212e9
01/18/2025 10:54:51:INFO:Received: train message 9241ca99-f5f0-4d9a-954b-55d2293212e9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:06:46:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:16:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:16:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c0d2f4f7-0bc5-4f4e-8b2e-7af3cc235ead
01/18/2025 11:16:02:INFO:Received: evaluate message c0d2f4f7-0bc5-4f4e-8b2e-7af3cc235ead
[92mINFO [0m:      Sent reply
01/18/2025 11:20:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:21:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:21:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 22873363-9510-480d-9a36-ac5cde1efbb8
01/18/2025 11:21:25:INFO:Received: train message 22873363-9510-480d-9a36-ac5cde1efbb8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:33:12:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:43:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:43:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 64794481-6972-41bf-af98-6fcc8938fb46
01/18/2025 11:43:02:INFO:Received: evaluate message 64794481-6972-41bf-af98-6fcc8938fb46
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_1', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_1']
BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962], 'accuracy': [0.3415223519935562], 'auc': [0.5447577460866678]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633], 'accuracy': [0.3415223519935562, 0.3403141361256545], 'auc': [0.5447577460866678, 0.5869994805277825]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/18/2025 11:47:20:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:47:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:47:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7d5ed989-8038-46dc-ac88-b867f2098953
01/18/2025 11:47:57:INFO:Received: train message 7d5ed989-8038-46dc-ac88-b867f2098953
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:59:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:10:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:10:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8fc46622-eec0-4f87-9806-96f88f6016c5
01/18/2025 12:10:16:INFO:Received: evaluate message 8fc46622-eec0-4f87-9806-96f88f6016c5
[92mINFO [0m:      Sent reply
01/18/2025 12:15:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:16:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:16:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fe4a593e-97b4-4cc8-8638-3a5de0be2753
01/18/2025 12:16:01:INFO:Received: train message fe4a593e-97b4-4cc8-8638-3a5de0be2753
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:27:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:37:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:37:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2ccffec3-c6cd-4d7c-8a00-0223aa4d2382
01/18/2025 12:37:15:INFO:Received: evaluate message 2ccffec3-c6cd-4d7c-8a00-0223aa4d2382
[92mINFO [0m:      Sent reply
01/18/2025 12:42:19:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:43:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:43:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 61aa878b-11c2-49fa-81b3-3838392b5bff
01/18/2025 12:43:04:INFO:Received: train message 61aa878b-11c2-49fa-81b3-3838392b5bff
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:54:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:05:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:05:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 476b117e-7b31-4ce6-8dbe-46cc0a777cd1
01/18/2025 13:05:01:INFO:Received: evaluate message 476b117e-7b31-4ce6-8dbe-46cc0a777cd1
[92mINFO [0m:      Sent reply
01/18/2025 13:09:21:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:10:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:10:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ca5577cd-5eca-46cb-a1f0-51dd0d5e07cf
01/18/2025 13:10:27:INFO:Received: train message ca5577cd-5eca-46cb-a1f0-51dd0d5e07cf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:22:42:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:31:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:31:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4bf772a3-3553-430f-aabe-6a5faa11342d
01/18/2025 13:31:22:INFO:Received: evaluate message 4bf772a3-3553-430f-aabe-6a5faa11342d
[92mINFO [0m:      Sent reply
01/18/2025 13:35:37:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:36:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:36:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9e12b5a9-c60d-403b-afe3-1eabbd392636
01/18/2025 13:36:31:INFO:Received: train message 9e12b5a9-c60d-403b-afe3-1eabbd392636
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:49:10:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:59:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:59:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 66f4d572-5a09-43bf-9827-b379dec0e29f
01/18/2025 13:59:01:INFO:Received: evaluate message 66f4d572-5a09-43bf-9827-b379dec0e29f
[92mINFO [0m:      Sent reply
01/18/2025 14:02:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:04:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:04:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message da1fa995-e13a-4682-a89f-5bc40746d032
01/18/2025 14:04:08:INFO:Received: train message da1fa995-e13a-4682-a89f-5bc40746d032
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:15:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:26:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:26:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f4e5c814-7196-42da-94e1-2a4207b6289a
01/18/2025 14:26:09:INFO:Received: evaluate message f4e5c814-7196-42da-94e1-2a4207b6289a
[92mINFO [0m:      Sent reply
01/18/2025 14:29:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:31:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:31:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 41a6f73b-9954-4ef0-b8e7-5657a929e59e
01/18/2025 14:31:22:INFO:Received: train message 41a6f73b-9954-4ef0-b8e7-5657a929e59e

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.254431664943695
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 2.1041362206141154
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 1.9538407762845358
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 1.8035453319549561
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:43:04:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:53:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:53:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 275dd1a0-baf7-4cad-99dc-8ca2c5cb5f71
01/18/2025 14:53:35:INFO:Received: evaluate message 275dd1a0-baf7-4cad-99dc-8ca2c5cb5f71
[92mINFO [0m:      Sent reply
01/18/2025 14:57:48:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:58:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:58:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 980e7088-c9dd-4e10-86cf-7354054d049c
01/18/2025 14:58:28:INFO:Received: train message 980e7088-c9dd-4e10-86cf-7354054d049c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:10:15:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:21:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:21:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a12ad859-020f-41c3-834a-749fa3c9ae95
01/18/2025 15:21:06:INFO:Received: evaluate message a12ad859-020f-41c3-834a-749fa3c9ae95
[92mINFO [0m:      Sent reply
01/18/2025 15:25:34:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:26:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:26:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 87c6941b-91e3-448b-b76a-6a8ad9bdcd7c
01/18/2025 15:26:13:INFO:Received: train message 87c6941b-91e3-448b-b76a-6a8ad9bdcd7c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:37:53:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:48:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:48:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0317f06f-d4b0-45e6-b082-40932b13302f
01/18/2025 15:48:50:INFO:Received: evaluate message 0317f06f-d4b0-45e6-b082-40932b13302f
[92mINFO [0m:      Sent reply
01/18/2025 15:53:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:53:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:53:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f2ae2bba-e1e9-43c1-ba8e-bbc2effcbedb
01/18/2025 15:53:38:INFO:Received: train message f2ae2bba-e1e9-43c1-ba8e-bbc2effcbedb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:05:15:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:17:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:17:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a157900c-b713-4bd9-804a-ec566f3acd47
01/18/2025 16:17:08:INFO:Received: evaluate message a157900c-b713-4bd9-804a-ec566f3acd47
[92mINFO [0m:      Sent reply
01/18/2025 16:21:57:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:22:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:22:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6de22267-ef11-4f88-a3f9-b24b34028d68
01/18/2025 16:22:27:INFO:Received: train message 6de22267-ef11-4f88-a3f9-b24b34028d68
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:34:01:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:46:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:46:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4a57a8ad-93c6-4217-9c38-a0a1a1ee9aaf
01/18/2025 16:46:39:INFO:Received: evaluate message 4a57a8ad-93c6-4217-9c38-a0a1a1ee9aaf
[92mINFO [0m:      Sent reply
01/18/2025 16:51:36:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:52:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:52:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 019b555a-eb73-4f5b-b93e-9db1c592a173
01/18/2025 16:52:51:INFO:Received: train message 019b555a-eb73-4f5b-b93e-9db1c592a173
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:04:36:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:18:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:18:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b9fcf738-a1fd-4a3f-85ab-f694c7a6371e
01/18/2025 17:18:21:INFO:Received: evaluate message b9fcf738-a1fd-4a3f-85ab-f694c7a6371e
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 1.6532498876253765
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 1.5029544432957969
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 1.352658998966217
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 1.2023635546366374
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 1.0520681103070577
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 0.9017726659774781
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/18/2025 17:23:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:24:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:24:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 586d6bf9-4a20-4746-94ea-dbf6c0bdee77
01/18/2025 17:24:07:INFO:Received: train message 586d6bf9-4a20-4746-94ea-dbf6c0bdee77
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:35:49:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:52:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:52:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 29cd90a8-7703-4ca1-b47f-c1a7dc3979b5
01/18/2025 17:52:59:INFO:Received: evaluate message 29cd90a8-7703-4ca1-b47f-c1a7dc3979b5
[92mINFO [0m:      Sent reply
01/18/2025 17:57:00:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:57:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:57:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 551b5511-21d3-4815-bcf2-20df4885d06d
01/18/2025 17:57:12:INFO:Received: train message 551b5511-21d3-4815-bcf2-20df4885d06d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:09:11:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:24:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:24:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5981b19b-9f7e-4707-b0d1-8c6ba56c39cd
01/18/2025 18:24:44:INFO:Received: evaluate message 5981b19b-9f7e-4707-b0d1-8c6ba56c39cd
[92mINFO [0m:      Sent reply
01/18/2025 18:28:44:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:29:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:29:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4afdb103-ecc9-4c14-b9f7-410ff6da2616
01/18/2025 18:29:21:INFO:Received: train message 4afdb103-ecc9-4c14-b9f7-410ff6da2616
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:41:20:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:54:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:54:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e68678e0-de4d-437b-b989-2b7e5bdfe6eb
01/18/2025 18:54:42:INFO:Received: evaluate message e68678e0-de4d-437b-b989-2b7e5bdfe6eb
[92mINFO [0m:      Sent reply
01/18/2025 18:58:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:59:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:59:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e2640e44-61ad-4da7-a87b-a4e05dd7acda
01/18/2025 18:59:37:INFO:Received: train message e2640e44-61ad-4da7-a87b-a4e05dd7acda
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:11:15:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:22:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:22:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1d5f3411-123b-4766-b4b5-9e46a34841e4
01/18/2025 19:22:20:INFO:Received: evaluate message 1d5f3411-123b-4766-b4b5-9e46a34841e4

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 0.7514772216478984
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 0.6011817773183188
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349, 119.47592175006866], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838, 0.5251711639146194], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765, 0.7122401309258455]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 0.4508863329887389
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349, 119.47592175006866, 119.1336715221405], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838, 0.5251711639146194, 0.527184857027789], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765, 0.7122401309258455, 0.7145001845596337]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 0.3005908886591593
Epsilon = 1.00
[92mINFO [0m:      Sent reply
01/18/2025 19:26:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:27:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:27:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d13460f6-04b0-47e0-87bf-a452840f1489
01/18/2025 19:27:09:INFO:Received: train message d13460f6-04b0-47e0-87bf-a452840f1489
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:39:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:49:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:49:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 21f8981e-5b0b-4ac1-bc33-0dc37aae9f90
01/18/2025 19:49:50:INFO:Received: evaluate message 21f8981e-5b0b-4ac1-bc33-0dc37aae9f90
[92mINFO [0m:      Sent reply
01/18/2025 19:54:11:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:54:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:54:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message acf122c0-f7dc-4ac4-907a-acf3db3734a2
01/18/2025 19:54:57:INFO:Received: train message acf122c0-f7dc-4ac4-907a-acf3db3734a2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:08:30:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:19:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:19:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message da0a7cb3-15e0-43e0-82f9-ce94973a11e9
01/18/2025 20:19:04:INFO:Received: evaluate message da0a7cb3-15e0-43e0-82f9-ce94973a11e9
[92mINFO [0m:      Sent reply
01/18/2025 20:23:43:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:23:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:23:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 83fe0fa1-8836-4806-9018-f48e09157d26
01/18/2025 20:23:46:INFO:Received: reconnect message 83fe0fa1-8836-4806-9018-f48e09157d26
01/18/2025 20:23:46:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/18/2025 20:23:46:INFO:Disconnect and shut down

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349, 119.47592175006866, 119.1336715221405, 118.44663709402084], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838, 0.5251711639146194, 0.527184857027789, 0.5283930728956907], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765, 0.7122401309258455, 0.7145001845596337, 0.7172991897557845]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 0.15029544432957964
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349, 119.47592175006866, 119.1336715221405, 118.44663709402084, 118.22421109676361], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838, 0.5251711639146194, 0.527184857027789, 0.5283930728956907, 0.5308095046314941], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765, 0.7122401309258455, 0.7145001845596337, 0.7172991897557845, 0.7193194506724596]}

BaseNM 3.06640625
noise multiplier 2.254431664943695
Noise multiplier before  adjustment: 2.254431664943695
Noise multiplier before convergence adjustment: 2.254431664943695
Updated noise multiplier after convergence adjustment: 0.0
Epsilon = 1.00

{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349, 119.47592175006866, 119.1336715221405, 118.44663709402084, 118.22421109676361, 118.26175034046173], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838, 0.5251711639146194, 0.527184857027789, 0.5283930728956907, 0.5308095046314941, 0.5300040273862263], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765, 0.7122401309258455, 0.7145001845596337, 0.7172991897557845, 0.7193194506724596, 0.7209046546349261]}



Final client history:
{'loss': [142.10614502429962, 135.5050299167633, 137.47201240062714, 140.40874767303467, 141.34832644462585, 141.1794821023941, 139.22257089614868, 137.55157935619354, 134.25717842578888, 132.35167920589447, 130.7151244878769, 129.98683059215546, 128.34206986427307, 127.50094366073608, 126.48802548646927, 125.56411898136139, 124.3446934223175, 124.26051634550095, 123.44350230693817, 122.49547052383423, 121.92903017997742, 121.34707772731781, 120.57707929611206, 119.99816364049911, 119.52394533157349, 119.47592175006866, 119.1336715221405, 118.44663709402084, 118.22421109676361, 118.26175034046173], 'accuracy': [0.3415223519935562, 0.3403141361256545, 0.3411196133709223, 0.3415223519935562, 0.3459524768425292, 0.35118807893677, 0.3681031010873943, 0.3813934756343133, 0.4027386226339106, 0.4176399516713653, 0.43455497382198954, 0.441401530406766, 0.45388642770841725, 0.4599275070479259, 0.4675795408779702, 0.47321788159484496, 0.48086991542488927, 0.48610551751913006, 0.49295207410390657, 0.500201369311317, 0.5030205396697544, 0.5074506645187273, 0.5138944824808699, 0.5199355618203786, 0.5235602094240838, 0.5251711639146194, 0.527184857027789, 0.5283930728956907, 0.5308095046314941, 0.5300040273862263], 'auc': [0.5447577460866678, 0.5869994805277825, 0.6108693952517427, 0.6233983124795475, 0.6335404197828883, 0.6397362295037701, 0.6467459697108222, 0.6524602597191902, 0.6576649289239014, 0.6610682888712689, 0.6659904126530913, 0.6701545798920383, 0.6736646874963987, 0.677940976600335, 0.6808937111757158, 0.6848946908358893, 0.6881373732627478, 0.6904252009133054, 0.6931431210476737, 0.6967410528246826, 0.6991049824377151, 0.7016687400980424, 0.704518178459855, 0.7072666509607928, 0.7101596625893765, 0.7122401309258455, 0.7145001845596337, 0.7172991897557845, 0.7193194506724596, 0.7209046546349261]}

