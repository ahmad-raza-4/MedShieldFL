nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/18/2025 06:22:47:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/18/2025 06:22:47:DEBUG:ChannelConnectivity.IDLE
01/18/2025 06:22:47:DEBUG:ChannelConnectivity.CONNECTING
01/18/2025 06:22:47:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/18/2025 06:25:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:25:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 733bf013-fc39-4436-8775-bc7cdd1a44cb
01/18/2025 06:25:23:INFO:Received: train message 733bf013-fc39-4436-8775-bc7cdd1a44cb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 06:38:24:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:44:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:44:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e2862f19-e116-477f-95ea-9c4883a6f9da
01/18/2025 06:44:29:INFO:Received: evaluate message e2862f19-e116-477f-95ea-9c4883a6f9da
[92mINFO [0m:      Sent reply
01/18/2025 06:48:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:49:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:49:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 56fd8f33-b480-43fb-b2a1-609783794859
01/18/2025 06:49:16:INFO:Received: train message 56fd8f33-b480-43fb-b2a1-609783794859
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:02:06:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:08:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:08:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b14bd48e-498f-459b-96dc-4cb9a6bad35b
01/18/2025 07:08:16:INFO:Received: evaluate message b14bd48e-498f-459b-96dc-4cb9a6bad35b
[92mINFO [0m:      Sent reply
01/18/2025 07:11:51:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:12:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:12:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f77ee356-80d1-4981-835e-1cd077241210
01/18/2025 07:12:48:INFO:Received: train message f77ee356-80d1-4981-835e-1cd077241210
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:25:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:32:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:32:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 367ece55-7f2b-4d43-b6bb-39cbe4e2f760
01/18/2025 07:32:00:INFO:Received: evaluate message 367ece55-7f2b-4d43-b6bb-39cbe4e2f760
[92mINFO [0m:      Sent reply
01/18/2025 07:35:52:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:36:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:36:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b7773185-a72c-4279-b1e2-8c4876f6ca0d
01/18/2025 07:36:41:INFO:Received: train message b7773185-a72c-4279-b1e2-8c4876f6ca0d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:49:20:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:55:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:55:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c041e375-bd9f-4d6b-a08c-6e1ccf1c7a69
01/18/2025 07:55:49:INFO:Received: evaluate message c041e375-bd9f-4d6b-a08c-6e1ccf1c7a69
[92mINFO [0m:      Sent reply
01/18/2025 07:59:46:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:59:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:59:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f8c91557-dcd3-4453-b9c4-062ebf65c1ba
01/18/2025 07:59:59:INFO:Received: train message f8c91557-dcd3-4453-b9c4-062ebf65c1ba
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:12:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:19:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:19:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ea6c7ad1-2fc0-46ac-b637-8e830ba30401
01/18/2025 08:19:36:INFO:Received: evaluate message ea6c7ad1-2fc0-46ac-b637-8e830ba30401
[92mINFO [0m:      Sent reply
01/18/2025 08:23:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:24:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:24:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2ec8e169-b2ac-45d9-9cb3-c40b3156e76e
01/18/2025 08:24:28:INFO:Received: train message 2ec8e169-b2ac-45d9-9cb3-c40b3156e76e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:38:52:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:49:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:49:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 755e57c5-acc9-48b9-be38-0f2ef92edcda
01/18/2025 08:49:27:INFO:Received: evaluate message 755e57c5-acc9-48b9-be38-0f2ef92edcda
[92mINFO [0m:      Sent reply
01/18/2025 08:53:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:54:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:54:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a64ecfa8-8524-44db-ad20-ca7e1ee942ce
01/18/2025 08:54:13:INFO:Received: train message a64ecfa8-8524-44db-ad20-ca7e1ee942ce
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:09:55:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:19:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:19:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6dfc3175-80ed-47e2-98bb-4694a894f2e9
01/18/2025 09:19:41:INFO:Received: evaluate message 6dfc3175-80ed-47e2-98bb-4694a894f2e9
[92mINFO [0m:      Sent reply
01/18/2025 09:23:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:24:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:24:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 696b1538-3b2c-40b8-a433-a268a129239a
01/18/2025 09:24:22:INFO:Received: train message 696b1538-3b2c-40b8-a433-a268a129239a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:40:31:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:49:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:49:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 29d4dd44-6eb8-4c8c-9d81-298f8187ca1e
01/18/2025 09:49:40:INFO:Received: evaluate message 29d4dd44-6eb8-4c8c-9d81-298f8187ca1e
[92mINFO [0m:      Sent reply
01/18/2025 09:53:41:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:54:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:54:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5ddd55a3-ef2b-4e75-a1d7-a9130b12348a
01/18/2025 09:54:32:INFO:Received: train message 5ddd55a3-ef2b-4e75-a1d7-a9130b12348a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:10:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:19:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:19:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e57eb410-fac6-4c69-9ba8-955072e81795
01/18/2025 10:19:00:INFO:Received: evaluate message e57eb410-fac6-4c69-9ba8-955072e81795
[92mINFO [0m:      Sent reply
01/18/2025 10:23:01:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:24:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:24:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2dc0da47-1910-45bd-85f6-710a845ba826
01/18/2025 10:24:14:INFO:Received: train message 2dc0da47-1910-45bd-85f6-710a845ba826
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:40:40:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:48:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:48:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eae8720f-eaac-406a-bce2-3d96ace70947
01/18/2025 10:48:40:INFO:Received: evaluate message eae8720f-eaac-406a-bce2-3d96ace70947
[92mINFO [0m:      Sent reply
01/18/2025 10:53:20:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:53:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:53:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ef56a8f7-b08d-4b3c-9319-bede31b32960
01/18/2025 10:53:51:INFO:Received: train message ef56a8f7-b08d-4b3c-9319-bede31b32960
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:07:24:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:15:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:15:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1023035f-1202-4315-8def-97ba72e395f7
01/18/2025 11:15:33:INFO:Received: evaluate message 1023035f-1202-4315-8def-97ba72e395f7
[92mINFO [0m:      Sent reply
01/18/2025 11:19:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:20:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:20:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message be63ebf7-1fc8-45b0-8b27-41673acc5e79
01/18/2025 11:20:37:INFO:Received: train message be63ebf7-1fc8-45b0-8b27-41673acc5e79
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:33:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:42:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:42:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 774b4c5c-2778-4f0f-aa11-71272ae07d07
01/18/2025 11:42:26:INFO:Received: evaluate message 774b4c5c-2778-4f0f-aa11-71272ae07d07
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise']
BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275], 'accuracy': [0.3419250906161901], 'auc': [0.5457303512912005]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585], 'accuracy': [0.3419250906161901, 0.3407168747482884], 'auc': [0.5457303512912005, 0.5868275242355229]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 11:46:12:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:48:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:48:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d83808ac-5a2a-41ce-b2e0-9e1ec2d859da
01/18/2025 11:48:10:INFO:Received: train message d83808ac-5a2a-41ce-b2e0-9e1ec2d859da
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:01:13:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:10:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:10:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message caaa8e68-7026-4bb4-8007-f71b7e8b26f8
01/18/2025 12:10:26:INFO:Received: evaluate message caaa8e68-7026-4bb4-8007-f71b7e8b26f8
[92mINFO [0m:      Sent reply
01/18/2025 12:15:13:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:16:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:16:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e9c6aec4-7b9a-4d00-a433-b8b5595fbaa0
01/18/2025 12:16:40:INFO:Received: train message e9c6aec4-7b9a-4d00-a433-b8b5595fbaa0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:29:41:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:39:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:39:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b0676669-d904-485a-a8f4-5a65e6ae7e77
01/18/2025 12:39:39:INFO:Received: evaluate message b0676669-d904-485a-a8f4-5a65e6ae7e77
[92mINFO [0m:      Sent reply
01/18/2025 12:45:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:45:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:45:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1b2786f7-62c4-4daa-80d5-9a30775c5521
01/18/2025 12:45:52:INFO:Received: train message 1b2786f7-62c4-4daa-80d5-9a30775c5521
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:58:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:10:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:10:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e3bdf9b6-dc6d-48e9-b36f-cd35b93acf91
01/18/2025 13:10:06:INFO:Received: evaluate message e3bdf9b6-dc6d-48e9-b36f-cd35b93acf91
[92mINFO [0m:      Sent reply
01/18/2025 13:15:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:16:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:16:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 90f14577-1427-4739-89a9-6b1d871a6bd8
01/18/2025 13:16:08:INFO:Received: train message 90f14577-1427-4739-89a9-6b1d871a6bd8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:29:24:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:44:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:44:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 11fa8aab-ce30-4b1a-a44e-f52d50c85837
01/18/2025 13:44:04:INFO:Received: evaluate message 11fa8aab-ce30-4b1a-a44e-f52d50c85837
[92mINFO [0m:      Sent reply
01/18/2025 13:49:06:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:49:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:49:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd61a09a-40f2-4e67-b4b0-dbf6b2a00195
01/18/2025 13:49:40:INFO:Received: train message cd61a09a-40f2-4e67-b4b0-dbf6b2a00195
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:02:16:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:17:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:17:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c8d5d427-0013-4af3-969b-ebe561b656bd
01/18/2025 14:17:49:INFO:Received: evaluate message c8d5d427-0013-4af3-969b-ebe561b656bd
[92mINFO [0m:      Sent reply
01/18/2025 14:21:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:22:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:22:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b9886ddf-eb28-4c32-a713-f8e489143b5e
01/18/2025 14:22:16:INFO:Received: train message b9886ddf-eb28-4c32-a713-f8e489143b5e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:35:25:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:48:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:48:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 27769dd6-1029-4290-b449-ed68ef2280d1
01/18/2025 14:48:52:INFO:Received: evaluate message 27769dd6-1029-4290-b449-ed68ef2280d1
[92mINFO [0m:      Sent reply
01/18/2025 14:52:56:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:53:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:53:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d4738de1-f7bf-4667-85f0-d69fc9cce0d9
01/18/2025 14:53:17:INFO:Received: train message d4738de1-f7bf-4667-85f0-d69fc9cce0d9

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.5144704151898623
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.4801723875105381
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.445874359831214
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.4115763321518898
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088]}

BaseNM 0.72113037109375
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:06:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:17:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:17:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c8402d6d-a474-4e68-8292-d293ad3b4e0b
01/18/2025 15:17:17:INFO:Received: evaluate message c8402d6d-a474-4e68-8292-d293ad3b4e0b
[92mINFO [0m:      Sent reply
01/18/2025 15:21:25:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:22:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:22:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 94e0150a-736a-4ac0-9ebc-f92c71bd5042
01/18/2025 15:22:03:INFO:Received: train message 94e0150a-736a-4ac0-9ebc-f92c71bd5042
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:35:31:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:45:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:45:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9f065816-f60e-4274-b371-56c1bc2dbaec
01/18/2025 15:45:38:INFO:Received: evaluate message 9f065816-f60e-4274-b371-56c1bc2dbaec
[92mINFO [0m:      Sent reply
01/18/2025 15:49:57:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:50:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:50:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f4ff2567-d4a8-42d7-824a-479aa8e25a5c
01/18/2025 15:50:25:INFO:Received: train message f4ff2567-d4a8-42d7-824a-479aa8e25a5c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:03:57:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:13:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:13:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a56fa4f2-2219-4491-8311-fee0feb389c6
01/18/2025 16:13:55:INFO:Received: evaluate message a56fa4f2-2219-4491-8311-fee0feb389c6
[92mINFO [0m:      Sent reply
01/18/2025 16:18:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:19:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:19:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5e8db2f7-be12-48e6-b5a4-bbba08c3a4ee
01/18/2025 16:19:07:INFO:Received: train message 5e8db2f7-be12-48e6-b5a4-bbba08c3a4ee
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:32:20:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:41:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:41:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8df22737-17d9-44f2-a8a0-83014e7a8203
01/18/2025 16:41:52:INFO:Received: evaluate message 8df22737-17d9-44f2-a8a0-83014e7a8203
[92mINFO [0m:      Sent reply
01/18/2025 16:46:19:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:47:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:47:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 312ce10e-edc9-4363-bd62-e59967a3da46
01/18/2025 16:47:08:INFO:Received: train message 312ce10e-edc9-4363-bd62-e59967a3da46
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:00:14:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:08:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:08:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d6387b0b-cd1b-4a11-93d2-5906c58353a7
01/18/2025 17:08:57:INFO:Received: evaluate message d6387b0b-cd1b-4a11-93d2-5906c58353a7
[92mINFO [0m:      Sent reply
01/18/2025 17:13:23:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:14:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:14:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message efc05261-a516-4bdf-b807-370a4e3ebd2c
01/18/2025 17:14:16:INFO:Received: train message efc05261-a516-4bdf-b807-370a4e3ebd2c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:27:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:36:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:36:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fe4892f6-574c-49ff-99ed-aabdbab9d55d
01/18/2025 17:36:20:INFO:Received: evaluate message fe4892f6-574c-49ff-99ed-aabdbab9d55d
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.37727830447256566
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.34298027679324156
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.30868224911391734
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.2743842214345932
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.24008619375526905
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.2057881660759449
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 17:40:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:41:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:41:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 78b97e73-c63c-4fbc-a046-89b573de2cbb
01/18/2025 17:41:29:INFO:Received: train message 78b97e73-c63c-4fbc-a046-89b573de2cbb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:54:52:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:04:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:04:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6ec35e47-c043-4808-a456-c6b52f423c74
01/18/2025 18:04:44:INFO:Received: evaluate message 6ec35e47-c043-4808-a456-c6b52f423c74
[92mINFO [0m:      Sent reply
01/18/2025 18:09:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:10:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:10:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 16868467-fca5-47c0-82c2-a5f93d5ea4b7
01/18/2025 18:10:03:INFO:Received: train message 16868467-fca5-47c0-82c2-a5f93d5ea4b7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:23:07:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:32:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:32:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5dcb868d-9bb9-4ae0-95e9-93fc33a582c2
01/18/2025 18:32:42:INFO:Received: evaluate message 5dcb868d-9bb9-4ae0-95e9-93fc33a582c2
[92mINFO [0m:      Sent reply
01/18/2025 18:37:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:38:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:38:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b7dcb3f9-47b2-40ea-bf5f-f0f428fc14f3
01/18/2025 18:38:15:INFO:Received: train message b7dcb3f9-47b2-40ea-bf5f-f0f428fc14f3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:51:25:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:01:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:01:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0ac3db7b-ef16-4b81-9df9-943f2cd71584
01/18/2025 19:01:09:INFO:Received: evaluate message 0ac3db7b-ef16-4b81-9df9-943f2cd71584
[92mINFO [0m:      Sent reply
01/18/2025 19:05:48:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:06:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:06:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 95408170-5bf2-4676-84dd-f28bd8182b86
01/18/2025 19:06:34:INFO:Received: train message 95408170-5bf2-4676-84dd-f28bd8182b86
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:19:43:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:30:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:30:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 26f11da8-c84e-4d1a-9588-877454b8ca1e
01/18/2025 19:30:20:INFO:Received: evaluate message 26f11da8-c84e-4d1a-9588-877454b8ca1e

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.17149013839662078
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.13719211071729662
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.10289408303797243
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.06859605535864828
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 19:35:30:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:35:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:35:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a6ba67b0-5055-4eac-acc6-39e7020ea3fe
01/18/2025 19:35:52:INFO:Received: train message a6ba67b0-5055-4eac-acc6-39e7020ea3fe
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:48:56:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:59:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:59:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 558d4907-38b1-4d41-a97a-497e41ed6413
01/18/2025 19:59:59:INFO:Received: evaluate message 558d4907-38b1-4d41-a97a-497e41ed6413
[92mINFO [0m:      Sent reply
01/18/2025 20:05:30:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:06:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:06:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3abd3bec-11f9-4142-9fc0-467715c621da
01/18/2025 20:06:19:INFO:Received: train message 3abd3bec-11f9-4142-9fc0-467715c621da
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:19:30:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:28:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:28:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3aad4d54-8e0e-4f4c-9380-50c44b8add6e
01/18/2025 20:28:57:INFO:Received: evaluate message 3aad4d54-8e0e-4f4c-9380-50c44b8add6e
[92mINFO [0m:      Sent reply
01/18/2025 20:32:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:32:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:32:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 634bf674-9753-41c9-8a06-9a0febd66b67
01/18/2025 20:32:58:INFO:Received: reconnect message 634bf674-9753-41c9-8a06-9a0febd66b67
01/18/2025 20:32:58:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/18/2025 20:32:58:INFO:Disconnect and shut down

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.03429802767932414
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554]}

BaseNM 0.72113037109375
noise multiplier 0.5144704151898623
Noise multiplier before  adjustment: 0.5144704151898623
Noise multiplier before convergence adjustment: 0.5144704151898623
Updated noise multiplier after convergence adjustment: 0.0
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864, 118.1268835067749], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585, 0.5316149818767619], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554, 0.7213411777674161]}



Final client history:
{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864, 118.1268835067749], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585, 0.5316149818767619], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554, 0.7213411777674161]}

