nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/18/2025 06:22:25:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/18/2025 06:22:25:DEBUG:ChannelConnectivity.IDLE
01/18/2025 06:22:25:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/18/2025 06:25:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:25:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 822fb1d4-2fa8-422c-a829-86bcbacaf4bd
01/18/2025 06:25:23:INFO:Received: train message 822fb1d4-2fa8-422c-a829-86bcbacaf4bd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 06:37:29:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:44:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:44:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7b19631e-991b-4a11-b31b-6d53e0603886
01/18/2025 06:44:39:INFO:Received: evaluate message 7b19631e-991b-4a11-b31b-6d53e0603886
[92mINFO [0m:      Sent reply
01/18/2025 06:48:43:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:48:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:48:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e302b213-9f8a-45b7-a371-dd63f67bd01c
01/18/2025 06:48:59:INFO:Received: train message e302b213-9f8a-45b7-a371-dd63f67bd01c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:00:40:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:08:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:08:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c679b9b0-47d8-4478-a6cf-c4e5b09c0209
01/18/2025 07:08:30:INFO:Received: evaluate message c679b9b0-47d8-4478-a6cf-c4e5b09c0209
[92mINFO [0m:      Sent reply
01/18/2025 07:12:32:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:13:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:13:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message db4c3496-390d-4b7f-9a13-ba223f210de9
01/18/2025 07:13:00:INFO:Received: train message db4c3496-390d-4b7f-9a13-ba223f210de9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:25:00:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:32:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:32:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eeada221-224a-4f0f-9b0b-166321060f5c
01/18/2025 07:32:11:INFO:Received: evaluate message eeada221-224a-4f0f-9b0b-166321060f5c
[92mINFO [0m:      Sent reply
01/18/2025 07:36:11:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:36:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:36:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9c2eaba8-9525-4672-95b5-f271044ab545
01/18/2025 07:36:44:INFO:Received: train message 9c2eaba8-9525-4672-95b5-f271044ab545
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:48:23:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:55:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:55:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3232c80d-0cd7-430d-b21c-b05829a3c505
01/18/2025 07:55:52:INFO:Received: evaluate message 3232c80d-0cd7-430d-b21c-b05829a3c505
[92mINFO [0m:      Sent reply
01/18/2025 07:59:48:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:00:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:00:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9335e848-e573-4502-89c0-af3ee7a84fb4
01/18/2025 08:00:17:INFO:Received: train message 9335e848-e573-4502-89c0-af3ee7a84fb4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:12:06:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:19:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:19:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bc9771a9-c21b-49f1-8ea5-c29b4a48210b
01/18/2025 08:19:51:INFO:Received: evaluate message bc9771a9-c21b-49f1-8ea5-c29b4a48210b
[92mINFO [0m:      Sent reply
01/18/2025 08:24:04:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:24:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:24:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8f242826-262a-4653-8046-15d17062bf4d
01/18/2025 08:24:37:INFO:Received: train message 8f242826-262a-4653-8046-15d17062bf4d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:37:10:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:49:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:49:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2a834e2f-5eac-458e-acc1-4e09dd9a5617
01/18/2025 08:49:27:INFO:Received: evaluate message 2a834e2f-5eac-458e-acc1-4e09dd9a5617
[92mINFO [0m:      Sent reply
01/18/2025 08:53:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:54:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:54:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 710f2adf-d432-410e-8de3-e84cb41adb5e
01/18/2025 08:54:00:INFO:Received: train message 710f2adf-d432-410e-8de3-e84cb41adb5e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:07:30:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:19:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:19:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fa7a226e-4d4c-4f10-9bef-9158a54bec34
01/18/2025 09:19:52:INFO:Received: evaluate message fa7a226e-4d4c-4f10-9bef-9158a54bec34
[92mINFO [0m:      Sent reply
01/18/2025 09:24:03:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:24:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:24:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5c3b1adb-7619-42e8-a92d-035f21ef5cdd
01/18/2025 09:24:31:INFO:Received: train message 5c3b1adb-7619-42e8-a92d-035f21ef5cdd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:38:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:49:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:49:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 85d334ae-dcd8-4ccc-a635-d780852923db
01/18/2025 09:49:50:INFO:Received: evaluate message 85d334ae-dcd8-4ccc-a635-d780852923db
[92mINFO [0m:      Sent reply
01/18/2025 09:53:56:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:54:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:54:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d9c69da1-919b-4392-8a23-d19a7110e3eb
01/18/2025 09:54:22:INFO:Received: train message d9c69da1-919b-4392-8a23-d19a7110e3eb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:07:46:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:19:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:19:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c27d42df-caed-42af-bcc6-47e24e470756
01/18/2025 10:19:17:INFO:Received: evaluate message c27d42df-caed-42af-bcc6-47e24e470756
[92mINFO [0m:      Sent reply
01/18/2025 10:23:42:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:24:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:24:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 44735546-3281-435e-8ea4-037213b9ac07
01/18/2025 10:24:02:INFO:Received: train message 44735546-3281-435e-8ea4-037213b9ac07
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:37:44:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:48:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:48:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c16d4c15-86e2-4ff8-a2a6-272c8c9d1a0c
01/18/2025 10:48:24:INFO:Received: evaluate message c16d4c15-86e2-4ff8-a2a6-272c8c9d1a0c
[92mINFO [0m:      Sent reply
01/18/2025 10:52:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:53:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:53:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4372e17f-5318-4448-8ab3-1a8bb67abd61
01/18/2025 10:53:57:INFO:Received: train message 4372e17f-5318-4448-8ab3-1a8bb67abd61
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:06:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:15:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:15:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8a88bd3b-65a9-4d12-8de9-7032d67e9a9f
01/18/2025 11:15:20:INFO:Received: evaluate message 8a88bd3b-65a9-4d12-8de9-7032d67e9a9f
[92mINFO [0m:      Sent reply
01/18/2025 11:19:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:20:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:20:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e52c5cdc-fe4c-427b-9731-4cf2b1a8e824
01/18/2025 11:20:42:INFO:Received: train message e52c5cdc-fe4c-427b-9731-4cf2b1a8e824
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:32:49:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:42:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:42:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 56e96f5d-7828-495f-8aed-3c742152b9e2
01/18/2025 11:42:39:INFO:Received: evaluate message 56e96f5d-7828-495f-8aed-3c742152b9e2
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise']
BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275], 'accuracy': [0.3419250906161901], 'auc': [0.5457303512912005]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585], 'accuracy': [0.3419250906161901, 0.3407168747482884], 'auc': [0.5457303512912005, 0.5868275242355229]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 11:47:34:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:48:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:48:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 23f5840b-c68b-489f-9321-a715d210950a
01/18/2025 11:48:11:INFO:Received: train message 23f5840b-c68b-489f-9321-a715d210950a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:00:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:10:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:10:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 39879146-a769-461e-bf7d-5fcce00d377c
01/18/2025 12:10:41:INFO:Received: evaluate message 39879146-a769-461e-bf7d-5fcce00d377c
[92mINFO [0m:      Sent reply
01/18/2025 12:15:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:16:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:16:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5ebe9ed7-07b0-4ef1-a317-fceb2223b0eb
01/18/2025 12:16:11:INFO:Received: train message 5ebe9ed7-07b0-4ef1-a317-fceb2223b0eb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:28:28:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:39:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:39:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2c36e0d7-49b2-4e94-9703-d233789068f9
01/18/2025 12:39:17:INFO:Received: evaluate message 2c36e0d7-49b2-4e94-9703-d233789068f9
[92mINFO [0m:      Sent reply
01/18/2025 12:44:37:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:45:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:45:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4b7c89fb-192f-4c13-b95c-f5d073537e29
01/18/2025 12:45:27:INFO:Received: train message 4b7c89fb-192f-4c13-b95c-f5d073537e29
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:57:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:10:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:10:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b2b3558e-0081-4264-9273-f144fa25a134
01/18/2025 13:10:13:INFO:Received: evaluate message b2b3558e-0081-4264-9273-f144fa25a134
[92mINFO [0m:      Sent reply
01/18/2025 13:15:49:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:16:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:16:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dcc2889c-d87e-489d-a23a-b3ebaf1d5bde
01/18/2025 13:16:35:INFO:Received: train message dcc2889c-d87e-489d-a23a-b3ebaf1d5bde
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:29:03:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:44:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:44:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c2c178b9-37a8-436b-93ea-0bc89d5ba476
01/18/2025 13:44:06:INFO:Received: evaluate message c2c178b9-37a8-436b-93ea-0bc89d5ba476
[92mINFO [0m:      Sent reply
01/18/2025 13:49:01:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:49:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:49:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6c8b6b69-e1ae-42f0-99f0-eaaa2ebeb85d
01/18/2025 13:49:26:INFO:Received: train message 6c8b6b69-e1ae-42f0-99f0-eaaa2ebeb85d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:01:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:17:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:17:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 21a71dbf-b115-4f7a-a205-bfd959126b3b
01/18/2025 14:17:31:INFO:Received: evaluate message 21a71dbf-b115-4f7a-a205-bfd959126b3b
[92mINFO [0m:      Sent reply
01/18/2025 14:21:36:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:22:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:22:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c5304066-5eef-417d-aa18-efbdaf74f972
01/18/2025 14:22:23:INFO:Received: train message c5304066-5eef-417d-aa18-efbdaf74f972
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:34:53:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:48:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:48:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7df611c5-298d-4ea3-b916-782460c0ec53
01/18/2025 14:48:54:INFO:Received: evaluate message 7df611c5-298d-4ea3-b916-782460c0ec53
[92mINFO [0m:      Sent reply
01/18/2025 14:52:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:53:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:53:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ca55e6f7-0bf3-410f-acac-9647701ce98e
01/18/2025 14:53:22:INFO:Received: train message ca55e6f7-0bf3-410f-acac-9647701ce98e

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.531806864310056
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.4963530733560522
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.4608992824020485
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.4254454914480448
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088]}

BaseNM 0.72113037109375
noise multiplier /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:05:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:17:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:17:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ec32ed49-6add-4f19-b9fa-59d839bb788e
01/18/2025 15:17:10:INFO:Received: evaluate message ec32ed49-6add-4f19-b9fa-59d839bb788e
[92mINFO [0m:      Sent reply
01/18/2025 15:21:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:22:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:22:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7dc256b5-31fc-435c-bc95-274fe9673fcd
01/18/2025 15:22:09:INFO:Received: train message 7dc256b5-31fc-435c-bc95-274fe9673fcd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:34:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:45:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:45:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e0e2fd0a-086b-4fb4-9eec-a33f8d091478
01/18/2025 15:45:25:INFO:Received: evaluate message e0e2fd0a-086b-4fb4-9eec-a33f8d091478
[92mINFO [0m:      Sent reply
01/18/2025 15:49:43:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:50:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:50:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 099a24b9-2702-4f26-b247-4aa1f25be4a2
01/18/2025 15:50:23:INFO:Received: train message 099a24b9-2702-4f26-b247-4aa1f25be4a2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:02:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:13:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:13:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 48f4f328-836a-463a-8757-5695708a87b0
01/18/2025 16:13:46:INFO:Received: evaluate message 48f4f328-836a-463a-8757-5695708a87b0
[92mINFO [0m:      Sent reply
01/18/2025 16:18:30:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:19:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:19:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 88d81770-c0ab-4a6d-a27d-12f1187d7d33
01/18/2025 16:19:02:INFO:Received: train message 88d81770-c0ab-4a6d-a27d-12f1187d7d33
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:31:06:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:41:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:41:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9afc6d5e-98ee-4ff7-b078-8e362f08e60a
01/18/2025 16:41:43:INFO:Received: evaluate message 9afc6d5e-98ee-4ff7-b078-8e362f08e60a
[92mINFO [0m:      Sent reply
01/18/2025 16:46:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:46:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:46:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e99d0d05-a2e1-414d-9cab-8ec588bea745
01/18/2025 16:46:49:INFO:Received: train message e99d0d05-a2e1-414d-9cab-8ec588bea745
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:58:56:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:08:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:08:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5582eab1-f854-4b86-9274-cbffafce2e86
01/18/2025 17:08:56:INFO:Received: evaluate message 5582eab1-f854-4b86-9274-cbffafce2e86
[92mINFO [0m:      Sent reply
01/18/2025 17:13:16:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:14:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:14:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 79438346-98ca-442a-bce0-8d59a9fb7e38
01/18/2025 17:14:17:INFO:Received: train message 79438346-98ca-442a-bce0-8d59a9fb7e38
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:26:23:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:36:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:36:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ae2e8493-9b85-4136-a9d6-d47fd12ce370
01/18/2025 17:36:26:INFO:Received: evaluate message ae2e8493-9b85-4136-a9d6-d47fd12ce370
0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.3899917004940411
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.35453790954003733
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.3190841185860336
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.28363032763202983
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.2481765366780261
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.2127227457240224
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 17:40:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:41:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:41:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9c6b0424-d950-49cf-a87e-03370c2788cc
01/18/2025 17:41:45:INFO:Received: train message 9c6b0424-d950-49cf-a87e-03370c2788cc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:53:53:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:04:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:04:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5596ef4e-8a57-481f-b7b0-289cca43aee1
01/18/2025 18:04:28:INFO:Received: evaluate message 5596ef4e-8a57-481f-b7b0-289cca43aee1
[92mINFO [0m:      Sent reply
01/18/2025 18:08:52:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:09:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:09:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eaf595e1-65fd-4982-a4fd-ffb5896ac3f7
01/18/2025 18:09:50:INFO:Received: train message eaf595e1-65fd-4982-a4fd-ffb5896ac3f7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:21:52:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:32:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:32:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5abea3d9-b79f-403c-b839-6578f6ad01c2
01/18/2025 18:32:42:INFO:Received: evaluate message 5abea3d9-b79f-403c-b839-6578f6ad01c2
[92mINFO [0m:      Sent reply
01/18/2025 18:37:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:38:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:38:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8575d1bc-6d9f-4cce-8915-2d10e22bfc85
01/18/2025 18:38:13:INFO:Received: train message 8575d1bc-6d9f-4cce-8915-2d10e22bfc85
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:50:19:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:01:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:01:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 94f6484f-84b3-4f46-93e4-5066ea27c49c
01/18/2025 19:01:24:INFO:Received: evaluate message 94f6484f-84b3-4f46-93e4-5066ea27c49c
[92mINFO [0m:      Sent reply
01/18/2025 19:05:53:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:06:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:06:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1ee9d75b-272c-449d-8c42-a488ad194415
01/18/2025 19:06:38:INFO:Received: train message 1ee9d75b-272c-449d-8c42-a488ad194415
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:18:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:30:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:30:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2c55291b-4855-4c18-8af1-d93e612e17c3
01/18/2025 19:30:21:INFO:Received: evaluate message 2c55291b-4855-4c18-8af1-d93e612e17c3

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.17726895477001867
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.14181516381601494
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.10636137286201117
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.07090758190800744
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 19:34:41:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:35:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:35:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 46911bf7-8611-4325-b8dc-f8229adde5ad
01/18/2025 19:35:44:INFO:Received: train message 46911bf7-8611-4325-b8dc-f8229adde5ad
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:47:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:00:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:00:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 84eb4bd0-9e16-4def-9dae-a9c053f6f6f2
01/18/2025 20:00:24:INFO:Received: evaluate message 84eb4bd0-9e16-4def-9dae-a9c053f6f6f2
[92mINFO [0m:      Sent reply
01/18/2025 20:04:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:06:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:06:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e751c38e-770c-42ba-a57b-65c1d719055c
01/18/2025 20:06:04:INFO:Received: train message e751c38e-770c-42ba-a57b-65c1d719055c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:18:06:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:28:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:28:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message acb01ef0-e7b5-4145-bf53-1c17ef70e296
01/18/2025 20:28:36:INFO:Received: evaluate message acb01ef0-e7b5-4145-bf53-1c17ef70e296
[92mINFO [0m:      Sent reply
01/18/2025 20:32:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:32:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:32:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message ca4979d2-8df9-40ad-8c32-cc9f9ae54d57
01/18/2025 20:32:58:INFO:Received: reconnect message ca4979d2-8df9-40ad-8c32-cc9f9ae54d57
01/18/2025 20:32:58:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/18/2025 20:32:58:INFO:Disconnect and shut down

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.03545379095400372
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554]}

BaseNM 0.72113037109375
noise multiplier 0.531806864310056
Noise multiplier before  adjustment: 0.531806864310056
Noise multiplier before convergence adjustment: 0.531806864310056
Updated noise multiplier after convergence adjustment: 0.0
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864, 118.1268835067749], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585, 0.5316149818767619], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554, 0.7213411777674161]}



Final client history:
{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864, 118.1268835067749], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585, 0.5316149818767619], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554, 0.7213411777674161]}

