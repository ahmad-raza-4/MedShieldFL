nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/18/2025 06:20:16:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/18/2025 06:20:16:DEBUG:ChannelConnectivity.IDLE
01/18/2025 06:20:16:DEBUG:ChannelConnectivity.CONNECTING
01/18/2025 06:20:16:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/18/2025 06:25:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:25:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 50bf5056-fd67-4e1e-adf9-c682b9b81e2f
01/18/2025 06:25:23:INFO:Received: train message 50bf5056-fd67-4e1e-adf9-c682b9b81e2f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 06:29:43:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:44:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:44:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 417957c8-d64f-40a5-bf4e-8ffab7f4bed6
01/18/2025 06:44:38:INFO:Received: evaluate message 417957c8-d64f-40a5-bf4e-8ffab7f4bed6
[92mINFO [0m:      Sent reply
01/18/2025 06:48:44:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:49:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:49:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 865cb982-54ce-4375-81be-afa7957944b1
01/18/2025 06:49:16:INFO:Received: train message 865cb982-54ce-4375-81be-afa7957944b1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 06:53:37:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:08:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:08:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 571c8ee9-72b1-41cf-a087-5da899449171
01/18/2025 07:08:31:INFO:Received: evaluate message 571c8ee9-72b1-41cf-a087-5da899449171
[92mINFO [0m:      Sent reply
01/18/2025 07:12:34:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:13:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:13:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 06d375f4-807d-472a-bc37-dc8562c8d75b
01/18/2025 07:13:06:INFO:Received: train message 06d375f4-807d-472a-bc37-dc8562c8d75b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:17:20:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:32:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:32:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3c32e150-a639-4111-996d-ee0b14ebbdba
01/18/2025 07:32:00:INFO:Received: evaluate message 3c32e150-a639-4111-996d-ee0b14ebbdba
[92mINFO [0m:      Sent reply
01/18/2025 07:35:51:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:36:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:36:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message edacd770-d5bf-414c-b63a-7f34606904a2
01/18/2025 07:36:40:INFO:Received: train message edacd770-d5bf-414c-b63a-7f34606904a2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:40:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:55:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:55:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ab451f78-2983-4926-b710-5bf02abbf1f4
01/18/2025 07:55:38:INFO:Received: evaluate message ab451f78-2983-4926-b710-5bf02abbf1f4
[92mINFO [0m:      Sent reply
01/18/2025 07:59:31:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:00:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:00:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d0ec6380-d596-47d4-b811-90fa8abbb49b
01/18/2025 08:00:09:INFO:Received: train message d0ec6380-d596-47d4-b811-90fa8abbb49b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:04:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:19:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:19:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 039c6dbc-3fa5-4dcc-8a6a-2c02122b44fe
01/18/2025 08:19:44:INFO:Received: evaluate message 039c6dbc-3fa5-4dcc-8a6a-2c02122b44fe
[92mINFO [0m:      Sent reply
01/18/2025 08:24:00:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:24:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:24:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4092a5bb-ff88-439c-beb7-11eebacee918
01/18/2025 08:24:29:INFO:Received: train message 4092a5bb-ff88-439c-beb7-11eebacee918
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:28:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:49:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:49:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c8b786b2-dd17-4354-a897-99d0db0a115a
01/18/2025 08:49:05:INFO:Received: evaluate message c8b786b2-dd17-4354-a897-99d0db0a115a
[92mINFO [0m:      Sent reply
01/18/2025 08:52:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:54:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:54:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 74b31e35-9944-4823-8d00-24a15db49dfb
01/18/2025 08:54:13:INFO:Received: train message 74b31e35-9944-4823-8d00-24a15db49dfb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:58:34:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:19:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:19:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 59b8f593-0914-4fd7-8fd9-525f02226d00
01/18/2025 09:19:52:INFO:Received: evaluate message 59b8f593-0914-4fd7-8fd9-525f02226d00
[92mINFO [0m:      Sent reply
01/18/2025 09:24:03:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:24:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:24:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 89f4b018-a3e8-4e9e-a0fc-7d85f2a7aa12
01/18/2025 09:24:20:INFO:Received: train message 89f4b018-a3e8-4e9e-a0fc-7d85f2a7aa12
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:28:22:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:49:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:49:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8bc4a18b-397b-46d7-9abf-0fc9d3a83404
01/18/2025 09:49:30:INFO:Received: evaluate message 8bc4a18b-397b-46d7-9abf-0fc9d3a83404
[92mINFO [0m:      Sent reply
01/18/2025 09:53:15:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:54:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:54:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 20df64e7-0013-42da-8adc-fe7bde49a316
01/18/2025 09:54:29:INFO:Received: train message 20df64e7-0013-42da-8adc-fe7bde49a316
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:59:00:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:19:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:19:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e30240e4-a80d-4a97-98bd-fded9f9ebd98
01/18/2025 10:19:15:INFO:Received: evaluate message e30240e4-a80d-4a97-98bd-fded9f9ebd98
[92mINFO [0m:      Sent reply
01/18/2025 10:23:43:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:24:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:24:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 532f9923-ed5f-4d35-9a15-28f0b2753185
01/18/2025 10:24:09:INFO:Received: train message 532f9923-ed5f-4d35-9a15-28f0b2753185
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:28:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:48:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:48:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 580eaa23-8512-47ac-b2ff-4d5c5f803897
01/18/2025 10:48:31:INFO:Received: evaluate message 580eaa23-8512-47ac-b2ff-4d5c5f803897
[92mINFO [0m:      Sent reply
01/18/2025 10:53:10:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:53:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:53:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a0e844a6-1f13-43ac-ab16-80bcb30fcd0f
01/18/2025 10:53:39:INFO:Received: train message a0e844a6-1f13-43ac-ab16-80bcb30fcd0f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:57:49:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:15:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:15:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ebb78d5c-d794-4d02-b473-f5735823eab5
01/18/2025 11:15:20:INFO:Received: evaluate message ebb78d5c-d794-4d02-b473-f5735823eab5
[92mINFO [0m:      Sent reply
01/18/2025 11:19:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:20:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:20:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a24e52c2-e9f8-4f10-8c94-d9668cd161bf
01/18/2025 11:20:31:INFO:Received: train message a24e52c2-e9f8-4f10-8c94-d9668cd161bf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:24:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:42:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:42:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 774a3311-9bec-4adc-82f9-34a55f522ac4
01/18/2025 11:42:35:INFO:Received: evaluate message 774a3311-9bec-4adc-82f9-34a55f522ac4
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise']
BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275], 'accuracy': [0.3419250906161901], 'auc': [0.5457303512912005]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585], 'accuracy': [0.3419250906161901, 0.3407168747482884], 'auc': [0.5457303512912005, 0.5868275242355229]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 11:47:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:47:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:47:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 11469a2e-d0dd-434d-9b2a-907b0ac9f666
01/18/2025 11:47:57:INFO:Received: train message 11469a2e-d0dd-434d-9b2a-907b0ac9f666
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:52:26:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:10:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:10:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0ddf0297-fb43-4409-927a-794cb2af6042
01/18/2025 12:10:42:INFO:Received: evaluate message 0ddf0297-fb43-4409-927a-794cb2af6042
[92mINFO [0m:      Sent reply
01/18/2025 12:15:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:16:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:16:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a882fd70-c00b-474b-a2ec-a3d0226f00b9
01/18/2025 12:16:31:INFO:Received: train message a882fd70-c00b-474b-a2ec-a3d0226f00b9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:21:04:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:39:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:39:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 62bd6750-9513-44fb-b5fa-dc5be66811f7
01/18/2025 12:39:17:INFO:Received: evaluate message 62bd6750-9513-44fb-b5fa-dc5be66811f7
[92mINFO [0m:      Sent reply
01/18/2025 12:44:34:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:45:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:45:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b7b270b1-8bee-4c1e-a658-405195e9b86a
01/18/2025 12:45:46:INFO:Received: train message b7b270b1-8bee-4c1e-a658-405195e9b86a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:50:31:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:10:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:10:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d8e38bfb-12e9-476b-b2a0-a0c738d686e5
01/18/2025 13:10:10:INFO:Received: evaluate message d8e38bfb-12e9-476b-b2a0-a0c738d686e5
[92mINFO [0m:      Sent reply
01/18/2025 13:15:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:16:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:16:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b6822761-c751-47f5-9df1-6e87a5ed2f15
01/18/2025 13:16:23:INFO:Received: train message b6822761-c751-47f5-9df1-6e87a5ed2f15
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:20:55:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:43:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:43:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cfe91f5b-25f5-429e-8ff0-bc5dfaadc8ff
01/18/2025 13:43:50:INFO:Received: evaluate message cfe91f5b-25f5-429e-8ff0-bc5dfaadc8ff
[92mINFO [0m:      Sent reply
01/18/2025 13:48:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:49:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:49:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message afb8978d-363e-45ea-bf71-6034093678ae
01/18/2025 13:49:47:INFO:Received: train message afb8978d-363e-45ea-bf71-6034093678ae
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:54:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:17:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:17:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5aabe4d-54ff-4196-8315-fb2291df28d4
01/18/2025 14:17:48:INFO:Received: evaluate message b5aabe4d-54ff-4196-8315-fb2291df28d4
[92mINFO [0m:      Sent reply
01/18/2025 14:21:53:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:22:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:22:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 554acaef-fb6c-44c2-8f03-383c77617cfc
01/18/2025 14:22:25:INFO:Received: train message 554acaef-fb6c-44c2-8f03-383c77617cfc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:26:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:48:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:48:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3d42152f-7fae-4e54-b74a-cd6036db3edf
01/18/2025 14:48:52:INFO:Received: evaluate message 3d42152f-7fae-4e54-b74a-cd6036db3edf
[92mINFO [0m:      Sent reply
01/18/2025 14:52:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:53:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:53:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2e2473d1-570a-4912-8b47-684b81763c9d
01/18/2025 14:53:31:INFO:Received: train message 2e2473d1-570a-4912-8b47-684b81763c9d

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.5282484986819327
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.49303193210313717
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.45781536552434166
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.4225987989455462
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088]}

BaseNM 0.72113037109375
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:57:51:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:17:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:17:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 29b2f9d0-a8c3-4abf-8d68-44b6810da368
01/18/2025 15:17:21:INFO:Received: evaluate message 29b2f9d0-a8c3-4abf-8d68-44b6810da368
[92mINFO [0m:      Sent reply
01/18/2025 15:21:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:22:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:22:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c38db4a3-0b99-436d-90d2-9e88851a5d16
01/18/2025 15:22:10:INFO:Received: train message c38db4a3-0b99-436d-90d2-9e88851a5d16
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:26:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:45:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:45:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 21c4acf8-e925-4c36-8680-168fc25fb779
01/18/2025 15:45:41:INFO:Received: evaluate message 21c4acf8-e925-4c36-8680-168fc25fb779
[92mINFO [0m:      Sent reply
01/18/2025 15:50:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:50:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:50:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4a5da850-1a0f-46f5-a8c4-f1bd18e83b42
01/18/2025 15:50:44:INFO:Received: train message 4a5da850-1a0f-46f5-a8c4-f1bd18e83b42
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:55:16:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:14:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:14:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8ec6cce4-6bf7-45a4-bd6f-fb4c5a4391c9
01/18/2025 16:14:00:INFO:Received: evaluate message 8ec6cce4-6bf7-45a4-bd6f-fb4c5a4391c9
[92mINFO [0m:      Sent reply
01/18/2025 16:18:46:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:19:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:19:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c277b157-0323-4d10-851c-4d0fd2ccc82e
01/18/2025 16:19:26:INFO:Received: train message c277b157-0323-4d10-851c-4d0fd2ccc82e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:23:53:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:42:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:42:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c2d9454e-d4b9-4d8a-a3bb-25be25e9ac21
01/18/2025 16:42:02:INFO:Received: evaluate message c2d9454e-d4b9-4d8a-a3bb-25be25e9ac21
[92mINFO [0m:      Sent reply
01/18/2025 16:46:31:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:47:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:47:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 20ec00cb-41cd-43df-8e23-80f89eb7ba85
01/18/2025 16:47:08:INFO:Received: train message 20ec00cb-41cd-43df-8e23-80f89eb7ba85
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:51:34:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:08:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:08:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9c372b76-67aa-4b41-af1a-81b2d5e4a37a
01/18/2025 17:08:48:INFO:Received: evaluate message 9c372b76-67aa-4b41-af1a-81b2d5e4a37a
[92mINFO [0m:      Sent reply
01/18/2025 17:13:01:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:14:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:14:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c9f05db5-8871-4cc0-83c6-1f708d2fe6fe
01/18/2025 17:14:12:INFO:Received: train message c9f05db5-8871-4cc0-83c6-1f708d2fe6fe
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:18:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:36:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:36:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 78c15634-799a-40cd-b8c6-749fb2c23009
01/18/2025 17:36:26:INFO:Received: evaluate message 78c15634-799a-40cd-b8c6-749fb2c23009
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.3873822323667507
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.35216566578795516
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.3169490992091596
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.2817325326303641
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.24651596605156859
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.2112993994727731
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 17:40:49:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:41:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:41:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7aa5113b-8213-481f-ada6-1dd29ef966ab
01/18/2025 17:41:42:INFO:Received: train message 7aa5113b-8213-481f-ada6-1dd29ef966ab
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:46:00:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:04:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:04:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1141b5e5-cf77-4248-ba81-b1fc69505aeb
01/18/2025 18:04:36:INFO:Received: evaluate message 1141b5e5-cf77-4248-ba81-b1fc69505aeb
[92mINFO [0m:      Sent reply
01/18/2025 18:08:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:09:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:09:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bd55bd27-ac64-4d4d-9cf2-41ee9caddc68
01/18/2025 18:09:42:INFO:Received: train message bd55bd27-ac64-4d4d-9cf2-41ee9caddc68
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:13:29:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:32:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:32:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 351a878a-89a8-4c77-a381-3f3746cbb9bf
01/18/2025 18:32:37:INFO:Received: evaluate message 351a878a-89a8-4c77-a381-3f3746cbb9bf
[92mINFO [0m:      Sent reply
01/18/2025 18:37:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:38:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:38:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 64ef5cba-bd16-4cc9-aa0a-f151b53a1354
01/18/2025 18:38:06:INFO:Received: train message 64ef5cba-bd16-4cc9-aa0a-f151b53a1354
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:42:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:01:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:01:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ec0cc3b1-9d8e-4d67-ac7f-3206225b1b59
01/18/2025 19:01:18:INFO:Received: evaluate message ec0cc3b1-9d8e-4d67-ac7f-3206225b1b59
[92mINFO [0m:      Sent reply
01/18/2025 19:05:48:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:06:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:06:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 27254bf2-8568-474b-8e13-10e2e16ae66a
01/18/2025 19:06:27:INFO:Received: train message 27254bf2-8568-474b-8e13-10e2e16ae66a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:10:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:30:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:30:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 43dd5edf-8d8a-475b-ae7f-b2ff17f579b2
01/18/2025 19:30:16:INFO:Received: evaluate message 43dd5edf-8d8a-475b-ae7f-b2ff17f579b2

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.17608283289397758
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.14086626631518206
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.10564969973638652
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.070433133157591
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 19:34:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:36:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:36:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cf43270e-09e7-4e00-8fbd-ca6ed22af214
01/18/2025 19:36:07:INFO:Received: train message cf43270e-09e7-4e00-8fbd-ca6ed22af214
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:40:30:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:00:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:00:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 91227b2a-fd34-48ee-b93e-14e666eb25a0
01/18/2025 20:00:12:INFO:Received: evaluate message 91227b2a-fd34-48ee-b93e-14e666eb25a0
[92mINFO [0m:      Sent reply
01/18/2025 20:04:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:06:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:06:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 59f7afa3-5934-4d09-a3c7-f5583150f1fe
01/18/2025 20:06:22:INFO:Received: train message 59f7afa3-5934-4d09-a3c7-f5583150f1fe
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:10:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:28:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:28:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4bd3da8a-a9b6-4fef-a9b2-5a510b7b92f3
01/18/2025 20:28:43:INFO:Received: evaluate message 4bd3da8a-a9b6-4fef-a9b2-5a510b7b92f3
[92mINFO [0m:      Sent reply
01/18/2025 20:32:40:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:32:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:32:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message acb65e69-1aac-4fea-840c-aa53f82a60e7
01/18/2025 20:32:58:INFO:Received: reconnect message acb65e69-1aac-4fea-840c-aa53f82a60e7
01/18/2025 20:32:58:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/18/2025 20:32:58:INFO:Disconnect and shut down

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.0352165665787955
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554]}

BaseNM 0.72113037109375
noise multiplier 0.5282484986819327
Noise multiplier before  adjustment: 0.5282484986819327
Noise multiplier before convergence adjustment: 0.5282484986819327
Updated noise multiplier after convergence adjustment: 0.0
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864, 118.1268835067749], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585, 0.5316149818767619], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554, 0.7213411777674161]}



Final client history:
{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864, 118.1268835067749], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585, 0.5316149818767619], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554, 0.7213411777674161]}

