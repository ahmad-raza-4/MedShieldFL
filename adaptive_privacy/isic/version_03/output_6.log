nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/28/2025 21:54:31:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/28/2025 21:54:31:DEBUG:ChannelConnectivity.IDLE
01/28/2025 21:54:31:DEBUG:ChannelConnectivity.CONNECTING
01/28/2025 21:54:31:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/28/2025 21:54:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 21:54:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message a3b054b9-734c-4352-b6a4-eadc07f8d108
01/28/2025 21:54:31:INFO:Received: get_parameters message a3b054b9-734c-4352-b6a4-eadc07f8d108
[92mINFO [0m:      Sent reply
01/28/2025 21:54:35:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:01:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:01:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ff52ca69-9829-4dd0-b804-ee13fef037f5
01/28/2025 22:01:19:INFO:Received: train message ff52ca69-9829-4dd0-b804-ee13fef037f5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 22:03:12:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:21:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:21:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 36b42ab2-305c-4be0-82de-fc2b4dea88ff
01/28/2025 22:21:45:INFO:Received: evaluate message 36b42ab2-305c-4be0-82de-fc2b4dea88ff
[92mINFO [0m:      Sent reply
01/28/2025 22:25:30:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:26:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:26:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 47865edd-7872-4fc3-8f48-69d321c0ef13
01/28/2025 22:26:27:INFO:Received: train message 47865edd-7872-4fc3-8f48-69d321c0ef13
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 22:29:00:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:48:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:48:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a6942a0c-6cab-4f03-8a46-a4645ec67bc5
01/28/2025 22:48:16:INFO:Received: evaluate message a6942a0c-6cab-4f03-8a46-a4645ec67bc5
[92mINFO [0m:      Sent reply
01/28/2025 22:51:54:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:52:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:52:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f93a851c-5904-48f2-82b9-f6bd164c3475
01/28/2025 22:52:53:INFO:Received: train message f93a851c-5904-48f2-82b9-f6bd164c3475
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 22:55:22:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:13:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:13:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fd2a05f6-9bee-4395-a850-150870ec3dd5
01/28/2025 23:13:22:INFO:Received: evaluate message fd2a05f6-9bee-4395-a850-150870ec3dd5
[92mINFO [0m:      Sent reply
01/28/2025 23:17:24:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:17:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:17:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3f7f2577-ed03-47a0-84b3-ba91cab3ee08
01/28/2025 23:17:54:INFO:Received: train message 3f7f2577-ed03-47a0-84b3-ba91cab3ee08
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 23:20:28:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:38:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:38:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cff50d71-051d-4221-b4db-616a322efd93
01/28/2025 23:38:38:INFO:Received: evaluate message cff50d71-051d-4221-b4db-616a322efd93
[92mINFO [0m:      Sent reply
01/28/2025 23:42:33:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:43:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:43:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 42f89fe9-4ba8-468f-a82b-48cd60847803
01/28/2025 23:43:08:INFO:Received: train message 42f89fe9-4ba8-468f-a82b-48cd60847803
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 23:45:38:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:04:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:04:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 851d224e-e36a-4fa3-8019-d8a4d023edae
01/29/2025 00:04:53:INFO:Received: evaluate message 851d224e-e36a-4fa3-8019-d8a4d023edae
[92mINFO [0m:      Sent reply
01/29/2025 00:08:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:09:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:09:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3c1722ee-fc07-4cbc-a458-850a73fca58a
01/29/2025 00:09:32:INFO:Received: train message 3c1722ee-fc07-4cbc-a458-850a73fca58a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 00:12:21:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:31:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:31:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a25fd353-77b5-4bf5-bbd7-07419d54393f
01/29/2025 00:31:52:INFO:Received: evaluate message a25fd353-77b5-4bf5-bbd7-07419d54393f
[92mINFO [0m:      Sent reply
01/29/2025 00:35:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:36:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:36:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5972f9f5-f3b9-45f7-9a6e-7b3132b7e278
01/29/2025 00:36:14:INFO:Received: train message 5972f9f5-f3b9-45f7-9a6e-7b3132b7e278
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007], 'accuracy': [0.5823600483286347], 'auc': [0.844982871338986]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152], 'accuracy': [0.5823600483286347, 0.6182037857430528], 'auc': [0.844982871338986, 0.8747687441438666]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 00:38:16:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:58:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:58:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message abd44cbb-4c37-4486-937a-5ff4341f9bf1
01/29/2025 00:58:10:INFO:Received: evaluate message abd44cbb-4c37-4486-937a-5ff4341f9bf1
[92mINFO [0m:      Sent reply
01/29/2025 01:02:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:02:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:02:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3557570b-a23e-4cea-bbf5-e49cd2c0ae2d
01/29/2025 01:02:55:INFO:Received: train message 3557570b-a23e-4cea-bbf5-e49cd2c0ae2d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 01:05:33:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:28:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:28:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 35d7b5d2-3c43-4f2a-9c40-7904126624d2
01/29/2025 01:28:51:INFO:Received: evaluate message 35d7b5d2-3c43-4f2a-9c40-7904126624d2
[92mINFO [0m:      Sent reply
01/29/2025 01:32:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:33:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:33:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e2ec3f63-5440-4385-98b1-683da3c5793f
01/29/2025 01:33:19:INFO:Received: train message e2ec3f63-5440-4385-98b1-683da3c5793f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 01:36:01:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:00:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:00:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message de2ac4ff-98fa-4ac9-8618-7968617405e7
01/29/2025 02:00:57:INFO:Received: evaluate message de2ac4ff-98fa-4ac9-8618-7968617405e7
[92mINFO [0m:      Sent reply
01/29/2025 02:04:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:05:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:05:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6b0dce16-0305-437f-8fd6-4a7f09718202
01/29/2025 02:05:33:INFO:Received: train message 6b0dce16-0305-437f-8fd6-4a7f09718202
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 02:08:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:29:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:29:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 820e3109-c151-4a73-869f-63ea1f464d82
01/29/2025 02:29:25:INFO:Received: evaluate message 820e3109-c151-4a73-869f-63ea1f464d82
[92mINFO [0m:      Sent reply
01/29/2025 02:32:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:33:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:33:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e8c89fab-bd00-4223-9f46-eb1e17f96515
01/29/2025 02:33:56:INFO:Received: train message e8c89fab-bd00-4223-9f46-eb1e17f96515
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 02:36:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:01:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:01:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 01dddac2-e0cf-465f-a196-266862350f04
01/29/2025 03:01:29:INFO:Received: evaluate message 01dddac2-e0cf-465f-a196-266862350f04
[92mINFO [0m:      Sent reply
01/29/2025 03:05:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:05:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:05:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4f03dfac-4596-4a8c-9902-e129d7b00831
01/29/2025 03:05:49:INFO:Received: train message 4f03dfac-4596-4a8c-9902-e129d7b00831
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 03:08:07:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:26:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:26:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 27080a0c-3099-45ec-b2f1-aa23a853f37b
01/29/2025 03:26:45:INFO:Received: evaluate message 27080a0c-3099-45ec-b2f1-aa23a853f37b
[92mINFO [0m:      Sent reply
01/29/2025 03:30:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:31:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:31:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 23dce76a-605e-4c3f-975f-0c66ee014e22
01/29/2025 03:31:06:INFO:Received: train message 23dce76a-605e-4c3f-975f-0c66ee014e22
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 03:32:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:55:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:55:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message de8c86d1-68fb-44f7-92b7-2f0a8822a859
01/29/2025 03:55:22:INFO:Received: evaluate message de8c86d1-68fb-44f7-92b7-2f0a8822a859
[92mINFO [0m:      Sent reply
01/29/2025 03:59:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:59:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:59:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7f9bf6da-c332-4614-ba79-fafe125bcbf6
01/29/2025 03:59:42:INFO:Received: train message 7f9bf6da-c332-4614-ba79-fafe125bcbf6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 04:02:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:20:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:20:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 94cbca34-9975-4225-af36-9f66dd83cceb
01/29/2025 04:20:21:INFO:Received: evaluate message 94cbca34-9975-4225-af36-9f66dd83cceb
[92mINFO [0m:      Sent reply
01/29/2025 04:24:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:24:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:24:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3ce82d27-054b-4c57-b93d-50e7ac0cff2a
01/29/2025 04:24:33:INFO:Received: train message 3ce82d27-054b-4c57-b93d-50e7ac0cff2a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 04:26:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:47:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:47:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0ed3aa13-feac-44a1-9804-d8050dd22e68
01/29/2025 04:47:38:INFO:Received: evaluate message 0ed3aa13-feac-44a1-9804-d8050dd22e68
[92mINFO [0m:      Sent reply
01/29/2025 04:51:38:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:52:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:52:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2a3baf21-d9bf-4581-bb04-9f910e1d4b12
01/29/2025 04:52:08:INFO:Received: train message 2a3baf21-d9bf-4581-bb04-9f910e1d4b12
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 04:54:44:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:13:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:13:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 55785698-d2a8-4d29-b2f7-1632143ac0ec
01/29/2025 05:13:12:INFO:Received: evaluate message 55785698-d2a8-4d29-b2f7-1632143ac0ec
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 05:17:03:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:17:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:17:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 08e7998e-c952-487f-9c16-2cfee86670da
01/29/2025 05:17:33:INFO:Received: train message 08e7998e-c952-487f-9c16-2cfee86670da
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 05:20:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:40:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:40:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3df09912-3320-44eb-8cab-e9575d4996e6
01/29/2025 05:40:12:INFO:Received: evaluate message 3df09912-3320-44eb-8cab-e9575d4996e6
[92mINFO [0m:      Sent reply
01/29/2025 05:43:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:44:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:44:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a2935f83-5285-4c7f-8fc9-a8d327d57e09
01/29/2025 05:44:36:INFO:Received: train message a2935f83-5285-4c7f-8fc9-a8d327d57e09
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 05:46:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 06:10:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 06:10:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5db850c-e3e1-40fb-a68b-503a941ab008
01/29/2025 06:10:07:INFO:Received: evaluate message b5db850c-e3e1-40fb-a68b-503a941ab008
[92mINFO [0m:      Sent reply
01/29/2025 06:15:42:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 06:16:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 06:16:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ad62f9f6-2424-477b-9e50-164770459e75
01/29/2025 06:16:19:INFO:Received: train message ad62f9f6-2424-477b-9e50-164770459e75
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 06:19:43:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:01:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:01:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 43ecb2d0-4f4a-434b-9c31-5d029425e2f4
01/29/2025 07:01:59:INFO:Received: evaluate message 43ecb2d0-4f4a-434b-9c31-5d029425e2f4
[92mINFO [0m:      Sent reply
01/29/2025 07:07:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:08:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:08:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 40129402-818d-43d9-bfea-50c86907d1c2
01/29/2025 07:08:13:INFO:Received: train message 40129402-818d-43d9-bfea-50c86907d1c2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 07:11:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:56:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:56:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7e324993-9f08-4bea-ae36-daf418a18708
01/29/2025 07:56:55:INFO:Received: evaluate message 7e324993-9f08-4bea-ae36-daf418a18708

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826, 1.3427717304978695], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846, 0.6512283527990335], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434, 0.9055308040441616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009605006780475378, 0.003906686790287495, 0.00012848229380324483, 3.58792494807858e-05, 0.0002883541164919734, 0.00015977041039150208, 0.000180816714419052, 5.813915777252987e-05]
Noise Multiplier after list and tensor:  0.0007148286763367651
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
[92mINFO [0m:      Sent reply
01/29/2025 08:02:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:04:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:04:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 813a6154-e580-46dd-b826-aa55a98094b2
01/29/2025 08:04:09:INFO:Received: train message 813a6154-e580-46dd-b826-aa55a98094b2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:07:33:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:53:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:53:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e6a54864-4df2-45e8-b745-e70560236ac4
01/29/2025 08:53:59:INFO:Received: evaluate message e6a54864-4df2-45e8-b745-e70560236ac4
[92mINFO [0m:      Sent reply
01/29/2025 09:00:43:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:01:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:01:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a674a12a-6ba5-4640-9bb0-c919a38eab50
01/29/2025 09:01:47:INFO:Received: train message a674a12a-6ba5-4640-9bb0-c919a38eab50
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:05:12:INFO:Sent reply
nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/30/2025 04:35:54:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/30/2025 04:35:54:DEBUG:ChannelConnectivity.IDLE
01/30/2025 04:35:54:DEBUG:ChannelConnectivity.CONNECTING
01/30/2025 04:35:54:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/30/2025 04:45:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:45:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 525af3fe-9d12-446a-96ed-4bc485a41cad
01/30/2025 04:45:45:INFO:Received: train message 525af3fe-9d12-446a-96ed-4bc485a41cad
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:48:36:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:17:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:17:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 906fe282-fd6d-4e31-ae27-a5e7f2627f1d
01/30/2025 05:17:47:INFO:Received: evaluate message 906fe282-fd6d-4e31-ae27-a5e7f2627f1d
[92mINFO [0m:      Sent reply
01/30/2025 05:22:22:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:22:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:22:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dc818d18-d0c6-4670-970d-0d8f76e361a3
01/30/2025 05:22:52:INFO:Received: train message dc818d18-d0c6-4670-970d-0d8f76e361a3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:25:54:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:05:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:05:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8afb8d6d-3bc2-4356-a5df-a60c06058ce5
01/30/2025 06:05:01:INFO:Received: evaluate message 8afb8d6d-3bc2-4356-a5df-a60c06058ce5
[92mINFO [0m:      Sent reply
01/30/2025 06:09:49:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:10:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:10:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7f5332c9-20cd-45b9-b3e8-1f30243bf05b
01/30/2025 06:10:31:INFO:Received: train message 7f5332c9-20cd-45b9-b3e8-1f30243bf05b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 06:14:26:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:50:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:50:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ff533dd1-9d2d-4d32-bdcf-c543e2e071bb
01/30/2025 06:50:35:INFO:Received: evaluate message ff533dd1-9d2d-4d32-bdcf-c543e2e071bb
[92mINFO [0m:      Sent reply
01/30/2025 06:55:31:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:56:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:56:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 679a81d7-9c8e-47e3-a88b-41f034794288
01/30/2025 06:56:12:INFO:Received: train message 679a81d7-9c8e-47e3-a88b-41f034794288
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 06:59:12:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:29:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:29:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d6b1f681-03e3-446f-9553-2634e69d3463
01/30/2025 07:29:13:INFO:Received: evaluate message d6b1f681-03e3-446f-9553-2634e69d3463
[92mINFO [0m:      Sent reply
01/30/2025 07:34:08:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:34:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:34:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0be33e3c-07cc-40a3-a683-8419e8836d64
01/30/2025 07:34:39:INFO:Received: train message 0be33e3c-07cc-40a3-a683-8419e8836d64
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 07:37:25:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 08:18:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 08:18:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3388cda6-f109-4ca6-b44b-4a175a636703
01/30/2025 08:18:07:INFO:Received: evaluate message 3388cda6-f109-4ca6-b44b-4a175a636703
[92mINFO [0m:      Sent reply
01/30/2025 08:22:05:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 08:22:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 08:22:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 0b273545-6ce1-4ba7-ae94-7fd79062d64f
01/30/2025 08:22:16:INFO:Received: reconnect message 0b273545-6ce1-4ba7-ae94-7fd79062d64f
01/30/2025 08:22:16:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 08:22:16:INFO:Disconnect and shut down
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009586053201928735, 0.003904718905687332, 0.00012498894648160785, 3.7852165405638516e-05, 0.00029622833244502544, 0.00015951642126310617, 0.00017604832828510553, 5.6824155763024464e-05]
Noise Multiplier after list and tensor:  0.0007143478219404642
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644], 'accuracy': [0.6741844542891663], 'auc': [0.9136487085523237]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009586053201928735, 0.003904718905687332, 0.00012498894648160785, 3.7852165405638516e-05, 0.00029622833244502544, 0.00015951642126310617, 0.00017604832828510553, 5.6824155763024464e-05]
Noise Multiplier after list and tensor:  0.0007143478219404642
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512], 'accuracy': [0.6741844542891663, 0.6677406363270237], 'auc': [0.9136487085523237, 0.9122972121312665]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009586053201928735, 0.003904718905687332, 0.00012498894648160785, 3.7852165405638516e-05, 0.00029622833244502544, 0.00015951642126310617, 0.00017604832828510553, 5.6824155763024464e-05]
Noise Multiplier after list and tensor:  0.0007143478219404642
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009586053201928735, 0.003904718905687332, 0.00012498894648160785, 3.7852165405638516e-05, 0.00029622833244502544, 0.00015951642126310617, 0.00017604832828510553, 5.6824155763024464e-05]
Noise Multiplier after list and tensor:  0.0007143478219404642
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.34732818603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.0009586053201928735, 0.003904718905687332, 0.00012498894648160785, 3.7852165405638516e-05, 0.00029622833244502544, 0.00015951642126310617, 0.00017604832828510553, 5.6824155763024464e-05]
Noise Multiplier after list and tensor:  0.0007143478219404642
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765, 1.4033728663449896], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438, 0.6524365686669351], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285, 0.9103161857948601]}



Final client history:
{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765, 1.4033728663449896], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438, 0.6524365686669351], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285, 0.9103161857948601]}

