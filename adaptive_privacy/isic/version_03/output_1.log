nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.1 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/27/2025 04:44:40:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 04:44:40:DEBUG:ChannelConnectivity.IDLE
01/27/2025 04:44:40:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 04:44:40:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 04:44:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 04:44:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message ab872086-14a6-48e7-9690-15039c67cb8c
01/27/2025 04:44:40:INFO:Received: get_parameters message ab872086-14a6-48e7-9690-15039c67cb8c
[92mINFO [0m:      Sent reply
01/27/2025 04:44:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 04:44:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 04:44:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7eeb8b74-2d66-4679-9451-6164873c37c6
01/27/2025 04:44:52:INFO:Received: train message 7eeb8b74-2d66-4679-9451-6164873c37c6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 05:11:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:11:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:11:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cae1764b-bf19-4c1a-a3fc-172f917ff8ae
01/27/2025 05:11:21:INFO:Received: evaluate message cae1764b-bf19-4c1a-a3fc-172f917ff8ae
[92mINFO [0m:      Sent reply
01/27/2025 05:13:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:13:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:13:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3703afe8-8431-4d2b-9adc-c0b90c128d29
01/27/2025 05:13:17:INFO:Received: train message 3703afe8-8431-4d2b-9adc-c0b90c128d29
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 05:39:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:39:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:39:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6c97bc6a-4151-46be-bc34-714ad27c7c5f
01/27/2025 05:39:51:INFO:Received: evaluate message 6c97bc6a-4151-46be-bc34-714ad27c7c5f
[92mINFO [0m:      Sent reply
01/27/2025 05:41:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:41:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:41:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4b7caecd-018b-46f5-a9bb-c86ce192f449
01/27/2025 05:41:47:INFO:Received: train message 4b7caecd-018b-46f5-a9bb-c86ce192f449
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:09:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:10:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:10:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 452e37f3-e215-4fe9-9245-f0c8c4748565
01/27/2025 06:10:05:INFO:Received: evaluate message 452e37f3-e215-4fe9-9245-f0c8c4748565
[92mINFO [0m:      Sent reply
01/27/2025 06:11:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:12:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:12:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e4bb48f2-1dbe-4223-8c98-a5d5a17b098a
01/27/2025 06:12:04:INFO:Received: train message e4bb48f2-1dbe-4223-8c98-a5d5a17b098a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:30:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:30:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:30:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fba52e4f-b7b1-42d5-9be7-5b5c6f42a02c
01/27/2025 06:30:46:INFO:Received: evaluate message fba52e4f-b7b1-42d5-9be7-5b5c6f42a02c
[92mINFO [0m:      Sent reply
01/27/2025 06:31:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:31:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:31:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c1a4dae6-b688-4f6a-b6ea-9cb4570c1470
01/27/2025 06:31:37:INFO:Received: train message c1a4dae6-b688-4f6a-b6ea-9cb4570c1470
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:44:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:44:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:44:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f0524e9e-6c54-4830-a753-241958ae2626
01/27/2025 06:44:36:INFO:Received: evaluate message f0524e9e-6c54-4830-a753-241958ae2626
[92mINFO [0m:      Sent reply
01/27/2025 06:45:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:45:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:45:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d2d041cd-8f01-4a33-ad1f-5274c40fc99b
01/27/2025 06:45:31:INFO:Received: train message d2d041cd-8f01-4a33-ad1f-5274c40fc99b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 07:05:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:05:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:05:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ab6f42a6-5581-44a2-9e93-fade8353a494
01/27/2025 07:05:26:INFO:Received: evaluate message ab6f42a6-5581-44a2-9e93-fade8353a494
[92mINFO [0m:      Sent reply
01/27/2025 07:06:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:06:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:06:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 566f4516-d85a-49e5-a8a8-a5e7f6d4ff80
01/27/2025 07:06:41:INFO:Received: train message 566f4516-d85a-49e5-a8a8-a5e7f6d4ff80
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826], 'accuracy': [0.6222311719693918], 'auc': [0.8608385557108893]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644], 'accuracy': [0.6222311719693918, 0.6387434554973822], 'auc': [0.8608385557108893, 0.8776805448953071]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 07:24:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:24:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:24:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 02e02aaf-0feb-40bd-be43-a469bcb1f16a
01/27/2025 07:24:33:INFO:Received: evaluate message 02e02aaf-0feb-40bd-be43-a469bcb1f16a
[92mINFO [0m:      Sent reply
01/27/2025 07:25:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:25:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:25:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 228b20d3-ef69-4b9b-ab2d-0fa2d8788257
01/27/2025 07:25:35:INFO:Received: train message 228b20d3-ef69-4b9b-ab2d-0fa2d8788257
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 07:41:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:41:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:41:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 34e70454-4d9f-4bcf-9a22-6daa6cd54f68
01/27/2025 07:41:56:INFO:Received: evaluate message 34e70454-4d9f-4bcf-9a22-6daa6cd54f68
[92mINFO [0m:      Sent reply
01/27/2025 07:44:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:44:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:44:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 73558078-9aac-4656-b087-da74f058af6e
01/27/2025 07:44:12:INFO:Received: train message 73558078-9aac-4656-b087-da74f058af6e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 08:05:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:05:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:05:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e5f0a848-d549-43dc-a98c-224f5c23de6d
01/27/2025 08:05:49:INFO:Received: evaluate message e5f0a848-d549-43dc-a98c-224f5c23de6d
[92mINFO [0m:      Sent reply
01/27/2025 08:06:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:06:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:06:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1a3b6f0a-8306-4aa0-b2b6-36cda4dae87d
01/27/2025 08:06:53:INFO:Received: train message 1a3b6f0a-8306-4aa0-b2b6-36cda4dae87d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 08:25:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:25:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:25:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7fb5a6e4-ce1b-42e8-bad6-c3c88890edd9
01/27/2025 08:25:33:INFO:Received: evaluate message 7fb5a6e4-ce1b-42e8-bad6-c3c88890edd9
[92mINFO [0m:      Sent reply
01/27/2025 08:27:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:28:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:28:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 50148e4b-1560-482b-93c9-fbfbcb098607
01/27/2025 08:28:07:INFO:Received: train message 50148e4b-1560-482b-93c9-fbfbcb098607
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 08:48:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:48:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:48:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 726082ba-6e3a-4f2f-bba6-7f01f6db9971
01/27/2025 08:48:56:INFO:Received: evaluate message 726082ba-6e3a-4f2f-bba6-7f01f6db9971
[92mINFO [0m:      Sent reply
01/27/2025 08:50:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:50:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:50:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 94f188c6-39f1-4daa-9137-1cc11bf35e81
01/27/2025 08:50:35:INFO:Received: train message 94f188c6-39f1-4daa-9137-1cc11bf35e81
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 09:07:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:07:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:07:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 66f106ee-6b2c-4cca-8309-fd3952b48ec0
01/27/2025 09:07:30:INFO:Received: evaluate message 66f106ee-6b2c-4cca-8309-fd3952b48ec0
[92mINFO [0m:      Sent reply
01/27/2025 09:10:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:10:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:10:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cfcb8f99-f409-42d3-b764-e8a86ff7e98a
01/27/2025 09:10:31:INFO:Received: train message cfcb8f99-f409-42d3-b764-e8a86ff7e98a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 09:31:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:32:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:32:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aadef519-8dea-4f11-b002-6828ae350183
01/27/2025 09:32:06:INFO:Received: evaluate message aadef519-8dea-4f11-b002-6828ae350183
[92mINFO [0m:      Sent reply
01/27/2025 09:33:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:33:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:33:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ab918b9-7bff-4f21-b996-05086fced25f
01/27/2025 09:33:51:INFO:Received: train message 8ab918b9-7bff-4f21-b996-05086fced25f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 09:58:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:58:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:58:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5389975-6ca5-4aae-b7f0-306ba5afb202
01/27/2025 09:58:48:INFO:Received: evaluate message b5389975-6ca5-4aae-b7f0-306ba5afb202
[92mINFO [0m:      Sent reply
01/27/2025 09:59:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:59:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:59:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d1914af5-88b6-4a45-b734-be3c2e5c3a84
01/27/2025 09:59:57:INFO:Received: train message d1914af5-88b6-4a45-b734-be3c2e5c3a84
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 10:23:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:24:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:24:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f1114feb-54f6-4369-954d-96329cc2a815
01/27/2025 10:24:00:INFO:Received: evaluate message f1114feb-54f6-4369-954d-96329cc2a815
[92mINFO [0m:      Sent reply
01/27/2025 10:25:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:25:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:25:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d47c38f7-ffea-4d46-8e0e-c4bf030ccbfb
01/27/2025 10:25:11:INFO:Received: train message d47c38f7-ffea-4d46-8e0e-c4bf030ccbfb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 10:54:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:54:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:54:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ba98cacd-a491-4bae-9180-003598260824
01/27/2025 10:54:36:INFO:Received: evaluate message ba98cacd-a491-4bae-9180-003598260824
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 10:56:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:56:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:56:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 001e67e3-e6bd-44c1-8243-a3a0bb030e6a
01/27/2025 10:56:14:INFO:Received: train message 001e67e3-e6bd-44c1-8243-a3a0bb030e6a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 11:16:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 11:17:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:17:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ea189138-5974-4eb6-b5ea-8d23889e67c0
01/27/2025 11:17:04:INFO:Received: evaluate message ea189138-5974-4eb6-b5ea-8d23889e67c0
[92mINFO [0m:      Sent reply
01/27/2025 11:18:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 11:18:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:18:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3938c375-05dc-4378-b350-46173f432789
01/27/2025 11:18:11:INFO:Received: train message 3938c375-05dc-4378-b350-46173f432789
