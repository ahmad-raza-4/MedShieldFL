nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.1 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/27/2025 04:44:40:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 04:44:40:DEBUG:ChannelConnectivity.IDLE
01/27/2025 04:44:40:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 04:44:40:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 04:44:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 04:44:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message ab872086-14a6-48e7-9690-15039c67cb8c
01/27/2025 04:44:40:INFO:Received: get_parameters message ab872086-14a6-48e7-9690-15039c67cb8c
[92mINFO [0m:      Sent reply
01/27/2025 04:44:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 04:44:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 04:44:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7eeb8b74-2d66-4679-9451-6164873c37c6
01/27/2025 04:44:52:INFO:Received: train message 7eeb8b74-2d66-4679-9451-6164873c37c6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 05:11:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:11:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:11:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cae1764b-bf19-4c1a-a3fc-172f917ff8ae
01/27/2025 05:11:21:INFO:Received: evaluate message cae1764b-bf19-4c1a-a3fc-172f917ff8ae
[92mINFO [0m:      Sent reply
01/27/2025 05:13:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:13:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:13:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3703afe8-8431-4d2b-9adc-c0b90c128d29
01/27/2025 05:13:17:INFO:Received: train message 3703afe8-8431-4d2b-9adc-c0b90c128d29
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 05:39:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:39:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:39:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6c97bc6a-4151-46be-bc34-714ad27c7c5f
01/27/2025 05:39:51:INFO:Received: evaluate message 6c97bc6a-4151-46be-bc34-714ad27c7c5f
[92mINFO [0m:      Sent reply
01/27/2025 05:41:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:41:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:41:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4b7caecd-018b-46f5-a9bb-c86ce192f449
01/27/2025 05:41:47:INFO:Received: train message 4b7caecd-018b-46f5-a9bb-c86ce192f449
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:09:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:10:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:10:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 452e37f3-e215-4fe9-9245-f0c8c4748565
01/27/2025 06:10:05:INFO:Received: evaluate message 452e37f3-e215-4fe9-9245-f0c8c4748565
[92mINFO [0m:      Sent reply
01/27/2025 06:11:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:12:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:12:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e4bb48f2-1dbe-4223-8c98-a5d5a17b098a
01/27/2025 06:12:04:INFO:Received: train message e4bb48f2-1dbe-4223-8c98-a5d5a17b098a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:30:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:30:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:30:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fba52e4f-b7b1-42d5-9be7-5b5c6f42a02c
01/27/2025 06:30:46:INFO:Received: evaluate message fba52e4f-b7b1-42d5-9be7-5b5c6f42a02c
[92mINFO [0m:      Sent reply
01/27/2025 06:31:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:31:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:31:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c1a4dae6-b688-4f6a-b6ea-9cb4570c1470
01/27/2025 06:31:37:INFO:Received: train message c1a4dae6-b688-4f6a-b6ea-9cb4570c1470
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:44:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:44:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:44:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f0524e9e-6c54-4830-a753-241958ae2626
01/27/2025 06:44:36:INFO:Received: evaluate message f0524e9e-6c54-4830-a753-241958ae2626
[92mINFO [0m:      Sent reply
01/27/2025 06:45:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:45:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:45:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d2d041cd-8f01-4a33-ad1f-5274c40fc99b
01/27/2025 06:45:31:INFO:Received: train message d2d041cd-8f01-4a33-ad1f-5274c40fc99b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 07:05:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:05:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:05:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ab6f42a6-5581-44a2-9e93-fade8353a494
01/27/2025 07:05:26:INFO:Received: evaluate message ab6f42a6-5581-44a2-9e93-fade8353a494
[92mINFO [0m:      Sent reply
01/27/2025 07:06:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:06:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:06:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 566f4516-d85a-49e5-a8a8-a5e7f6d4ff80
01/27/2025 07:06:41:INFO:Received: train message 566f4516-d85a-49e5-a8a8-a5e7f6d4ff80
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826], 'accuracy': [0.6222311719693918], 'auc': [0.8608385557108893]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644], 'accuracy': [0.6222311719693918, 0.6387434554973822], 'auc': [0.8608385557108893, 0.8776805448953071]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 07:24:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:24:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:24:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 02e02aaf-0feb-40bd-be43-a469bcb1f16a
01/27/2025 07:24:33:INFO:Received: evaluate message 02e02aaf-0feb-40bd-be43-a469bcb1f16a
[92mINFO [0m:      Sent reply
01/27/2025 07:25:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:25:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:25:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 228b20d3-ef69-4b9b-ab2d-0fa2d8788257
01/27/2025 07:25:35:INFO:Received: train message 228b20d3-ef69-4b9b-ab2d-0fa2d8788257
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 07:41:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:41:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:41:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 34e70454-4d9f-4bcf-9a22-6daa6cd54f68
01/27/2025 07:41:56:INFO:Received: evaluate message 34e70454-4d9f-4bcf-9a22-6daa6cd54f68
[92mINFO [0m:      Sent reply
01/27/2025 07:44:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:44:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:44:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 73558078-9aac-4656-b087-da74f058af6e
01/27/2025 07:44:12:INFO:Received: train message 73558078-9aac-4656-b087-da74f058af6e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 08:05:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:05:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:05:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e5f0a848-d549-43dc-a98c-224f5c23de6d
01/27/2025 08:05:49:INFO:Received: evaluate message e5f0a848-d549-43dc-a98c-224f5c23de6d
[92mINFO [0m:      Sent reply
01/27/2025 08:06:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:06:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:06:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1a3b6f0a-8306-4aa0-b2b6-36cda4dae87d
01/27/2025 08:06:53:INFO:Received: train message 1a3b6f0a-8306-4aa0-b2b6-36cda4dae87d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 08:25:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:25:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:25:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7fb5a6e4-ce1b-42e8-bad6-c3c88890edd9
01/27/2025 08:25:33:INFO:Received: evaluate message 7fb5a6e4-ce1b-42e8-bad6-c3c88890edd9
[92mINFO [0m:      Sent reply
01/27/2025 08:27:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:28:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:28:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 50148e4b-1560-482b-93c9-fbfbcb098607
01/27/2025 08:28:07:INFO:Received: train message 50148e4b-1560-482b-93c9-fbfbcb098607
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 08:48:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:48:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:48:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 726082ba-6e3a-4f2f-bba6-7f01f6db9971
01/27/2025 08:48:56:INFO:Received: evaluate message 726082ba-6e3a-4f2f-bba6-7f01f6db9971
[92mINFO [0m:      Sent reply
01/27/2025 08:50:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:50:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:50:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 94f188c6-39f1-4daa-9137-1cc11bf35e81
01/27/2025 08:50:35:INFO:Received: train message 94f188c6-39f1-4daa-9137-1cc11bf35e81
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 09:07:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:07:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:07:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 66f106ee-6b2c-4cca-8309-fd3952b48ec0
01/27/2025 09:07:30:INFO:Received: evaluate message 66f106ee-6b2c-4cca-8309-fd3952b48ec0
[92mINFO [0m:      Sent reply
01/27/2025 09:10:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:10:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:10:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cfcb8f99-f409-42d3-b764-e8a86ff7e98a
01/27/2025 09:10:31:INFO:Received: train message cfcb8f99-f409-42d3-b764-e8a86ff7e98a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 09:31:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:32:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:32:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aadef519-8dea-4f11-b002-6828ae350183
01/27/2025 09:32:06:INFO:Received: evaluate message aadef519-8dea-4f11-b002-6828ae350183
[92mINFO [0m:      Sent reply
01/27/2025 09:33:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:33:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:33:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ab918b9-7bff-4f21-b996-05086fced25f
01/27/2025 09:33:51:INFO:Received: train message 8ab918b9-7bff-4f21-b996-05086fced25f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 09:58:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:58:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:58:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5389975-6ca5-4aae-b7f0-306ba5afb202
01/27/2025 09:58:48:INFO:Received: evaluate message b5389975-6ca5-4aae-b7f0-306ba5afb202
[92mINFO [0m:      Sent reply
01/27/2025 09:59:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:59:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:59:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d1914af5-88b6-4a45-b734-be3c2e5c3a84
01/27/2025 09:59:57:INFO:Received: train message d1914af5-88b6-4a45-b734-be3c2e5c3a84
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 10:23:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:24:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:24:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f1114feb-54f6-4369-954d-96329cc2a815
01/27/2025 10:24:00:INFO:Received: evaluate message f1114feb-54f6-4369-954d-96329cc2a815
[92mINFO [0m:      Sent reply
01/27/2025 10:25:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:25:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:25:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d47c38f7-ffea-4d46-8e0e-c4bf030ccbfb
01/27/2025 10:25:11:INFO:Received: train message d47c38f7-ffea-4d46-8e0e-c4bf030ccbfb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 10:54:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:54:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:54:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ba98cacd-a491-4bae-9180-003598260824
01/27/2025 10:54:36:INFO:Received: evaluate message ba98cacd-a491-4bae-9180-003598260824
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 10:56:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:56:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:56:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 001e67e3-e6bd-44c1-8243-a3a0bb030e6a
01/27/2025 10:56:14:INFO:Received: train message 001e67e3-e6bd-44c1-8243-a3a0bb030e6a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 11:16:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 11:17:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:17:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ea189138-5974-4eb6-b5ea-8d23889e67c0
01/27/2025 11:17:04:INFO:Received: evaluate message ea189138-5974-4eb6-b5ea-8d23889e67c0
[92mINFO [0m:      Sent reply
01/27/2025 11:18:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 11:18:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:18:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3938c375-05dc-4378-b350-46173f432789
01/27/2025 11:18:11:INFO:Received: train message 3938c375-05dc-4378-b350-46173f432789
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 11:53:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 11:53:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:53:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 18747241-6c41-4100-a0e9-522723a5ca45
01/27/2025 11:53:37:INFO:Received: evaluate message 18747241-6c41-4100-a0e9-522723a5ca45
[92mINFO [0m:      Sent reply
01/27/2025 11:55:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 11:55:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:55:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b13caaab-2971-4db4-a055-fcff158a5fe5
01/27/2025 11:55:41:INFO:Received: train message b13caaab-2971-4db4-a055-fcff158a5fe5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 12:33:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:33:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:33:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 01667c55-03a2-4e39-9655-341f1e45eed7
01/27/2025 12:33:56:INFO:Received: evaluate message 01667c55-03a2-4e39-9655-341f1e45eed7
[92mINFO [0m:      Sent reply
01/27/2025 12:36:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:36:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:36:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 547f9014-7189-4969-a242-f63212bd3a9d
01/27/2025 12:36:07:INFO:Received: train message 547f9014-7189-4969-a242-f63212bd3a9d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 13:13:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:14:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:14:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0298c286-aaf5-4675-98da-80351ffdc27b
01/27/2025 13:14:12:INFO:Received: evaluate message 0298c286-aaf5-4675-98da-80351ffdc27b

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217, 1.2577753355557815], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054, 0.6786145791381394], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492, 0.9094003399304822]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 13:16:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:16:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:16:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8737f55b-65b0-4658-918a-c85d0e73cea3
01/27/2025 13:16:21:INFO:Received: train message 8737f55b-65b0-4658-918a-c85d0e73cea3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 13:56:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:56:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:56:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 01901ceb-58fc-4470-b988-d9304f35742a
01/27/2025 13:56:47:INFO:Received: evaluate message 01901ceb-58fc-4470-b988-d9304f35742a
[92mINFO [0m:      Sent reply
01/27/2025 13:58:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:58:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:58:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 76df42d1-f741-4417-ae3c-306c27ebfbf3
01/27/2025 13:58:36:INFO:Received: train message 76df42d1-f741-4417-ae3c-306c27ebfbf3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 14:37:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:37:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:37:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 27ed8ca2-ffcb-484f-b065-7bcf4fc8d829
01/27/2025 14:37:48:INFO:Received: evaluate message 27ed8ca2-ffcb-484f-b065-7bcf4fc8d829
[92mINFO [0m:      Sent reply
01/27/2025 14:39:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:39:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:39:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9acfc403-5372-404f-8c26-b8872231e980
01/27/2025 14:39:38:INFO:Received: train message 9acfc403-5372-404f-8c26-b8872231e980
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 15:18:04:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 15:18:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 15:18:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c9fcae94-c509-4aec-be12-8e75fd1a09a5
01/27/2025 15:18:20:INFO:Received: evaluate message c9fcae94-c509-4aec-be12-8e75fd1a09a5
[92mINFO [0m:      Sent reply
01/27/2025 15:20:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 15:20:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 15:20:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6370a126-0ec1-4176-9402-05a3c7102916
01/27/2025 15:20:32:INFO:Received: train message 6370a126-0ec1-4176-9402-05a3c7102916

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217, 1.2577753355557815, 1.2371053248223787], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054, 0.6786145791381394, 0.6697543294401933], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492, 0.9094003399304822, 0.9101596366893427]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217, 1.2577753355557815, 1.2371053248223787, 1.2368399795389349], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054, 0.6786145791381394, 0.6697543294401933, 0.6774063632702376], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492, 0.9094003399304822, 0.9101596366893427, 0.9095700910483251]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217, 1.2577753355557815, 1.2371053248223787, 1.2368399795389349, 1.2447566510783152], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054, 0.6786145791381394, 0.6697543294401933, 0.6774063632702376, 0.6810310108739428], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492, 0.9094003399304822, 0.9101596366893427, 0.9095700910483251, 0.9115160686752171]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217, 1.2577753355557815, 1.2371053248223787, 1.2368399795389349, 1.2447566510783152, 1.2530351522177205], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054, 0.6786145791381394, 0.6697543294401933, 0.6774063632702376, 0.6810310108739428, 0.6818364881192106], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492, 0.9094003399304822, 0.9101596366893427, 0.9095700910483251, 0.9115160686752171, 0.9115129196481417]}

/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 15:58:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 15:59:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 15:59:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b8e2a522-c32d-4e17-b880-a704dca52118
01/27/2025 15:59:06:INFO:Received: evaluate message b8e2a522-c32d-4e17-b880-a704dca52118
[92mINFO [0m:      Sent reply
01/27/2025 16:00:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:00:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:00:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a9da4099-d8bf-4c75-9847-fe3c005acfd8
01/27/2025 16:00:45:INFO:Received: train message a9da4099-d8bf-4c75-9847-fe3c005acfd8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 16:37:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:38:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:38:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 37918f47-e184-4f3c-82de-53cca652771b
01/27/2025 16:38:02:INFO:Received: evaluate message 37918f47-e184-4f3c-82de-53cca652771b
[92mINFO [0m:      Sent reply
01/27/2025 16:39:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:39:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:39:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fe25dbe1-12a5-43eb-a93a-2322d85db471
01/27/2025 16:39:38:INFO:Received: train message fe25dbe1-12a5-43eb-a93a-2322d85db471
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 17:17:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:17:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:17:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a5975b41-7e82-4208-bc70-93d654185dbd
01/27/2025 17:17:48:INFO:Received: evaluate message a5975b41-7e82-4208-bc70-93d654185dbd
[92mINFO [0m:      Sent reply
01/27/2025 17:19:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:19:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:19:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cf5fa372-2017-44f5-97bc-b60658d0bdb0
01/27/2025 17:19:38:INFO:Received: train message cf5fa372-2017-44f5-97bc-b60658d0bdb0
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217, 1.2577753355557815, 1.2371053248223787, 1.2368399795389349, 1.2447566510783152, 1.2530351522177205, 1.2918795700088606], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054, 0.6786145791381394, 0.6697543294401933, 0.6774063632702376, 0.6810310108739428, 0.6818364881192106, 0.680628272251309], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492, 0.9094003399304822, 0.9101596366893427, 0.9095700910483251, 0.9115160686752171, 0.9115129196481417, 0.9103216769970813]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217, 1.2577753355557815, 1.2371053248223787, 1.2368399795389349, 1.2447566510783152, 1.2530351522177205, 1.2918795700088606, 1.25550067768047], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054, 0.6786145791381394, 0.6697543294401933, 0.6774063632702376, 0.6810310108739428, 0.6818364881192106, 0.680628272251309, 0.6810310108739428], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492, 0.9094003399304822, 0.9101596366893427, 0.9095700910483251, 0.9115160686752171, 0.9115129196481417, 0.9103216769970813, 0.9127658027268697]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217, 1.2577753355557815, 1.2371053248223787, 1.2368399795389349, 1.2447566510783152, 1.2530351522177205, 1.2918795700088606, 1.25550067768047, 1.2753770947936462], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054, 0.6786145791381394, 0.6697543294401933, 0.6774063632702376, 0.6810310108739428, 0.6818364881192106, 0.680628272251309, 0.6810310108739428, 0.680628272251309], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492, 0.9094003399304822, 0.9101596366893427, 0.9095700910483251, 0.9115160686752171, 0.9115129196481417, 0.9103216769970813, 0.9127658027268697, 0.9119610818996204]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 17:55:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:56:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:56:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2fd5f6ef-c412-42d3-8fb5-1767a9e627d3
01/27/2025 17:56:10:INFO:Received: evaluate message 2fd5f6ef-c412-42d3-8fb5-1767a9e627d3
[92mINFO [0m:      Sent reply
01/27/2025 17:58:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:59:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:59:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ce487f82-90f3-4e37-b4e8-269a6f96f838
01/27/2025 17:59:00:INFO:Received: train message ce487f82-90f3-4e37-b4e8-269a6f96f838
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 18:45:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:45:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:45:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4dabb1af-b876-42b4-9f92-03a3700e2ab0
01/27/2025 18:45:11:INFO:Received: evaluate message 4dabb1af-b876-42b4-9f92-03a3700e2ab0
[92mINFO [0m:      Sent reply
01/27/2025 18:47:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:47:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:47:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 830b9640-47e4-4e60-9e94-df77fe6840da
01/27/2025 18:47:27:INFO:Received: train message 830b9640-47e4-4e60-9e94-df77fe6840da
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 19:33:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:33:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:33:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 662197dc-9cb5-4e1a-8df9-7e98ba198cf4
01/27/2025 19:33:55:INFO:Received: evaluate message 662197dc-9cb5-4e1a-8df9-7e98ba198cf4
[92mINFO [0m:      Sent reply
01/27/2025 19:36:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:36:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:36:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 228f3f44-ef23-4f6d-b948-945e4abcd411
01/27/2025 19:36:15:INFO:Received: train message 228f3f44-ef23-4f6d-b948-945e4abcd411
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217, 1.2577753355557815, 1.2371053248223787, 1.2368399795389349, 1.2447566510783152, 1.2530351522177205, 1.2918795700088606, 1.25550067768047, 1.2753770947936462, 1.3867592845671703], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054, 0.6786145791381394, 0.6697543294401933, 0.6774063632702376, 0.6810310108739428, 0.6818364881192106, 0.680628272251309, 0.6810310108739428, 0.680628272251309, 0.6729762384212646], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492, 0.9094003399304822, 0.9101596366893427, 0.9095700910483251, 0.9115160686752171, 0.9115129196481417, 0.9103216769970813, 0.9127658027268697, 0.9119610818996204, 0.9117852557371656]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217, 1.2577753355557815, 1.2371053248223787, 1.2368399795389349, 1.2447566510783152, 1.2530351522177205, 1.2918795700088606, 1.25550067768047, 1.2753770947936462, 1.3867592845671703, 1.2588181414769342], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054, 0.6786145791381394, 0.6697543294401933, 0.6774063632702376, 0.6810310108739428, 0.6818364881192106, 0.680628272251309, 0.6810310108739428, 0.680628272251309, 0.6729762384212646, 0.6826419653644784], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492, 0.9094003399304822, 0.9101596366893427, 0.9095700910483251, 0.9115160686752171, 0.9115129196481417, 0.9103216769970813, 0.9127658027268697, 0.9119610818996204, 0.9117852557371656, 0.9128525966782297]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217, 1.2577753355557815, 1.2371053248223787, 1.2368399795389349, 1.2447566510783152, 1.2530351522177205, 1.2918795700088606, 1.25550067768047, 1.2753770947936462, 1.3867592845671703, 1.2588181414769342, 1.260038985703868], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054, 0.6786145791381394, 0.6697543294401933, 0.6774063632702376, 0.6810310108739428, 0.6818364881192106, 0.680628272251309, 0.6810310108739428, 0.680628272251309, 0.6729762384212646, 0.6826419653644784, 0.6761981474023359], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492, 0.9094003399304822, 0.9101596366893427, 0.9095700910483251, 0.9115160686752171, 0.9115129196481417, 0.9103216769970813, 0.9127658027268697, 0.9119610818996204, 0.9117852557371656, 0.9128525966782297, 0.9127338976337543]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.041674789041280746, 0.06296811252832413, 0.04216014966368675, 0.014849129132926464, 0.01870434172451496, 0.008090374059975147, 0.009285001084208488, 0.008094659075140953]
Noise Multiplier after list and tensor:  0.025728319538757205
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 20:23:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:24:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:24:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bb3ef553-b499-4ad2-abc9-e6ce68e2c25f
01/27/2025 20:24:00:INFO:Received: evaluate message bb3ef553-b499-4ad2-abc9-e6ce68e2c25f
[92mINFO [0m:      Sent reply
01/27/2025 20:26:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:26:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:26:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message bc70899f-8d2a-4834-8bd5-19a2f2cd1d21
01/27/2025 20:26:32:INFO:Received: reconnect message bc70899f-8d2a-4834-8bd5-19a2f2cd1d21
01/27/2025 20:26:32:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 20:26:32:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217, 1.2577753355557815, 1.2371053248223787, 1.2368399795389349, 1.2447566510783152, 1.2530351522177205, 1.2918795700088606, 1.25550067768047, 1.2753770947936462, 1.3867592845671703, 1.2588181414769342, 1.260038985703868, 1.3074782269403333], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054, 0.6786145791381394, 0.6697543294401933, 0.6774063632702376, 0.6810310108739428, 0.6818364881192106, 0.680628272251309, 0.6810310108739428, 0.680628272251309, 0.6729762384212646, 0.6826419653644784, 0.6761981474023359, 0.6737817156665324], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492, 0.9094003399304822, 0.9101596366893427, 0.9095700910483251, 0.9115160686752171, 0.9115129196481417, 0.9103216769970813, 0.9127658027268697, 0.9119610818996204, 0.9117852557371656, 0.9128525966782297, 0.9127338976337543, 0.9113236096082424]}



Final client history:
{'loss': [1.3708904687790826, 1.3394183183071644, 1.2830710319612375, 1.2534348987496, 1.3023897142262406, 1.284439037751633, 1.2353403653485902, 1.2791143270735292, 1.2498522781576786, 1.2289945832241749, 1.2522555962955093, 1.276531464646432, 1.2844760273667626, 1.2610239283336635, 1.25943131685449, 1.2561995628808613, 1.2458938377737951, 1.200949357490217, 1.2577753355557815, 1.2371053248223787, 1.2368399795389349, 1.2447566510783152, 1.2530351522177205, 1.2918795700088606, 1.25550067768047, 1.2753770947936462, 1.3867592845671703, 1.2588181414769342, 1.260038985703868, 1.3074782269403333], 'accuracy': [0.6222311719693918, 0.6387434554973822, 0.6524365686669351, 0.6568666935159082, 0.6472009665726943, 0.6641159887233186, 0.6697543294401933, 0.6673378977043899, 0.6669351590817559, 0.6673378977043899, 0.6705598066854611, 0.675795408779702, 0.6745871929118002, 0.6689488521949255, 0.6717680225533629, 0.6693515908175593, 0.680628272251309, 0.6782118405155054, 0.6786145791381394, 0.6697543294401933, 0.6774063632702376, 0.6810310108739428, 0.6818364881192106, 0.680628272251309, 0.6810310108739428, 0.680628272251309, 0.6729762384212646, 0.6826419653644784, 0.6761981474023359, 0.6737817156665324], 'auc': [0.8608385557108893, 0.8776805448953071, 0.8882719311804788, 0.8949967380720788, 0.8940045524646061, 0.8990922364450595, 0.9015008220020249, 0.9000591260516232, 0.8984615792805317, 0.9042723133997554, 0.9037108651444874, 0.9048924475240755, 0.9055651309586983, 0.9072824611959808, 0.9067095936985394, 0.9085736662698435, 0.9086784879019362, 0.9097115850206492, 0.9094003399304822, 0.9101596366893427, 0.9095700910483251, 0.9115160686752171, 0.9115129196481417, 0.9103216769970813, 0.9127658027268697, 0.9119610818996204, 0.9117852557371656, 0.9128525966782297, 0.9127338976337543, 0.9113236096082424]}

