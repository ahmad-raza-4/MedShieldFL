nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.1 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/27/2025 05:28:55:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 05:28:55:DEBUG:ChannelConnectivity.IDLE
01/27/2025 05:28:55:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 05:28:55:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 05:28:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:28:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message efdcf5d1-a3cc-422f-bafa-4009b33d3d49
01/27/2025 05:28:55:INFO:Received: get_parameters message efdcf5d1-a3cc-422f-bafa-4009b33d3d49
[92mINFO [0m:      Sent reply
01/27/2025 05:29:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:29:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:29:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message da60d364-6b24-4f74-b1c5-d8d81c051ba4
01/27/2025 05:29:08:INFO:Received: train message da60d364-6b24-4f74-b1c5-d8d81c051ba4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 05:44:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:44:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:44:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e1965ecf-5795-48c0-942f-dca5c28246a1
01/27/2025 05:44:45:INFO:Received: evaluate message e1965ecf-5795-48c0-942f-dca5c28246a1
[92mINFO [0m:      Sent reply
01/27/2025 05:46:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:46:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:46:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b68299e4-ab08-4a61-91d8-e111f1ed8210
01/27/2025 05:46:26:INFO:Received: train message b68299e4-ab08-4a61-91d8-e111f1ed8210
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:01:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:02:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:02:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 150c0c75-721e-430e-8b70-921ec902a6ff
01/27/2025 06:02:10:INFO:Received: evaluate message 150c0c75-721e-430e-8b70-921ec902a6ff
[92mINFO [0m:      Sent reply
01/27/2025 06:03:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:03:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:03:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 21f89fff-46bc-4ef8-a12c-2fb4f42cf9f7
01/27/2025 06:03:09:INFO:Received: train message 21f89fff-46bc-4ef8-a12c-2fb4f42cf9f7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:17:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:17:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:17:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b74b1651-308f-4cb6-8bd0-85e0f512212c
01/27/2025 06:17:55:INFO:Received: evaluate message b74b1651-308f-4cb6-8bd0-85e0f512212c
[92mINFO [0m:      Sent reply
01/27/2025 06:18:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:18:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:18:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e5889253-9835-4a27-bfd7-9f41d9be9d7c
01/27/2025 06:18:41:INFO:Received: train message e5889253-9835-4a27-bfd7-9f41d9be9d7c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:33:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:34:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:34:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 79a0c138-559c-44bd-8f7b-078ca87ab316
01/27/2025 06:34:00:INFO:Received: evaluate message 79a0c138-559c-44bd-8f7b-078ca87ab316
[92mINFO [0m:      Sent reply
01/27/2025 06:34:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:34:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:34:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 916f50b4-0e8e-46bd-b3a3-9eafea3aa759
01/27/2025 06:34:50:INFO:Received: train message 916f50b4-0e8e-46bd-b3a3-9eafea3aa759
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:47:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:47:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:47:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e910e830-5a07-46ee-bf28-724744af9f44
01/27/2025 06:47:52:INFO:Received: evaluate message e910e830-5a07-46ee-bf28-724744af9f44
[92mINFO [0m:      Sent reply
01/27/2025 06:48:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:48:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:48:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e62827fa-75f4-43f1-a22b-8bfdfcb07611
01/27/2025 06:48:49:INFO:Received: train message e62827fa-75f4-43f1-a22b-8bfdfcb07611
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 07:10:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:10:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:10:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4e098727-7592-46dd-be7b-097d1d2b7c53
01/27/2025 07:10:58:INFO:Received: evaluate message 4e098727-7592-46dd-be7b-097d1d2b7c53
[92mINFO [0m:      Sent reply
01/27/2025 07:11:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:12:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:12:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a7403e71-6824-4b05-8cee-6aaef9ae82f3
01/27/2025 07:12:01:INFO:Received: train message a7403e71-6824-4b05-8cee-6aaef9ae82f3
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222], 'accuracy': [0.5912202980265807], 'auc': [0.7983149516674793]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046], 'accuracy': [0.5912202980265807, 0.6129681836488119], 'auc': [0.7983149516674793, 0.8269634608151732]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 07:28:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:28:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:28:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b65737d6-1b85-497c-ace9-97bf0e0c960c
01/27/2025 07:28:20:INFO:Received: evaluate message b65737d6-1b85-497c-ace9-97bf0e0c960c
[92mINFO [0m:      Sent reply
01/27/2025 07:29:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:29:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:29:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 75d154ab-2296-4384-bace-243571dc3d21
01/27/2025 07:29:27:INFO:Received: train message 75d154ab-2296-4384-bace-243571dc3d21
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 07:48:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:49:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:49:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message faf0a57b-0de1-4388-bbd4-6b5ffc6b16cf
01/27/2025 07:49:12:INFO:Received: evaluate message faf0a57b-0de1-4388-bbd4-6b5ffc6b16cf
[92mINFO [0m:      Sent reply
01/27/2025 07:50:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:50:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:50:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b75fae3f-9738-4893-ae15-3f34765d667f
01/27/2025 07:50:56:INFO:Received: train message b75fae3f-9738-4893-ae15-3f34765d667f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 08:09:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:09:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:09:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6f334650-1289-4454-b4d3-b76c467cda17
01/27/2025 08:09:26:INFO:Received: evaluate message 6f334650-1289-4454-b4d3-b76c467cda17
[92mINFO [0m:      Sent reply
01/27/2025 08:10:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:10:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:10:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 176e6879-898a-4a81-8cb2-dd04fb8366af
01/27/2025 08:10:32:INFO:Received: train message 176e6879-898a-4a81-8cb2-dd04fb8366af
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 08:31:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:31:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:31:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2b25d310-db00-47f2-a7d2-2388c3d8d2bc
01/27/2025 08:31:16:INFO:Received: evaluate message 2b25d310-db00-47f2-a7d2-2388c3d8d2bc
[92mINFO [0m:      Sent reply
01/27/2025 08:32:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:32:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:32:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f60ba55f-91a4-45cc-b774-9ff46a01dcf7
01/27/2025 08:32:24:INFO:Received: train message f60ba55f-91a4-45cc-b774-9ff46a01dcf7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 08:53:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:53:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:53:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eb6353f7-6263-4654-94ed-8e33f7629169
01/27/2025 08:53:16:INFO:Received: evaluate message eb6353f7-6263-4654-94ed-8e33f7629169
[92mINFO [0m:      Sent reply
01/27/2025 08:54:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:54:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:54:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5fa70763-5038-40ae-a53a-5288bf9ba9da
01/27/2025 08:54:22:INFO:Received: train message 5fa70763-5038-40ae-a53a-5288bf9ba9da
[0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 09:14:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:14:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:14:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 481fc351-b838-4d0d-a3ec-2605170cb528
01/27/2025 09:14:53:INFO:Received: evaluate message 481fc351-b838-4d0d-a3ec-2605170cb528
[92mINFO [0m:      Sent reply
01/27/2025 09:16:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:16:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:16:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd627735-0d9f-4be8-a86e-69ec40d22766
01/27/2025 09:16:10:INFO:Received: train message cd627735-0d9f-4be8-a86e-69ec40d22766
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 09:36:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:36:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:36:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 36ce6ddf-867b-45ea-91db-1e4429c6770c
01/27/2025 09:36:39:INFO:Received: evaluate message 36ce6ddf-867b-45ea-91db-1e4429c6770c
[92mINFO [0m:      Sent reply
01/27/2025 09:38:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:38:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:38:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0e46b75a-3bcf-465f-8068-e1641d6ec068
01/27/2025 09:38:22:INFO:Received: train message 0e46b75a-3bcf-465f-8068-e1641d6ec068
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 10:01:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:02:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:02:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 65281062-9eef-4d08-97f6-3da24df65848
01/27/2025 10:02:04:INFO:Received: evaluate message 65281062-9eef-4d08-97f6-3da24df65848
[92mINFO [0m:      Sent reply
01/27/2025 10:03:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:03:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:03:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c6468c9f-970a-457b-84c5-c04cc96ba0c2
01/27/2025 10:03:16:INFO:Received: train message c6468c9f-970a-457b-84c5-c04cc96ba0c2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 10:26:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:27:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:27:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 641d9246-a9b9-49f7-b533-95f55162d74c
01/27/2025 10:27:00:INFO:Received: evaluate message 641d9246-a9b9-49f7-b533-95f55162d74c
[92mINFO [0m:      Sent reply
01/27/2025 10:29:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:29:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:29:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2515161c-0752-4434-8186-98e7fa7dc139
01/27/2025 10:29:56:INFO:Received: train message 2515161c-0752-4434-8186-98e7fa7dc139
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 11:06:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 11:06:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:06:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ff0c4681-e036-4a29-afbd-eb46e521f588
01/27/2025 11:06:56:INFO:Received: evaluate message ff0c4681-e036-4a29-afbd-eb46e521f588
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 11:08:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 11:08:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:08:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 98efbdc9-fa8d-478a-bcb5-94c6c7ed1156
01/27/2025 11:08:42:INFO:Received: train message 98efbdc9-fa8d-478a-bcb5-94c6c7ed1156
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 11:40:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 11:40:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:40:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0ca67735-730e-471d-90e0-9e5e2fb1ccb9
01/27/2025 11:40:24:INFO:Received: evaluate message 0ca67735-730e-471d-90e0-9e5e2fb1ccb9
[92mINFO [0m:      Sent reply
01/27/2025 11:43:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 11:43:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:43:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fe22f212-74d5-4953-bb31-d3aa02c7b5f2
01/27/2025 11:43:11:INFO:Received: train message fe22f212-74d5-4953-bb31-d3aa02c7b5f2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 12:14:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:14:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:14:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1b626274-4dc7-406c-bdce-b4b3e012785a
01/27/2025 12:14:55:INFO:Received: evaluate message 1b626274-4dc7-406c-bdce-b4b3e012785a
[92mINFO [0m:      Sent reply
01/27/2025 12:17:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:18:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:18:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d89f6932-dcbd-4f99-b48a-ac4c483cdd58
01/27/2025 12:18:02:INFO:Received: train message d89f6932-dcbd-4f99-b48a-ac4c483cdd58
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 12:54:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:54:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:54:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4702c5a8-c8ac-4775-886b-ccc6cacb693e
01/27/2025 12:54:49:INFO:Received: evaluate message 4702c5a8-c8ac-4775-886b-ccc6cacb693e
[92mINFO [0m:      Sent reply
01/27/2025 12:57:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:57:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:57:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ca49e386-b778-4d7b-a28f-2934808489af
01/27/2025 12:57:14:INFO:Received: train message ca49e386-b778-4d7b-a28f-2934808489af
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 13:33:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:33:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:33:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 40358236-ebd7-498a-bd0f-039d60739a79
01/27/2025 13:33:33:INFO:Received: evaluate message 40358236-ebd7-498a-bd0f-039d60739a79

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175, 1.355353427807074], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961, 0.6516310914216673], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762, 0.8784244678609513]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 13:36:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:36:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:36:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 92953700-fabb-4216-b0bb-3a7fc9aa34e3
01/27/2025 13:36:07:INFO:Received: train message 92953700-fabb-4216-b0bb-3a7fc9aa34e3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 14:08:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:09:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:09:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1f935805-d4f0-426c-a58c-967c2a4bc2d6
01/27/2025 14:09:11:INFO:Received: evaluate message 1f935805-d4f0-426c-a58c-967c2a4bc2d6
[92mINFO [0m:      Sent reply
01/27/2025 14:11:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:11:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:11:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 98df9049-30f9-4990-91ac-65235b12db64
01/27/2025 14:11:28:INFO:Received: train message 98df9049-30f9-4990-91ac-65235b12db64
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 14:49:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:49:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:49:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 851e1cd7-8c6b-487c-ac3c-ef5ae421146b
01/27/2025 14:49:25:INFO:Received: evaluate message 851e1cd7-8c6b-487c-ac3c-ef5ae421146b
[92mINFO [0m:      Sent reply
01/27/2025 14:51:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:51:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:51:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 49af8050-7a2f-4861-9442-513ff4982220
01/27/2025 14:51:32:INFO:Received: train message 49af8050-7a2f-4861-9442-513ff4982220
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 15:29:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 15:29:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 15:29:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 48ae2da6-edd2-4c99-82cd-0a96c293fa19
01/27/2025 15:29:20:INFO:Received: evaluate message 48ae2da6-edd2-4c99-82cd-0a96c293fa19
[92mINFO [0m:      Sent reply
01/27/2025 15:31:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 15:31:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 15:31:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 79a8031e-00ca-4b59-8413-41acf5ee2f75
01/27/2025 15:31:26:INFO:Received: train message 79a8031e-00ca-4b59-8413-41acf5ee2f75

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175, 1.355353427807074, 1.322129089578812], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961, 0.6516310914216673, 0.6516310914216673], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762, 0.8784244678609513, 0.8809379713534655]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175, 1.355353427807074, 1.322129089578812, 1.3277917421252987], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961, 0.6516310914216673, 0.6516310914216673, 0.6508256141763995], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762, 0.8784244678609513, 0.8809379713534655, 0.8816715121052386]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175, 1.355353427807074, 1.322129089578812, 1.3277917421252987, 1.3738822126119692], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961, 0.6516310914216673, 0.6516310914216673, 0.6508256141763995, 0.6512283527990335], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762, 0.8784244678609513, 0.8809379713534655, 0.8816715121052386, 0.8824866082823349]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175, 1.355353427807074, 1.322129089578812, 1.3277917421252987, 1.3738822126119692, 1.3545606162967674], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961, 0.6516310914216673, 0.6516310914216673, 0.6508256141763995, 0.6512283527990335, 0.6564639548932742], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762, 0.8784244678609513, 0.8809379713534655, 0.8816715121052386, 0.8824866082823349, 0.8843497079263107]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 16:01:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:01:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:01:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bfa981ad-dc01-4128-b560-ce9a20efe1b4
01/27/2025 16:01:59:INFO:Received: evaluate message bfa981ad-dc01-4128-b560-ce9a20efe1b4
[92mINFO [0m:      Sent reply
01/27/2025 16:04:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:04:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:04:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8f9d380b-ea35-4038-b0b4-7e62da919264
01/27/2025 16:04:12:INFO:Received: train message 8f9d380b-ea35-4038-b0b4-7e62da919264
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 16:34:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:35:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:35:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d4bab26a-cffb-4a32-8206-8a03a3f73ce1
01/27/2025 16:35:06:INFO:Received: evaluate message d4bab26a-cffb-4a32-8206-8a03a3f73ce1
[92mINFO [0m:      Sent reply
01/27/2025 16:36:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:36:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:36:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6d619b29-78c3-4381-b1fa-d32cfe6f59d8
01/27/2025 16:36:57:INFO:Received: train message 6d619b29-78c3-4381-b1fa-d32cfe6f59d8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 17:08:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:08:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:08:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2de3c826-e8c3-470b-86b0-21f480dcf1e7
01/27/2025 17:08:25:INFO:Received: evaluate message 2de3c826-e8c3-470b-86b0-21f480dcf1e7
[92mINFO [0m:      Sent reply
01/27/2025 17:10:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:10:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:10:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cf2d50da-7e61-4494-a3c4-1c000d989418
01/27/2025 17:10:21:INFO:Received: train message cf2d50da-7e61-4494-a3c4-1c000d989418
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175, 1.355353427807074, 1.322129089578812, 1.3277917421252987, 1.3738822126119692, 1.3545606162967674, 1.3785960841361333], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961, 0.6516310914216673, 0.6516310914216673, 0.6508256141763995, 0.6512283527990335, 0.6564639548932742, 0.6568666935159082], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762, 0.8784244678609513, 0.8809379713534655, 0.8816715121052386, 0.8824866082823349, 0.8843497079263107, 0.8828378248514602]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175, 1.355353427807074, 1.322129089578812, 1.3277917421252987, 1.3738822126119692, 1.3545606162967674, 1.3785960841361333, 1.3600123661254808], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961, 0.6516310914216673, 0.6516310914216673, 0.6508256141763995, 0.6512283527990335, 0.6564639548932742, 0.6568666935159082, 0.6564639548932742], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762, 0.8784244678609513, 0.8809379713534655, 0.8816715121052386, 0.8824866082823349, 0.8843497079263107, 0.8828378248514602, 0.8855009572514095]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175, 1.355353427807074, 1.322129089578812, 1.3277917421252987, 1.3738822126119692, 1.3545606162967674, 1.3785960841361333, 1.3600123661254808, 1.339842028618819], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961, 0.6516310914216673, 0.6516310914216673, 0.6508256141763995, 0.6512283527990335, 0.6564639548932742, 0.6568666935159082, 0.6564639548932742, 0.6568666935159082], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762, 0.8784244678609513, 0.8809379713534655, 0.8816715121052386, 0.8824866082823349, 0.8843497079263107, 0.8828378248514602, 0.8855009572514095, 0.8864640972064196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 17:38:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:39:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:39:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d7159850-7161-4838-960a-2606632368ba
01/27/2025 17:39:03:INFO:Received: evaluate message d7159850-7161-4838-960a-2606632368ba
[92mINFO [0m:      Sent reply
01/27/2025 17:40:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:40:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:40:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9450602e-9dfc-45b4-bc27-7fcbea776df3
01/27/2025 17:40:47:INFO:Received: train message 9450602e-9dfc-45b4-bc27-7fcbea776df3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 18:07:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:07:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:07:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c2d3596b-fa21-4caf-a870-b23cb26ff560
01/27/2025 18:07:18:INFO:Received: evaluate message c2d3596b-fa21-4caf-a870-b23cb26ff560
[92mINFO [0m:      Sent reply
01/27/2025 18:08:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:08:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:08:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 468ea407-a7ae-4b3e-9d16-d5d3064c17f8
01/27/2025 18:08:57:INFO:Received: train message 468ea407-a7ae-4b3e-9d16-d5d3064c17f8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 18:34:04:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:34:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:34:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9a864fcf-9156-4a63-a4e0-c092b993581a
01/27/2025 18:34:14:INFO:Received: evaluate message 9a864fcf-9156-4a63-a4e0-c092b993581a
[92mINFO [0m:      Sent reply
01/27/2025 18:35:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:35:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:35:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 807f999a-7b79-4078-b4c2-8b4b0f6d7c18
01/27/2025 18:35:55:INFO:Received: train message 807f999a-7b79-4078-b4c2-8b4b0f6d7c18
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175, 1.355353427807074, 1.322129089578812, 1.3277917421252987, 1.3738822126119692, 1.3545606162967674, 1.3785960841361333, 1.3600123661254808, 1.339842028618819, 1.3592655752581835], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961, 0.6516310914216673, 0.6516310914216673, 0.6508256141763995, 0.6512283527990335, 0.6564639548932742, 0.6568666935159082, 0.6564639548932742, 0.6568666935159082, 0.657269432138542], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762, 0.8784244678609513, 0.8809379713534655, 0.8816715121052386, 0.8824866082823349, 0.8843497079263107, 0.8828378248514602, 0.8855009572514095, 0.8864640972064196, 0.8885909566438681]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175, 1.355353427807074, 1.322129089578812, 1.3277917421252987, 1.3738822126119692, 1.3545606162967674, 1.3785960841361333, 1.3600123661254808, 1.339842028618819, 1.3592655752581835, 1.3627562105919726], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961, 0.6516310914216673, 0.6516310914216673, 0.6508256141763995, 0.6512283527990335, 0.6564639548932742, 0.6568666935159082, 0.6564639548932742, 0.6568666935159082, 0.657269432138542, 0.657672170761176], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762, 0.8784244678609513, 0.8809379713534655, 0.8816715121052386, 0.8824866082823349, 0.8843497079263107, 0.8828378248514602, 0.8855009572514095, 0.8864640972064196, 0.8885909566438681, 0.8905615988830349]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175, 1.355353427807074, 1.322129089578812, 1.3277917421252987, 1.3738822126119692, 1.3545606162967674, 1.3785960841361333, 1.3600123661254808, 1.339842028618819, 1.3592655752581835, 1.3627562105919726, 1.3499042758814903], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961, 0.6516310914216673, 0.6516310914216673, 0.6508256141763995, 0.6512283527990335, 0.6564639548932742, 0.6568666935159082, 0.6564639548932742, 0.6568666935159082, 0.657269432138542, 0.657672170761176, 0.6588803866290778], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762, 0.8784244678609513, 0.8809379713534655, 0.8816715121052386, 0.8824866082823349, 0.8843497079263107, 0.8828378248514602, 0.8855009572514095, 0.8864640972064196, 0.8885909566438681, 0.8905615988830349, 0.8912102514984852]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 18:59:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:59:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:59:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cfd23a0d-8596-49a6-bf24-0620fa9cfff3
01/27/2025 18:59:41:INFO:Received: evaluate message cfd23a0d-8596-49a6-bf24-0620fa9cfff3
[92mINFO [0m:      Sent reply
01/27/2025 19:01:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:01:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:01:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 39d622c0-5c56-4490-8150-cab26279b2a9
01/27/2025 19:01:13:INFO:Received: reconnect message 39d622c0-5c56-4490-8150-cab26279b2a9
01/27/2025 19:01:13:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 19:01:13:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175, 1.355353427807074, 1.322129089578812, 1.3277917421252987, 1.3738822126119692, 1.3545606162967674, 1.3785960841361333, 1.3600123661254808, 1.339842028618819, 1.3592655752581835, 1.3627562105919726, 1.3499042758814903, 1.3706497979701837], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961, 0.6516310914216673, 0.6516310914216673, 0.6508256141763995, 0.6512283527990335, 0.6564639548932742, 0.6568666935159082, 0.6564639548932742, 0.6568666935159082, 0.657269432138542, 0.657672170761176, 0.6588803866290778, 0.6520338300443013], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762, 0.8784244678609513, 0.8809379713534655, 0.8816715121052386, 0.8824866082823349, 0.8843497079263107, 0.8828378248514602, 0.8855009572514095, 0.8864640972064196, 0.8885909566438681, 0.8905615988830349, 0.8912102514984852, 0.8904872337390467]}



Final client history:
{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428, 1.3391361280981686, 1.3608950483333282, 1.3337914491679175, 1.355353427807074, 1.322129089578812, 1.3277917421252987, 1.3738822126119692, 1.3545606162967674, 1.3785960841361333, 1.3600123661254808, 1.339842028618819, 1.3592655752581835, 1.3627562105919726, 1.3499042758814903, 1.3706497979701837], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265, 0.6476037051953283, 0.6500201369311317, 0.6484091824405961, 0.6516310914216673, 0.6516310914216673, 0.6508256141763995, 0.6512283527990335, 0.6564639548932742, 0.6568666935159082, 0.6564639548932742, 0.6568666935159082, 0.657269432138542, 0.657672170761176, 0.6588803866290778, 0.6520338300443013], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922, 0.8789743725101526, 0.8775463246751841, 0.8787560597854762, 0.8784244678609513, 0.8809379713534655, 0.8816715121052386, 0.8824866082823349, 0.8843497079263107, 0.8828378248514602, 0.8855009572514095, 0.8864640972064196, 0.8885909566438681, 0.8905615988830349, 0.8912102514984852, 0.8904872337390467]}

