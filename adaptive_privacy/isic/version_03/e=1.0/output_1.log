nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.1 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/27/2025 05:28:55:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 05:28:55:DEBUG:ChannelConnectivity.IDLE
01/27/2025 05:28:55:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 05:28:55:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 05:28:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:28:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message efdcf5d1-a3cc-422f-bafa-4009b33d3d49
01/27/2025 05:28:55:INFO:Received: get_parameters message efdcf5d1-a3cc-422f-bafa-4009b33d3d49
[92mINFO [0m:      Sent reply
01/27/2025 05:29:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:29:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:29:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message da60d364-6b24-4f74-b1c5-d8d81c051ba4
01/27/2025 05:29:08:INFO:Received: train message da60d364-6b24-4f74-b1c5-d8d81c051ba4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 05:44:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:44:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:44:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e1965ecf-5795-48c0-942f-dca5c28246a1
01/27/2025 05:44:45:INFO:Received: evaluate message e1965ecf-5795-48c0-942f-dca5c28246a1
[92mINFO [0m:      Sent reply
01/27/2025 05:46:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 05:46:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 05:46:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b68299e4-ab08-4a61-91d8-e111f1ed8210
01/27/2025 05:46:26:INFO:Received: train message b68299e4-ab08-4a61-91d8-e111f1ed8210
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:01:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:02:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:02:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 150c0c75-721e-430e-8b70-921ec902a6ff
01/27/2025 06:02:10:INFO:Received: evaluate message 150c0c75-721e-430e-8b70-921ec902a6ff
[92mINFO [0m:      Sent reply
01/27/2025 06:03:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:03:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:03:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 21f89fff-46bc-4ef8-a12c-2fb4f42cf9f7
01/27/2025 06:03:09:INFO:Received: train message 21f89fff-46bc-4ef8-a12c-2fb4f42cf9f7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:17:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:17:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:17:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b74b1651-308f-4cb6-8bd0-85e0f512212c
01/27/2025 06:17:55:INFO:Received: evaluate message b74b1651-308f-4cb6-8bd0-85e0f512212c
[92mINFO [0m:      Sent reply
01/27/2025 06:18:35:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:18:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:18:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e5889253-9835-4a27-bfd7-9f41d9be9d7c
01/27/2025 06:18:41:INFO:Received: train message e5889253-9835-4a27-bfd7-9f41d9be9d7c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:33:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:34:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:34:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 79a0c138-559c-44bd-8f7b-078ca87ab316
01/27/2025 06:34:00:INFO:Received: evaluate message 79a0c138-559c-44bd-8f7b-078ca87ab316
[92mINFO [0m:      Sent reply
01/27/2025 06:34:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:34:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:34:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 916f50b4-0e8e-46bd-b3a3-9eafea3aa759
01/27/2025 06:34:50:INFO:Received: train message 916f50b4-0e8e-46bd-b3a3-9eafea3aa759
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 06:47:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:47:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:47:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e910e830-5a07-46ee-bf28-724744af9f44
01/27/2025 06:47:52:INFO:Received: evaluate message e910e830-5a07-46ee-bf28-724744af9f44
[92mINFO [0m:      Sent reply
01/27/2025 06:48:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 06:48:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 06:48:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e62827fa-75f4-43f1-a22b-8bfdfcb07611
01/27/2025 06:48:49:INFO:Received: train message e62827fa-75f4-43f1-a22b-8bfdfcb07611
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 07:10:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:10:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:10:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4e098727-7592-46dd-be7b-097d1d2b7c53
01/27/2025 07:10:58:INFO:Received: evaluate message 4e098727-7592-46dd-be7b-097d1d2b7c53
[92mINFO [0m:      Sent reply
01/27/2025 07:11:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:12:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:12:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a7403e71-6824-4b05-8cee-6aaef9ae82f3
01/27/2025 07:12:01:INFO:Received: train message a7403e71-6824-4b05-8cee-6aaef9ae82f3
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222], 'accuracy': [0.5912202980265807], 'auc': [0.7983149516674793]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046], 'accuracy': [0.5912202980265807, 0.6129681836488119], 'auc': [0.7983149516674793, 0.8269634608151732]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 07:28:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:28:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:28:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b65737d6-1b85-497c-ace9-97bf0e0c960c
01/27/2025 07:28:20:INFO:Received: evaluate message b65737d6-1b85-497c-ace9-97bf0e0c960c
[92mINFO [0m:      Sent reply
01/27/2025 07:29:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:29:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:29:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 75d154ab-2296-4384-bace-243571dc3d21
01/27/2025 07:29:27:INFO:Received: train message 75d154ab-2296-4384-bace-243571dc3d21
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 07:48:59:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:49:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:49:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message faf0a57b-0de1-4388-bbd4-6b5ffc6b16cf
01/27/2025 07:49:12:INFO:Received: evaluate message faf0a57b-0de1-4388-bbd4-6b5ffc6b16cf
[92mINFO [0m:      Sent reply
01/27/2025 07:50:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 07:50:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 07:50:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b75fae3f-9738-4893-ae15-3f34765d667f
01/27/2025 07:50:56:INFO:Received: train message b75fae3f-9738-4893-ae15-3f34765d667f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 08:09:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:09:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:09:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6f334650-1289-4454-b4d3-b76c467cda17
01/27/2025 08:09:26:INFO:Received: evaluate message 6f334650-1289-4454-b4d3-b76c467cda17
[92mINFO [0m:      Sent reply
01/27/2025 08:10:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:10:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:10:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 176e6879-898a-4a81-8cb2-dd04fb8366af
01/27/2025 08:10:32:INFO:Received: train message 176e6879-898a-4a81-8cb2-dd04fb8366af
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 08:31:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:31:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:31:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2b25d310-db00-47f2-a7d2-2388c3d8d2bc
01/27/2025 08:31:16:INFO:Received: evaluate message 2b25d310-db00-47f2-a7d2-2388c3d8d2bc
[92mINFO [0m:      Sent reply
01/27/2025 08:32:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:32:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:32:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f60ba55f-91a4-45cc-b774-9ff46a01dcf7
01/27/2025 08:32:24:INFO:Received: train message f60ba55f-91a4-45cc-b774-9ff46a01dcf7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 08:53:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:53:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:53:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eb6353f7-6263-4654-94ed-8e33f7629169
01/27/2025 08:53:16:INFO:Received: evaluate message eb6353f7-6263-4654-94ed-8e33f7629169
[92mINFO [0m:      Sent reply
01/27/2025 08:54:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 08:54:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 08:54:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5fa70763-5038-40ae-a53a-5288bf9ba9da
01/27/2025 08:54:22:INFO:Received: train message 5fa70763-5038-40ae-a53a-5288bf9ba9da
[0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 09:14:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:14:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:14:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 481fc351-b838-4d0d-a3ec-2605170cb528
01/27/2025 09:14:53:INFO:Received: evaluate message 481fc351-b838-4d0d-a3ec-2605170cb528
[92mINFO [0m:      Sent reply
01/27/2025 09:16:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:16:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:16:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd627735-0d9f-4be8-a86e-69ec40d22766
01/27/2025 09:16:10:INFO:Received: train message cd627735-0d9f-4be8-a86e-69ec40d22766
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 09:36:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:36:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:36:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 36ce6ddf-867b-45ea-91db-1e4429c6770c
01/27/2025 09:36:39:INFO:Received: evaluate message 36ce6ddf-867b-45ea-91db-1e4429c6770c
[92mINFO [0m:      Sent reply
01/27/2025 09:38:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 09:38:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 09:38:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0e46b75a-3bcf-465f-8068-e1641d6ec068
01/27/2025 09:38:22:INFO:Received: train message 0e46b75a-3bcf-465f-8068-e1641d6ec068
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 10:01:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:02:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:02:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 65281062-9eef-4d08-97f6-3da24df65848
01/27/2025 10:02:04:INFO:Received: evaluate message 65281062-9eef-4d08-97f6-3da24df65848
[92mINFO [0m:      Sent reply
01/27/2025 10:03:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:03:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:03:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c6468c9f-970a-457b-84c5-c04cc96ba0c2
01/27/2025 10:03:16:INFO:Received: train message c6468c9f-970a-457b-84c5-c04cc96ba0c2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 10:26:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:27:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:27:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 641d9246-a9b9-49f7-b533-95f55162d74c
01/27/2025 10:27:00:INFO:Received: evaluate message 641d9246-a9b9-49f7-b533-95f55162d74c
[92mINFO [0m:      Sent reply
01/27/2025 10:29:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 10:29:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 10:29:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2515161c-0752-4434-8186-98e7fa7dc139
01/27/2025 10:29:56:INFO:Received: train message 2515161c-0752-4434-8186-98e7fa7dc139
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 11:06:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 11:06:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:06:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ff0c4681-e036-4a29-afbd-eb46e521f588
01/27/2025 11:06:56:INFO:Received: evaluate message ff0c4681-e036-4a29-afbd-eb46e521f588
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.5024531785394222, 1.4056714188414046, 1.388591115051251, 1.341721743309253, 1.3622973431179324, 1.343331170571796, 1.3781681470077844, 1.4127157360519367, 1.3380122226042177, 1.3478734843729967, 1.3650561665941332, 1.3397577966214955, 1.3573613001654046, 1.3382597132839122, 1.4010196621745428], 'accuracy': [0.5912202980265807, 0.6129681836488119, 0.6165928312525171, 0.6202174788562224, 0.6266612968183649, 0.6379379782521144, 0.6355215465163109, 0.6375352396294804, 0.6367297623842126, 0.6359242851389448, 0.6423681031010874, 0.6472009665726943, 0.6431735803463552, 0.6484091824405961, 0.63954893274265], 'auc': [0.7983149516674793, 0.8269634608151732, 0.8429952684621963, 0.8526022433251008, 0.8527325251375046, 0.8564517144228334, 0.8610275899823032, 0.8634875573022367, 0.866208811657971, 0.8683085663138757, 0.8708748754471416, 0.8736842758829847, 0.871373540179897, 0.8775959906713258, 0.8738257396432922]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  4.8828125
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.49381157755851746, 0.7460201978683472, 0.4987291395664215, 0.17615827918052673, 0.22133322060108185, 0.09497257322072983, 0.11002083867788315, 0.09601432085037231]
Noise Multiplier after list and tensor:  0.304632518440485
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 11:08:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 11:08:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:08:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 98efbdc9-fa8d-478a-bcb5-94c6c7ed1156
01/27/2025 11:08:42:INFO:Received: train message 98efbdc9-fa8d-478a-bcb5-94c6c7ed1156
