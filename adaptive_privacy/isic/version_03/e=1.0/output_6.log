nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 07:09:29:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 07:09:29:DEBUG:ChannelConnectivity.IDLE
01/29/2025 07:09:29:DEBUG:ChannelConnectivity.CONNECTING
01/29/2025 07:09:29:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 07:09:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:09:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message 38827892-c1cd-4d0e-919f-1d070419dd46
01/29/2025 07:09:29:INFO:Received: get_parameters message 38827892-c1cd-4d0e-919f-1d070419dd46
[92mINFO [0m:      Sent reply
01/29/2025 07:09:35:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:18:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:18:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1d9f934a-e4a1-4e1b-aee4-0266df3bc49f
01/29/2025 07:18:29:INFO:Received: train message 1d9f934a-e4a1-4e1b-aee4-0266df3bc49f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 07:24:30:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:46:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:46:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a8299691-8151-4125-b19e-7c873638d27c
01/29/2025 07:46:42:INFO:Received: evaluate message a8299691-8151-4125-b19e-7c873638d27c
[92mINFO [0m:      Sent reply
01/29/2025 07:50:49:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:51:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:51:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c2da8f13-79e9-4701-bd82-a6d18cc588f4
01/29/2025 07:51:32:INFO:Received: train message c2da8f13-79e9-4701-bd82-a6d18cc588f4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 07:53:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:18:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:18:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f65d9fb0-77ef-4718-ae61-ab4cc8442484
01/29/2025 08:18:34:INFO:Received: evaluate message f65d9fb0-77ef-4718-ae61-ab4cc8442484
[92mINFO [0m:      Sent reply
01/29/2025 08:22:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:23:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:23:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c6a16a49-98a7-41b5-a88d-036fb1a02a9f
01/29/2025 08:23:26:INFO:Received: train message c6a16a49-98a7-41b5-a88d-036fb1a02a9f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:25:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:47:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:47:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3897527d-f3a9-4281-80b6-5685eeba323c
01/29/2025 08:47:13:INFO:Received: evaluate message 3897527d-f3a9-4281-80b6-5685eeba323c
[92mINFO [0m:      Sent reply
01/29/2025 08:51:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:51:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:51:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d71fbb8e-5bac-4df0-ac4c-0658bc7e881f
01/29/2025 08:51:52:INFO:Received: train message d71fbb8e-5bac-4df0-ac4c-0658bc7e881f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:54:08:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:18:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:18:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7ba066cf-ffb7-40c9-ad9c-8f7d0f0297bf
01/29/2025 09:18:57:INFO:Received: evaluate message 7ba066cf-ffb7-40c9-ad9c-8f7d0f0297bf
[92mINFO [0m:      Sent reply
01/29/2025 09:25:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:26:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:26:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4e3dee8e-7121-4504-b35a-e5dd810d55e1
01/29/2025 09:26:31:INFO:Received: train message 4e3dee8e-7121-4504-b35a-e5dd810d55e1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:29:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:58:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:58:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 13119c2f-0c7e-46c4-a0d8-09a8cf2de7a6
01/29/2025 09:58:35:INFO:Received: evaluate message 13119c2f-0c7e-46c4-a0d8-09a8cf2de7a6
[92mINFO [0m:      Sent reply
01/29/2025 10:03:16:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:04:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:04:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a2618f20-e902-4622-aad7-f6efd86293b6
01/29/2025 10:04:01:INFO:Received: train message a2618f20-e902-4622-aad7-f6efd86293b6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 10:06:21:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:26:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:26:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7334d275-3bce-4527-be8b-e2762f61d474
01/29/2025 10:26:51:INFO:Received: evaluate message 7334d275-3bce-4527-be8b-e2762f61d474
[92mINFO [0m:      Sent reply
01/29/2025 10:31:07:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:31:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:31:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b8b1e312-f2aa-4d3f-ab84-fc5bb213be6d
01/29/2025 10:31:29:INFO:Received: train message b8b1e312-f2aa-4d3f-ab84-fc5bb213be6d
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657], 'accuracy': [0.517921868707209], 'auc': [0.7610659482721037]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953], 'accuracy': [0.517921868707209, 0.5819573097060008], 'auc': [0.7610659482721037, 0.8125282828523395]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 10:33:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:55:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:55:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 64cb7055-2563-413f-b894-914fdcb7bd0a
01/29/2025 10:55:13:INFO:Received: evaluate message 64cb7055-2563-413f-b894-914fdcb7bd0a
[92mINFO [0m:      Sent reply
01/29/2025 10:59:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:00:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:00:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bb306ac5-24d0-4104-88c4-72ca5010f453
01/29/2025 11:00:14:INFO:Received: train message bb306ac5-24d0-4104-88c4-72ca5010f453
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:02:15:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:26:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:26:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f1a04e4b-6ab5-43b5-bc7c-59ad2ea3c06c
01/29/2025 11:26:51:INFO:Received: evaluate message f1a04e4b-6ab5-43b5-bc7c-59ad2ea3c06c
[92mINFO [0m:      Sent reply
01/29/2025 11:32:27:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:33:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:33:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ec5196a7-00f1-46c1-8f62-b594b033c7f1
01/29/2025 11:33:01:INFO:Received: train message ec5196a7-00f1-46c1-8f62-b594b033c7f1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:36:10:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:13:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:13:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e0519aef-68ee-47c8-ae3f-5c1eee0fafd0
01/29/2025 12:13:44:INFO:Received: evaluate message e0519aef-68ee-47c8-ae3f-5c1eee0fafd0
[92mINFO [0m:      Sent reply
01/29/2025 12:18:30:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:19:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:19:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bbbe9618-7cc8-4ee2-aa47-2f7e31e921aa
01/29/2025 12:19:25:INFO:Received: train message bbbe9618-7cc8-4ee2-aa47-2f7e31e921aa
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:22:16:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:49:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:49:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7c6bfadf-315c-4f6c-b6dd-a2273a463302
01/29/2025 12:49:58:INFO:Received: evaluate message 7c6bfadf-315c-4f6c-b6dd-a2273a463302
[92mINFO [0m:      Sent reply
01/29/2025 12:54:43:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:55:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:55:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b6411118-6dba-45c1-acbb-8703179e9c47
01/29/2025 12:55:45:INFO:Received: train message b6411118-6dba-45c1-acbb-8703179e9c47
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:58:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:31:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:31:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 31f34484-fd53-4d15-8438-f504ad77ef5b
01/29/2025 13:31:59:INFO:Received: evaluate message 31f34484-fd53-4d15-8438-f504ad77ef5b
[92mINFO [0m:      Sent reply
01/29/2025 13:36:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:37:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:37:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message effba67f-fdd1-4c8c-a795-fd59732b7e52
01/29/2025 13:37:32:INFO:Received: train message effba67f-fdd1-4c8c-a795-fd59732b7e52
[0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:40:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:08:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:08:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6bcc89c6-5d45-4504-9021-d09b6332ced3
01/29/2025 14:08:08:INFO:Received: evaluate message 6bcc89c6-5d45-4504-9021-d09b6332ced3
[92mINFO [0m:      Sent reply
01/29/2025 14:13:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:14:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:14:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1963079b-abe7-43e1-b366-4ed55b006876
01/29/2025 14:14:33:INFO:Received: train message 1963079b-abe7-43e1-b366-4ed55b006876
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:17:28:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:51:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:51:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 67446cfc-3819-4e1d-8ffa-e8301fea7cda
01/29/2025 14:51:59:INFO:Received: evaluate message 67446cfc-3819-4e1d-8ffa-e8301fea7cda
[92mINFO [0m:      Sent reply
01/29/2025 14:56:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:57:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:57:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4111eae2-fdf2-40fc-ba19-0de11e7cb12b
01/29/2025 14:57:10:INFO:Received: train message 4111eae2-fdf2-40fc-ba19-0de11e7cb12b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:59:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:31:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:31:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 320cd87d-801e-4abb-9025-3cb3189b01d6
01/29/2025 15:31:05:INFO:Received: evaluate message 320cd87d-801e-4abb-9025-3cb3189b01d6
[92mINFO [0m:      Sent reply
01/29/2025 15:35:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:36:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:36:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 85a23dec-adfb-4408-9499-b835764f25c4
01/29/2025 15:36:53:INFO:Received: train message 85a23dec-adfb-4408-9499-b835764f25c4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:39:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:08:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:08:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7e92a287-5cbf-43bf-a86a-55e3363a3e68
01/29/2025 16:08:43:INFO:Received: evaluate message 7e92a287-5cbf-43bf-a86a-55e3363a3e68
[92mINFO [0m:      Sent reply
01/29/2025 16:14:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message def5324d-14f9-4823-819d-c2a558c7a789
01/29/2025 16:15:05:INFO:Received: train message def5324d-14f9-4823-819d-c2a558c7a789
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:18:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:48:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:48:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1bc8ba60-21b4-492a-bc64-1853d837f5b0
01/29/2025 16:48:58:INFO:Received: evaluate message 1bc8ba60-21b4-492a-bc64-1853d837f5b0
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 16:56:08:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:56:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:56:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e9b3f504-0835-4e77-8b38-096ea122fb03
01/29/2025 16:56:49:INFO:Received: train message e9b3f504-0835-4e77-8b38-096ea122fb03
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:59:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:31:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:31:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3e0eef20-94bc-49f1-8585-254ae02b5b95
01/29/2025 17:31:10:INFO:Received: evaluate message 3e0eef20-94bc-49f1-8585-254ae02b5b95
[92mINFO [0m:      Sent reply
01/29/2025 17:35:50:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:36:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:36:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d082dd03-1c92-404f-bf37-901f4dfc876f
01/29/2025 17:36:25:INFO:Received: train message d082dd03-1c92-404f-bf37-901f4dfc876f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:38:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:12:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:12:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aa0fbc5e-f9c7-490a-baab-4e045d1c3885
01/29/2025 18:12:52:INFO:Received: evaluate message aa0fbc5e-f9c7-490a-baab-4e045d1c3885
[92mINFO [0m:      Sent reply
01/29/2025 18:17:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:18:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:18:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7625c875-d475-4ea4-a9cf-f3e6fe502ba1
01/29/2025 18:18:30:INFO:Received: train message 7625c875-d475-4ea4-a9cf-f3e6fe502ba1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:21:21:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:51:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:51:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b0c9652c-4af4-471b-a526-d25ea21c6cdc
01/29/2025 18:51:38:INFO:Received: evaluate message b0c9652c-4af4-471b-a526-d25ea21c6cdc
[92mINFO [0m:      Sent reply
01/29/2025 18:55:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:57:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:57:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b4858d33-3d7d-4726-ae8a-1fc277b9938c
01/29/2025 18:57:14:INFO:Received: train message b4858d33-3d7d-4726-ae8a-1fc277b9938c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:00:23:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:30:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:30:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e775b632-7ab4-44ee-ab3f-76483772a773
01/29/2025 19:30:18:INFO:Received: evaluate message e775b632-7ab4-44ee-ab3f-76483772a773

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 19:34:43:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:35:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:35:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 49877478-773b-4b93-b08f-9ce629ce23f0
01/29/2025 19:35:26:INFO:Received: train message 49877478-773b-4b93-b08f-9ce629ce23f0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:38:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:09:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:09:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8495843b-8294-4176-a734-7d52fdc56a21
01/29/2025 20:09:42:INFO:Received: evaluate message 8495843b-8294-4176-a734-7d52fdc56a21
[92mINFO [0m:      Sent reply
01/29/2025 20:14:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:14:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:14:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e927a06b-db20-4bf0-981d-e91351c2792c
01/29/2025 20:14:51:INFO:Received: train message e927a06b-db20-4bf0-981d-e91351c2792c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:17:08:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1c26fde1-567f-4e04-a688-21f49b272267
01/29/2025 20:48:16:INFO:Received: evaluate message 1c26fde1-567f-4e04-a688-21f49b272267
[92mINFO [0m:      Sent reply
01/29/2025 20:52:17:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:54:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:54:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bd9b145d-cc20-46cd-9874-75e8c949d40c
01/29/2025 20:54:17:INFO:Received: train message bd9b145d-cc20-46cd-9874-75e8c949d40c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:56:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:28:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:28:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a957f2fe-a88c-48a7-91d0-c1666b8ac233
01/29/2025 21:28:47:INFO:Received: evaluate message a957f2fe-a88c-48a7-91d0-c1666b8ac233
[92mINFO [0m:      Sent reply
01/29/2025 21:34:29:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:35:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:35:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 70a8d990-57b7-4dd3-8c78-a88137e95b74
01/29/2025 21:35:21:INFO:Received: train message 70a8d990-57b7-4dd3-8c78-a88137e95b74

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:38:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:11:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:11:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c0187cad-e0f3-4af0-ad43-efda6893633e
01/29/2025 22:11:06:INFO:Received: evaluate message c0187cad-e0f3-4af0-ad43-efda6893633e
[92mINFO [0m:      Sent reply
01/29/2025 22:15:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:17:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:17:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2462d4c5-6f9e-439e-8aad-605e480d3188
01/29/2025 22:17:10:INFO:Received: train message 2462d4c5-6f9e-439e-8aad-605e480d3188
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:19:07:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:54:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:54:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c428294d-4ddd-47e3-840f-308e08669fb2
01/29/2025 22:54:06:INFO:Received: evaluate message c428294d-4ddd-47e3-840f-308e08669fb2
[92mINFO [0m:      Sent reply
01/29/2025 22:58:49:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:59:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:59:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ec6587c4-dbb8-4413-884e-3befc484ec92
01/29/2025 22:59:34:INFO:Received: train message ec6587c4-dbb8-4413-884e-3befc484ec92
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:01:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:38:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:38:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message de05b99a-64e9-4de8-8649-51d06782f8da
01/29/2025 23:38:49:INFO:Received: evaluate message de05b99a-64e9-4de8-8649-51d06782f8da
[92mINFO [0m:      Sent reply
01/29/2025 23:44:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:45:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:45:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0d616e92-59d6-4bec-8dcd-243b492bbfce
01/29/2025 23:45:19:INFO:Received: train message 0d616e92-59d6-4bec-8dcd-243b492bbfce
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:48:11:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:21:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:21:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bcb26aab-6385-405f-b00a-80cc254b5b70
01/30/2025 00:21:05:INFO:Received: evaluate message bcb26aab-6385-405f-b00a-80cc254b5b70
[92mINFO [0m:      Sent reply
01/30/2025 00:25:09:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:27:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:27:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message afed4b52-39a5-4700-b717-3bf0a0d907a5
01/30/2025 00:27:05:INFO:Received: train message afed4b52-39a5-4700-b717-3bf0a0d907a5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:29:03:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:08:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:08:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4fadacf9-c01c-4a92-bdad-82eabe9a2fb6
01/30/2025 01:08:06:INFO:Received: evaluate message 4fadacf9-c01c-4a92-bdad-82eabe9a2fb6
[92mINFO [0m:      Sent reply
01/30/2025 01:12:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:13:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:13:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 91a6f150-bfdc-4c89-b67b-e4e7ce0e2187
01/30/2025 01:13:37:INFO:Received: train message 91a6f150-bfdc-4c89-b67b-e4e7ce0e2187
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:15:30:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:55:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:55:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b7307fe3-09df-451c-9c26-b998f141e36f
01/30/2025 01:55:30:INFO:Received: evaluate message b7307fe3-09df-451c-9c26-b998f141e36f
[92mINFO [0m:      Sent reply
01/30/2025 01:59:31:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:00:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:00:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd2355bd-243f-4371-adcf-6e7f21b6fd81
01/30/2025 02:00:18:INFO:Received: train message cd2355bd-243f-4371-adcf-6e7f21b6fd81
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.25
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.003451052587479353, 0.014065812341868877, 0.0004647714376915246, 0.000122997080325149, 0.0010464267106726766, 0.0005606114282272756, 0.000663566286675632, 0.00020367460092529655]
Noise Multiplier after list and tensor:  0.002572364059233223
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:02:29:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:37:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:37:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a4d694f9-1bff-4d7e-bf34-f5f76aee6f5a
01/30/2025 02:37:34:INFO:Received: evaluate message a4d694f9-1bff-4d7e-bf34-f5f76aee6f5a
[92mINFO [0m:      Sent reply
01/30/2025 02:42:01:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:42:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:42:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 112766b2-dedc-42d8-adf4-6547e8769e4d
01/30/2025 02:42:27:INFO:Received: reconnect message 112766b2-dedc-42d8-adf4-6547e8769e4d
01/30/2025 02:42:27:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 02:42:27:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047, 1.3677138108857638], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804, 0.6331051147805075], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232, 0.8855101957814356]}



Final client history:
{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047, 1.3677138108857638], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804, 0.6331051147805075], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232, 0.8855101957814356]}

