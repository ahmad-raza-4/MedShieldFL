nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 11:09:30:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 11:09:30:DEBUG:ChannelConnectivity.IDLE
01/29/2025 11:09:30:DEBUG:ChannelConnectivity.CONNECTING
01/29/2025 11:09:30:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 11:09:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:09:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message ad65ff74-629b-44c2-91f1-53cc375785ba
01/29/2025 11:09:30:INFO:Received: get_parameters message ad65ff74-629b-44c2-91f1-53cc375785ba
[92mINFO [0m:      Sent reply
01/29/2025 11:09:36:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:18:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:18:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b3b211ab-3769-4737-97c2-851ddbc857ef
01/29/2025 11:18:33:INFO:Received: train message b3b211ab-3769-4737-97c2-851ddbc857ef
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:21:29:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:10:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:10:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 50c81c1a-21a2-4cf3-9be6-08eb51b8c768
01/29/2025 12:10:05:INFO:Received: evaluate message 50c81c1a-21a2-4cf3-9be6-08eb51b8c768
[92mINFO [0m:      Sent reply
01/29/2025 12:14:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:14:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:14:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 381876fb-c6c6-4dff-b46b-2676d11181e6
01/29/2025 12:14:54:INFO:Received: train message 381876fb-c6c6-4dff-b46b-2676d11181e6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:17:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:47:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:47:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6e88712c-7922-46e4-855f-bedcc0f776a4
01/29/2025 12:47:07:INFO:Received: evaluate message 6e88712c-7922-46e4-855f-bedcc0f776a4
[92mINFO [0m:      Sent reply
01/29/2025 12:51:23:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:52:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:52:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9fa90f05-2d58-4b4f-b8df-29b173d70502
01/29/2025 12:52:28:INFO:Received: train message 9fa90f05-2d58-4b4f-b8df-29b173d70502
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:55:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:30:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:30:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c732b30e-3446-4d2f-aae0-3da80ea2cbd1
01/29/2025 13:30:53:INFO:Received: evaluate message c732b30e-3446-4d2f-aae0-3da80ea2cbd1
[92mINFO [0m:      Sent reply
01/29/2025 13:35:01:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:36:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:36:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 68aba71d-ea23-426d-9df2-f728e6dca183
01/29/2025 13:36:33:INFO:Received: train message 68aba71d-ea23-426d-9df2-f728e6dca183
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:39:49:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:08:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:08:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 51a8970a-bda4-495d-9b1b-74b607540843
01/29/2025 14:08:19:INFO:Received: evaluate message 51a8970a-bda4-495d-9b1b-74b607540843
[92mINFO [0m:      Sent reply
01/29/2025 14:13:30:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:14:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:14:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1544dda1-adfe-4a18-8c1a-1df104619748
01/29/2025 14:14:25:INFO:Received: train message 1544dda1-adfe-4a18-8c1a-1df104619748
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:16:32:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:50:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:50:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8a0aa66e-0a80-41a5-a239-536d452ed884
01/29/2025 14:50:25:INFO:Received: evaluate message 8a0aa66e-0a80-41a5-a239-536d452ed884
[92mINFO [0m:      Sent reply
01/29/2025 14:54:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:55:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:55:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ffbd93bb-7836-4587-9949-c6baa07366be
01/29/2025 14:55:34:INFO:Received: train message ffbd93bb-7836-4587-9949-c6baa07366be
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:57:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:29:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:29:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a8564303-9b9c-401d-a516-96436fc6a13b
01/29/2025 15:29:52:INFO:Received: evaluate message a8564303-9b9c-401d-a516-96436fc6a13b
[92mINFO [0m:      Sent reply
01/29/2025 15:34:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:34:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:34:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e02519e8-21b5-4ec7-8cd6-2a2d4c26ee98
01/29/2025 15:34:55:INFO:Received: train message e02519e8-21b5-4ec7-8cd6-2a2d4c26ee98
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345], 'accuracy': [0.5690696737817157], 'auc': [0.830051779313479]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765], 'accuracy': [0.5690696737817157, 0.6153846153846154], 'auc': [0.830051779313479, 0.8663737657560014]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:37:31:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:08:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:08:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6bf01091-f41f-4634-9959-83e919cea41d
01/29/2025 16:08:32:INFO:Received: evaluate message 6bf01091-f41f-4634-9959-83e919cea41d
[92mINFO [0m:      Sent reply
01/29/2025 16:13:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f5d8246f-2d85-4c5a-9da4-b505d6135c6e
01/29/2025 16:15:13:INFO:Received: train message f5d8246f-2d85-4c5a-9da4-b505d6135c6e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:18:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:49:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:49:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 604df19d-d279-4166-aa8c-0a2209aa79e3
01/29/2025 16:49:51:INFO:Received: evaluate message 604df19d-d279-4166-aa8c-0a2209aa79e3
[92mINFO [0m:      Sent reply
01/29/2025 16:56:50:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:57:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:57:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bb8b7a23-7bb6-4496-9d97-2f93907a0d8a
01/29/2025 16:57:46:INFO:Received: train message bb8b7a23-7bb6-4496-9d97-2f93907a0d8a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:00:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:31:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:31:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9e768a63-e656-41ea-87fe-02a1d6d64eaa
01/29/2025 17:31:53:INFO:Received: evaluate message 9e768a63-e656-41ea-87fe-02a1d6d64eaa
[92mINFO [0m:      Sent reply
01/29/2025 17:36:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:36:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:36:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 88efabc2-b04f-4c15-8b79-fc519fd3fb26
01/29/2025 17:36:53:INFO:Received: train message 88efabc2-b04f-4c15-8b79-fc519fd3fb26
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:39:33:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:13:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:13:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message db40a633-fd89-4a36-b353-9c9b91648ef9
01/29/2025 18:13:37:INFO:Received: evaluate message db40a633-fd89-4a36-b353-9c9b91648ef9
[92mINFO [0m:      Sent reply
01/29/2025 18:18:01:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:19:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:19:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7cad6170-e784-4102-9c54-69bb66f7d001
01/29/2025 18:19:18:INFO:Received: train message 7cad6170-e784-4102-9c54-69bb66f7d001
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:21:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:52:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:52:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 25682421-c939-4c3b-bc38-8336aa0bbcf4
01/29/2025 18:52:48:INFO:Received: evaluate message 25682421-c939-4c3b-bc38-8336aa0bbcf4
[92mINFO [0m:      Sent reply
01/29/2025 18:56:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:58:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:58:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bf2bc8a2-2efd-420b-a30e-d2a75470e661
01/29/2025 18:58:45:INFO:Received: train message bf2bc8a2-2efd-420b-a30e-d2a75470e661
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:00:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:31:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:31:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bb48e3e8-8007-4a62-b1cc-24ea54e10fe4
01/29/2025 19:31:25:INFO:Received: evaluate message bb48e3e8-8007-4a62-b1cc-24ea54e10fe4
[92mINFO [0m:      Sent reply
01/29/2025 19:34:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:36:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:36:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9548783c-5b87-4d09-b437-16958f04346f
01/29/2025 19:36:48:INFO:Received: train message 9548783c-5b87-4d09-b437-16958f04346f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:40:38:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:10:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:10:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b6e970e1-da6d-4452-98ca-06df8229283e
01/29/2025 20:10:45:INFO:Received: evaluate message b6e970e1-da6d-4452-98ca-06df8229283e
[92mINFO [0m:      Sent reply
01/29/2025 20:15:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:16:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:16:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d15a1268-2bc7-4fa0-a000-5864db08997f
01/29/2025 20:16:13:INFO:Received: train message d15a1268-2bc7-4fa0-a000-5864db08997f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:19:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 868ddaed-3809-4e51-a409-a7890ff25464
01/29/2025 20:48:23:INFO:Received: evaluate message 868ddaed-3809-4e51-a409-a7890ff25464
[92mINFO [0m:      Sent reply
01/29/2025 20:53:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:53:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:53:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 26fdcf3c-289d-422d-9629-b1b530ce74ac
01/29/2025 20:53:39:INFO:Received: train message 26fdcf3c-289d-422d-9629-b1b530ce74ac
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:57:01:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:26:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:26:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 454e693b-bd54-4e41-9f44-12d0a1b25ca6
01/29/2025 21:26:56:INFO:Received: evaluate message 454e693b-bd54-4e41-9f44-12d0a1b25ca6
[92mINFO [0m:      Sent reply
01/29/2025 21:32:03:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:33:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:33:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ed8b16b5-1ca6-4f57-8e95-4704350b0014
01/29/2025 21:33:03:INFO:Received: train message ed8b16b5-1ca6-4f57-8e95-4704350b0014
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:35:57:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:06:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:06:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 80e17fda-1897-4e4d-ace8-7ec5a9c603c0
01/29/2025 22:06:13:INFO:Received: evaluate message 80e17fda-1897-4e4d-ace8-7ec5a9c603c0
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 22:11:23:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:12:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:12:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cbb4ab92-dd63-4568-9823-596e34833c1c
01/29/2025 22:12:00:INFO:Received: train message cbb4ab92-dd63-4568-9823-596e34833c1c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:15:32:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:44:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:44:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5cdfc27a-05d0-45b6-baf1-3c7881ee3283
01/29/2025 22:44:33:INFO:Received: evaluate message 5cdfc27a-05d0-45b6-baf1-3c7881ee3283
[92mINFO [0m:      Sent reply
01/29/2025 22:49:34:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:50:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:50:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c84c55d5-b502-405b-b1c4-2c3c7e1e3051
01/29/2025 22:50:08:INFO:Received: train message c84c55d5-b502-405b-b1c4-2c3c7e1e3051
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:52:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:27:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:27:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9be7430c-5639-41c3-9635-b9301d4ab91d
01/29/2025 23:27:53:INFO:Received: evaluate message 9be7430c-5639-41c3-9635-b9301d4ab91d
[92mINFO [0m:      Sent reply
01/29/2025 23:31:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:33:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:33:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1eeef610-f0bc-47b9-a436-fb5c54b79a85
01/29/2025 23:33:59:INFO:Received: train message 1eeef610-f0bc-47b9-a436-fb5c54b79a85
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:36:31:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:07:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:07:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message be23f328-951c-47e8-9bbd-e63db0dd096a
01/30/2025 00:07:25:INFO:Received: evaluate message be23f328-951c-47e8-9bbd-e63db0dd096a
[92mINFO [0m:      Sent reply
01/30/2025 00:11:14:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:12:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:12:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b76d8275-9895-4d40-92b6-1431a700a873
01/30/2025 00:12:46:INFO:Received: train message b76d8275-9895-4d40-92b6-1431a700a873
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:16:13:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:47:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:47:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 05b74c9d-46e4-47fe-ad21-e8a4dbd903a1
01/30/2025 00:47:25:INFO:Received: evaluate message 05b74c9d-46e4-47fe-ad21-e8a4dbd903a1

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
[92mINFO [0m:      Sent reply
01/30/2025 00:52:33:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:52:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:52:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5277b7dc-0a84-41a5-bfc2-e0dda485cb81
01/30/2025 00:52:59:INFO:Received: train message 5277b7dc-0a84-41a5-bfc2-e0dda485cb81
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:56:13:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:33:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:33:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fbb4336d-286f-4f7e-8b0f-c312a3c48c61
01/30/2025 01:33:47:INFO:Received: evaluate message fbb4336d-286f-4f7e-8b0f-c312a3c48c61
[92mINFO [0m:      Sent reply
01/30/2025 01:38:59:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:39:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:39:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ae2f005-f0ea-410c-a64b-234b8a6019b4
01/30/2025 01:39:43:INFO:Received: train message 8ae2f005-f0ea-410c-a64b-234b8a6019b4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:43:03:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:15:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:15:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 04420645-75a2-4584-b912-099b058997f0
01/30/2025 02:15:31:INFO:Received: evaluate message 04420645-75a2-4584-b912-099b058997f0
[92mINFO [0m:      Sent reply
01/30/2025 02:20:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:21:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:21:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1bee2eb8-5386-4d65-8ab9-07f213b5d649
01/30/2025 02:21:13:INFO:Received: train message 1bee2eb8-5386-4d65-8ab9-07f213b5d649
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:23:34:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:56:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:56:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3566a3a6-0ff4-425b-9000-148d5caa2457
01/30/2025 02:56:05:INFO:Received: evaluate message 3566a3a6-0ff4-425b-9000-148d5caa2457
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 03:01:31:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:01:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:01:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1ec51bb5-9b62-4cc1-bb44-bb5b81d1bccb
01/30/2025 03:01:57:INFO:Received: train message 1ec51bb5-9b62-4cc1-bb44-bb5b81d1bccb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:04:28:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:31:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:31:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b41e473a-1fb9-4037-bb71-10f2e8392a29
01/30/2025 03:31:58:INFO:Received: evaluate message b41e473a-1fb9-4037-bb71-10f2e8392a29
[92mINFO [0m:      Sent reply
01/30/2025 03:36:03:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:36:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:36:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 071c2691-d184-4034-9b1b-28ec22d42947
01/30/2025 03:36:56:INFO:Received: train message 071c2691-d184-4034-9b1b-28ec22d42947
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:39:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:11:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:11:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 75777129-c8af-43d2-90fc-bd4fde22361e
01/30/2025 04:11:14:INFO:Received: evaluate message 75777129-c8af-43d2-90fc-bd4fde22361e
[92mINFO [0m:      Sent reply
01/30/2025 04:15:44:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:17:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:17:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 21661acc-608f-4858-bb7c-f72ed9998b43
01/30/2025 04:17:14:INFO:Received: train message 21661acc-608f-4858-bb7c-f72ed9998b43
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:20:24:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:54:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:54:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e2135a06-069c-47bf-b3d5-889cb8846719
01/30/2025 04:54:41:INFO:Received: evaluate message e2135a06-069c-47bf-b3d5-889cb8846719

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.514068603515625
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001421525375917554, 0.005804517772048712, 0.00018538771837484092, 5.468328527058475e-05, 0.0004363921470940113, 0.00023537431843578815, 0.0002567874325904995, 8.672752301208675e-05]
Noise Multiplier after list and tensor:  0.0010601744465930096
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 05:00:26:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:01:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:01:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 207337fa-8bc4-40ea-9df0-2caa96d3766a
01/30/2025 05:01:15:INFO:Received: train message 207337fa-8bc4-40ea-9df0-2caa96d3766a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:04:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:40:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:40:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b664a1bc-ff2a-45be-8fcb-60d1cb6574c2
01/30/2025 05:40:26:INFO:Received: evaluate message b664a1bc-ff2a-45be-8fcb-60d1cb6574c2
[92mINFO [0m:      Sent reply
01/30/2025 05:44:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:46:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:46:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 19fbe0fe-266d-456e-bee2-dfde85b98320
01/30/2025 05:46:27:INFO:Received: train message 19fbe0fe-266d-456e-bee2-dfde85b98320
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:48:53:INFO:Sent reply
