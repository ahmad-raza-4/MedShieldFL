nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 11:13:21:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 11:13:21:DEBUG:ChannelConnectivity.IDLE
01/29/2025 11:13:21:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 11:18:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:18:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 34d693d7-cd42-4046-a45e-7f9509e6147f
01/29/2025 11:18:27:INFO:Received: train message 34d693d7-cd42-4046-a45e-7f9509e6147f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:29:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:09:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:09:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bcb68685-ec2a-45a5-afdb-f8269ec2ae85
01/29/2025 12:09:59:INFO:Received: evaluate message bcb68685-ec2a-45a5-afdb-f8269ec2ae85
[92mINFO [0m:      Sent reply
01/29/2025 12:14:27:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:15:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:15:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2358ec96-773b-44a9-a62f-54869b30c57f
01/29/2025 12:15:12:INFO:Received: train message 2358ec96-773b-44a9-a62f-54869b30c57f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:31:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:47:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:47:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message db368897-e409-4b68-94f7-085486f7e060
01/29/2025 12:47:16:INFO:Received: evaluate message db368897-e409-4b68-94f7-085486f7e060
[92mINFO [0m:      Sent reply
01/29/2025 12:51:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:52:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:52:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ea2cd41e-65aa-4828-a420-b7fcac92f931
01/29/2025 12:52:28:INFO:Received: train message ea2cd41e-65aa-4828-a420-b7fcac92f931
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:07:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:30:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:30:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e5c0ffc0-68cc-4010-9c68-8617d1d93351
01/29/2025 13:30:55:INFO:Received: evaluate message e5c0ffc0-68cc-4010-9c68-8617d1d93351
[92mINFO [0m:      Sent reply
01/29/2025 13:35:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:36:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:36:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4ed83c0e-e8c3-471e-904a-756c9b3df8e3
01/29/2025 13:36:16:INFO:Received: train message 4ed83c0e-e8c3-471e-904a-756c9b3df8e3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:53:39:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:08:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:08:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a2a01450-4cd3-409a-9ca6-81482e8eba42
01/29/2025 14:08:22:INFO:Received: evaluate message a2a01450-4cd3-409a-9ca6-81482e8eba42
[92mINFO [0m:      Sent reply
01/29/2025 14:14:07:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:14:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:14:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c99f41b0-bbd5-4e42-ab28-87db75b01ded
01/29/2025 14:14:49:INFO:Received: train message c99f41b0-bbd5-4e42-ab28-87db75b01ded
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:30:50:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:50:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:50:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4f9df370-d9a1-4528-a074-a279a4abfa26
01/29/2025 14:50:18:INFO:Received: evaluate message 4f9df370-d9a1-4528-a074-a279a4abfa26
[92mINFO [0m:      Sent reply
01/29/2025 14:55:08:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:55:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:55:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8b270411-813f-4048-8284-84cf68472511
01/29/2025 14:55:58:INFO:Received: train message 8b270411-813f-4048-8284-84cf68472511
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:15:02:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:29:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:29:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c6858a97-a89e-4a60-b2f6-3a996101f3b5
01/29/2025 15:29:56:INFO:Received: evaluate message c6858a97-a89e-4a60-b2f6-3a996101f3b5
[92mINFO [0m:      Sent reply
01/29/2025 15:34:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:34:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:34:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 24861df4-0145-45c5-874b-12110d2b3a49
01/29/2025 15:34:49:INFO:Received: train message 24861df4-0145-45c5-874b-12110d2b3a49
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345], 'accuracy': [0.5690696737817157], 'auc': [0.830051779313479]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765], 'accuracy': [0.5690696737817157, 0.6153846153846154], 'auc': [0.830051779313479, 0.8663737657560014]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:53:28:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:08:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:08:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 45acab5f-f6a3-402b-9718-facc4303c94f
01/29/2025 16:08:36:INFO:Received: evaluate message 45acab5f-f6a3-402b-9718-facc4303c94f
[92mINFO [0m:      Sent reply
01/29/2025 16:14:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ed8a676e-6189-4de7-a4eb-00cd619f54e6
01/29/2025 16:15:27:INFO:Received: train message ed8a676e-6189-4de7-a4eb-00cd619f54e6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:36:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:49:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:49:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4c1c8c20-df69-42d0-97ff-7494b26766ae
01/29/2025 16:49:40:INFO:Received: evaluate message 4c1c8c20-df69-42d0-97ff-7494b26766ae
[92mINFO [0m:      Sent reply
01/29/2025 16:56:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:57:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:57:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 121756d1-1e23-4ab2-ab5d-d13cb9995541
01/29/2025 16:57:29:INFO:Received: train message 121756d1-1e23-4ab2-ab5d-d13cb9995541
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:14:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:31:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:31:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 718a4993-3c70-4dda-b8fa-5ba9a28bac05
01/29/2025 17:31:52:INFO:Received: evaluate message 718a4993-3c70-4dda-b8fa-5ba9a28bac05
[92mINFO [0m:      Sent reply
01/29/2025 17:36:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:36:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:36:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a46acaec-2f28-48a8-92e7-cb93cc31d962
01/29/2025 17:36:56:INFO:Received: train message a46acaec-2f28-48a8-92e7-cb93cc31d962
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:54:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:13:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:13:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 42d27ae9-6e82-4739-9283-515ab20cea9f
01/29/2025 18:13:41:INFO:Received: evaluate message 42d27ae9-6e82-4739-9283-515ab20cea9f
[92mINFO [0m:      Sent reply
01/29/2025 18:18:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:19:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:19:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c8300c1c-3441-48d7-99b3-a9834a5376aa
01/29/2025 18:19:19:INFO:Received: train message c8300c1c-3441-48d7-99b3-a9834a5376aa
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:35:57:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:53:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:53:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 69dc290d-54ed-4a26-8388-2f715c006ee5
01/29/2025 18:53:01:INFO:Received: evaluate message 69dc290d-54ed-4a26-8388-2f715c006ee5
[92mINFO [0m:      Sent reply
01/29/2025 18:57:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:58:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:58:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4bc0b230-e348-45e3-a021-50d5fe336a2d
01/29/2025 18:58:24:INFO:Received: train message 4bc0b230-e348-45e3-a021-50d5fe336a2d
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:17:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:31:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:31:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7e183760-a275-4f3b-a36f-7be3d18f00dd
01/29/2025 19:31:50:INFO:Received: evaluate message 7e183760-a275-4f3b-a36f-7be3d18f00dd
[92mINFO [0m:      Sent reply
01/29/2025 19:36:16:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:36:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:36:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 60033188-1d28-4dee-ab89-01a66668487e
01/29/2025 19:36:44:INFO:Received: train message 60033188-1d28-4dee-ab89-01a66668487e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:53:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:10:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:10:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 753d699b-3f16-4ade-b18e-60ef65e8b696
01/29/2025 20:10:54:INFO:Received: evaluate message 753d699b-3f16-4ade-b18e-60ef65e8b696
[92mINFO [0m:      Sent reply
01/29/2025 20:15:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:15:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:15:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 10731c74-a69c-4c6e-a1a9-3ca33c925ac6
01/29/2025 20:15:56:INFO:Received: train message 10731c74-a69c-4c6e-a1a9-3ca33c925ac6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:33:08:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message efd7f7c3-e5bb-4227-9497-5e0acd5127f6
01/29/2025 20:48:25:INFO:Received: evaluate message efd7f7c3-e5bb-4227-9497-5e0acd5127f6
[92mINFO [0m:      Sent reply
01/29/2025 20:53:05:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:53:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:53:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9f26482c-855d-4d3a-9ee4-ecca6e8f5704
01/29/2025 20:53:50:INFO:Received: train message 9f26482c-855d-4d3a-9ee4-ecca6e8f5704
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:09:44:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:26:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:26:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b92407af-7aff-43ca-9e5e-6aa4372f6492
01/29/2025 21:26:52:INFO:Received: evaluate message b92407af-7aff-43ca-9e5e-6aa4372f6492
[92mINFO [0m:      Sent reply
01/29/2025 21:32:23:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:32:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:32:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4a1d7bde-b8d8-4007-9662-a730a65e7096
01/29/2025 21:32:54:INFO:Received: train message 4a1d7bde-b8d8-4007-9662-a730a65e7096
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:47:31:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:06:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:06:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c4931eee-db42-41d4-8b14-d4ec7f6c231e
01/29/2025 22:06:01:INFO:Received: evaluate message c4931eee-db42-41d4-8b14-d4ec7f6c231e
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 22:10:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:12:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:12:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e6d5f9c1-5e5d-40d1-b448-6624afbcf5c4
01/29/2025 22:12:01:INFO:Received: train message e6d5f9c1-5e5d-40d1-b448-6624afbcf5c4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:24:44:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:44:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:44:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5ad48bff-3d39-45df-9d48-337bcfafc1a5
01/29/2025 22:44:33:INFO:Received: evaluate message 5ad48bff-3d39-45df-9d48-337bcfafc1a5
[92mINFO [0m:      Sent reply
01/29/2025 22:49:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:50:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:50:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 20bcdda7-aee9-4236-98da-b3875160ef18
01/29/2025 22:50:21:INFO:Received: train message 20bcdda7-aee9-4236-98da-b3875160ef18
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:04:16:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:27:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:27:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8907006c-526b-432f-be7d-7e7cd32fd353
01/29/2025 23:27:35:INFO:Received: evaluate message 8907006c-526b-432f-be7d-7e7cd32fd353
[92mINFO [0m:      Sent reply
01/29/2025 23:33:01:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:34:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:34:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eb60e48e-e612-47c7-a90d-3284b700affa
01/29/2025 23:34:07:INFO:Received: train message eb60e48e-e612-47c7-a90d-3284b700affa
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:52:32:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:07:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:07:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4fb1b5da-30e6-4bbb-992b-903092214887
01/30/2025 00:07:26:INFO:Received: evaluate message 4fb1b5da-30e6-4bbb-992b-903092214887
[92mINFO [0m:      Sent reply
01/30/2025 00:12:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:12:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:12:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c1ff9f0b-9227-4827-8215-3470b80783ce
01/30/2025 00:12:46:INFO:Received: train message c1ff9f0b-9227-4827-8215-3470b80783ce
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:29:32:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:47:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:47:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a6404d6b-21a3-4d79-bdb7-c1e2cfd25d61
01/30/2025 00:47:16:INFO:Received: evaluate message a6404d6b-21a3-4d79-bdb7-c1e2cfd25d61

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 00:52:18:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:53:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:53:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1f00e124-2ea8-4b78-8999-999963f0dc08
01/30/2025 00:53:16:INFO:Received: train message 1f00e124-2ea8-4b78-8999-999963f0dc08
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:09:39:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:33:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:33:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 371ffb02-6c46-4946-b9af-36eff66a168c
01/30/2025 01:33:24:INFO:Received: evaluate message 371ffb02-6c46-4946-b9af-36eff66a168c
[92mINFO [0m:      Sent reply
01/30/2025 01:38:46:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:39:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:39:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0804ff11-f58d-4d28-8918-66764bb4974f
01/30/2025 01:39:22:INFO:Received: train message 0804ff11-f58d-4d28-8918-66764bb4974f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:54:03:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:15:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:15:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 18f0a752-ac85-477e-b478-7dd7c519196c
01/30/2025 02:15:32:INFO:Received: evaluate message 18f0a752-ac85-477e-b478-7dd7c519196c
[92mINFO [0m:      Sent reply
01/30/2025 02:20:06:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:21:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:21:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 14f98e96-6fd9-42cc-993d-0aff1a8f125b
01/30/2025 02:21:13:INFO:Received: train message 14f98e96-6fd9-42cc-993d-0aff1a8f125b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:34:38:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:55:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:55:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 91cde850-9634-434a-a299-3507c0c641be
01/30/2025 02:55:53:INFO:Received: evaluate message 91cde850-9634-434a-a299-3507c0c641be
[92mINFO [0m:      Sent reply
01/30/2025 03:01:19:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:02:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:02:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 20b8f29a-ee39-4ffc-9397-fa9b9bb3844e
01/30/2025 03:02:08:INFO:Received: train message 20b8f29a-ee39-4ffc-9397-fa9b9bb3844e

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:16:44:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:31:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:31:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 80ebc4a4-1503-4dbb-9794-12a14e2b3979
01/30/2025 03:31:57:INFO:Received: evaluate message 80ebc4a4-1503-4dbb-9794-12a14e2b3979
[92mINFO [0m:      Sent reply
01/30/2025 03:36:10:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:36:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:36:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 968703fe-2752-4f3c-8bd3-c49827987599
01/30/2025 03:36:50:INFO:Received: train message 968703fe-2752-4f3c-8bd3-c49827987599
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:51:21:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:11:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:11:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 265a5c98-8546-41ee-921a-acf2f64a9679
01/30/2025 04:11:23:INFO:Received: evaluate message 265a5c98-8546-41ee-921a-acf2f64a9679
[92mINFO [0m:      Sent reply
01/30/2025 04:16:04:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:17:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:17:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 118854dc-60d1-42e8-9278-913204bf17c6
01/30/2025 04:17:26:INFO:Received: train message 118854dc-60d1-42e8-9278-913204bf17c6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:33:21:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:54:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:54:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 58aa0651-87e1-4678-8b55-0b1bab835ece
01/30/2025 04:54:35:INFO:Received: evaluate message 58aa0651-87e1-4678-8b55-0b1bab835ece
[92mINFO [0m:      Sent reply
01/30/2025 05:00:15:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:01:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:01:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 909ec824-e81e-4b07-9c51-61a423850d93
01/30/2025 05:01:06:INFO:Received: train message 909ec824-e81e-4b07-9c51-61a423850d93
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:14:28:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:40:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:40:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b132669c-3d4a-4e24-9287-f00d40382482
01/30/2025 05:40:43:INFO:Received: evaluate message b132669c-3d4a-4e24-9287-f00d40382482
[92mINFO [0m:      Sent reply
01/30/2025 05:45:14:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:46:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:46:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 77a54a9b-d98e-492b-8868-893aeb4a29df
01/30/2025 05:46:55:INFO:Received: train message 77a54a9b-d98e-492b-8868-893aeb4a29df
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 06:00:45:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:33:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:33:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 552e6585-3d99-4e60-9c7f-4278edc5b3ae
01/30/2025 06:33:58:INFO:Received: evaluate message 552e6585-3d99-4e60-9c7f-4278edc5b3ae
[92mINFO [0m:      Sent reply
01/30/2025 06:38:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:39:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:39:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c4f6a5f0-4ad5-4dc6-80a8-d343e352e25d
01/30/2025 06:39:29:INFO:Received: train message c4f6a5f0-4ad5-4dc6-80a8-d343e352e25d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 06:53:22:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:10:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:10:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0214d61a-bb3b-44dd-a148-edc6264c8021
01/30/2025 07:10:39:INFO:Received: evaluate message 0214d61a-bb3b-44dd-a148-edc6264c8021
[92mINFO [0m:      Sent reply
01/30/2025 07:15:09:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:16:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:16:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5184b984-78ca-4f80-a59e-965cae0f133e
01/30/2025 07:16:28:INFO:Received: train message 5184b984-78ca-4f80-a59e-965cae0f133e
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434, 1.3048963505944273], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351, 0.653242045912203], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195, 0.9039743690802352]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434, 1.3048963505944273, 1.329263680124302], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351, 0.653242045912203, 0.6472009665726943], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195, 0.9039743690802352, 0.9046808053879412]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434, 1.3048963505944273, 1.329263680124302, 1.34127657160469], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351, 0.653242045912203, 0.6472009665726943, 0.6484091824405961], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195, 0.9039743690802352, 0.9046808053879412, 0.9038223419023863]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6976318359375
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.017823895439505577, 0.04378039762377739, 0.006034344434738159, 0.0013245376758277416, 0.014932413585484028, 0.002979007549583912, 0.0033056142274290323, 0.002667730674147606]
Noise Multiplier after list and tensor:  0.01160599265131168
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 07:30:03:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:51:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:51:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 77f67545-7bb2-4656-b4e2-d59dbd37a3ab
01/30/2025 07:51:34:INFO:Received: evaluate message 77f67545-7bb2-4656-b4e2-d59dbd37a3ab
[92mINFO [0m:      Sent reply
01/30/2025 07:56:35:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:58:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:58:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 27b820b5-51bb-4a89-a135-80a537b29176
01/30/2025 07:58:38:INFO:Received: reconnect message 27b820b5-51bb-4a89-a135-80a537b29176
01/30/2025 07:58:38:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 07:58:38:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434, 1.3048963505944273, 1.329263680124302, 1.34127657160469, 1.3089735003858145], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351, 0.653242045912203, 0.6472009665726943, 0.6484091824405961, 0.6604913411196134], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195, 0.9039743690802352, 0.9046808053879412, 0.9038223419023863, 0.9054790163872304]}



Final client history:
{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434, 1.3048963505944273, 1.329263680124302, 1.34127657160469, 1.3089735003858145], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351, 0.653242045912203, 0.6472009665726943, 0.6484091824405961, 0.6604913411196134], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195, 0.9039743690802352, 0.9046808053879412, 0.9038223419023863, 0.9054790163872304]}

