nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 11:10:15:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 11:10:15:DEBUG:ChannelConnectivity.IDLE
01/29/2025 11:10:15:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 11:18:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:18:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 21689bfc-2cf6-4a47-b7e6-def0dc708691
01/29/2025 11:18:32:INFO:Received: train message 21689bfc-2cf6-4a47-b7e6-def0dc708691
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:27:28:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:10:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:10:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6e89aa1f-96d6-41fc-9352-fcdea4d06dea
01/29/2025 12:10:05:INFO:Received: evaluate message 6e89aa1f-96d6-41fc-9352-fcdea4d06dea
[92mINFO [0m:      Sent reply
01/29/2025 12:14:29:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:15:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:15:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message beb32bdd-23f6-4a02-b529-c4d114908a33
01/29/2025 12:15:13:INFO:Received: train message beb32bdd-23f6-4a02-b529-c4d114908a33
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:20:10:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:47:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:47:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e69de3de-3a8b-40e1-9fd0-3568f26ab512
01/29/2025 12:47:07:INFO:Received: evaluate message e69de3de-3a8b-40e1-9fd0-3568f26ab512
[92mINFO [0m:      Sent reply
01/29/2025 12:51:41:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:52:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:52:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e88c6daa-b68c-4cf1-a948-f19e9b777830
01/29/2025 12:52:07:INFO:Received: train message e88c6daa-b68c-4cf1-a948-f19e9b777830
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:56:32:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:30:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:30:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ba2b7ba7-abda-4515-b890-0e0f838a1196
01/29/2025 13:30:44:INFO:Received: evaluate message ba2b7ba7-abda-4515-b890-0e0f838a1196
[92mINFO [0m:      Sent reply
01/29/2025 13:35:23:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:36:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:36:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1a6fb591-0b35-4654-bde1-2bff57db9f30
01/29/2025 13:36:27:INFO:Received: train message 1a6fb591-0b35-4654-bde1-2bff57db9f30
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:42:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:08:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:08:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ffc82283-aced-496c-89bf-007ca44900c6
01/29/2025 14:08:31:INFO:Received: evaluate message ffc82283-aced-496c-89bf-007ca44900c6
[92mINFO [0m:      Sent reply
01/29/2025 14:14:07:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:14:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:14:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b7b2c7bb-8de1-4e9d-bfbb-a6d49b08dc22
01/29/2025 14:14:45:INFO:Received: train message b7b2c7bb-8de1-4e9d-bfbb-a6d49b08dc22
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:19:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:50:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:50:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c66475c0-a71e-461f-acac-05715c3f3238
01/29/2025 14:50:26:INFO:Received: evaluate message c66475c0-a71e-461f-acac-05715c3f3238
[92mINFO [0m:      Sent reply
01/29/2025 14:55:15:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:55:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:55:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9227ab30-a1ba-448d-b989-ca9bae1a49b0
01/29/2025 14:55:59:INFO:Received: train message 9227ab30-a1ba-448d-b989-ca9bae1a49b0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:01:16:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:29:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:29:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7e944c3d-0a02-45a3-ba54-b78741ec0ee7
01/29/2025 15:29:38:INFO:Received: evaluate message 7e944c3d-0a02-45a3-ba54-b78741ec0ee7
[92mINFO [0m:      Sent reply
01/29/2025 15:33:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:34:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:34:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fddba4e7-f928-4aa8-82a7-350ed6180e80
01/29/2025 15:34:56:INFO:Received: train message fddba4e7-f928-4aa8-82a7-350ed6180e80
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345], 'accuracy': [0.5690696737817157], 'auc': [0.830051779313479]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765], 'accuracy': [0.5690696737817157, 0.6153846153846154], 'auc': [0.830051779313479, 0.8663737657560014]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:40:21:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:08:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:08:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eb95099e-e418-4011-913b-52276252cac4
01/29/2025 16:08:36:INFO:Received: evaluate message eb95099e-e418-4011-913b-52276252cac4
[92mINFO [0m:      Sent reply
01/29/2025 16:14:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dc7d6da2-4adc-4250-bc45-f9bf3decf6c3
01/29/2025 16:15:31:INFO:Received: train message dc7d6da2-4adc-4250-bc45-f9bf3decf6c3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:22:27:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:49:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:49:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 364a87fb-a843-462e-9da7-2a7f94cdb6e3
01/29/2025 16:49:52:INFO:Received: evaluate message 364a87fb-a843-462e-9da7-2a7f94cdb6e3
[92mINFO [0m:      Sent reply
01/29/2025 16:57:05:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:57:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:57:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 986b3c7d-7e6b-45a3-ab5f-7a0d75eae631
01/29/2025 16:57:49:INFO:Received: train message 986b3c7d-7e6b-45a3-ab5f-7a0d75eae631
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:03:16:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:31:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:31:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e34ba908-22ac-4dca-8ba8-53041c1696e2
01/29/2025 17:31:58:INFO:Received: evaluate message e34ba908-22ac-4dca-8ba8-53041c1696e2
[92mINFO [0m:      Sent reply
01/29/2025 17:36:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:36:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:36:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7c9a905c-ec99-49ab-8d9e-4a75fa8180a8
01/29/2025 17:36:39:INFO:Received: train message 7c9a905c-ec99-49ab-8d9e-4a75fa8180a8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:41:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:13:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:13:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 42b0bfe5-10b7-4d16-b26a-61db60d698c8
01/29/2025 18:13:37:INFO:Received: evaluate message 42b0bfe5-10b7-4d16-b26a-61db60d698c8
[92mINFO [0m:      Sent reply
01/29/2025 18:18:35:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:19:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:19:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3f1a3ae1-a32f-42cd-ac0e-84a5661b89c6
01/29/2025 18:19:32:INFO:Received: train message 3f1a3ae1-a32f-42cd-ac0e-84a5661b89c6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:24:43:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:53:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:53:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bfceda80-6686-41bb-94d1-75d0d984603e
01/29/2025 18:53:06:INFO:Received: evaluate message bfceda80-6686-41bb-94d1-75d0d984603e
[92mINFO [0m:      Sent reply
01/29/2025 18:57:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:58:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:58:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5219e131-2430-400d-b3fa-db936ed1ae1e
01/29/2025 18:58:37:INFO:Received: train message 5219e131-2430-400d-b3fa-db936ed1ae1e
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:05:36:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:31:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:31:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f2eff66b-ab89-4f96-bffe-964961ae2e56
01/29/2025 19:31:43:INFO:Received: evaluate message f2eff66b-ab89-4f96-bffe-964961ae2e56
[92mINFO [0m:      Sent reply
01/29/2025 19:36:07:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:36:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:36:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 91699c5d-5f4f-4ac7-b472-503332bddc4d
01/29/2025 19:36:57:INFO:Received: train message 91699c5d-5f4f-4ac7-b472-503332bddc4d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:42:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:10:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:10:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 44cfd27a-a99e-4cf1-80b0-d7eaab88475b
01/29/2025 20:10:38:INFO:Received: evaluate message 44cfd27a-a99e-4cf1-80b0-d7eaab88475b
[92mINFO [0m:      Sent reply
01/29/2025 20:14:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:16:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:16:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0508934e-a486-4a84-8f5e-ecb3ec058d9d
01/29/2025 20:16:15:INFO:Received: train message 0508934e-a486-4a84-8f5e-ecb3ec058d9d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:22:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 12c4ace0-0103-4b83-b238-44bdf5319a27
01/29/2025 20:48:15:INFO:Received: evaluate message 12c4ace0-0103-4b83-b238-44bdf5319a27
[92mINFO [0m:      Sent reply
01/29/2025 20:52:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:53:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:53:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9b90a912-63f8-4cc5-b781-477e850804b8
01/29/2025 20:53:31:INFO:Received: train message 9b90a912-63f8-4cc5-b781-477e850804b8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:57:34:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:26:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:26:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b8c6e0ae-78db-4ca5-a33c-6e467c569157
01/29/2025 21:26:43:INFO:Received: evaluate message b8c6e0ae-78db-4ca5-a33c-6e467c569157
[92mINFO [0m:      Sent reply
01/29/2025 21:32:02:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:33:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:33:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 760f5d5d-b23d-4512-96c7-67292a888f87
01/29/2025 21:33:00:INFO:Received: train message 760f5d5d-b23d-4512-96c7-67292a888f87
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:37:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:05:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:05:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6da0811e-43c9-4065-80f0-fcad323c6d01
01/29/2025 22:05:51:INFO:Received: evaluate message 6da0811e-43c9-4065-80f0-fcad323c6d01
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 22:09:41:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:12:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:12:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 53bd116c-14ed-4759-a550-b3ed2d3f33e3
01/29/2025 22:12:07:INFO:Received: train message 53bd116c-14ed-4759-a550-b3ed2d3f33e3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:16:34:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:44:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:44:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e400590f-dbd3-49d3-9c9c-45f49c5e227d
01/29/2025 22:44:33:INFO:Received: evaluate message e400590f-dbd3-49d3-9c9c-45f49c5e227d
[92mINFO [0m:      Sent reply
01/29/2025 22:49:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:50:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:50:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 40818556-91d2-4dc0-be4b-b937d2272aaf
01/29/2025 22:50:08:INFO:Received: train message 40818556-91d2-4dc0-be4b-b937d2272aaf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:54:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:27:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:27:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3d8057f3-b2e7-4f24-abdb-f39f280d3335
01/29/2025 23:27:55:INFO:Received: evaluate message 3d8057f3-b2e7-4f24-abdb-f39f280d3335
[92mINFO [0m:      Sent reply
01/29/2025 23:33:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:33:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:33:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9c25cd82-ee47-4c7a-9025-d75e02e7b414
01/29/2025 23:33:55:INFO:Received: train message 9c25cd82-ee47-4c7a-9025-d75e02e7b414
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:39:39:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:07:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:07:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 30b43b53-7eb5-4396-9bcc-8cbd91bbfc09
01/30/2025 00:07:26:INFO:Received: evaluate message 30b43b53-7eb5-4396-9bcc-8cbd91bbfc09
[92mINFO [0m:      Sent reply
01/30/2025 00:12:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:13:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:13:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 88bf4225-cf23-426a-8df7-9246b9a0a126
01/30/2025 00:13:06:INFO:Received: train message 88bf4225-cf23-426a-8df7-9246b9a0a126
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:17:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:47:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:47:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e9114264-ebed-4217-8f9a-6ac0ea16ad48
01/30/2025 00:47:18:INFO:Received: evaluate message e9114264-ebed-4217-8f9a-6ac0ea16ad48

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
[92mINFO [0m:      Sent reply
01/30/2025 00:51:08:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:53:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:53:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4a76a07f-6d13-41b7-a5ce-7c2310f1a036
01/30/2025 00:53:07:INFO:Received: train message 4a76a07f-6d13-41b7-a5ce-7c2310f1a036
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:57:20:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:33:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:33:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 646bf05b-e369-4d9d-a872-1d96cf31d7af
01/30/2025 01:33:34:INFO:Received: evaluate message 646bf05b-e369-4d9d-a872-1d96cf31d7af
[92mINFO [0m:      Sent reply
01/30/2025 01:37:44:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:39:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:39:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 912369f1-b484-4fad-9536-1e56fb96521e
01/30/2025 01:39:30:INFO:Received: train message 912369f1-b484-4fad-9536-1e56fb96521e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:43:35:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:15:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:15:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5aa7e58f-c3d1-41fc-b61e-90d30e7e5095
01/30/2025 02:15:34:INFO:Received: evaluate message 5aa7e58f-c3d1-41fc-b61e-90d30e7e5095
[92mINFO [0m:      Sent reply
01/30/2025 02:20:38:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:20:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:20:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6850f580-46e1-45e5-8708-1442923ddc27
01/30/2025 02:20:54:INFO:Received: train message 6850f580-46e1-45e5-8708-1442923ddc27
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:25:06:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:55:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:55:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1d0da028-23f5-461d-9816-84585c7f63e6
01/30/2025 02:55:53:INFO:Received: evaluate message 1d0da028-23f5-461d-9816-84585c7f63e6
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 03:01:08:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:02:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:02:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 607723c5-59e7-48fb-b9a1-550fbbb09aa0
01/30/2025 03:02:16:INFO:Received: train message 607723c5-59e7-48fb-b9a1-550fbbb09aa0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:06:46:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:31:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:31:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0002d38f-71f8-4560-b3d7-11fe686c6ffd
01/30/2025 03:31:52:INFO:Received: evaluate message 0002d38f-71f8-4560-b3d7-11fe686c6ffd
[92mINFO [0m:      Sent reply
01/30/2025 03:36:24:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:37:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:37:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c8252b50-284e-4246-8a63-2b28e8e81a5f
01/30/2025 03:37:17:INFO:Received: train message c8252b50-284e-4246-8a63-2b28e8e81a5f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:42:46:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:11:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:11:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5834bb94-369a-4731-b343-b94433073fc3
01/30/2025 04:11:09:INFO:Received: evaluate message 5834bb94-369a-4731-b343-b94433073fc3
[92mINFO [0m:      Sent reply
01/30/2025 04:16:27:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:17:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:17:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 922e1e43-740e-4270-8e36-275df5ae2501
01/30/2025 04:17:07:INFO:Received: train message 922e1e43-740e-4270-8e36-275df5ae2501
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:22:27:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:54:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:54:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c5e769d2-22c7-482e-837c-94f14dc0b836
01/30/2025 04:54:41:INFO:Received: evaluate message c5e769d2-22c7-482e-837c-94f14dc0b836

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.558319091796875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.004416628275066614, 0.00794275663793087, 0.00021626570378430188, 0.000746621866710484, 0.0037375164683908224, 0.0007953407475724816, 0.0003978984896093607, 0.00018095519044436514]
Noise Multiplier after list and tensor:  0.0023042479224386625
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 05:00:11:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:01:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:01:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4d61cf38-5e85-446e-b218-fb2c17eaba3f
01/30/2025 05:01:18:INFO:Received: train message 4d61cf38-5e85-446e-b218-fb2c17eaba3f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:06:35:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:40:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:40:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 34a271b0-5312-466c-a947-b2095c17af6a
01/30/2025 05:40:38:INFO:Received: evaluate message 34a271b0-5312-466c-a947-b2095c17af6a
[92mINFO [0m:      Sent reply
01/30/2025 05:45:48:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:46:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:46:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7604327a-2781-43b8-a3d7-5e76ed7fb9e1
01/30/2025 05:46:38:INFO:Received: train message 7604327a-2781-43b8-a3d7-5e76ed7fb9e1
