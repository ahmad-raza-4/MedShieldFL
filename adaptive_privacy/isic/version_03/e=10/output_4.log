nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 11:12:09:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 11:12:09:DEBUG:ChannelConnectivity.IDLE
01/29/2025 11:12:09:DEBUG:ChannelConnectivity.CONNECTING
01/29/2025 11:12:09:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 11:18:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:18:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 94d2c4ff-6b92-48dd-8176-3798177803bb
01/29/2025 11:18:32:INFO:Received: train message 94d2c4ff-6b92-48dd-8176-3798177803bb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:37:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:09:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:09:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0110acd9-e28f-4f30-81a7-3575576403c6
01/29/2025 12:09:45:INFO:Received: evaluate message 0110acd9-e28f-4f30-81a7-3575576403c6
[92mINFO [0m:      Sent reply
01/29/2025 12:13:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:15:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:15:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 77c78e0f-fa1a-484b-839c-771813e13518
01/29/2025 12:15:13:INFO:Received: train message 77c78e0f-fa1a-484b-839c-771813e13518
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:26:33:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:46:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:46:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 89f77380-fd0b-4f58-90c1-10530fc70c92
01/29/2025 12:46:57:INFO:Received: evaluate message 89f77380-fd0b-4f58-90c1-10530fc70c92
[92mINFO [0m:      Sent reply
01/29/2025 12:50:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:52:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:52:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8b1fa921-6922-4b64-b46e-1f8cc067b7b6
01/29/2025 12:52:20:INFO:Received: train message 8b1fa921-6922-4b64-b46e-1f8cc067b7b6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:02:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:31:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:31:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a042f0ae-7b36-4d26-8fdc-bece1d8a1f51
01/29/2025 13:31:01:INFO:Received: evaluate message a042f0ae-7b36-4d26-8fdc-bece1d8a1f51
[92mINFO [0m:      Sent reply
01/29/2025 13:35:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:36:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:36:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 13dd3544-eba1-4bd6-afe3-5f03726fa390
01/29/2025 13:36:34:INFO:Received: train message 13dd3544-eba1-4bd6-afe3-5f03726fa390
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:49:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:08:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:08:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4b162fe6-dc37-4fc9-8b29-713af800bac0
01/29/2025 14:08:31:INFO:Received: evaluate message 4b162fe6-dc37-4fc9-8b29-713af800bac0
[92mINFO [0m:      Sent reply
01/29/2025 14:14:10:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:14:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:14:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e2f500a2-6b68-4ea1-be10-a0735e9091d0
01/29/2025 14:14:48:INFO:Received: train message e2f500a2-6b68-4ea1-be10-a0735e9091d0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:25:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:49:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:49:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a9cebe12-eb27-468b-89a4-82cf0bfb7a47
01/29/2025 14:49:52:INFO:Received: evaluate message a9cebe12-eb27-468b-89a4-82cf0bfb7a47
[92mINFO [0m:      Sent reply
01/29/2025 14:54:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:55:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:55:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eb67eed9-9ff7-4409-8f0d-c68a37f4c3c1
01/29/2025 14:55:39:INFO:Received: train message eb67eed9-9ff7-4409-8f0d-c68a37f4c3c1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:10:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:29:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:29:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5c5f800b-e697-4f24-9d20-c3d77c2a1e27
01/29/2025 15:29:48:INFO:Received: evaluate message 5c5f800b-e697-4f24-9d20-c3d77c2a1e27
[92mINFO [0m:      Sent reply
01/29/2025 15:34:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:35:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:35:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 40c75555-115e-45c9-846f-1a2829edc34b
01/29/2025 15:35:06:INFO:Received: train message 40c75555-115e-45c9-846f-1a2829edc34b
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345], 'accuracy': [0.5690696737817157], 'auc': [0.830051779313479]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765], 'accuracy': [0.5690696737817157, 0.6153846153846154], 'auc': [0.830051779313479, 0.8663737657560014]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:47:30:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:08:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:08:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6cc10645-5551-441b-a576-37e3a0236865
01/29/2025 16:08:26:INFO:Received: evaluate message 6cc10645-5551-441b-a576-37e3a0236865
[92mINFO [0m:      Sent reply
01/29/2025 16:14:32:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6a357c8d-12d1-4313-9603-be6e3906a3bd
01/29/2025 16:15:57:INFO:Received: train message 6a357c8d-12d1-4313-9603-be6e3906a3bd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:33:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:49:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:49:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 28f9328c-48d8-4ed6-a1ef-cea94d54e184
01/29/2025 16:49:30:INFO:Received: evaluate message 28f9328c-48d8-4ed6-a1ef-cea94d54e184
[92mINFO [0m:      Sent reply
01/29/2025 16:55:42:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:57:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:57:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f95ed1ef-6fe1-4783-b63a-c13d94a8c099
01/29/2025 16:57:46:INFO:Received: train message f95ed1ef-6fe1-4783-b63a-c13d94a8c099
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:10:16:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:31:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:31:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d47fa58f-62f4-4793-984b-f648b4786dab
01/29/2025 17:31:39:INFO:Received: evaluate message d47fa58f-62f4-4793-984b-f648b4786dab
[92mINFO [0m:      Sent reply
01/29/2025 17:35:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:36:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:36:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4ab4876e-888e-402d-8d6b-9c0782a3dc10
01/29/2025 17:36:55:INFO:Received: train message 4ab4876e-888e-402d-8d6b-9c0782a3dc10
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:50:10:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:13:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:13:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8c726159-dba9-4714-a14c-1035ccc20f9f
01/29/2025 18:13:27:INFO:Received: evaluate message 8c726159-dba9-4714-a14c-1035ccc20f9f
[92mINFO [0m:      Sent reply
01/29/2025 18:18:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:19:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:19:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7462621c-a896-4b10-8f87-769fd0159e7d
01/29/2025 18:19:16:INFO:Received: train message 7462621c-a896-4b10-8f87-769fd0159e7d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:31:31:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:52:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:52:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bbac94ce-3de3-4b50-868e-cd365daa1957
01/29/2025 18:52:57:INFO:Received: evaluate message bbac94ce-3de3-4b50-868e-cd365daa1957
[92mINFO [0m:      Sent reply
01/29/2025 18:57:41:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:58:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:58:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8e82c23d-b036-4455-92ce-39eec786a50d
01/29/2025 18:58:37:INFO:Received: train message 8e82c23d-b036-4455-92ce-39eec786a50d
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:11:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:31:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:31:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 67a95311-8887-431b-bb04-102485f2daf9
01/29/2025 19:31:51:INFO:Received: evaluate message 67a95311-8887-431b-bb04-102485f2daf9
[92mINFO [0m:      Sent reply
01/29/2025 19:36:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:37:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:37:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2611363d-79cb-493e-85cb-3cf4e7461d7f
01/29/2025 19:37:00:INFO:Received: train message 2611363d-79cb-493e-85cb-3cf4e7461d7f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:49:21:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:11:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:11:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2d88dde5-75f2-4564-ac08-baf656ce2287
01/29/2025 20:11:00:INFO:Received: evaluate message 2d88dde5-75f2-4564-ac08-baf656ce2287
[92mINFO [0m:      Sent reply
01/29/2025 20:15:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:16:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:16:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e558d997-d88a-4ba8-b0aa-8af1ea787c18
01/29/2025 20:16:06:INFO:Received: train message e558d997-d88a-4ba8-b0aa-8af1ea787c18
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:28:57:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 45157317-d80d-437a-9068-565371ae7a0c
01/29/2025 20:48:11:INFO:Received: evaluate message 45157317-d80d-437a-9068-565371ae7a0c
[92mINFO [0m:      Sent reply
01/29/2025 20:52:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:53:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:53:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0a11a390-f816-477f-8df1-fa55fb9e036c
01/29/2025 20:53:52:INFO:Received: train message 0a11a390-f816-477f-8df1-fa55fb9e036c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:03:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:26:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:26:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6eed7d6f-3d7b-41f9-a8f4-80b7b2c6878a
01/29/2025 21:26:54:INFO:Received: evaluate message 6eed7d6f-3d7b-41f9-a8f4-80b7b2c6878a
[92mINFO [0m:      Sent reply
01/29/2025 21:32:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:32:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:32:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 699aae49-14ce-4da2-b9df-03b9ca53920e
01/29/2025 21:32:48:INFO:Received: train message 699aae49-14ce-4da2-b9df-03b9ca53920e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:43:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:06:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:06:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c38d3a1a-8919-4400-8202-fc4bd19ce5b2
01/29/2025 22:06:12:INFO:Received: evaluate message c38d3a1a-8919-4400-8202-fc4bd19ce5b2
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 22:10:41:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:12:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:12:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7e4dd4ec-aa55-43cb-afd2-af706603661a
01/29/2025 22:12:01:INFO:Received: train message 7e4dd4ec-aa55-43cb-afd2-af706603661a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:21:38:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:44:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:44:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2c8f1add-4688-4b10-b142-66c2181f8afd
01/29/2025 22:44:35:INFO:Received: evaluate message 2c8f1add-4688-4b10-b142-66c2181f8afd
[92mINFO [0m:      Sent reply
01/29/2025 22:49:11:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:49:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:49:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message db173c33-71a1-4542-ab79-3161816cb1a5
01/29/2025 22:49:57:INFO:Received: train message db173c33-71a1-4542-ab79-3161816cb1a5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:59:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:27:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:27:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a48e4944-fe87-4964-9c12-bbd6d4ded116
01/29/2025 23:27:40:INFO:Received: evaluate message a48e4944-fe87-4964-9c12-bbd6d4ded116
[92mINFO [0m:      Sent reply
01/29/2025 23:33:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:34:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:34:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8f5254f1-d6e8-4904-a0d2-f7a196256f6b
01/29/2025 23:34:06:INFO:Received: train message 8f5254f1-d6e8-4904-a0d2-f7a196256f6b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:48:03:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:07:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:07:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 89f631d2-c054-4ba5-a2ae-92dc98515a12
01/30/2025 00:07:15:INFO:Received: evaluate message 89f631d2-c054-4ba5-a2ae-92dc98515a12
[92mINFO [0m:      Sent reply
01/30/2025 00:12:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:13:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:13:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6932fa6d-dd56-4e55-8411-69e7ce941632
01/30/2025 00:13:09:INFO:Received: train message 6932fa6d-dd56-4e55-8411-69e7ce941632
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:22:56:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:47:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:47:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 14211935-4940-480c-a032-e45221112c5e
01/30/2025 00:47:22:INFO:Received: evaluate message 14211935-4940-480c-a032-e45221112c5e

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 00:51:27:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:53:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:53:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 46b9183f-b8e2-4506-8110-197237eb28a7
01/30/2025 00:53:15:INFO:Received: train message 46b9183f-b8e2-4506-8110-197237eb28a7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:02:25:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:33:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:33:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 50376cc2-e12b-4e32-8d7f-540cc06d48b4
01/30/2025 01:33:54:INFO:Received: evaluate message 50376cc2-e12b-4e32-8d7f-540cc06d48b4
[92mINFO [0m:      Sent reply
01/30/2025 01:38:13:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:39:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:39:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d00e371b-aa61-4906-a140-770f45975730
01/30/2025 01:39:48:INFO:Received: train message d00e371b-aa61-4906-a140-770f45975730
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:49:08:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:15:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:15:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 046d0a9f-d283-4ae8-b81c-80e09ee1929d
01/30/2025 02:15:45:INFO:Received: evaluate message 046d0a9f-d283-4ae8-b81c-80e09ee1929d
[92mINFO [0m:      Sent reply
01/30/2025 02:20:43:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:21:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:21:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d96a3314-06a3-4feb-a6b5-7f9f18e8d747
01/30/2025 02:21:14:INFO:Received: train message d96a3314-06a3-4feb-a6b5-7f9f18e8d747
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:30:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:56:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:56:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3c6fe30b-9a6c-4ba6-99cc-d4890acfc2ef
01/30/2025 02:56:02:INFO:Received: evaluate message 3c6fe30b-9a6c-4ba6-99cc-d4890acfc2ef
[92mINFO [0m:      Sent reply
01/30/2025 03:01:15:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:02:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:02:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 30f7da13-0536-4211-bf53-354c0a8e2c06
01/30/2025 03:02:23:INFO:Received: train message 30f7da13-0536-4211-bf53-354c0a8e2c06

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:12:27:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:32:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:32:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4bb52931-e4df-46d4-93da-12cbe4b9f2f4
01/30/2025 03:32:09:INFO:Received: evaluate message 4bb52931-e4df-46d4-93da-12cbe4b9f2f4
[92mINFO [0m:      Sent reply
01/30/2025 03:36:40:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:37:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:37:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 44843205-2cd7-4170-b45a-69f197e322f9
01/30/2025 03:37:17:INFO:Received: train message 44843205-2cd7-4170-b45a-69f197e322f9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:48:22:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:11:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:11:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2a65d31a-25ee-44a6-b65f-7a1c6c0a43b9
01/30/2025 04:11:23:INFO:Received: evaluate message 2a65d31a-25ee-44a6-b65f-7a1c6c0a43b9
[92mINFO [0m:      Sent reply
01/30/2025 04:16:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:17:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:17:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6f276c67-c80e-4dbc-8890-b9da8c73ff38
01/30/2025 04:17:06:INFO:Received: train message 6f276c67-c80e-4dbc-8890-b9da8c73ff38
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:28:14:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:54:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:54:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c0ba2cae-21f7-46f2-afe6-864964678f6a
01/30/2025 04:54:46:INFO:Received: evaluate message c0ba2cae-21f7-46f2-afe6-864964678f6a
[92mINFO [0m:      Sent reply
01/30/2025 05:00:16:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:00:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:00:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6334ba18-3cb0-4a29-ace1-462d8e395269
01/30/2025 05:00:51:INFO:Received: train message 6334ba18-3cb0-4a29-ace1-462d8e395269
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.6512451171875
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.008247390389442444, 0.020679054781794548, 0.00699745025485754, 0.004425711464136839, 0.011750944890081882, 0.00214773160405457, 0.000740863848477602, 0.0047605703584849834]
Noise Multiplier after list and tensor:  0.007468714698916301
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:11:41:INFO:Sent reply
