nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 11:13:50:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 11:13:51:DEBUG:ChannelConnectivity.IDLE
01/29/2025 11:13:51:DEBUG:ChannelConnectivity.CONNECTING
01/29/2025 11:13:51:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 11:18:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:18:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8f836d80-64fd-4662-95ef-0d2a8a398927
01/29/2025 11:18:32:INFO:Received: train message 8f836d80-64fd-4662-95ef-0d2a8a398927
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:39:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:10:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:10:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 26eff7c6-01f4-446f-8390-7059232b246d
01/29/2025 12:10:04:INFO:Received: evaluate message 26eff7c6-01f4-446f-8390-7059232b246d
[92mINFO [0m:      Sent reply
01/29/2025 12:14:30:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:15:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:15:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f5e4b61f-67ca-4640-aefb-b84827ef023a
01/29/2025 12:15:12:INFO:Received: train message f5e4b61f-67ca-4640-aefb-b84827ef023a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:32:15:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:47:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:47:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 750fd56f-2922-4f1d-bf97-a0f382b3e420
01/29/2025 12:47:13:INFO:Received: evaluate message 750fd56f-2922-4f1d-bf97-a0f382b3e420
[92mINFO [0m:      Sent reply
01/29/2025 12:51:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:52:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:52:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e7d7b318-85d4-4dbf-9481-cc3daaa34446
01/29/2025 12:52:25:INFO:Received: train message e7d7b318-85d4-4dbf-9481-cc3daaa34446
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:09:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:31:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:31:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e4b6a873-06e9-4d3b-8477-b179c2683657
01/29/2025 13:31:01:INFO:Received: evaluate message e4b6a873-06e9-4d3b-8477-b179c2683657
[92mINFO [0m:      Sent reply
01/29/2025 13:35:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:36:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:36:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d8cebaa9-cd60-48e9-895f-dee6c6e7669c
01/29/2025 13:36:31:INFO:Received: train message d8cebaa9-cd60-48e9-895f-dee6c6e7669c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:55:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:07:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:07:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 19e14128-a216-493d-83ea-878a0853f165
01/29/2025 14:07:49:INFO:Received: evaluate message 19e14128-a216-493d-83ea-878a0853f165
[92mINFO [0m:      Sent reply
01/29/2025 14:14:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:14:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:14:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message df78f666-a346-4848-afc9-e7f8ab38c8b0
01/29/2025 14:14:49:INFO:Received: train message df78f666-a346-4848-afc9-e7f8ab38c8b0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:33:21:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:50:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:50:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4bf7ac92-30b8-48ad-9e10-1d35c0c60074
01/29/2025 14:50:25:INFO:Received: evaluate message 4bf7ac92-30b8-48ad-9e10-1d35c0c60074
[92mINFO [0m:      Sent reply
01/29/2025 14:55:17:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:55:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:55:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 73176ef6-347a-4705-91b1-a52dd0fd1218
01/29/2025 14:55:39:INFO:Received: train message 73176ef6-347a-4705-91b1-a52dd0fd1218
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:16:43:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:29:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:29:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5d05a34-5b12-4d78-8959-4f652638739d
01/29/2025 15:29:56:INFO:Received: evaluate message b5d05a34-5b12-4d78-8959-4f652638739d
[92mINFO [0m:      Sent reply
01/29/2025 15:34:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:35:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:35:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e6323110-1f36-476c-aa42-1564efd3824a
01/29/2025 15:35:05:INFO:Received: train message e6323110-1f36-476c-aa42-1564efd3824a
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 10.0, target_epsilon: 10.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345], 'accuracy': [0.5690696737817157], 'auc': [0.830051779313479]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765], 'accuracy': [0.5690696737817157, 0.6153846153846154], 'auc': [0.830051779313479, 0.8663737657560014]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:56:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:08:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:08:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 73073a13-0463-40ed-8e80-9dd8221bd177
01/29/2025 16:08:22:INFO:Received: evaluate message 73073a13-0463-40ed-8e80-9dd8221bd177
[92mINFO [0m:      Sent reply
01/29/2025 16:14:01:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ebd507d5-096b-4a63-b7c5-3972ef5554ac
01/29/2025 16:15:50:INFO:Received: train message ebd507d5-096b-4a63-b7c5-3972ef5554ac
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:38:30:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:49:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:49:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 122ea099-25a7-4d27-b194-e777f9ae364d
01/29/2025 16:49:47:INFO:Received: evaluate message 122ea099-25a7-4d27-b194-e777f9ae364d
[92mINFO [0m:      Sent reply
01/29/2025 16:57:03:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:57:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:57:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1c2f0788-6c00-48e3-abe6-1a784bb0ff37
01/29/2025 16:57:42:INFO:Received: train message 1c2f0788-6c00-48e3-abe6-1a784bb0ff37
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:15:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:31:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:31:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eb6c590a-87c3-43eb-8b47-285bf90b81d3
01/29/2025 17:31:59:INFO:Received: evaluate message eb6c590a-87c3-43eb-8b47-285bf90b81d3
[92mINFO [0m:      Sent reply
01/29/2025 17:36:17:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:36:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:36:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 12e205cb-99ce-4a21-b94c-0a5353da1dbe
01/29/2025 17:36:54:INFO:Received: train message 12e205cb-99ce-4a21-b94c-0a5353da1dbe
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:57:07:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:13:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:13:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 447e6a2a-16aa-4e28-91a8-51c9c9a83f50
01/29/2025 18:13:41:INFO:Received: evaluate message 447e6a2a-16aa-4e28-91a8-51c9c9a83f50
[92mINFO [0m:      Sent reply
01/29/2025 18:18:49:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:19:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:19:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f804cfb3-5ac7-4195-8af6-7ffe4ff4f59e
01/29/2025 18:19:36:INFO:Received: train message f804cfb3-5ac7-4195-8af6-7ffe4ff4f59e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:38:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:53:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:53:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 47563192-3bc0-4cd6-bf51-577c63a95086
01/29/2025 18:53:04:INFO:Received: evaluate message 47563192-3bc0-4cd6-bf51-577c63a95086
[92mINFO [0m:      Sent reply
01/29/2025 18:58:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:58:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:58:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b9a135d3-19a5-4ca4-b1cb-cd9c07201536
01/29/2025 18:58:44:INFO:Received: train message b9a135d3-19a5-4ca4-b1cb-cd9c07201536
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:18:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:31:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:31:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8633f71c-178f-4b6f-8fe3-1f62f9c8ad8c
01/29/2025 19:31:46:INFO:Received: evaluate message 8633f71c-178f-4b6f-8fe3-1f62f9c8ad8c
[92mINFO [0m:      Sent reply
01/29/2025 19:36:11:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:36:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:36:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1708e1b7-7e78-4737-88ba-bc0c4e758a37
01/29/2025 19:36:42:INFO:Received: train message 1708e1b7-7e78-4737-88ba-bc0c4e758a37
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:54:17:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:10:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:10:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0ffb41c9-0758-4f26-9e33-ca64ce5901b9
01/29/2025 20:10:59:INFO:Received: evaluate message 0ffb41c9-0758-4f26-9e33-ca64ce5901b9
[92mINFO [0m:      Sent reply
01/29/2025 20:15:39:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:16:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:16:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 036c43e6-2b92-4012-ba8a-bde461aacb62
01/29/2025 20:16:19:INFO:Received: train message 036c43e6-2b92-4012-ba8a-bde461aacb62
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:34:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fd82b9f0-77c6-46ab-aeef-2a1f9a985a34
01/29/2025 20:48:23:INFO:Received: evaluate message fd82b9f0-77c6-46ab-aeef-2a1f9a985a34
[92mINFO [0m:      Sent reply
01/29/2025 20:52:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:53:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:53:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3c93db3a-67ef-4a0b-8567-456464c2ac37
01/29/2025 20:53:40:INFO:Received: train message 3c93db3a-67ef-4a0b-8567-456464c2ac37
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:09:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:26:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:26:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 375f01a9-2f8d-456a-aaf8-3bdee30cac30
01/29/2025 21:26:54:INFO:Received: evaluate message 375f01a9-2f8d-456a-aaf8-3bdee30cac30
[92mINFO [0m:      Sent reply
01/29/2025 21:32:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:33:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:33:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a63f4734-abdb-487c-87fa-2e41bcd48318
01/29/2025 21:33:08:INFO:Received: train message a63f4734-abdb-487c-87fa-2e41bcd48318
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:48:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:05:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:05:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 87a39231-3d9f-49e0-8ffd-5b2d17e10ece
01/29/2025 22:05:58:INFO:Received: evaluate message 87a39231-3d9f-49e0-8ffd-5b2d17e10ece
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 22:10:11:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:12:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:12:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f087d6b0-e9b0-4862-800c-e3a20c541e7a
01/29/2025 22:12:07:INFO:Received: train message f087d6b0-e9b0-4862-800c-e3a20c541e7a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:27:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:44:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:44:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 688b165d-4f36-4d9c-9fd9-0c356a762b6b
01/29/2025 22:44:08:INFO:Received: evaluate message 688b165d-4f36-4d9c-9fd9-0c356a762b6b
[92mINFO [0m:      Sent reply
01/29/2025 22:48:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:50:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:50:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c6a50aef-1cd6-4f2a-b899-077c6d5e2d1b
01/29/2025 22:50:16:INFO:Received: train message c6a50aef-1cd6-4f2a-b899-077c6d5e2d1b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:06:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:27:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:27:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 232eac7f-3b80-41f6-8ab5-5994c59da98a
01/29/2025 23:27:40:INFO:Received: evaluate message 232eac7f-3b80-41f6-8ab5-5994c59da98a
[92mINFO [0m:      Sent reply
01/29/2025 23:33:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:34:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:34:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f9987775-dd54-4374-9e6d-62b4b5a35ed0
01/29/2025 23:34:01:INFO:Received: train message f9987775-dd54-4374-9e6d-62b4b5a35ed0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:54:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:07:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:07:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3be0ada9-7671-4294-a1e2-114900eb9e5f
01/30/2025 00:07:29:INFO:Received: evaluate message 3be0ada9-7671-4294-a1e2-114900eb9e5f
[92mINFO [0m:      Sent reply
01/30/2025 00:12:22:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:13:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:13:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7d4d172a-5a73-4900-a827-d0226420918b
01/30/2025 00:13:03:INFO:Received: train message 7d4d172a-5a73-4900-a827-d0226420918b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:31:30:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:47:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:47:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3d673338-6c4f-4709-8ada-7b95d3df359a
01/30/2025 00:47:09:INFO:Received: evaluate message 3d673338-6c4f-4709-8ada-7b95d3df359a

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 00:51:49:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:53:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:53:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e6136ef2-b209-4087-82ae-4664c8f4e2ee
01/30/2025 00:53:16:INFO:Received: train message e6136ef2-b209-4087-82ae-4664c8f4e2ee
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:08:11:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:33:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:33:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7be06cb9-2704-4695-a71c-f7d625c4f35b
01/30/2025 01:33:33:INFO:Received: evaluate message 7be06cb9-2704-4695-a71c-f7d625c4f35b
[92mINFO [0m:      Sent reply
01/30/2025 01:38:10:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:39:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:39:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8f5b5c23-f6fb-4f3c-b01c-36b27a453217
01/30/2025 01:39:49:INFO:Received: train message 8f5b5c23-f6fb-4f3c-b01c-36b27a453217
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:53:38:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:15:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:15:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 558f0e86-c3c3-42d4-a399-2b9be326e682
01/30/2025 02:15:25:INFO:Received: evaluate message 558f0e86-c3c3-42d4-a399-2b9be326e682
[92mINFO [0m:      Sent reply
01/30/2025 02:20:27:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:21:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:21:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 239a4c3c-baa9-4dd3-9424-24c7e3b3ca32
01/30/2025 02:21:19:INFO:Received: train message 239a4c3c-baa9-4dd3-9424-24c7e3b3ca32
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:35:11:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:55:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:55:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 18844435-259f-4719-acd3-4bd6b2215e88
01/30/2025 02:55:51:INFO:Received: evaluate message 18844435-259f-4719-acd3-4bd6b2215e88
[92mINFO [0m:      Sent reply
01/30/2025 03:01:05:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:02:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:02:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1a0532af-6537-429e-8523-05e3b271ac8c
01/30/2025 03:02:21:INFO:Received: train message 1a0532af-6537-429e-8523-05e3b271ac8c

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629]}

Step 2a: Compute base noise multiplier/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:17:25:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:32:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:32:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9cee563e-eafa-4ef3-beb3-6d86f556a09d
01/30/2025 03:32:07:INFO:Received: evaluate message 9cee563e-eafa-4ef3-beb3-6d86f556a09d
[92mINFO [0m:      Sent reply
01/30/2025 03:36:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:37:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:37:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c33c16e4-0ae1-42bd-ae90-d27896665eaf
01/30/2025 03:37:19:INFO:Received: train message c33c16e4-0ae1-42bd-ae90-d27896665eaf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:53:03:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:11:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:11:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c290aeeb-16b6-4fee-b5b7-d8fca0d2cc65
01/30/2025 04:11:14:INFO:Received: evaluate message c290aeeb-16b6-4fee-b5b7-d8fca0d2cc65
[92mINFO [0m:      Sent reply
01/30/2025 04:16:34:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:17:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:17:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 615ee819-3fe6-4610-b7d0-8bdb6ad565fe
01/30/2025 04:17:21:INFO:Received: train message 615ee819-3fe6-4610-b7d0-8bdb6ad565fe
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:36:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:54:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:54:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7965aed3-ce6a-4cdc-b21b-7bd3068c598c
01/30/2025 04:54:47:INFO:Received: evaluate message 7965aed3-ce6a-4cdc-b21b-7bd3068c598c
[92mINFO [0m:      Sent reply
01/30/2025 05:00:12:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:01:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:01:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4bf777bc-c7dd-4d10-915e-5f713bada4fa
01/30/2025 05:01:15:INFO:Received: train message 4bf777bc-c7dd-4d10-915e-5f713bada4fa

Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.40763587597345, 1.2927908420658765, 1.3203699378010922, 1.2888170360222584, 1.3413953279421496, 1.2619433705719354, 1.2799618099420436, 1.2819091984210742, 1.2951343906199408, 1.2564055997183665, 1.2727300038848537, 1.3154285673263617, 1.3600333542339849, 1.3524245549678995, 1.2939806056272296, 1.284216887039664, 1.2980858089460081, 1.3043666386191421, 1.320201873539246, 1.320213367753676, 1.285732830022066, 1.3034096350604567, 1.337240922105346, 1.3405357957654453, 1.2980719281212725, 1.3269906935315434], 'accuracy': [0.5690696737817157, 0.6153846153846154, 0.6270640354409988, 0.6339105920257753, 0.6230366492146597, 0.6447845348368909, 0.6419653644784535, 0.6407571486105518, 0.639951671365284, 0.6500201369311317, 0.6508256141763995, 0.6415626258558196, 0.6359242851389448, 0.6379379782521144, 0.6524365686669351, 0.6564639548932742, 0.6592831252517116, 0.657672170761176, 0.6516310914216673, 0.644381796214257, 0.6552557390253725, 0.6528393072895691, 0.6476037051953283, 0.6411598872331856, 0.6580749093838099, 0.6524365686669351], 'auc': [0.830051779313479, 0.8663737657560014, 0.8742704942126016, 0.8822818005260119, 0.8830940014106516, 0.8885906663544167, 0.8910285322992733, 0.8921989028210153, 0.8934183051732519, 0.8966042952214901, 0.8972558310621438, 0.8975133233118824, 0.8968163976313428, 0.898663738027341, 0.8993827986132272, 0.9010360647122826, 0.9008979727517417, 0.9022023692845734, 0.9011228000547002, 0.900593152202005, 0.9017445346135617, 0.9024747575615352, 0.9017265537076629, 0.9009619550492176, 0.9032434784723347, 0.9027519769585195]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.7183837890625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.005528035573661327, 0.08163753151893616, 0.0013687282335013151, 0.0017256689025089145, 0.00419113552197814, 0.0016321626026183367, 0.002031985903158784, 0.0042375135235488415]
Noise Multiplier after list and tensor:  0.012794095222488977
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:16:12:INFO:Sent reply
