nohup: ignoring input
Traceback (most recent call last):
  File "client_6.py", line 16, in <module>
    from flamby.datasets.fed_isic2019 import FedIsic2019
ModuleNotFoundError: No module named 'flamby'
nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=20/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 10:42:08:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 10:42:08:DEBUG:ChannelConnectivity.IDLE
01/29/2025 10:42:08:DEBUG:ChannelConnectivity.CONNECTING
01/29/2025 10:42:08:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 10:42:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:42:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message 742d7d02-0659-458d-8d55-6b49f6e7562d
01/29/2025 10:42:08:INFO:Received: get_parameters message 742d7d02-0659-458d-8d55-6b49f6e7562d
[92mINFO [0m:      Sent reply
01/29/2025 10:42:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:50:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:50:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e7cf9a8e-3284-4c37-821f-3b72a81186c5
01/29/2025 10:50:02:INFO:Received: train message e7cf9a8e-3284-4c37-821f-3b72a81186c5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 10:56:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:26:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:26:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7800442a-651b-4871-8576-7a05dbcfc060
01/29/2025 11:26:08:INFO:Received: evaluate message 7800442a-651b-4871-8576-7a05dbcfc060
[92mINFO [0m:      Sent reply
01/29/2025 11:31:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:32:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:32:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 48c4a821-4e65-4c70-8936-8a6ab5cf8836
01/29/2025 11:32:48:INFO:Received: train message 48c4a821-4e65-4c70-8936-8a6ab5cf8836
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:36:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:12:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:12:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f6714dbb-fd86-4209-acbb-4f99942ba42a
01/29/2025 12:12:26:INFO:Received: evaluate message f6714dbb-fd86-4209-acbb-4f99942ba42a
[92mINFO [0m:      Sent reply
01/29/2025 12:17:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:17:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:17:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4e1a8576-9234-4999-ae99-e03db80e284a
01/29/2025 12:17:51:INFO:Received: train message 4e1a8576-9234-4999-ae99-e03db80e284a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:20:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:48:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:48:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cbabce67-486a-46bf-9d98-911941edd444
01/29/2025 12:48:46:INFO:Received: evaluate message cbabce67-486a-46bf-9d98-911941edd444
[92mINFO [0m:      Sent reply
01/29/2025 12:53:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:54:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:54:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f24e9cb7-9a3c-4855-bf62-8313ed897ff1
01/29/2025 12:54:13:INFO:Received: train message f24e9cb7-9a3c-4855-bf62-8313ed897ff1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:56:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:33:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:33:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 28044055-23f1-4e5c-94d8-761108e1c559
01/29/2025 13:33:04:INFO:Received: evaluate message 28044055-23f1-4e5c-94d8-761108e1c559
[92mINFO [0m:      Sent reply
01/29/2025 13:38:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:39:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:39:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6d05f7c6-466a-41d0-ba47-6e097418a7b3
01/29/2025 13:39:21:INFO:Received: train message 6d05f7c6-466a-41d0-ba47-6e097418a7b3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:43:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:12:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:12:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 221fb51f-c56a-495b-9c28-c9f59103a5b8
01/29/2025 14:12:11:INFO:Received: evaluate message 221fb51f-c56a-495b-9c28-c9f59103a5b8
[92mINFO [0m:      Sent reply
01/29/2025 14:17:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:18:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:18:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 12c26ca9-36c8-4e53-9b17-c22e4f55ba8b
01/29/2025 14:18:04:INFO:Received: train message 12c26ca9-36c8-4e53-9b17-c22e4f55ba8b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:21:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:55:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:55:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c8c33348-2a69-4e91-9435-b2a5b6b62412
01/29/2025 14:55:54:INFO:Received: evaluate message c8c33348-2a69-4e91-9435-b2a5b6b62412
[92mINFO [0m:      Sent reply
01/29/2025 15:01:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:01:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:01:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 746ceac8-04f4-4ca3-bc5b-bb7c4b345949
01/29/2025 15:01:55:INFO:Received: train message 746ceac8-04f4-4ca3-bc5b-bb7c4b345949
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=20', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=20']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683], 'accuracy': [0.5783326621022956], 'auc': [0.840446426141872]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418], 'accuracy': [0.5783326621022956, 0.621425694724124], 'auc': [0.840446426141872, 0.872547279925616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:06:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:34:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:34:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3a5e2a33-a8f1-4a16-ac0a-8196936948d3
01/29/2025 15:34:57:INFO:Received: evaluate message 3a5e2a33-a8f1-4a16-ac0a-8196936948d3
[92mINFO [0m:      Sent reply
01/29/2025 15:40:10:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:40:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:40:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6f6e9caf-49a2-4ceb-875e-f2705296fe85
01/29/2025 15:40:44:INFO:Received: train message 6f6e9caf-49a2-4ceb-875e-f2705296fe85
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:43:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9b313b04-3079-4edd-97a4-f390862b0c04
01/29/2025 16:15:17:INFO:Received: evaluate message 9b313b04-3079-4edd-97a4-f390862b0c04
[92mINFO [0m:      Sent reply
01/29/2025 16:22:11:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:23:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:23:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3f7b277f-87f0-49b8-a830-6429394fc878
01/29/2025 16:23:06:INFO:Received: train message 3f7b277f-87f0-49b8-a830-6429394fc878
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:27:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:59:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:59:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a416bab5-8f9e-4c38-bbc5-0cd089b73fc4
01/29/2025 16:59:01:INFO:Received: evaluate message a416bab5-8f9e-4c38-bbc5-0cd089b73fc4
[92mINFO [0m:      Sent reply
01/29/2025 17:04:36:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:05:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:05:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ee64bf53-10b6-49f3-a528-14db5ee8ea4f
01/29/2025 17:05:23:INFO:Received: train message ee64bf53-10b6-49f3-a528-14db5ee8ea4f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:08:29:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:39:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:39:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3471e23b-4a34-4618-b514-6ce55a3ca716
01/29/2025 17:39:03:INFO:Received: evaluate message 3471e23b-4a34-4618-b514-6ce55a3ca716
[92mINFO [0m:      Sent reply
01/29/2025 17:44:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:44:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:44:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 92d067c5-1d83-463e-9347-e720e8153b97
01/29/2025 17:44:44:INFO:Received: train message 92d067c5-1d83-463e-9347-e720e8153b97
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:48:36:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:18:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:18:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6c1306b1-fd31-4e5f-b678-eb42f90eef9c
01/29/2025 18:18:31:INFO:Received: evaluate message 6c1306b1-fd31-4e5f-b678-eb42f90eef9c
[92mINFO [0m:      Sent reply
01/29/2025 18:23:57:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:24:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:24:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e94c9626-bfa4-43ce-9754-05cf32b68d30
01/29/2025 18:24:38:INFO:Received: train message e94c9626-bfa4-43ce-9754-05cf32b68d30
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:27:38:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:55:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:55:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f39834be-72d5-430a-aa18-7dae194d5c99
01/29/2025 18:55:48:INFO:Received: evaluate message f39834be-72d5-430a-aa18-7dae194d5c99
[92mINFO [0m:      Sent reply
01/29/2025 19:01:02:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:01:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:01:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 625cf292-58f4-44cd-a3a5-678d54202155
01/29/2025 19:01:36:INFO:Received: train message 625cf292-58f4-44cd-a3a5-678d54202155
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:04:38:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:33:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:33:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 16415172-a88b-4a8d-b617-0014ddd6c00b
01/29/2025 19:33:05:INFO:Received: evaluate message 16415172-a88b-4a8d-b617-0014ddd6c00b
[92mINFO [0m:      Sent reply
01/29/2025 19:37:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:39:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:39:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c96adb7e-327a-426c-90fa-2eb94ae5507f
01/29/2025 19:39:51:INFO:Received: train message c96adb7e-327a-426c-90fa-2eb94ae5507f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:43:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:12:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:12:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b8936a26-32ec-4525-a0c6-7436d67d932b
01/29/2025 20:12:01:INFO:Received: evaluate message b8936a26-32ec-4525-a0c6-7436d67d932b
[92mINFO [0m:      Sent reply
01/29/2025 20:16:44:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:17:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:17:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 904fc6c4-c77b-4ed6-b36d-6b24ff718f0a
01/29/2025 20:17:22:INFO:Received: train message 904fc6c4-c77b-4ed6-b36d-6b24ff718f0a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:20:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message be8042df-0caa-4878-877b-39a1905b44df
01/29/2025 20:48:06:INFO:Received: evaluate message be8042df-0caa-4878-877b-39a1905b44df
[92mINFO [0m:      Sent reply
01/29/2025 20:52:30:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:53:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:53:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 82a578bc-7c0a-4397-aa34-21444bf0b343
01/29/2025 20:53:37:INFO:Received: train message 82a578bc-7c0a-4397-aa34-21444bf0b343
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:56:30:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:27:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:27:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 858adbd2-77d9-4d04-a00f-15a120216328
01/29/2025 21:27:04:INFO:Received: evaluate message 858adbd2-77d9-4d04-a00f-15a120216328
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 21:32:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:33:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:33:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8438dffe-e647-4874-94ef-c0319f17e578
01/29/2025 21:33:34:INFO:Received: train message 8438dffe-e647-4874-94ef-c0319f17e578
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:36:31:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:06:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:06:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a1f32ade-f0ad-475c-86d4-394f7e287513
01/29/2025 22:06:20:INFO:Received: evaluate message a1f32ade-f0ad-475c-86d4-394f7e287513
[92mINFO [0m:      Sent reply
01/29/2025 22:10:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:12:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:12:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 937e5d7a-6f02-4ca2-b83f-5dd42ee267f4
01/29/2025 22:12:17:INFO:Received: train message 937e5d7a-6f02-4ca2-b83f-5dd42ee267f4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:15:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:41:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:41:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f66f780c-15d0-4ff8-90fe-e0c1261c7fcc
01/29/2025 22:41:28:INFO:Received: evaluate message f66f780c-15d0-4ff8-90fe-e0c1261c7fcc
[92mINFO [0m:      Sent reply
01/29/2025 22:45:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:46:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:46:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 170c4693-e686-46a1-b9f5-d71b01b14db7
01/29/2025 22:46:40:INFO:Received: train message 170c4693-e686-46a1-b9f5-d71b01b14db7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:49:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:23:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:23:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3bd8ca42-8c1d-456e-9467-080a674b31b1
01/29/2025 23:23:25:INFO:Received: evaluate message 3bd8ca42-8c1d-456e-9467-080a674b31b1
[92mINFO [0m:      Sent reply
01/29/2025 23:28:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:29:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:29:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd59f1ec-2c8e-4813-93ce-ef7f5210f560
01/29/2025 23:29:40:INFO:Received: train message cd59f1ec-2c8e-4813-93ce-ef7f5210f560
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:33:21:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:12:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:12:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a1a5eb6e-48ff-4551-8242-fb8200c405d6
01/30/2025 00:12:56:INFO:Received: evaluate message a1a5eb6e-48ff-4551-8242-fb8200c405d6

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally[92mINFO [0m:      Sent reply
01/30/2025 00:17:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:18:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:18:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e1dccf88-4691-4033-8a86-4a48a33685ec
01/30/2025 00:18:55:INFO:Received: train message e1dccf88-4691-4033-8a86-4a48a33685ec
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:21:38:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:46:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:46:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 20dbe741-e317-4b50-beba-ad5cfe52781a
01/30/2025 00:46:00:INFO:Received: evaluate message 20dbe741-e317-4b50-beba-ad5cfe52781a
[92mINFO [0m:      Sent reply
01/30/2025 00:50:06:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:52:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:52:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message df37df8e-4b23-4ef6-9e69-7c8d5f8db04a
01/30/2025 00:52:08:INFO:Received: train message df37df8e-4b23-4ef6-9e69-7c8d5f8db04a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:54:53:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:24:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:24:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9714bef3-534e-49e9-b6d6-0d40377c92de
01/30/2025 01:24:27:INFO:Received: evaluate message 9714bef3-534e-49e9-b6d6-0d40377c92de
[92mINFO [0m:      Sent reply
01/30/2025 01:28:24:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:29:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:29:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 75bfe355-6a57-4f9e-b1c5-7fc6a4601229
01/30/2025 01:29:50:INFO:Received: train message 75bfe355-6a57-4f9e-b1c5-7fc6a4601229
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:32:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:00:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:00:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 173fecbb-676f-4e2d-8bdd-992a9c2e398a
01/30/2025 02:00:12:INFO:Received: evaluate message 173fecbb-676f-4e2d-8bdd-992a9c2e398a
[92mINFO [0m:      Sent reply
01/30/2025 02:03:58:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:05:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:05:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ceada044-8182-4447-a735-d081bb494c5b
01/30/2025 02:05:41:INFO:Received: train message ceada044-8182-4447-a735-d081bb494c5b


{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886]}

/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:08:22:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:40:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:40:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0d678b64-7205-44b7-bc39-610ca374a61b
01/30/2025 02:40:42:INFO:Received: evaluate message 0d678b64-7205-44b7-bc39-610ca374a61b
[92mINFO [0m:      Sent reply
01/30/2025 02:45:20:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:45:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:45:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8fdfcd98-f322-4991-9b8c-aad2ad67f10f
01/30/2025 02:45:55:INFO:Received: train message 8fdfcd98-f322-4991-9b8c-aad2ad67f10f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:48:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:21:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:21:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ccdd4ea0-3534-46cc-a59b-b216d381f522
01/30/2025 03:21:22:INFO:Received: evaluate message ccdd4ea0-3534-46cc-a59b-b216d381f522
[92mINFO [0m:      Sent reply
01/30/2025 03:25:28:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:25:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:25:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5235f9ba-5334-49b7-b1e9-141758b15ee4
01/30/2025 03:25:59:INFO:Received: train message 5235f9ba-5334-49b7-b1e9-141758b15ee4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:29:04:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:56:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:56:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 76ac80a2-9f84-4734-8fb8-1d12b218baae
01/30/2025 03:56:25:INFO:Received: evaluate message 76ac80a2-9f84-4734-8fb8-1d12b218baae
[92mINFO [0m:      Sent reply
01/30/2025 04:00:51:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:01:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:01:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 31277386-e42e-4d2e-be66-5b19f9c852bc
01/30/2025 04:01:25:INFO:Received: train message 31277386-e42e-4d2e-be66-5b19f9c852bc
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843, 1.3140666796658915], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299, 0.6564639548932742], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921, 0.9071987834331172]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 351, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843, 1.3140666796658915, 1.3254329497133395], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299, 0.6564639548932742, 0.6568666935159082], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921, 0.9071987834331172, 0.9064201878633371]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.402069091796875
Data Scaling Factor: 0.01887401193740926 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [0.001116227125748992, 0.004524558316916227, 0.0001487184636062011, 4.3935298890573904e-05, 0.0003475733392406255, 0.0001859283511294052, 0.0002006726572290063, 6.815188680775464e-05]
Noise Multiplier after list and tensor:  0.0008294706799460982
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:03:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:40:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:40:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bde08029-654d-4c28-95c3-856f6685602b
01/30/2025 04:40:24:INFO:Received: evaluate message bde08029-654d-4c28-95c3-856f6685602b
[92mINFO [0m:      Sent reply
01/30/2025 04:44:43:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:45:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:45:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 13bebff1-67df-4707-853e-15c27c801e0b
01/30/2025 04:45:03:INFO:Received: train message 13bebff1-67df-4707-853e-15c27c801e0b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:47:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:18:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:18:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4a161efa-c8a9-4147-8676-d2c536e85307
01/30/2025 05:18:10:INFO:Received: evaluate message 4a161efa-c8a9-4147-8676-d2c536e85307
[92mINFO [0m:      Sent reply
01/30/2025 05:22:57:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:23:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:23:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 28f453ad-4b82-4427-9be2-bd90b0b4efee
01/30/2025 05:23:30:INFO:Received: train message 28f453ad-4b82-4427-9be2-bd90b0b4efee
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:26:38:INFO:Sent reply
