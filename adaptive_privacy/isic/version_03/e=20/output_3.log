nohup: ignoring input
Traceback (most recent call last):
  File "client_3.py", line 16, in <module>
    from flamby.datasets.fed_isic2019 import FedIsic2019
ModuleNotFoundError: No module named 'flamby'
nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=20/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 10:45:44:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 10:45:44:DEBUG:ChannelConnectivity.IDLE
01/29/2025 10:45:44:DEBUG:ChannelConnectivity.CONNECTING
01/29/2025 10:45:44:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 10:50:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:50:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 84d4fe46-8bc6-48e1-939a-3da270c461ac
01/29/2025 10:50:05:INFO:Received: train message 84d4fe46-8bc6-48e1-939a-3da270c461ac
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:05:43:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:26:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:26:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5400cd3e-5d68-440f-b84b-2b87a8e453a1
01/29/2025 11:26:25:INFO:Received: evaluate message 5400cd3e-5d68-440f-b84b-2b87a8e453a1
[92mINFO [0m:      Sent reply
01/29/2025 11:32:07:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:32:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:32:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cb2ab398-5d67-4545-9bcf-775e66da8c46
01/29/2025 11:32:59:INFO:Received: train message cb2ab398-5d67-4545-9bcf-775e66da8c46
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:53:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:12:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:12:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a03374b8-25f7-4abf-90cc-aa224dd9a8fa
01/29/2025 12:12:31:INFO:Received: evaluate message a03374b8-25f7-4abf-90cc-aa224dd9a8fa
[92mINFO [0m:      Sent reply
01/29/2025 12:17:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:18:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:18:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2a4ccdcd-6bd7-4378-a07b-15501f8a0cfd
01/29/2025 12:18:08:INFO:Received: train message 2a4ccdcd-6bd7-4378-a07b-15501f8a0cfd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:33:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:48:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:48:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3c50f743-39a5-4b8a-b81d-70719b362a30
01/29/2025 12:48:46:INFO:Received: evaluate message 3c50f743-39a5-4b8a-b81d-70719b362a30
[92mINFO [0m:      Sent reply
01/29/2025 12:53:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:54:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:54:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 78f5e28f-f291-47a6-aa3c-9b35478b44ae
01/29/2025 12:54:11:INFO:Received: train message 78f5e28f-f291-47a6-aa3c-9b35478b44ae
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:10:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:33:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:33:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 86d74f8e-9229-4ad6-a391-6d6bfc6904eb
01/29/2025 13:33:13:INFO:Received: evaluate message 86d74f8e-9229-4ad6-a391-6d6bfc6904eb
[92mINFO [0m:      Sent reply
01/29/2025 13:38:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:39:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:39:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5456e817-7eac-4e77-8e40-23376cd7bbb0
01/29/2025 13:39:28:INFO:Received: train message 5456e817-7eac-4e77-8e40-23376cd7bbb0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:55:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:12:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:12:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dbcefcf2-4cc2-4776-909a-9c2c6559ede1
01/29/2025 14:12:06:INFO:Received: evaluate message dbcefcf2-4cc2-4776-909a-9c2c6559ede1
[92mINFO [0m:      Sent reply
01/29/2025 14:17:15:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:17:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:17:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 28825af0-55d2-4dab-beec-a5f771a6c345
01/29/2025 14:17:52:INFO:Received: train message 28825af0-55d2-4dab-beec-a5f771a6c345
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:34:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:55:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:55:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d84ed31e-8bae-494d-aa03-b5ee510ce88c
01/29/2025 14:55:49:INFO:Received: evaluate message d84ed31e-8bae-494d-aa03-b5ee510ce88c
[92mINFO [0m:      Sent reply
01/29/2025 15:01:08:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:01:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:01:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 875015db-4799-4bed-a7ab-464e58cbf82e
01/29/2025 15:01:55:INFO:Received: train message 875015db-4799-4bed-a7ab-464e58cbf82e
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=20', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=20']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683], 'accuracy': [0.5783326621022956], 'auc': [0.840446426141872]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418], 'accuracy': [0.5783326621022956, 0.621425694724124], 'auc': [0.840446426141872, 0.872547279925616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:19:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:34:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:34:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bca714fa-bf65-4a26-ba81-64d721ce6924
01/29/2025 15:34:37:INFO:Received: evaluate message bca714fa-bf65-4a26-ba81-64d721ce6924
[92mINFO [0m:      Sent reply
01/29/2025 15:39:23:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:40:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:40:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message acf0d350-2213-440c-8f3f-de9e68b8d971
01/29/2025 15:40:56:INFO:Received: train message acf0d350-2213-440c-8f3f-de9e68b8d971
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:57:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a9e4e8c1-77d1-446a-bbf8-065cfb5d5eb9
01/29/2025 16:15:02:INFO:Received: evaluate message a9e4e8c1-77d1-446a-bbf8-065cfb5d5eb9
[92mINFO [0m:      Sent reply
01/29/2025 16:21:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:23:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:23:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6dab61ea-bbeb-4cc6-bbb6-a674f31661c9
01/29/2025 16:23:09:INFO:Received: train message 6dab61ea-bbeb-4cc6-bbb6-a674f31661c9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:40:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:58:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:58:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f0374e85-238c-4bfd-817b-1ee7076abebc
01/29/2025 16:58:54:INFO:Received: evaluate message f0374e85-238c-4bfd-817b-1ee7076abebc
[92mINFO [0m:      Sent reply
01/29/2025 17:04:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:05:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:05:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fff6b245-aefb-43a4-abac-b38e0da3d12f
01/29/2025 17:05:22:INFO:Received: train message fff6b245-aefb-43a4-abac-b38e0da3d12f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:19:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:39:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:39:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2766bdfa-ae2c-4659-88fe-6a2c337e7494
01/29/2025 17:39:14:INFO:Received: evaluate message 2766bdfa-ae2c-4659-88fe-6a2c337e7494
[92mINFO [0m:      Sent reply
01/29/2025 17:44:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:45:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:45:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6b7d18f1-0347-45e1-88af-7f8c94985ac3
01/29/2025 17:45:09:INFO:Received: train message 6b7d18f1-0347-45e1-88af-7f8c94985ac3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:02:08:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:18:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:18:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 72d57add-1c7b-4056-931e-54029935c79a
01/29/2025 18:18:38:INFO:Received: evaluate message 72d57add-1c7b-4056-931e-54029935c79a
[92mINFO [0m:      Sent reply
01/29/2025 18:24:03:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:24:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:24:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a18553f7-c3d2-404a-9d5a-c5d22e26e335
01/29/2025 18:24:48:INFO:Received: train message a18553f7-c3d2-404a-9d5a-c5d22e26e335
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:41:30:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:55:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:55:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2e26757a-72b6-4889-b050-98a2d192e2d0
01/29/2025 18:55:39:INFO:Received: evaluate message 2e26757a-72b6-4889-b050-98a2d192e2d0
[92mINFO [0m:      Sent reply
01/29/2025 19:00:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:01:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:01:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ba9a7080-e3fb-411f-b44d-db7c98a4f408
01/29/2025 19:01:27:INFO:Received: train message ba9a7080-e3fb-411f-b44d-db7c98a4f408
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:18:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:33:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:33:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 284d27ad-b8a2-4791-8f34-5e496d5714c8
01/29/2025 19:33:15:INFO:Received: evaluate message 284d27ad-b8a2-4791-8f34-5e496d5714c8
[92mINFO [0m:      Sent reply
01/29/2025 19:38:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:40:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:40:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fc190e06-edcc-4068-9279-0202501da3d3
01/29/2025 19:40:00:INFO:Received: train message fc190e06-edcc-4068-9279-0202501da3d3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:54:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:11:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:11:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 79724100-771c-40d8-8ba3-bbf96f308690
01/29/2025 20:11:41:INFO:Received: evaluate message 79724100-771c-40d8-8ba3-bbf96f308690
[92mINFO [0m:      Sent reply
01/29/2025 20:16:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:17:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:17:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b457a1e9-9d16-41cf-a114-9d0714c8d735
01/29/2025 20:17:27:INFO:Received: train message b457a1e9-9d16-41cf-a114-9d0714c8d735
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:34:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7985eb87-dced-4148-9bf8-d0c55a2cdf9b
01/29/2025 20:48:03:INFO:Received: evaluate message 7985eb87-dced-4148-9bf8-d0c55a2cdf9b
[92mINFO [0m:      Sent reply
01/29/2025 20:52:29:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:53:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:53:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7fff279c-5e10-4d68-8ff4-597d79e080b7
01/29/2025 20:53:37:INFO:Received: train message 7fff279c-5e10-4d68-8ff4-597d79e080b7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:07:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:27:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:27:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 88158576-96da-4e78-933c-1b6d0633bdf8
01/29/2025 21:27:19:INFO:Received: evaluate message 88158576-96da-4e78-933c-1b6d0633bdf8
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 21:33:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:33:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:33:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 27fbba40-4555-4def-b1ec-d83c77bff3e7
01/29/2025 21:33:35:INFO:Received: train message 27fbba40-4555-4def-b1ec-d83c77bff3e7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:47:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:06:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:06:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e275ede5-d8d9-44d6-a8be-fb5cbcb8ff7f
01/29/2025 22:06:22:INFO:Received: evaluate message e275ede5-d8d9-44d6-a8be-fb5cbcb8ff7f
[92mINFO [0m:      Sent reply
01/29/2025 22:10:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:12:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:12:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e932fd4b-e9fb-4ba7-ba79-68ec660ff76d
01/29/2025 22:12:16:INFO:Received: train message e932fd4b-e9fb-4ba7-ba79-68ec660ff76d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:25:49:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:41:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:41:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 06a8e9c4-6ba9-4e26-8b4f-154ff2be852a
01/29/2025 22:41:28:INFO:Received: evaluate message 06a8e9c4-6ba9-4e26-8b4f-154ff2be852a
[92mINFO [0m:      Sent reply
01/29/2025 22:45:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:46:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:46:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a3c8ab7b-dac1-4b10-a182-7064bc178a95
01/29/2025 22:46:55:INFO:Received: train message a3c8ab7b-dac1-4b10-a182-7064bc178a95
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:00:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:23:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:23:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 33779737-9581-4f0c-b55f-c8c337e2bb1c
01/29/2025 23:23:35:INFO:Received: evaluate message 33779737-9581-4f0c-b55f-c8c337e2bb1c
[92mINFO [0m:      Sent reply
01/29/2025 23:28:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:29:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:29:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 349ce13e-7a0b-40b5-afa3-ca24f8459c44
01/29/2025 23:29:45:INFO:Received: train message 349ce13e-7a0b-40b5-afa3-ca24f8459c44
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:48:33:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:12:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:12:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 75399e67-7ad8-4514-a350-213bbcdc3857
01/30/2025 00:12:49:INFO:Received: evaluate message 75399e67-7ad8-4514-a350-213bbcdc3857

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 00:17:16:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:18:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:18:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2bde414d-28e8-4a83-9486-529f0b9c0a08
01/30/2025 00:18:32:INFO:Received: train message 2bde414d-28e8-4a83-9486-529f0b9c0a08
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:31:51:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:45:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:45:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4d298916-02d1-4d3d-941c-0d7769f9e0b6
01/30/2025 00:45:57:INFO:Received: evaluate message 4d298916-02d1-4d3d-941c-0d7769f9e0b6
[92mINFO [0m:      Sent reply
01/30/2025 00:49:53:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:52:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:52:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fd3bc635-753c-4802-ac8b-2f1ea030bef9
01/30/2025 00:52:08:INFO:Received: train message fd3bc635-753c-4802-ac8b-2f1ea030bef9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:05:36:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:24:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:24:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 149dc2a4-eab0-44c4-b4a3-4349b0df99ca
01/30/2025 01:24:27:INFO:Received: evaluate message 149dc2a4-eab0-44c4-b4a3-4349b0df99ca
[92mINFO [0m:      Sent reply
01/30/2025 01:28:26:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:29:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:29:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d9aa6126-9606-4e70-8f05-d43e9851b1f0
01/30/2025 01:29:55:INFO:Received: train message d9aa6126-9606-4e70-8f05-d43e9851b1f0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:43:53:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:00:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:00:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dece89e6-da16-48b1-a46f-8647ed6ff958
01/30/2025 02:00:26:INFO:Received: evaluate message dece89e6-da16-48b1-a46f-8647ed6ff958
[92mINFO [0m:      Sent reply
01/30/2025 02:04:44:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:05:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:05:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b3d309b5-c188-4a90-8dd0-74af31866f33
01/30/2025 02:05:30:INFO:Received: train message b3d309b5-c188-4a90-8dd0-74af31866f33

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:20:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:40:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:40:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 47c661a2-c4a0-4c42-837f-100308e803f0
01/30/2025 02:40:38:INFO:Received: evaluate message 47c661a2-c4a0-4c42-837f-100308e803f0
[92mINFO [0m:      Sent reply
01/30/2025 02:45:15:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:46:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:46:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1f284fdf-a11a-42c1-85f7-beb3699b2c0a
01/30/2025 02:46:05:INFO:Received: train message 1f284fdf-a11a-42c1-85f7-beb3699b2c0a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:01:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:21:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:21:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3e728ed8-f655-4ed9-a9dc-a6699211fae6
01/30/2025 03:21:17:INFO:Received: evaluate message 3e728ed8-f655-4ed9-a9dc-a6699211fae6
[92mINFO [0m:      Sent reply
01/30/2025 03:25:26:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:25:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:25:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 99986124-4be0-4bd8-b1f0-8b0a3de69742
01/30/2025 03:25:57:INFO:Received: train message 99986124-4be0-4bd8-b1f0-8b0a3de69742
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:41:38:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:56:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:56:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f23dfa62-82eb-448a-8b1a-e8ced90bceae
01/30/2025 03:56:32:INFO:Received: evaluate message f23dfa62-82eb-448a-8b1a-e8ced90bceae
[92mINFO [0m:      Sent reply
01/30/2025 04:01:03:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:01:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:01:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d086279f-1252-4f06-811a-1ca784e2f8bf
01/30/2025 04:01:46:INFO:Received: train message d086279f-1252-4f06-811a-1ca784e2f8bf
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843, 1.3140666796658915], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299, 0.6564639548932742], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921, 0.9071987834331172]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843, 1.3140666796658915, 1.3254329497133395], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299, 0.6564639548932742, 0.6568666935159082], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921, 0.9071987834331172, 0.9064201878633371]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:17:20:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:40:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:40:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eedb0c0c-8180-4c36-a857-beaf73eb4908
01/30/2025 04:40:15:INFO:Received: evaluate message eedb0c0c-8180-4c36-a857-beaf73eb4908
[92mINFO [0m:      Sent reply
01/30/2025 04:44:28:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:45:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:45:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 21c60f97-2c67-479f-add9-aa8583444e53
01/30/2025 04:45:03:INFO:Received: train message 21c60f97-2c67-479f-add9-aa8583444e53
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:00:44:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:17:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:17:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dbcfaf65-2f40-4290-91fa-11577f2895e4
01/30/2025 05:17:48:INFO:Received: evaluate message dbcfaf65-2f40-4290-91fa-11577f2895e4
[92mINFO [0m:      Sent reply
01/30/2025 05:22:42:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:23:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:23:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0eb9ca57-1163-47db-907e-8bd9bb02e98d
01/30/2025 05:23:30:INFO:Received: train message 0eb9ca57-1163-47db-907e-8bd9bb02e98d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:40:10:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:09:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:09:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ceba3b77-49f4-4f7e-af76-75348e4e85c9
01/30/2025 06:09:44:INFO:Received: evaluate message ceba3b77-49f4-4f7e-af76-75348e4e85c9
[92mINFO [0m:      Sent reply
01/30/2025 06:15:00:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:16:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:16:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 46e995de-610a-40cd-9bff-e4bb91acd524
01/30/2025 06:16:37:INFO:Received: train message 46e995de-610a-40cd-9bff-e4bb91acd524
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843, 1.3140666796658915, 1.3254329497133395, 1.2964139303227753], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299, 0.6564639548932742, 0.6568666935159082, 0.6608940797422472], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921, 0.9071987834331172, 0.9064201878633371, 0.9076969880859942]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843, 1.3140666796658915, 1.3254329497133395, 1.2964139303227753, 1.3163958567311291], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299, 0.6564639548932742, 0.6568666935159082, 0.6608940797422472, 0.6548530004027386], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921, 0.9071987834331172, 0.9064201878633371, 0.9076969880859942, 0.9078627397733976]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843, 1.3140666796658915, 1.3254329497133395, 1.2964139303227753, 1.3163958567311291, 1.373914302331615], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299, 0.6564639548932742, 0.6568666935159082, 0.6608940797422472, 0.6548530004027386, 0.6451872734595248], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921, 0.9071987834331172, 0.9064201878633371, 0.9076969880859942, 0.9078627397733976, 0.9065564478394451]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4973602294921875
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01265410240739584, 0.031223950907588005, 0.004284132272005081, 0.0009333062916994095, 0.01056517381221056, 0.0021320448722690344, 0.00238373433239758, 0.0018895053071901202]
Noise Multiplier after list and tensor:  0.008258243775344454
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 06:32:56:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:54:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:54:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 48865030-b170-4f0e-9d76-8e5d96371cd6
01/30/2025 06:54:26:INFO:Received: evaluate message 48865030-b170-4f0e-9d76-8e5d96371cd6
[92mINFO [0m:      Sent reply
01/30/2025 06:58:57:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:58:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:58:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 176390a4-7f34-432b-bca6-e13227f23cbd
01/30/2025 06:58:57:INFO:Received: reconnect message 176390a4-7f34-432b-bca6-e13227f23cbd
01/30/2025 06:58:58:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 06:58:58:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843, 1.3140666796658915, 1.3254329497133395, 1.2964139303227753, 1.3163958567311291, 1.373914302331615, 1.317547931937119], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299, 0.6564639548932742, 0.6568666935159082, 0.6608940797422472, 0.6548530004027386, 0.6451872734595248, 0.6612968183648812], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921, 0.9071987834331172, 0.9064201878633371, 0.9076969880859942, 0.9078627397733976, 0.9065564478394451, 0.9082396607052312]}



Final client history:
{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843, 1.3140666796658915, 1.3254329497133395, 1.2964139303227753, 1.3163958567311291, 1.373914302331615, 1.317547931937119], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299, 0.6564639548932742, 0.6568666935159082, 0.6608940797422472, 0.6548530004027386, 0.6451872734595248, 0.6612968183648812], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921, 0.9071987834331172, 0.9064201878633371, 0.9076969880859942, 0.9078627397733976, 0.9065564478394451, 0.9082396607052312]}

