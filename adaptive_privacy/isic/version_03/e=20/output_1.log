nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=20/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 10:49:34:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 10:49:34:DEBUG:ChannelConnectivity.IDLE
01/29/2025 10:49:34:DEBUG:ChannelConnectivity.CONNECTING
01/29/2025 10:49:34:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 10:50:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:50:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a31a2063-4d8c-48b4-b033-04886a998f1c
01/29/2025 10:50:02:INFO:Received: train message a31a2063-4d8c-48b4-b033-04886a998f1c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:18:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:26:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:26:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dbdec802-4920-4e3e-851a-b37685602753
01/29/2025 11:26:25:INFO:Received: evaluate message dbdec802-4920-4e3e-851a-b37685602753
[92mINFO [0m:      Sent reply
01/29/2025 11:32:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:32:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:32:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 03556116-c3c2-4ca4-a4bb-1e15b2af0b31
01/29/2025 11:32:58:INFO:Received: train message 03556116-c3c2-4ca4-a4bb-1e15b2af0b31
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:11:29:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:12:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:12:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8760f537-3dcc-4c92-91fc-a0173ce50020
01/29/2025 12:12:27:INFO:Received: evaluate message 8760f537-3dcc-4c92-91fc-a0173ce50020
[92mINFO [0m:      Sent reply
01/29/2025 12:17:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:17:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:17:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message df7963bb-11a8-4892-8de8-4c6800d4c484
01/29/2025 12:17:55:INFO:Received: train message df7963bb-11a8-4892-8de8-4c6800d4c484
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:48:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:49:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:49:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 75953cab-2a17-4708-8b78-bbd4891b1c56
01/29/2025 12:49:00:INFO:Received: evaluate message 75953cab-2a17-4708-8b78-bbd4891b1c56
[92mINFO [0m:      Sent reply
01/29/2025 12:53:39:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:54:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:54:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 77d16d3c-94a7-44a1-8425-075dec6ef3ef
01/29/2025 12:54:19:INFO:Received: train message 77d16d3c-94a7-44a1-8425-075dec6ef3ef
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:31:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:33:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:33:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d6397419-c825-4d1f-967f-027251333bf6
01/29/2025 13:33:03:INFO:Received: evaluate message d6397419-c825-4d1f-967f-027251333bf6
[92mINFO [0m:      Sent reply
01/29/2025 13:38:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:39:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:39:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2429aed1-18d4-4604-9ca7-b09ed1dc72cc
01/29/2025 13:39:06:INFO:Received: train message 2429aed1-18d4-4604-9ca7-b09ed1dc72cc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:11:21:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:12:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:12:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6b234876-7bca-4a2b-8ba7-57090fe0374f
01/29/2025 14:12:12:INFO:Received: evaluate message 6b234876-7bca-4a2b-8ba7-57090fe0374f
[92mINFO [0m:      Sent reply
01/29/2025 14:17:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:17:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:17:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 16e68c36-948c-41f1-b574-895108a86023
01/29/2025 14:17:58:INFO:Received: train message 16e68c36-948c-41f1-b574-895108a86023
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:52:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:55:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:55:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8067d1d4-a8c9-428d-a072-2401be315a56
01/29/2025 14:55:50:INFO:Received: evaluate message 8067d1d4-a8c9-428d-a072-2401be315a56
[92mINFO [0m:      Sent reply
01/29/2025 15:01:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:01:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:01:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d2aaf0b6-c151-4ec8-b58c-2ca1f1ab9d85
01/29/2025 15:01:24:INFO:Received: train message d2aaf0b6-c151-4ec8-b58c-2ca1f1ab9d85
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=20', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=20']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683], 'accuracy': [0.5783326621022956], 'auc': [0.840446426141872]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418], 'accuracy': [0.5783326621022956, 0.621425694724124], 'auc': [0.840446426141872, 0.872547279925616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:32:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:34:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:34:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6c4e1876-89b7-4927-a248-48aac8ed3992
01/29/2025 15:34:44:INFO:Received: evaluate message 6c4e1876-89b7-4927-a248-48aac8ed3992
[92mINFO [0m:      Sent reply
01/29/2025 15:39:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:41:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:41:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cc4a0a32-341f-4eaa-84f5-aee7a547084d
01/29/2025 15:41:00:INFO:Received: train message cc4a0a32-341f-4eaa-84f5-aee7a547084d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:14:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8863cc8a-6b87-453a-b953-adc90d54ef0e
01/29/2025 16:15:20:INFO:Received: evaluate message 8863cc8a-6b87-453a-b953-adc90d54ef0e
[92mINFO [0m:      Sent reply
01/29/2025 16:22:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:22:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:22:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0c6ce847-6a40-4080-89bd-dd97f371d96a
01/29/2025 16:22:58:INFO:Received: train message 0c6ce847-6a40-4080-89bd-dd97f371d96a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:57:41:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:58:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:58:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 079183d6-9832-4adf-a2a3-78305c70545c
01/29/2025 16:58:54:INFO:Received: evaluate message 079183d6-9832-4adf-a2a3-78305c70545c
[92mINFO [0m:      Sent reply
01/29/2025 17:04:33:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:05:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:05:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 07c8847b-b8a0-4247-ab8c-c577444e4863
01/29/2025 17:05:03:INFO:Received: train message 07c8847b-b8a0-4247-ab8c-c577444e4863
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:35:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:39:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:39:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f2738739-b762-4954-a737-250df6da1296
01/29/2025 17:39:14:INFO:Received: evaluate message f2738739-b762-4954-a737-250df6da1296
[92mINFO [0m:      Sent reply
01/29/2025 17:44:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:44:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:44:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a8a86684-dd48-42a7-9062-70cb4e052df6
01/29/2025 17:44:55:INFO:Received: train message a8a86684-dd48-42a7-9062-70cb4e052df6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:17:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:18:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:18:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d51acb10-ca69-489d-9d33-59f4d374ea6f
01/29/2025 18:18:40:INFO:Received: evaluate message d51acb10-ca69-489d-9d33-59f4d374ea6f
[92mINFO [0m:      Sent reply
01/29/2025 18:24:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:24:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:24:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message be6381d2-1083-4918-b06b-c234ac210621
01/29/2025 18:24:23:INFO:Received: train message be6381d2-1083-4918-b06b-c234ac210621
[0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:54:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:55:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:55:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 48214439-d99c-4209-8bc7-4afc87a6391e
01/29/2025 18:55:48:INFO:Received: evaluate message 48214439-d99c-4209-8bc7-4afc87a6391e
[92mINFO [0m:      Sent reply
01/29/2025 19:01:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:01:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:01:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 63127e9f-dded-4a3e-b6c0-74081c63018e
01/29/2025 19:01:48:INFO:Received: train message 63127e9f-dded-4a3e-b6c0-74081c63018e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:32:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:33:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:33:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8b939408-ca44-4341-a2b6-6f3f46973168
01/29/2025 19:33:27:INFO:Received: evaluate message 8b939408-ca44-4341-a2b6-6f3f46973168
[92mINFO [0m:      Sent reply
01/29/2025 19:38:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:40:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:40:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b125bccc-616b-436e-b26b-8055196e1f4e
01/29/2025 19:40:01:INFO:Received: train message b125bccc-616b-436e-b26b-8055196e1f4e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:10:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:11:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:11:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3560d551-2e29-4d26-88c2-41acdeb1746f
01/29/2025 20:11:41:INFO:Received: evaluate message 3560d551-2e29-4d26-88c2-41acdeb1746f
[92mINFO [0m:      Sent reply
01/29/2025 20:16:28:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:17:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:17:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 386a29d8-bc17-4e23-b712-99b2c308b7b5
01/29/2025 20:17:13:INFO:Received: train message 386a29d8-bc17-4e23-b712-99b2c308b7b5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:47:31:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7b0d1064-0ac4-4825-ad82-6e1f1e0a5875
01/29/2025 20:48:04:INFO:Received: evaluate message 7b0d1064-0ac4-4825-ad82-6e1f1e0a5875
[92mINFO [0m:      Sent reply
01/29/2025 20:52:31:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:53:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:53:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 401a9916-e40f-4192-86d7-61f610102004
01/29/2025 20:53:37:INFO:Received: train message 401a9916-e40f-4192-86d7-61f610102004
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:26:29:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:26:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:26:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f7b380ec-26a6-4006-853d-f7a51cc41dce
01/29/2025 21:26:59:INFO:Received: evaluate message f7b380ec-26a6-4006-853d-f7a51cc41dce
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 21:32:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:33:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:33:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd828c90-e123-4836-a0ec-9095b0006557
01/29/2025 21:33:35:INFO:Received: train message cd828c90-e123-4836-a0ec-9095b0006557
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:05:32:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:06:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:06:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ce24b6bc-74e8-4fae-bd6e-d7cc4f52e142
01/29/2025 22:06:12:INFO:Received: evaluate message ce24b6bc-74e8-4fae-bd6e-d7cc4f52e142
[92mINFO [0m:      Sent reply
01/29/2025 22:10:38:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:12:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:12:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 40db9271-8942-4be8-beac-6afc276de4f2
01/29/2025 22:12:05:INFO:Received: train message 40db9271-8942-4be8-beac-6afc276de4f2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:40:44:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:41:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:41:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dce2d9c4-52f1-499f-bd90-9a82d1deb379
01/29/2025 22:41:07:INFO:Received: evaluate message dce2d9c4-52f1-499f-bd90-9a82d1deb379
[92mINFO [0m:      Sent reply
01/29/2025 22:45:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:46:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:46:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d718cf0b-e22e-44ce-b87e-6931b1936cf5
01/29/2025 22:46:33:INFO:Received: train message d718cf0b-e22e-44ce-b87e-6931b1936cf5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:22:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:23:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:23:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 523d85ab-dd26-43e0-8c1e-2e85991c6b89
01/29/2025 23:23:41:INFO:Received: evaluate message 523d85ab-dd26-43e0-8c1e-2e85991c6b89
[92mINFO [0m:      Sent reply
01/29/2025 23:28:57:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:29:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:29:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 41b593a5-2d5c-4e5d-b089-0957a07e263f
01/29/2025 23:29:32:INFO:Received: train message 41b593a5-2d5c-4e5d-b089-0957a07e263f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:12:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:12:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:12:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2a43dadc-9634-45f9-be3b-35425d7b55d6
01/30/2025 00:12:37:INFO:Received: evaluate message 2a43dadc-9634-45f9-be3b-35425d7b55d6

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 00:17:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:18:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:18:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5346f34f-5b5e-4ac5-b466-d2efda65a7ac
01/30/2025 00:18:46:INFO:Received: train message 5346f34f-5b5e-4ac5-b466-d2efda65a7ac
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:45:27:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:46:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:46:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1069083e-3904-48f4-a249-b1061655beee
01/30/2025 00:46:03:INFO:Received: evaluate message 1069083e-3904-48f4-a249-b1061655beee
[92mINFO [0m:      Sent reply
01/30/2025 00:49:53:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:52:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:52:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 060b545b-18df-4c68-9f56-572ee7906288
01/30/2025 00:52:08:INFO:Received: train message 060b545b-18df-4c68-9f56-572ee7906288
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:23:45:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:24:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:24:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2e184185-92ff-425e-8afb-f7f529f3b649
01/30/2025 01:24:22:INFO:Received: evaluate message 2e184185-92ff-425e-8afb-f7f529f3b649
[92mINFO [0m:      Sent reply
01/30/2025 01:28:32:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:29:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:29:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dff2382f-e3d0-4ad4-a012-8d37b2cafae9
01/30/2025 01:29:39:INFO:Received: train message dff2382f-e3d0-4ad4-a012-8d37b2cafae9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:59:50:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:00:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:00:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b17e0709-a2dc-4e5b-8c27-ef1977a36011
01/30/2025 02:00:29:INFO:Received: evaluate message b17e0709-a2dc-4e5b-8c27-ef1977a36011
[92mINFO [0m:      Sent reply
01/30/2025 02:04:52:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:05:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:05:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fc6a5fc4-bdc4-4925-be53-d2d12c498c01
01/30/2025 02:05:37:INFO:Received: train message fc6a5fc4-bdc4-4925-be53-d2d12c498c01

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:39:20:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:40:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:40:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2152706a-d4a9-4e43-b39c-9bf4bab7177c
01/30/2025 02:40:23:INFO:Received: evaluate message 2152706a-d4a9-4e43-b39c-9bf4bab7177c
[92mINFO [0m:      Sent reply
01/30/2025 02:44:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:45:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:45:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2577e479-be4e-43e3-9b4d-2e48850c310b
01/30/2025 02:45:43:INFO:Received: train message 2577e479-be4e-43e3-9b4d-2e48850c310b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:19:33:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:21:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:21:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ee53554e-8ef2-490f-b788-cd4cca273543
01/30/2025 03:21:10:INFO:Received: evaluate message ee53554e-8ef2-490f-b788-cd4cca273543
[92mINFO [0m:      Sent reply
01/30/2025 03:25:16:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:25:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:25:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 47adb885-d19c-4cb0-9dbe-89c15d758692
01/30/2025 03:25:57:INFO:Received: train message 47adb885-d19c-4cb0-9dbe-89c15d758692
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 03:55:58:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 03:56:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 03:56:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4bcf91c0-36d7-470c-9b9e-9d0120a20d00
01/30/2025 03:56:33:INFO:Received: evaluate message 4bcf91c0-36d7-470c-9b9e-9d0120a20d00
[92mINFO [0m:      Sent reply
01/30/2025 04:01:06:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:01:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:01:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3765583f-bef9-463a-a22c-39df0bdf48f7
01/30/2025 04:01:46:INFO:Received: train message 3765583f-bef9-463a-a22c-39df0bdf48f7
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843, 1.3140666796658915], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299, 0.6564639548932742], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921, 0.9071987834331172]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3802482733263683, 1.2848279955307418, 1.3252230800787288, 1.3113089342103867, 1.3572609328090592, 1.2752561200735697, 1.2936724788519436, 1.2880099149082536, 1.283758559704211, 1.235683896586725, 1.269645951445238, 1.2974417839278822, 1.3427641978438611, 1.3458970871319273, 1.3074661073452323, 1.3337799130165908, 1.3133244120781815, 1.3285861998328363, 1.3355151647960664, 1.287368972216025, 1.3090093365082374, 1.322208833391994, 1.346993484782251, 1.3302753639470843, 1.3140666796658915, 1.3254329497133395], 'accuracy': [0.5783326621022956, 0.621425694724124, 0.6270640354409988, 0.6306886830447039, 0.6153846153846154, 0.6407571486105518, 0.63954893274265, 0.6411598872331856, 0.6451872734595248, 0.6592831252517116, 0.6524365686669351, 0.6476037051953283, 0.6431735803463552, 0.6463954893274265, 0.6504228755537657, 0.6447845348368909, 0.657269432138542, 0.6596858638743456, 0.6496173983084977, 0.653242045912203, 0.6524365686669351, 0.6504228755537657, 0.6528393072895691, 0.6488119210632299, 0.6564639548932742, 0.6568666935159082], 'auc': [0.840446426141872, 0.872547279925616, 0.8797351767331626, 0.8861772364572584, 0.8864686446174059, 0.891785044775849, 0.8939037688440359, 0.895249571854143, 0.8972516798750731, 0.9001076897440828, 0.8997896854303573, 0.9004685653635225, 0.8998240151062024, 0.9015544902026775, 0.9022765067434418, 0.9023669715385373, 0.9032001383276858, 0.9047004789559929, 0.9044503177808452, 0.9045678995072952, 0.9044392973572588, 0.9053367522517963, 0.9056255796470886, 0.9055175331414921, 0.9071987834331172, 0.9064201878633371]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.533599853515625
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.05517294630408287, 0.08231624960899353, 0.05339275673031807, 0.017633095383644104, 0.02499944530427456, 0.005828534718602896, 0.00734469760209322, 0.015467780642211437]
Noise Multiplier after list and tensor:  0.032769438286777586
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:39:46:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:40:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:40:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6f0a901b-313f-4d62-b9ee-3d8b22d162ed
01/30/2025 04:40:29:INFO:Received: evaluate message 6f0a901b-313f-4d62-b9ee-3d8b22d162ed
[92mINFO [0m:      Sent reply
01/30/2025 04:44:44:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 04:45:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:45:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1b600788-a685-41fb-b1a1-11740d746e93
01/30/2025 04:45:18:INFO:Received: train message 1b600788-a685-41fb-b1a1-11740d746e93
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:17:27:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:17:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:17:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0f98a0e7-1bcd-4438-ba02-069ad25f2276
01/30/2025 05:17:49:INFO:Received: evaluate message 0f98a0e7-1bcd-4438-ba02-069ad25f2276
[92mINFO [0m:      Sent reply
01/30/2025 05:22:40:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:23:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:23:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 297348e2-2f78-4d33-87c7-97a897085b27
01/30/2025 05:23:35:INFO:Received: train message 297348e2-2f78-4d33-87c7-97a897085b27
