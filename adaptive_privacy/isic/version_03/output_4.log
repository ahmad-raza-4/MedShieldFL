nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/28/2025 21:56:42:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/28/2025 21:56:42:DEBUG:ChannelConnectivity.IDLE
01/28/2025 21:56:42:DEBUG:ChannelConnectivity.CONNECTING
01/28/2025 21:56:42:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/28/2025 22:01:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:01:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f3171956-bbb5-4856-95d8-5009f45947e5
01/28/2025 22:01:36:INFO:Received: train message f3171956-bbb5-4856-95d8-5009f45947e5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 22:10:25:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:21:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:21:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 03fcb2f6-8056-4a3a-ac0e-6165e2d1b34e
01/28/2025 22:21:58:INFO:Received: evaluate message 03fcb2f6-8056-4a3a-ac0e-6165e2d1b34e
[92mINFO [0m:      Sent reply
01/28/2025 22:25:54:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:26:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:26:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 33334d4d-faa6-49bd-b103-4e8616f43e35
01/28/2025 22:26:24:INFO:Received: train message 33334d4d-faa6-49bd-b103-4e8616f43e35
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 22:35:24:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:48:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:48:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 157caa32-7b44-438b-84b4-af5ab8366548
01/28/2025 22:48:26:INFO:Received: evaluate message 157caa32-7b44-438b-84b4-af5ab8366548
[92mINFO [0m:      Sent reply
01/28/2025 22:52:22:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:52:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:52:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c954a8cd-7661-40ab-b240-0dd4f1a62bb4
01/28/2025 22:52:58:INFO:Received: train message c954a8cd-7661-40ab-b240-0dd4f1a62bb4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 23:01:46:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:13:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:13:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 363f00d7-db85-4853-8b45-6639efeda63e
01/28/2025 23:13:17:INFO:Received: evaluate message 363f00d7-db85-4853-8b45-6639efeda63e
[92mINFO [0m:      Sent reply
01/28/2025 23:17:15:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:17:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:17:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 01d4f56a-3b46-4c7a-985a-c9647a1a66d1
01/28/2025 23:17:57:INFO:Received: train message 01d4f56a-3b46-4c7a-985a-c9647a1a66d1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 23:26:54:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:38:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:38:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5f7ea978-1c86-4e68-8cb7-a39714b6d10b
01/28/2025 23:38:40:INFO:Received: evaluate message 5f7ea978-1c86-4e68-8cb7-a39714b6d10b
[92mINFO [0m:      Sent reply
01/28/2025 23:42:35:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:42:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:42:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d995421c-5b42-4d17-8932-5628137ba13e
01/28/2025 23:42:56:INFO:Received: train message d995421c-5b42-4d17-8932-5628137ba13e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 23:51:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:05:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:05:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 54d65575-ab0a-46ba-82bc-1e78ece146e2
01/29/2025 00:05:06:INFO:Received: evaluate message 54d65575-ab0a-46ba-82bc-1e78ece146e2
[92mINFO [0m:      Sent reply
01/29/2025 00:09:05:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:09:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:09:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bfbcccb3-aaf3-45e9-8b63-5a542f5a9edb
01/29/2025 00:09:26:INFO:Received: train message bfbcccb3-aaf3-45e9-8b63-5a542f5a9edb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 00:18:16:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:31:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:31:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 63dd8a4c-3e19-45f1-b933-c105af9d0481
01/29/2025 00:31:58:INFO:Received: evaluate message 63dd8a4c-3e19-45f1-b933-c105af9d0481
[92mINFO [0m:      Sent reply
01/29/2025 00:36:02:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:36:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:36:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 314c9a52-545e-4588-9efd-e6bfafe43f4c
01/29/2025 00:36:18:INFO:Received: train message 314c9a52-545e-4588-9efd-e6bfafe43f4c
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007], 'accuracy': [0.5823600483286347], 'auc': [0.844982871338986]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152], 'accuracy': [0.5823600483286347, 0.6182037857430528], 'auc': [0.844982871338986, 0.8747687441438666]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 00:44:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:58:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:58:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3782cfef-24d7-4690-9083-677de1dd6f86
01/29/2025 00:58:21:INFO:Received: evaluate message 3782cfef-24d7-4690-9083-677de1dd6f86
[92mINFO [0m:      Sent reply
01/29/2025 01:02:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:02:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:02:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 840dcef0-e7f8-491e-abb2-92c1b5b2de88
01/29/2025 01:02:53:INFO:Received: train message 840dcef0-e7f8-491e-abb2-92c1b5b2de88
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 01:12:43:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:28:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:28:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a526d7de-f825-4dde-a740-45d12046142a
01/29/2025 01:28:35:INFO:Received: evaluate message a526d7de-f825-4dde-a740-45d12046142a
[92mINFO [0m:      Sent reply
01/29/2025 01:32:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:33:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:33:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 868e4902-8841-4b8e-8dca-1ca2f9bca0df
01/29/2025 01:33:29:INFO:Received: train message 868e4902-8841-4b8e-8dca-1ca2f9bca0df
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 01:43:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:00:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:00:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2d75b4b9-176b-43a3-a366-f5d792500f63
01/29/2025 02:00:52:INFO:Received: evaluate message 2d75b4b9-176b-43a3-a366-f5d792500f63
[92mINFO [0m:      Sent reply
01/29/2025 02:04:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:05:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:05:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 81b48eb0-b333-490d-a527-6fa88953a648
01/29/2025 02:05:17:INFO:Received: train message 81b48eb0-b333-490d-a527-6fa88953a648
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 02:14:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:29:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:29:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bf3605d8-c757-4e3c-adec-3037be417819
01/29/2025 02:29:43:INFO:Received: evaluate message bf3605d8-c757-4e3c-adec-3037be417819
[92mINFO [0m:      Sent reply
01/29/2025 02:33:38:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:34:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:34:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6a378dc8-0f33-403b-918b-d284107a522f
01/29/2025 02:34:13:INFO:Received: train message 6a378dc8-0f33-403b-918b-d284107a522f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 02:43:35:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:01:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:01:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1b671548-d278-4825-a2fa-45f49e804ff8
01/29/2025 03:01:24:INFO:Received: evaluate message 1b671548-d278-4825-a2fa-45f49e804ff8
[92mINFO [0m:      Sent reply
01/29/2025 03:05:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:05:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:05:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f7372f6a-ebcd-426d-9e99-a7cb93ec3dd3
01/29/2025 03:05:54:INFO:Received: train message f7372f6a-ebcd-426d-9e99-a7cb93ec3dd3
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 03:14:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:26:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:26:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e7e7fa64-3503-420a-ba1a-c4aad073edb4
01/29/2025 03:26:43:INFO:Received: evaluate message e7e7fa64-3503-420a-ba1a-c4aad073edb4
[92mINFO [0m:      Sent reply
01/29/2025 03:30:07:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:31:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:31:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dd0b71ae-95c8-4a82-a1ab-1be850157746
01/29/2025 03:31:20:INFO:Received: train message dd0b71ae-95c8-4a82-a1ab-1be850157746
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 03:40:02:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:55:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:55:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 36f41c62-941d-4136-a039-2d6f1e48973a
01/29/2025 03:55:22:INFO:Received: evaluate message 36f41c62-941d-4136-a039-2d6f1e48973a
[92mINFO [0m:      Sent reply
01/29/2025 03:59:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:59:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:59:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c707a00a-bb29-44b5-b22b-1029c2cb209d
01/29/2025 03:59:42:INFO:Received: train message c707a00a-bb29-44b5-b22b-1029c2cb209d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 04:08:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:20:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:20:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2342c1b0-c28b-446d-8a1b-79e608442968
01/29/2025 04:20:24:INFO:Received: evaluate message 2342c1b0-c28b-446d-8a1b-79e608442968
[92mINFO [0m:      Sent reply
01/29/2025 04:24:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:24:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:24:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a835c9d1-dcd5-4f4e-b3a5-11e3c1eb343d
01/29/2025 04:24:55:INFO:Received: train message a835c9d1-dcd5-4f4e-b3a5-11e3c1eb343d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 04:33:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:47:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:47:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 30621cef-0e8f-4514-bff7-64095654f3e2
01/29/2025 04:47:30:INFO:Received: evaluate message 30621cef-0e8f-4514-bff7-64095654f3e2
[92mINFO [0m:      Sent reply
01/29/2025 04:50:57:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:52:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:52:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c0e8ad3a-442c-4b5b-bee7-3814d8aa39cb
01/29/2025 04:52:09:INFO:Received: train message c0e8ad3a-442c-4b5b-bee7-3814d8aa39cb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 05:00:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:13:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:13:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 28f4bc38-aa4e-4ebe-9094-d57b242f1c11
01/29/2025 05:13:03:INFO:Received: evaluate message 28f4bc38-aa4e-4ebe-9094-d57b242f1c11
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 05:16:49:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:17:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:17:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ec14f864-80e2-42c3-bded-9b1e7a69da74
01/29/2025 05:17:35:INFO:Received: train message ec14f864-80e2-42c3-bded-9b1e7a69da74
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 05:26:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:40:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:40:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 38499139-d122-428d-8fae-ad1316387a8e
01/29/2025 05:40:12:INFO:Received: evaluate message 38499139-d122-428d-8fae-ad1316387a8e
[92mINFO [0m:      Sent reply
01/29/2025 05:43:49:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:44:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:44:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9a684b44-e16c-4df9-8c91-c6a691efefa7
01/29/2025 05:44:39:INFO:Received: train message 9a684b44-e16c-4df9-8c91-c6a691efefa7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 05:53:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 06:10:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 06:10:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 15134759-b322-4354-ae50-a6c530402595
01/29/2025 06:10:11:INFO:Received: evaluate message 15134759-b322-4354-ae50-a6c530402595
[92mINFO [0m:      Sent reply
01/29/2025 06:15:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 06:16:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 06:16:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 01f2eed7-3245-425b-a3b4-f722e18c20bb
01/29/2025 06:16:16:INFO:Received: train message 01f2eed7-3245-425b-a3b4-f722e18c20bb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 06:29:27:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:02:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:02:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d9cec841-8930-4b9a-90bd-159f666751eb
01/29/2025 07:02:10:INFO:Received: evaluate message d9cec841-8930-4b9a-90bd-159f666751eb
[92mINFO [0m:      Sent reply
01/29/2025 07:07:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:08:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:08:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 120cda5f-8431-4bf6-8999-3cf27cda5010
01/29/2025 07:08:23:INFO:Received: train message 120cda5f-8431-4bf6-8999-3cf27cda5010
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 07:22:21:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:57:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:57:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dc25bdc8-aa4f-4a1c-a3e3-f59f1ee7e3ac
01/29/2025 07:57:21:INFO:Received: evaluate message dc25bdc8-aa4f-4a1c-a3e3-f59f1ee7e3ac

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826, 1.3427717304978695], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846, 0.6512283527990335], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434, 0.9055308040441616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
[92mINFO [0m:      Sent reply
01/29/2025 08:03:34:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:03:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:03:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a0f167ff-d883-49da-8553-ef8815d37b62
01/29/2025 08:03:47:INFO:Received: train message a0f167ff-d883-49da-8553-ef8815d37b62
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:16:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:53:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:53:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a24fc39a-8d22-4701-be60-7abd41d83583
01/29/2025 08:53:57:INFO:Received: evaluate message a24fc39a-8d22-4701-be60-7abd41d83583
[92mINFO [0m:      Sent reply
01/29/2025 09:01:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:01:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:01:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fcf712bb-d846-4ad7-a5b7-5dc64825b91b
01/29/2025 09:01:19:INFO:Received: train message fcf712bb-d846-4ad7-a5b7-5dc64825b91b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:14:30:INFO:Sent reply
01/29/2025 09:26:06:DEBUG:gRPC channel closed
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826, 1.3427717304978695, 1.3111424360887878], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846, 0.6512283527990335, 0.6556584776480064], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434, 0.9055308040441616, 0.90569082690568]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826, 1.3427717304978695, 1.3111424360887878, 1.3010476263741837], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846, 0.6512283527990335, 0.6556584776480064, 0.6536447845348369], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434, 0.9055308040441616, 0.90569082690568, 0.9057821824917216]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005002044606953859, 0.012594924308359623, 0.004244559910148382, 0.0026864863466471434, 0.007137167267501354, 0.0013044446241110563, 0.00044903080561198294, 0.002889486262574792]
Noise Multiplier after list and tensor:  0.004538518016488524
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Traceback (most recent call last):
  File "client_4.py", line 380, in <module>
    fl.client.start_client(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 291, in start_client
    _start_client_internal(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 453, in _start_client_internal
    message = receive()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/grpc_client/connection.py", line 138, in receive
    proto = next(server_message_iterator)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/grpc/_channel.py", line 543, in __next__
    return self._next()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/grpc/_channel.py", line 969, in _next
    raise self
grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:
	status = StatusCode.UNAVAILABLE
	details = "ping timeout"
	debug_error_string = "UNKNOWN:Error received from peer ipv4:127.0.0.1:8052 {created_time:"2025-01-29T09:26:06.818718769-08:00", grpc_status:14, grpc_message:"ping timeout"}"
>
nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/30/2025 04:38:39:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/30/2025 04:38:39:DEBUG:ChannelConnectivity.IDLE
01/30/2025 04:38:39:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/30/2025 04:45:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:45:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7bb2c257-77a6-49c4-a187-136e6184711d
01/30/2025 04:45:45:INFO:Received: train message 7bb2c257-77a6-49c4-a187-136e6184711d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:56:33:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:17:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:17:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5b62792-26f5-4ec3-a309-33c833a2ff78
01/30/2025 05:17:49:INFO:Received: evaluate message b5b62792-26f5-4ec3-a309-33c833a2ff78
[92mINFO [0m:      Sent reply
01/30/2025 05:22:20:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:23:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:23:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7945e193-b2fc-4fbc-bf13-9fef8054ea52
01/30/2025 05:23:02:INFO:Received: train message 7945e193-b2fc-4fbc-bf13-9fef8054ea52
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:33:41:INFO:Sent reply
