nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/28/2025 21:57:58:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/28/2025 21:57:58:DEBUG:ChannelConnectivity.IDLE
01/28/2025 21:57:58:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/28/2025 22:01:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:01:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fac16d1f-5ba5-424c-8246-36e17f750488
01/28/2025 22:01:20:INFO:Received: train message fac16d1f-5ba5-424c-8246-36e17f750488
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 22:12:48:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:21:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:21:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7e647393-186b-4274-a94a-50b60f2e4e83
01/28/2025 22:21:50:INFO:Received: evaluate message 7e647393-186b-4274-a94a-50b60f2e4e83
[92mINFO [0m:      Sent reply
01/28/2025 22:25:45:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:26:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:26:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a53c87b6-9c46-48d2-87c7-045a32d391c7
01/28/2025 22:26:11:INFO:Received: train message a53c87b6-9c46-48d2-87c7-045a32d391c7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 22:37:38:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:48:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:48:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e4d12a2b-af67-44ab-9d84-dc61a711c77f
01/28/2025 22:48:21:INFO:Received: evaluate message e4d12a2b-af67-44ab-9d84-dc61a711c77f
[92mINFO [0m:      Sent reply
01/28/2025 22:52:16:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:52:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:52:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e74e3f48-4adb-4c68-a8fb-fec86225a9b6
01/28/2025 22:52:37:INFO:Received: train message e74e3f48-4adb-4c68-a8fb-fec86225a9b6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 23:04:18:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:13:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:13:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0f6ee1b5-3b6f-43a1-a82c-73ae3ec91f69
01/28/2025 23:13:21:INFO:Received: evaluate message 0f6ee1b5-3b6f-43a1-a82c-73ae3ec91f69
[92mINFO [0m:      Sent reply
01/28/2025 23:17:22:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:17:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:17:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 56366b6f-fc52-441d-8fc4-e4cf5ff9fac6
01/28/2025 23:17:45:INFO:Received: train message 56366b6f-fc52-441d-8fc4-e4cf5ff9fac6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 23:29:47:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:38:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:38:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ea5223c0-c08e-4bb6-9b21-17efbdc6ddce
01/28/2025 23:38:34:INFO:Received: evaluate message ea5223c0-c08e-4bb6-9b21-17efbdc6ddce
[92mINFO [0m:      Sent reply
01/28/2025 23:42:24:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:42:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:42:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8d357f21-c59e-4833-882f-cafdfa8dc6fd
01/28/2025 23:42:56:INFO:Received: train message 8d357f21-c59e-4833-882f-cafdfa8dc6fd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 23:54:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:05:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:05:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 326d2db0-f54f-4152-be18-7fe5e6ffd0be
01/29/2025 00:05:05:INFO:Received: evaluate message 326d2db0-f54f-4152-be18-7fe5e6ffd0be
[92mINFO [0m:      Sent reply
01/29/2025 00:09:05:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:09:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:09:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eb04f7f0-239b-4946-9061-48d3b26a0c96
01/29/2025 00:09:35:INFO:Received: train message eb04f7f0-239b-4946-9061-48d3b26a0c96
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 00:22:17:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:31:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:31:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d1e2762d-b0a6-4e14-909a-4aa2c217e336
01/29/2025 00:31:57:INFO:Received: evaluate message d1e2762d-b0a6-4e14-909a-4aa2c217e336
[92mINFO [0m:      Sent reply
01/29/2025 00:36:02:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:36:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:36:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd311cae-188c-4fe4-9ad0-ef931020fb36
01/29/2025 00:36:33:INFO:Received: train message cd311cae-188c-4fe4-9ad0-ef931020fb36
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007], 'accuracy': [0.5823600483286347], 'auc': [0.844982871338986]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152], 'accuracy': [0.5823600483286347, 0.6182037857430528], 'auc': [0.844982871338986, 0.8747687441438666]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 00:48:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:58:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:58:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6064e0ba-9acf-4f99-b3ee-91cee39468c9
01/29/2025 00:58:08:INFO:Received: evaluate message 6064e0ba-9acf-4f99-b3ee-91cee39468c9
[92mINFO [0m:      Sent reply
01/29/2025 01:02:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:03:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:03:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7980eaa2-d363-4ff0-a21f-d85bfe9ea1c3
01/29/2025 01:03:01:INFO:Received: train message 7980eaa2-d363-4ff0-a21f-d85bfe9ea1c3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 01:15:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:28:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:28:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2a6b3738-b237-47f8-b26b-cbdd202e2b12
01/29/2025 01:28:51:INFO:Received: evaluate message 2a6b3738-b237-47f8-b26b-cbdd202e2b12
[92mINFO [0m:      Sent reply
01/29/2025 01:32:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:33:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:33:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4d801bc1-67d2-4116-ab66-4458b0a21aa9
01/29/2025 01:33:31:INFO:Received: train message 4d801bc1-67d2-4116-ab66-4458b0a21aa9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 01:47:02:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:00:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:00:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e3d2649f-e788-463f-b9dd-7e9c2a2f7d52
01/29/2025 02:00:58:INFO:Received: evaluate message e3d2649f-e788-463f-b9dd-7e9c2a2f7d52
[92mINFO [0m:      Sent reply
01/29/2025 02:04:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:05:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:05:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d8db8221-51a3-4d50-8bd1-c0bd45cff845
01/29/2025 02:05:14:INFO:Received: train message d8db8221-51a3-4d50-8bd1-c0bd45cff845
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 02:18:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:29:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:29:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 33a72dad-3005-4cf1-b89d-0bd0f31432e1
01/29/2025 02:29:39:INFO:Received: evaluate message 33a72dad-3005-4cf1-b89d-0bd0f31432e1
[92mINFO [0m:      Sent reply
01/29/2025 02:33:35:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:34:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:34:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 59c224d3-0bb6-4fb6-b84b-65c69c63431d
01/29/2025 02:34:05:INFO:Received: train message 59c224d3-0bb6-4fb6-b84b-65c69c63431d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 02:46:23:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:01:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:01:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4b4a39ae-7504-495a-96c6-e319ba58b900
01/29/2025 03:01:28:INFO:Received: evaluate message 4b4a39ae-7504-495a-96c6-e319ba58b900
[92mINFO [0m:      Sent reply
01/29/2025 03:05:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:05:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:05:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 20271499-77bf-48b0-9c9b-013dc72a5f0f
01/29/2025 03:05:56:INFO:Received: train message 20271499-77bf-48b0-9c9b-013dc72a5f0f

Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 03:17:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:27:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:27:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5285baaa-aedc-47e3-a8a5-4ad14cef09d3
01/29/2025 03:27:01:INFO:Received: evaluate message 5285baaa-aedc-47e3-a8a5-4ad14cef09d3
[92mINFO [0m:      Sent reply
01/29/2025 03:30:49:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:31:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:31:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 29df6e8e-3720-49aa-9f09-812ddb317914
01/29/2025 03:31:18:INFO:Received: train message 29df6e8e-3720-49aa-9f09-812ddb317914
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 03:43:03:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:55:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:55:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 076a4d67-7bbc-4b04-b967-b504c91f2ee0
01/29/2025 03:55:18:INFO:Received: evaluate message 076a4d67-7bbc-4b04-b967-b504c91f2ee0
[92mINFO [0m:      Sent reply
01/29/2025 03:59:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:59:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:59:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0df483af-f373-4c80-8c3c-fcbedd678911
01/29/2025 03:59:58:INFO:Received: train message 0df483af-f373-4c80-8c3c-fcbedd678911
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 04:11:36:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:20:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:20:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5a5e67f-8d73-47c2-af96-00220f236091
01/29/2025 04:20:24:INFO:Received: evaluate message b5a5e67f-8d73-47c2-af96-00220f236091
[92mINFO [0m:      Sent reply
01/29/2025 04:24:23:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:24:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:24:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8e8434a6-0b88-4f1b-91bf-c8b15614d828
01/29/2025 04:24:43:INFO:Received: train message 8e8434a6-0b88-4f1b-91bf-c8b15614d828
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 04:36:42:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:47:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:47:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b04ba406-e162-4532-86b1-0fc186f93307
01/29/2025 04:47:35:INFO:Received: evaluate message b04ba406-e162-4532-86b1-0fc186f93307
[92mINFO [0m:      Sent reply
01/29/2025 04:51:29:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:51:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:51:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1d6e10cf-70ef-42c9-81ea-6a71450f4279
01/29/2025 04:51:53:INFO:Received: train message 1d6e10cf-70ef-42c9-81ea-6a71450f4279
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 05:03:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:13:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:13:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8c881e7f-53a6-42a2-9a73-da1122ddf138
01/29/2025 05:13:12:INFO:Received: evaluate message 8c881e7f-53a6-42a2-9a73-da1122ddf138
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 05:17:03:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:17:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:17:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c872e590-ebaf-487a-a71f-41e39f049237
01/29/2025 05:17:14:INFO:Received: train message c872e590-ebaf-487a-a71f-41e39f049237
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 05:28:41:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:40:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:40:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f7cd77a9-480e-4480-a672-b426f2480405
01/29/2025 05:40:27:INFO:Received: evaluate message f7cd77a9-480e-4480-a672-b426f2480405
[92mINFO [0m:      Sent reply
01/29/2025 05:44:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:44:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:44:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ad100218-8954-41e3-bf80-dc8ebaddd897
01/29/2025 05:44:51:INFO:Received: train message ad100218-8954-41e3-bf80-dc8ebaddd897
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 05:56:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 06:10:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 06:10:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a09bd5af-3d71-48f6-ae72-6048c03980a5
01/29/2025 06:10:06:INFO:Received: evaluate message a09bd5af-3d71-48f6-ae72-6048c03980a5
[92mINFO [0m:      Sent reply
01/29/2025 06:15:44:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 06:16:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 06:16:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4fbf2065-95af-4aa8-92a8-4665ef4541ac
01/29/2025 06:16:00:INFO:Received: train message 4fbf2065-95af-4aa8-92a8-4665ef4541ac
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 06:33:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:02:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:02:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8c66df55-ffd3-45b8-a44c-68fab185e349
01/29/2025 07:02:05:INFO:Received: evaluate message 8c66df55-ffd3-45b8-a44c-68fab185e349
[92mINFO [0m:      Sent reply
01/29/2025 07:07:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:08:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:08:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 11da5797-b92c-49b8-969e-887bc453ee59
01/29/2025 07:08:29:INFO:Received: train message 11da5797-b92c-49b8-969e-887bc453ee59
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 07:28:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:57:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:57:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 53a0a683-e1d4-40d5-9c25-9bc8399eec0d
01/29/2025 07:57:21:INFO:Received: evaluate message 53a0a683-e1d4-40d5-9c25-9bc8399eec0d

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826, 1.3427717304978695], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846, 0.6512283527990335], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434, 0.9055308040441616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010430833324790001, 0.025769127532839775, 0.0035308445803821087, 0.0007674619555473328, 0.008809649385511875, 0.0017614802345633507, 0.001948355813510716, 0.0015640758210793138]
Noise Multiplier after list and tensor:  0.006822728581028059
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
[92mINFO [0m:      Sent reply
01/29/2025 08:03:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:04:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:04:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4148bf06-e98e-47f9-9564-852ce7b989ff
01/29/2025 08:04:09:INFO:Received: train message 4148bf06-e98e-47f9-9564-852ce7b989ff
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:22:43:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:53:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:53:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 898ee7b8-d47b-46d3-94c4-d91182a1d9ae
01/29/2025 08:53:43:INFO:Received: evaluate message 898ee7b8-d47b-46d3-94c4-d91182a1d9ae
[92mINFO [0m:      Sent reply
01/29/2025 09:00:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:01:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:01:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message becaa606-3312-4cae-901a-1ebcd6a90cb5
01/29/2025 09:01:35:INFO:Received: train message becaa606-3312-4cae-901a-1ebcd6a90cb5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:20:05:INFO:Sent reply
nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/30/2025 04:39:45:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/30/2025 04:39:45:DEBUG:ChannelConnectivity.IDLE
01/30/2025 04:39:45:DEBUG:ChannelConnectivity.CONNECTING
01/30/2025 04:39:45:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/30/2025 04:45:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:45:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 50a72161-8de8-478d-a4aa-7e1e933c4200
01/30/2025 04:45:33:INFO:Received: train message 50a72161-8de8-478d-a4aa-7e1e933c4200
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:01:22:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:17:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:17:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 91d87cb4-b47e-49c7-8fef-95c3b602f93c
01/30/2025 05:17:35:INFO:Received: evaluate message 91d87cb4-b47e-49c7-8fef-95c3b602f93c
[92mINFO [0m:      Sent reply
01/30/2025 05:22:00:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:23:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:23:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 64cf7897-db36-4832-bd50-f01bc35962c3
01/30/2025 05:23:00:INFO:Received: train message 64cf7897-db36-4832-bd50-f01bc35962c3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:38:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:04:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:04:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b32e999a-6c5f-4beb-923c-e415d8a4e386
01/30/2025 06:04:53:INFO:Received: evaluate message b32e999a-6c5f-4beb-923c-e415d8a4e386
[92mINFO [0m:      Sent reply
01/30/2025 06:09:40:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:10:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:10:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 96596df5-4480-40a1-843a-1604eff295c3
01/30/2025 06:10:36:INFO:Received: train message 96596df5-4480-40a1-843a-1604eff295c3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 06:29:19:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:50:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:50:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a0ae96f8-ee29-412c-846e-700e9aa42b43
01/30/2025 06:50:46:INFO:Received: evaluate message a0ae96f8-ee29-412c-846e-700e9aa42b43
[92mINFO [0m:      Sent reply
01/30/2025 06:55:40:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:56:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:56:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 643465b4-abe3-4162-89b2-6df5203352e2
01/30/2025 06:56:16:INFO:Received: train message 643465b4-abe3-4162-89b2-6df5203352e2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 07:10:08:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:29:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:29:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ccb64be5-079d-48e3-b9ec-5164c89c61d5
01/30/2025 07:29:09:INFO:Received: evaluate message ccb64be5-079d-48e3-b9ec-5164c89c61d5
[92mINFO [0m:      Sent reply
01/30/2025 07:34:06:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:34:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:34:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cfb86b68-23f7-49bf-b8c3-6ce88d43f0bd
01/30/2025 07:34:49:INFO:Received: train message cfb86b68-23f7-49bf-b8c3-6ce88d43f0bd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 07:48:55:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 08:18:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 08:18:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 565cc4ed-2dcd-4728-8ba2-6a13738fc471
01/30/2025 08:18:15:INFO:Received: evaluate message 565cc4ed-2dcd-4728-8ba2-6a13738fc471
[92mINFO [0m:      Sent reply
01/30/2025 08:22:16:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 08:22:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 08:22:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 150b650e-8a53-4674-b6ef-a46f6d57fd46
01/30/2025 08:22:16:INFO:Received: reconnect message 150b650e-8a53-4674-b6ef-a46f6d57fd46
01/30/2025 08:22:16:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 08:22:16:INFO:Disconnect and shut down
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010458774864673615, 0.025683237239718437, 0.003541535697877407, 0.0007729015778750181, 0.008693458512425423, 0.0017648584907874465, 0.0019559834618121386, 0.0015776988584548235]
Noise Multiplier after list and tensor:  0.0068060560879530385
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644], 'accuracy': [0.6741844542891663], 'auc': [0.9136487085523237]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010458774864673615, 0.025683237239718437, 0.003541535697877407, 0.0007729015778750181, 0.008693458512425423, 0.0017648584907874465, 0.0019559834618121386, 0.0015776988584548235]
Noise Multiplier after list and tensor:  0.0068060560879530385
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512], 'accuracy': [0.6741844542891663, 0.6677406363270237], 'auc': [0.9136487085523237, 0.9122972121312665]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010458774864673615, 0.025683237239718437, 0.003541535697877407, 0.0007729015778750181, 0.008693458512425423, 0.0017648584907874465, 0.0019559834618121386, 0.0015776988584548235]
Noise Multiplier after list and tensor:  0.0068060560879530385
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010458774864673615, 0.025683237239718437, 0.003541535697877407, 0.0007729015778750181, 0.008693458512425423, 0.0017648584907874465, 0.0019559834618121386, 0.0015776988584548235]
Noise Multiplier after list and tensor:  0.0068060560879530385
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.409698486328125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.010458774864673615, 0.025683237239718437, 0.003541535697877407, 0.0007729015778750181, 0.008693458512425423, 0.0017648584907874465, 0.0019559834618121386, 0.0015776988584548235]
Noise Multiplier after list and tensor:  0.0068060560879530385
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765, 1.4033728663449896], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438, 0.6524365686669351], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285, 0.9103161857948601]}



Final client history:
{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765, 1.4033728663449896], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438, 0.6524365686669351], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285, 0.9103161857948601]}

