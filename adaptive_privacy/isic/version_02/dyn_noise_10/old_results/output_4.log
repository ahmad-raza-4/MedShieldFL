nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/23/2025 11:21:06:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/23/2025 11:21:06:DEBUG:ChannelConnectivity.IDLE
01/23/2025 11:21:06:DEBUG:ChannelConnectivity.CONNECTING
01/23/2025 11:21:06:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/23/2025 11:24:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:24:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cf379677-c1b1-4330-8c27-68059c2845ed
01/23/2025 11:24:42:INFO:Received: train message cf379677-c1b1-4330-8c27-68059c2845ed
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:33:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:43:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:43:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ade18a1b-224a-418e-8452-50b79b0afd4f
01/23/2025 11:43:54:INFO:Received: evaluate message ade18a1b-224a-418e-8452-50b79b0afd4f
[92mINFO [0m:      Sent reply
01/23/2025 11:47:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:48:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:48:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ab42119-4ec2-4ba0-a9d0-5cdc56678e62
01/23/2025 11:48:18:INFO:Received: train message 8ab42119-4ec2-4ba0-a9d0-5cdc56678e62
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:57:02:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:07:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:07:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a4f35cad-f2c3-418f-95a8-d7e22a9c8ec1
01/23/2025 12:07:34:INFO:Received: evaluate message a4f35cad-f2c3-418f-95a8-d7e22a9c8ec1
[92mINFO [0m:      Sent reply
01/23/2025 12:11:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:12:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:12:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2d4a2436-02a6-42cd-a35f-025f2e79774b
01/23/2025 12:12:06:INFO:Received: train message 2d4a2436-02a6-42cd-a35f-025f2e79774b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:21:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:31:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:31:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 325cb572-d2c8-4cad-877a-5fe418f03fac
01/23/2025 12:31:06:INFO:Received: evaluate message 325cb572-d2c8-4cad-877a-5fe418f03fac
[92mINFO [0m:      Sent reply
01/23/2025 12:35:10:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:35:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:35:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c3a505a9-b3d6-42e5-9e38-7761450f38b7
01/23/2025 12:35:35:INFO:Received: train message c3a505a9-b3d6-42e5-9e38-7761450f38b7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:44:39:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:54:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:54:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a13fe2d1-945b-4ca6-930a-0e3a150109db
01/23/2025 12:54:44:INFO:Received: evaluate message a13fe2d1-945b-4ca6-930a-0e3a150109db
[92mINFO [0m:      Sent reply
01/23/2025 12:58:48:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:59:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:59:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9ef23ca0-65a0-47d2-a447-e0a8bb634d55
01/23/2025 12:59:18:INFO:Received: train message 9ef23ca0-65a0-47d2-a447-e0a8bb634d55
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:08:29:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:18:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:18:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a3954f86-9d21-47fa-9df4-f24dddd3f92f
01/23/2025 13:18:39:INFO:Received: evaluate message a3954f86-9d21-47fa-9df4-f24dddd3f92f
[92mINFO [0m:      Sent reply
01/23/2025 13:22:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:23:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:23:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 18addb6d-26a8-43d8-84d0-5af80674bf7e
01/23/2025 13:23:12:INFO:Received: train message 18addb6d-26a8-43d8-84d0-5af80674bf7e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:32:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:42:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:42:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 712ffaa2-98ec-4a34-a7fc-4df8b515f995
01/23/2025 13:42:13:INFO:Received: evaluate message 712ffaa2-98ec-4a34-a7fc-4df8b515f995
[92mINFO [0m:      Sent reply
01/23/2025 13:46:14:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:46:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:46:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 312d0b8d-ae80-4fe4-b1a4-8a736b54e559
01/23/2025 13:46:40:INFO:Received: train message 312d0b8d-ae80-4fe4-b1a4-8a736b54e559
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:55:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:05:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:05:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 21039904-dedd-467b-a3ea-09ddc31dd14c
01/23/2025 14:05:47:INFO:Received: evaluate message 21039904-dedd-467b-a3ea-09ddc31dd14c
[92mINFO [0m:      Sent reply
01/23/2025 14:09:42:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:10:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:10:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 93808b84-922e-4dab-8968-dd68b26d519b
01/23/2025 14:10:05:INFO:Received: train message 93808b84-922e-4dab-8968-dd68b26d519b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:19:23:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:29:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:29:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bdea0b9c-6b8e-43e4-8191-7f1866ae3c75
01/23/2025 14:29:27:INFO:Received: evaluate message bdea0b9c-6b8e-43e4-8191-7f1866ae3c75
[92mINFO [0m:      Sent reply
01/23/2025 14:33:29:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:33:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:33:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2179ad61-8cc9-46fd-8394-c11fb6f8d8c6
01/23/2025 14:33:49:INFO:Received: train message 2179ad61-8cc9-46fd-8394-c11fb6f8d8c6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:42:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:53:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:53:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1ba5008b-ca18-4b4d-bf17-3629897e3f41
01/23/2025 14:53:13:INFO:Received: evaluate message 1ba5008b-ca18-4b4d-bf17-3629897e3f41
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 10, target_epsilon: 10, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 2.12

{'loss': [137.23898267745972], 'accuracy': [0.3383004430124849], 'auc': [0.5877396505618926]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 1.98

{'loss': [137.23898267745972, 134.08246445655823], 'accuracy': [0.3383004430124849, 0.3407168747482884], 'auc': [0.5877396505618926, 0.620088227094536]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 1.94

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 2.10

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 1.63

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 2.65

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 2.44

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 2.63

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 2.43
[92mINFO [0m:      Sent reply
01/23/2025 14:57:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:57:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:57:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b8ae2e23-1b85-4e36-a311-f95806d9b19f
01/23/2025 14:57:36:INFO:Received: train message b8ae2e23-1b85-4e36-a311-f95806d9b19f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:06:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:16:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:16:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3439b06c-7b93-4c3b-97d0-4f5fcfcaf047
01/23/2025 15:16:41:INFO:Received: evaluate message 3439b06c-7b93-4c3b-97d0-4f5fcfcaf047
[92mINFO [0m:      Sent reply
01/23/2025 15:20:47:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:21:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:21:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c7846078-e2cb-449a-85d3-0ac30766da34
01/23/2025 15:21:15:INFO:Received: train message c7846078-e2cb-449a-85d3-0ac30766da34
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:30:22:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:40:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:40:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0ab21e89-3406-4df8-aeb1-5d7b1441b2ae
01/23/2025 15:40:27:INFO:Received: evaluate message 0ab21e89-3406-4df8-aeb1-5d7b1441b2ae
[92mINFO [0m:      Sent reply
01/23/2025 15:44:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:44:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:44:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dc611647-ced9-4eaa-9d7a-2002f2283bdc
01/23/2025 15:44:56:INFO:Received: train message dc611647-ced9-4eaa-9d7a-2002f2283bdc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:53:43:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:03:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:03:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fecc6a1c-ac0a-471e-8854-aecf06490f4f
01/23/2025 16:03:38:INFO:Received: evaluate message fecc6a1c-ac0a-471e-8854-aecf06490f4f
[92mINFO [0m:      Sent reply
01/23/2025 16:07:18:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:08:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:08:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b3d01c9d-7339-4fd8-a2d0-fa186e2b97cd
01/23/2025 16:08:03:INFO:Received: train message b3d01c9d-7339-4fd8-a2d0-fa186e2b97cd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:16:00:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:27:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:27:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3a095ac8-1144-4305-95dd-7af8ab38f593
01/23/2025 16:27:08:INFO:Received: evaluate message 3a095ac8-1144-4305-95dd-7af8ab38f593
[92mINFO [0m:      Sent reply
01/23/2025 16:31:12:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:31:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:31:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 01291f92-0a1a-4719-ab7f-9d735e95e984
01/23/2025 16:31:49:INFO:Received: train message 01291f92-0a1a-4719-ab7f-9d735e95e984
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:40:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:51:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:51:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 055c1885-ed84-4ff5-950f-594da12e4c4c
01/23/2025 16:51:03:INFO:Received: evaluate message 055c1885-ed84-4ff5-950f-594da12e4c4c
[92mINFO [0m:      Sent reply
01/23/2025 16:54:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:55:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:55:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fec288db-f951-4952-a914-dc85ee387e13
01/23/2025 16:55:12:INFO:Received: train message fec288db-f951-4952-a914-dc85ee387e13
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:03:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:14:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:14:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7d567625-2c8f-411a-a845-464154f4d171
01/23/2025 17:14:22:INFO:Received: evaluate message 7d567625-2c8f-411a-a845-464154f4d171

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 2.84

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 2.42

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 1.84

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 2.02

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 2.43

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09853870254009962
Epsilon = 10.00 and Loss = 2.49
[92mINFO [0m:      Sent reply
01/23/2025 17:18:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:18:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:18:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e2de0f9b-627b-4607-a776-bcd10fff7edd
01/23/2025 17:18:58:INFO:Received: train message e2de0f9b-627b-4607-a776-bcd10fff7edd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:27:39:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:38:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:38:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f3d03486-105c-43f1-ba13-d5f47ca18eb9
01/23/2025 17:38:04:INFO:Received: evaluate message f3d03486-105c-43f1-ba13-d5f47ca18eb9
[92mINFO [0m:      Sent reply
01/23/2025 17:41:47:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:42:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:42:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 280e7c0a-f67d-4a7b-a0c8-8bebb4e8a647
01/23/2025 17:42:43:INFO:Received: train message 280e7c0a-f67d-4a7b-a0c8-8bebb4e8a647
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:51:39:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:01:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:01:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 51f0c5b0-27ea-4205-83f4-cc04e42a0dec
01/23/2025 18:01:47:INFO:Received: evaluate message 51f0c5b0-27ea-4205-83f4-cc04e42a0dec
[92mINFO [0m:      Sent reply
01/23/2025 18:05:57:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:06:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:06:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3d7494c4-7c6e-4442-b8c2-144a1f8273d9
01/23/2025 18:06:17:INFO:Received: train message 3d7494c4-7c6e-4442-b8c2-144a1f8273d9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:14:49:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:25:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:25:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4c5ed45b-4e39-4794-8210-f11baeed89eb
01/23/2025 18:25:28:INFO:Received: evaluate message 4c5ed45b-4e39-4794-8210-f11baeed89eb
[92mINFO [0m:      Sent reply
01/23/2025 18:29:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:29:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:29:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2e8c8dc2-f8e7-4d86-86d7-2e46e4c55bea
01/23/2025 18:29:55:INFO:Received: train message 2e8c8dc2-f8e7-4d86-86d7-2e46e4c55bea
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:38:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:48:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:48:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0602ee1f-b0ee-4712-ae67-d4ccf0951251
01/23/2025 18:48:45:INFO:Received: evaluate message 0602ee1f-b0ee-4712-ae67-d4ccf0951251
[92mINFO [0m:      Sent reply
01/23/2025 18:52:33:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:53:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:53:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1de83138-c345-463d-823d-3f7f1b8e1fe1
01/23/2025 18:53:20:INFO:Received: train message 1de83138-c345-463d-823d-3f7f1b8e1fe1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:02:28:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:12:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:12:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b8b6ba31-98e1-4d22-93d9-1460545965fd
01/23/2025 19:12:18:INFO:Received: evaluate message b8b6ba31-98e1-4d22-93d9-1460545965fd

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.09196945570409298
Epsilon = 10.00 and Loss = 1.84

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.08540020886808634
Epsilon = 10.00 and Loss = 1.80

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.0788309620320797
Epsilon = 10.00 and Loss = 2.07

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.07226171519607306
Epsilon = 10.00 and Loss = 2.29

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.06569246836006642
Epsilon = 10.00 and Loss = 2.08
[92mINFO [0m:      Sent reply
01/23/2025 19:16:25:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:16:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:16:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3158a757-fb15-4bae-b5eb-a313ec9974f3
01/23/2025 19:16:59:INFO:Received: train message 3158a757-fb15-4bae-b5eb-a313ec9974f3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:25:44:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:35:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:35:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f2be0530-f450-4546-b256-73994aa4f9cc
01/23/2025 19:35:52:INFO:Received: evaluate message f2be0530-f450-4546-b256-73994aa4f9cc
[92mINFO [0m:      Sent reply
01/23/2025 19:39:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:40:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:40:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4b6b47e3-6f57-4ae7-8415-6a4d9f2ee305
01/23/2025 19:40:36:INFO:Received: train message 4b6b47e3-6f57-4ae7-8415-6a4d9f2ee305
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:49:19:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:59:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:59:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ed86f01d-3c9d-4394-8582-6963120d3788
01/23/2025 19:59:33:INFO:Received: evaluate message ed86f01d-3c9d-4394-8582-6963120d3788
[92mINFO [0m:      Sent reply
01/23/2025 20:03:33:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:04:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:04:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message be00be1a-b99d-4578-b12a-3bcbe4760ff7
01/23/2025 20:04:05:INFO:Received: train message be00be1a-b99d-4578-b12a-3bcbe4760ff7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:13:10:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:23:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:23:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ca64b93a-78cd-4331-8029-226876c5fa21
01/23/2025 20:23:10:INFO:Received: evaluate message ca64b93a-78cd-4331-8029-226876c5fa21
[92mINFO [0m:      Sent reply
01/23/2025 20:27:08:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:27:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:27:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6a3518b1-7630-4b9c-84ab-6fac31bee585
01/23/2025 20:27:41:INFO:Received: train message 6a3518b1-7630-4b9c-84ab-6fac31bee585
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:36:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:46:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:46:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6dd6974a-669a-4cb2-b9e3-0f9cdef3ccf6
01/23/2025 20:46:50:INFO:Received: evaluate message 6dd6974a-669a-4cb2-b9e3-0f9cdef3ccf6

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.05912322152405977
Epsilon = 10.00 and Loss = 2.74

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.05255397468805313
Epsilon = 10.00 and Loss = 1.83

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.04598472785204649
Epsilon = 10.00 and Loss = 2.48

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.03941548101603985
Epsilon = 10.00 and Loss = 1.71
[92mINFO [0m:      Sent reply
01/23/2025 20:50:46:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:51:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:51:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8aded57b-2059-4ece-b5cd-129ec8d03e2d
01/23/2025 20:51:12:INFO:Received: train message 8aded57b-2059-4ece-b5cd-129ec8d03e2d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:59:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:10:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:10:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b50364a5-e09e-4517-bd38-78998f3bcfd2
01/23/2025 21:10:29:INFO:Received: evaluate message b50364a5-e09e-4517-bd38-78998f3bcfd2
[92mINFO [0m:      Sent reply
01/23/2025 21:14:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:15:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:15:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1433656d-a7e0-4329-ae6a-b719566470c0
01/23/2025 21:15:07:INFO:Received: train message 1433656d-a7e0-4329-ae6a-b719566470c0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:23:54:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:34:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:34:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 41f7ae90-adbe-4458-bffb-849d1437d2e1
01/23/2025 21:34:39:INFO:Received: evaluate message 41f7ae90-adbe-4458-bffb-849d1437d2e1
[92mINFO [0m:      Sent reply
01/23/2025 21:38:46:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:39:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:39:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9a04dbf4-7982-4fc1-abbb-f52e4f7c86c6
01/23/2025 21:39:29:INFO:Received: train message 9a04dbf4-7982-4fc1-abbb-f52e4f7c86c6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:49:05:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:05:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:05:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d74dee2e-2e16-4cb7-ad76-47cb6af1823a
01/23/2025 22:05:27:INFO:Received: evaluate message d74dee2e-2e16-4cb7-ad76-47cb6af1823a
[92mINFO [0m:      Sent reply
01/23/2025 22:10:09:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:10:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:10:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 03338b18-58bd-4c29-8c93-717633d1cbfb
01/23/2025 22:10:48:INFO:Received: train message 03338b18-58bd-4c29-8c93-717633d1cbfb

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.03284623418003321
Epsilon = 10.00 and Loss = 2.13

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.026276987344026573
Epsilon = 10.00 and Loss = 1.84

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.01970774050801992
Epsilon = 10.00 and Loss = 1.97

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:21:31:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:37:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:37:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3d011d02-5912-4a0f-b5fa-accb5dfb9e00
01/23/2025 22:37:58:INFO:Received: evaluate message 3d011d02-5912-4a0f-b5fa-accb5dfb9e00
[92mINFO [0m:      Sent reply
01/23/2025 22:42:47:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:43:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:43:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2ed4708c-fc82-4eaf-bd49-a9d2d5fcd223
01/23/2025 22:43:25:INFO:Received: train message 2ed4708c-fc82-4eaf-bd49-a9d2d5fcd223
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:54:43:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:15:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:15:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1896f839-39e4-48e4-8704-debcddb8ac92
01/23/2025 23:15:19:INFO:Received: evaluate message 1896f839-39e4-48e4-8704-debcddb8ac92
[92mINFO [0m:      Sent reply
01/23/2025 23:20:03:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:20:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:20:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fd487420-a369-473a-ada4-fff6d89388b6
01/23/2025 23:20:39:INFO:Received: train message fd487420-a369-473a-ada4-fff6d89388b6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 23:31:18:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:48:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:48:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 02648ce4-efad-4da4-93e3-ee920803dbb7
01/23/2025 23:48:50:INFO:Received: evaluate message 02648ce4-efad-4da4-93e3-ee920803dbb7
[92mINFO [0m:      Sent reply
01/23/2025 23:53:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:53:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:53:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 39bcf9fa-022a-474d-bbb0-3c53c388825c
01/23/2025 23:53:49:INFO:Received: reconnect message 39bcf9fa-022a-474d-bbb0-3c53c388825c
01/23/2025 23:53:49:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/23/2025 23:53:49:INFO:Disconnect and shut down
0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.01313849367201328
Epsilon = 10.00 and Loss = 2.00

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.00656924683600664
Epsilon = 10.00 and Loss = 2.09

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [1.194388747215271, 2.5854246616363525, 0.9732822775840759, 0.4004799723625183, 1.719892978668213, 0.21782158315181732, 0.12028753757476807, 0.6715184450149536]
Noise Multiplier after list and tensor:  0.9853870254009962
Noise Multiplier after Epsilon Scaling:  0.09853870254009962
Noise Multiplier after Convergence: 0.0
Epsilon = 10.00 and Loss = 1.88

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529, 116.09026396274567], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993, 0.5372533225936368], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808, 0.731616827854312]}



Final client history:
{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529, 116.09026396274567], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993, 0.5372533225936368], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808, 0.731616827854312]}

