nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/23/2025 11:21:53:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/23/2025 11:21:53:DEBUG:ChannelConnectivity.IDLE
01/23/2025 11:21:53:DEBUG:ChannelConnectivity.CONNECTING
01/23/2025 11:21:53:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/23/2025 11:24:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:24:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3dd8b3bc-d894-4ac6-a966-1cd58423b9cf
01/23/2025 11:24:46:INFO:Received: train message 3dd8b3bc-d894-4ac6-a966-1cd58423b9cf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:36:33:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:43:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:43:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8a3b2234-ef56-444f-8921-7193e1b06555
01/23/2025 11:43:45:INFO:Received: evaluate message 8a3b2234-ef56-444f-8921-7193e1b06555
[92mINFO [0m:      Sent reply
01/23/2025 11:47:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:48:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:48:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9f8d3085-a1d2-49a0-824a-3910fd3b79f6
01/23/2025 11:48:23:INFO:Received: train message 9f8d3085-a1d2-49a0-824a-3910fd3b79f6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:00:05:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:07:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:07:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0b0f492a-1003-49e9-bb7a-bd9614eaa7f2
01/23/2025 12:07:40:INFO:Received: evaluate message 0b0f492a-1003-49e9-bb7a-bd9614eaa7f2
[92mINFO [0m:      Sent reply
01/23/2025 12:11:38:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:11:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:11:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7530710d-880d-4876-ab84-d79621fa10bb
01/23/2025 12:11:48:INFO:Received: train message 7530710d-880d-4876-ab84-d79621fa10bb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:22:57:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:31:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:31:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cde6c98a-6257-4456-bcf0-edf166a41e46
01/23/2025 12:31:07:INFO:Received: evaluate message cde6c98a-6257-4456-bcf0-edf166a41e46
[92mINFO [0m:      Sent reply
01/23/2025 12:35:12:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:35:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:35:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ee9e5e1-622e-474f-94d6-24bc30a00842
01/23/2025 12:35:43:INFO:Received: train message 8ee9e5e1-622e-474f-94d6-24bc30a00842
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:47:39:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:54:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:54:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ea1b346f-99ea-4ac9-8d12-fa665ca91d7a
01/23/2025 12:54:45:INFO:Received: evaluate message ea1b346f-99ea-4ac9-8d12-fa665ca91d7a
[92mINFO [0m:      Sent reply
01/23/2025 12:58:51:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:59:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:59:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b1525a25-1afb-4d37-834c-a3d2ed695e7e
01/23/2025 12:59:17:INFO:Received: train message b1525a25-1afb-4d37-834c-a3d2ed695e7e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:11:24:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:18:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:18:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 111e6f08-c61c-4c13-8668-a9704fe0fce1
01/23/2025 13:18:44:INFO:Received: evaluate message 111e6f08-c61c-4c13-8668-a9704fe0fce1
[92mINFO [0m:      Sent reply
01/23/2025 13:22:43:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:23:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:23:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e5c44566-9da1-408a-b035-aef271f4e88c
01/23/2025 13:23:05:INFO:Received: train message e5c44566-9da1-408a-b035-aef271f4e88c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:35:16:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:42:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:42:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cdad1aff-a703-4990-8bc1-9fe7a776a3b3
01/23/2025 13:42:03:INFO:Received: evaluate message cdad1aff-a703-4990-8bc1-9fe7a776a3b3
[92mINFO [0m:      Sent reply
01/23/2025 13:46:01:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:46:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:46:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 72199444-8416-4d5b-8827-104ce1712fcc
01/23/2025 13:46:24:INFO:Received: train message 72199444-8416-4d5b-8827-104ce1712fcc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:57:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:05:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:05:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message be07bec3-659b-4ce0-99aa-ef9b88cea2b5
01/23/2025 14:05:45:INFO:Received: evaluate message be07bec3-659b-4ce0-99aa-ef9b88cea2b5
[92mINFO [0m:      Sent reply
01/23/2025 14:09:42:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:10:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:10:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a7cc578c-7605-45eb-a07c-e31f0d28ee6a
01/23/2025 14:10:01:INFO:Received: train message a7cc578c-7605-45eb-a07c-e31f0d28ee6a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:22:08:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:29:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:29:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f06f52b1-b81b-4b54-a63b-3dafa003f85c
01/23/2025 14:29:19:INFO:Received: evaluate message f06f52b1-b81b-4b54-a63b-3dafa003f85c
[92mINFO [0m:      Sent reply
01/23/2025 14:33:16:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:33:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:33:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f1e7e789-51b8-45c3-bbc4-7557765cc51c
01/23/2025 14:33:58:INFO:Received: train message f1e7e789-51b8-45c3-bbc4-7557765cc51c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:45:53:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:53:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:53:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 012ca0ff-70ff-4357-9ddb-b7ce59223800
01/23/2025 14:53:10:INFO:Received: evaluate message 012ca0ff-70ff-4357-9ddb-b7ce59223800
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 10, target_epsilon: 10, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 1.75

{'loss': [137.23898267745972], 'accuracy': [0.3383004430124849], 'auc': [0.5877396505618926]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 1.47

{'loss': [137.23898267745972, 134.08246445655823], 'accuracy': [0.3383004430124849, 0.3407168747482884], 'auc': [0.5877396505618926, 0.620088227094536]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 1.51

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 1.94

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 1.73

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 1.50

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 0.97

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 1.96

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 1.78
[92mINFO [0m:      Sent reply
01/23/2025 14:57:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:57:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:57:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 771a4852-8d9e-4f02-be4c-0e17f38bce78
01/23/2025 14:57:38:INFO:Received: train message 771a4852-8d9e-4f02-be4c-0e17f38bce78
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:09:43:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:16:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:16:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 97318cff-1ab7-4c2b-8f8b-62645e7eb013
01/23/2025 15:16:38:INFO:Received: evaluate message 97318cff-1ab7-4c2b-8f8b-62645e7eb013
[92mINFO [0m:      Sent reply
01/23/2025 15:20:43:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:21:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:21:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 62e980d2-afae-4714-8560-777030559dcc
01/23/2025 15:21:17:INFO:Received: train message 62e980d2-afae-4714-8560-777030559dcc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:33:15:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:40:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:40:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f534ffc9-4dbe-4af1-9a77-c3b1ae0d5e50
01/23/2025 15:40:17:INFO:Received: evaluate message f534ffc9-4dbe-4af1-9a77-c3b1ae0d5e50
[92mINFO [0m:      Sent reply
01/23/2025 15:44:08:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:44:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:44:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 928deb2d-912c-40e3-8250-f2611e8d19f2
01/23/2025 15:44:59:INFO:Received: train message 928deb2d-912c-40e3-8250-f2611e8d19f2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:56:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:03:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:03:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 532a6ee3-0da5-4f47-bebe-dba4a46101dc
01/23/2025 16:03:52:INFO:Received: evaluate message 532a6ee3-0da5-4f47-bebe-dba4a46101dc
[92mINFO [0m:      Sent reply
01/23/2025 16:07:51:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:08:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:08:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1889336a-1adb-4828-9f44-d5f1d7c79ca8
01/23/2025 16:08:20:INFO:Received: train message 1889336a-1adb-4828-9f44-d5f1d7c79ca8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:19:55:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:27:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:27:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e96b7d15-e29a-4044-8114-96a7e2b4dd03
01/23/2025 16:27:18:INFO:Received: evaluate message e96b7d15-e29a-4044-8114-96a7e2b4dd03
[92mINFO [0m:      Sent reply
01/23/2025 16:31:21:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:31:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:31:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e8d8df6c-f8ed-4f44-982c-89bae2d8664d
01/23/2025 16:31:33:INFO:Received: train message e8d8df6c-f8ed-4f44-982c-89bae2d8664d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:43:34:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:51:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:51:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6128adb5-de69-4a81-a952-0f9f3e1b5937
01/23/2025 16:51:05:INFO:Received: evaluate message 6128adb5-de69-4a81-a952-0f9f3e1b5937
[92mINFO [0m:      Sent reply
01/23/2025 16:55:00:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:55:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:55:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 568258c8-d4fc-4476-8067-b63551ffb73e
01/23/2025 16:55:29:INFO:Received: train message 568258c8-d4fc-4476-8067-b63551ffb73e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:07:12:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:14:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:14:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d3f84011-c1fd-48de-9d17-8a5f934f3f3f
01/23/2025 17:14:34:INFO:Received: evaluate message d3f84011-c1fd-48de-9d17-8a5f934f3f3f

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 1.52

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 2.01

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 2.07

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 2.00

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 1.57

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.06546779628843069
Epsilon = 10.00 and Loss = 2.02
[92mINFO [0m:      Sent reply
01/23/2025 17:18:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:19:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:19:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 90d449d2-32bf-46b0-8ebf-1567a1155f54
01/23/2025 17:19:01:INFO:Received: train message 90d449d2-32bf-46b0-8ebf-1567a1155f54
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:30:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:38:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:38:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 950008ed-69b8-415b-80ca-254239ae272a
01/23/2025 17:38:19:INFO:Received: evaluate message 950008ed-69b8-415b-80ca-254239ae272a
[92mINFO [0m:      Sent reply
01/23/2025 17:42:15:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:42:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:42:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0d14708b-8e3d-495d-b287-2713a84e8d6b
01/23/2025 17:42:26:INFO:Received: train message 0d14708b-8e3d-495d-b287-2713a84e8d6b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:54:01:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:01:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:01:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bdffeca7-ee5e-49ec-a7c5-b45f025be276
01/23/2025 18:01:43:INFO:Received: evaluate message bdffeca7-ee5e-49ec-a7c5-b45f025be276
[92mINFO [0m:      Sent reply
01/23/2025 18:05:46:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:06:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:06:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a78c638e-0000-4b10-bc31-0768cedf232f
01/23/2025 18:06:26:INFO:Received: train message a78c638e-0000-4b10-bc31-0768cedf232f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:17:56:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:25:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:25:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3d792fda-1d42-491a-84ac-0d39f99d130d
01/23/2025 18:25:26:INFO:Received: evaluate message 3d792fda-1d42-491a-84ac-0d39f99d130d
[92mINFO [0m:      Sent reply
01/23/2025 18:29:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:29:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:29:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7471885b-876b-439b-aa26-33270a3ea8dc
01/23/2025 18:29:55:INFO:Received: train message 7471885b-876b-439b-aa26-33270a3ea8dc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:41:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:48:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:48:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 620e1179-613d-4b6f-b807-2c7f05f86dea
01/23/2025 18:48:55:INFO:Received: evaluate message 620e1179-613d-4b6f-b807-2c7f05f86dea
[92mINFO [0m:      Sent reply
01/23/2025 18:52:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:53:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:53:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fccf3539-a92d-4017-ad85-653537fddfa3
01/23/2025 18:53:23:INFO:Received: train message fccf3539-a92d-4017-ad85-653537fddfa3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:05:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:12:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:12:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 269f0fe1-0de3-47ae-a744-aea320f785f3
01/23/2025 19:12:17:INFO:Received: evaluate message 269f0fe1-0de3-47ae-a744-aea320f785f3

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.061103276535868646
Epsilon = 10.00 and Loss = 1.53

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.0567387567833066
Epsilon = 10.00 and Loss = 1.82

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.05237423703074456
Epsilon = 10.00 and Loss = 1.53

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.04800971727818251
Epsilon = 10.00 and Loss = 1.40

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.04364519752562047
Epsilon = 10.00 and Loss = 1.62
[92mINFO [0m:      Sent reply
01/23/2025 19:16:21:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:17:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:17:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ac1c1549-bd69-4dee-a9be-20a98392090e
01/23/2025 19:17:01:INFO:Received: train message ac1c1549-bd69-4dee-a9be-20a98392090e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:28:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:35:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:35:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ee3c9048-a0b6-4fed-9011-2ae6e831d99b
01/23/2025 19:35:58:INFO:Received: evaluate message ee3c9048-a0b6-4fed-9011-2ae6e831d99b
[92mINFO [0m:      Sent reply
01/23/2025 19:40:04:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:40:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:40:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4c66736f-a166-4863-b11d-0f05cce95eec
01/23/2025 19:40:36:INFO:Received: train message 4c66736f-a166-4863-b11d-0f05cce95eec
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:52:09:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:59:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:59:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5c8e444b-7d34-4251-9c21-902048f21555
01/23/2025 19:59:24:INFO:Received: evaluate message 5c8e444b-7d34-4251-9c21-902048f21555
[92mINFO [0m:      Sent reply
01/23/2025 20:03:21:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:03:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:03:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8e75e0e4-7ec6-43fc-a82b-358ba97b5794
01/23/2025 20:03:53:INFO:Received: train message 8e75e0e4-7ec6-43fc-a82b-358ba97b5794
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:15:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:23:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:23:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 85de7a74-1db9-4357-b958-97c39d98ff7f
01/23/2025 20:23:14:INFO:Received: evaluate message 85de7a74-1db9-4357-b958-97c39d98ff7f
[92mINFO [0m:      Sent reply
01/23/2025 20:27:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:27:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:27:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 781ef3a2-2001-4549-aca2-baacd5789764
01/23/2025 20:27:43:INFO:Received: train message 781ef3a2-2001-4549-aca2-baacd5789764
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:39:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:46:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:46:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ec080bfe-6d64-4965-bafb-954821ae0e60
01/23/2025 20:46:32:INFO:Received: evaluate message ec080bfe-6d64-4965-bafb-954821ae0e60

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.039280677773058416
Epsilon = 10.00 and Loss = 1.80

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.03491615802049637
Epsilon = 10.00 and Loss = 1.53

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.030551638267934323
Epsilon = 10.00 and Loss = 1.74

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.02618711851537228
Epsilon = 10.00 and Loss = 1.81
[92mINFO [0m:      Sent reply
01/23/2025 20:50:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:51:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:51:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4254fcf6-2529-4946-ad57-092e7a61d2d4
01/23/2025 20:51:16:INFO:Received: train message 4254fcf6-2529-4946-ad57-092e7a61d2d4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:02:50:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:10:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:10:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a2abd310-909f-4159-9de0-20af18cb2451
01/23/2025 21:10:26:INFO:Received: evaluate message a2abd310-909f-4159-9de0-20af18cb2451
[92mINFO [0m:      Sent reply
01/23/2025 21:14:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:15:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:15:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3c965aa5-ae88-434e-9510-1cab75276f0a
01/23/2025 21:15:11:INFO:Received: train message 3c965aa5-ae88-434e-9510-1cab75276f0a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:26:56:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:34:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:34:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9d19886c-b6c7-4195-abfe-bd7a6f425e1c
01/23/2025 21:34:37:INFO:Received: evaluate message 9d19886c-b6c7-4195-abfe-bd7a6f425e1c
[92mINFO [0m:      Sent reply
01/23/2025 21:38:52:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:39:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:39:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c37fb812-24b1-4d36-9ce2-d5762bac73b7
01/23/2025 21:39:24:INFO:Received: train message c37fb812-24b1-4d36-9ce2-d5762bac73b7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:52:42:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:05:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:05:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3f3d426b-9956-468d-a0d1-50b571599afb
01/23/2025 22:05:12:INFO:Received: evaluate message 3f3d426b-9956-468d-a0d1-50b571599afb
[92mINFO [0m:      Sent reply
01/23/2025 22:10:07:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:10:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:10:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 714b9560-2afc-401f-89e1-e5833fcc7e55
01/23/2025 22:10:38:INFO:Received: train message 714b9560-2afc-401f-89e1-e5833fcc7e55

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.021822598762810234
Epsilon = 10.00 and Loss = 2.14

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.01745807901024819
Epsilon = 10.00 and Loss = 1.80

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.013093559257686136
Epsilon = 10.00 and Loss = 1.30

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor: /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:25:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:37:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:37:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0d1b23c3-cabe-4194-bc8f-c1134fb6957c
01/23/2025 22:37:50:INFO:Received: evaluate message 0d1b23c3-cabe-4194-bc8f-c1134fb6957c
[92mINFO [0m:      Sent reply
01/23/2025 22:42:48:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:43:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:43:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e3c3fd3e-8e7c-4f0f-bc3d-fa36d4e65501
01/23/2025 22:43:25:INFO:Received: train message e3c3fd3e-8e7c-4f0f-bc3d-fa36d4e65501
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:59:15:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:15:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:15:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0b602ba6-bd3b-4c38-a4c6-41c9fc69ad12
01/23/2025 23:15:04:INFO:Received: evaluate message 0b602ba6-bd3b-4c38-a4c6-41c9fc69ad12
[92mINFO [0m:      Sent reply
01/23/2025 23:19:33:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:20:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:20:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3dd45ad9-f911-420b-8006-73422fe2e763
01/23/2025 23:20:45:INFO:Received: train message 3dd45ad9-f911-420b-8006-73422fe2e763
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 23:35:04:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:48:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:48:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0e8d184c-7eab-4fcf-a0ba-a42081d8b750
01/23/2025 23:48:47:INFO:Received: evaluate message 0e8d184c-7eab-4fcf-a0ba-a42081d8b750
[92mINFO [0m:      Sent reply
01/23/2025 23:53:41:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:53:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:53:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 1431b5a4-ba48-4ffc-be81-c37d23b31901
01/23/2025 23:53:49:INFO:Received: reconnect message 1431b5a4-ba48-4ffc-be81-c37d23b31901
01/23/2025 23:53:49:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/23/2025 23:53:49:INFO:Disconnect and shut down
 0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.00872903950512409
Epsilon = 10.00 and Loss = 1.22

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.004364519752562045
Epsilon = 10.00 and Loss = 1.26

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [1.0048576593399048, 2.466346263885498, 0.3998086452484131, 0.13892436027526855, 0.7649617791175842, 0.1282823532819748, 0.1611902266740799, 0.17305241525173187]
Noise Multiplier after list and tensor:  0.6546779628843069
Noise Multiplier after Epsilon Scaling:  0.06546779628843069
Noise Multiplier after Convergence: 0.0
Epsilon = 10.00 and Loss = 1.64

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529, 116.09026396274567], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993, 0.5372533225936368], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808, 0.731616827854312]}



Final client history:
{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529, 116.09026396274567], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993, 0.5372533225936368], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808, 0.731616827854312]}

