nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/23/2025 11:18:58:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/23/2025 11:18:58:DEBUG:ChannelConnectivity.IDLE
01/23/2025 11:18:58:DEBUG:ChannelConnectivity.CONNECTING
01/23/2025 11:18:58:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/23/2025 11:18:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:18:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message 322f06e4-e807-44d4-a2f9-0adc3048a87b
01/23/2025 11:18:58:INFO:Received: get_parameters message 322f06e4-e807-44d4-a2f9-0adc3048a87b
[92mINFO [0m:      Sent reply
01/23/2025 11:19:01:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:24:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:24:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f0061813-988a-4fbd-8604-1ea306ca57f9
01/23/2025 11:24:27:INFO:Received: train message f0061813-988a-4fbd-8604-1ea306ca57f9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:25:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:43:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:43:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e77a28f2-a904-4145-a33a-583f41beaa00
01/23/2025 11:43:51:INFO:Received: evaluate message e77a28f2-a904-4145-a33a-583f41beaa00
[92mINFO [0m:      Sent reply
01/23/2025 11:47:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:48:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:48:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0d05acfd-bb79-4772-878b-57c34cdeb84d
01/23/2025 11:48:27:INFO:Received: train message 0d05acfd-bb79-4772-878b-57c34cdeb84d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:50:54:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:07:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:07:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d2b495a7-87ef-490f-915d-d34ae8134a4b
01/23/2025 12:07:34:INFO:Received: evaluate message d2b495a7-87ef-490f-915d-d34ae8134a4b
[92mINFO [0m:      Sent reply
01/23/2025 12:11:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:12:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:12:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a957e049-986d-4780-a81a-03799e9286f2
01/23/2025 12:12:00:INFO:Received: train message a957e049-986d-4780-a81a-03799e9286f2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:14:21:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:31:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:31:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 76577565-d9f4-4f60-99ee-319495f3cdb2
01/23/2025 12:31:08:INFO:Received: evaluate message 76577565-d9f4-4f60-99ee-319495f3cdb2
[92mINFO [0m:      Sent reply
01/23/2025 12:35:14:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:35:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:35:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 65f553d3-b5dd-48da-821c-e7471c297d96
01/23/2025 12:35:40:INFO:Received: train message 65f553d3-b5dd-48da-821c-e7471c297d96
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:38:06:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:54:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:54:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aca9eb6a-9361-4b9a-8a55-e3b8de29a82d
01/23/2025 12:54:42:INFO:Received: evaluate message aca9eb6a-9361-4b9a-8a55-e3b8de29a82d
[92mINFO [0m:      Sent reply
01/23/2025 12:58:38:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:59:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:59:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fb021e8a-0a19-447e-8cd3-b97ee96b1620
01/23/2025 12:59:22:INFO:Received: train message fb021e8a-0a19-447e-8cd3-b97ee96b1620
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:01:50:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:18:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:18:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 275b7751-db22-4b05-b35b-e5ce075c85f7
01/23/2025 13:18:44:INFO:Received: evaluate message 275b7751-db22-4b05-b35b-e5ce075c85f7
[92mINFO [0m:      Sent reply
01/23/2025 13:22:43:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:23:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:23:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8adf0768-b284-44aa-a657-04cd970ba211
01/23/2025 13:23:05:INFO:Received: train message 8adf0768-b284-44aa-a657-04cd970ba211
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:25:29:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:42:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:42:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5010250a-198e-4448-a2dc-701af161948d
01/23/2025 13:42:00:INFO:Received: evaluate message 5010250a-198e-4448-a2dc-701af161948d
[92mINFO [0m:      Sent reply
01/23/2025 13:45:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:46:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:46:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 584e8484-00dc-4578-958e-3bff3098695c
01/23/2025 13:46:36:INFO:Received: train message 584e8484-00dc-4578-958e-3bff3098695c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:48:57:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:05:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:05:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 03c88a60-3386-491b-ad73-e5f3a492d9ca
01/23/2025 14:05:47:INFO:Received: evaluate message 03c88a60-3386-491b-ad73-e5f3a492d9ca
[92mINFO [0m:      Sent reply
01/23/2025 14:09:41:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:10:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:10:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e3743ed6-3822-4543-971e-a23a70003678
01/23/2025 14:10:12:INFO:Received: train message e3743ed6-3822-4543-971e-a23a70003678
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:12:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:29:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:29:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 777c75a4-397e-47ed-ace3-065fc137d007
01/23/2025 14:29:26:INFO:Received: evaluate message 777c75a4-397e-47ed-ace3-065fc137d007
[92mINFO [0m:      Sent reply
01/23/2025 14:33:29:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:33:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:33:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 95f48a7a-9e1a-4b75-acf2-26aae85aab38
01/23/2025 14:33:49:INFO:Received: train message 95f48a7a-9e1a-4b75-acf2-26aae85aab38
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:36:06:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:53:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:53:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9bf67136-59c2-4f20-afd4-c9a953aa3844
01/23/2025 14:53:10:INFO:Received: evaluate message 9bf67136-59c2-4f20-afd4-c9a953aa3844
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 10, target_epsilon: 10, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 2.08

{'loss': [137.23898267745972], 'accuracy': [0.3383004430124849], 'auc': [0.5877396505618926]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 1.10

{'loss': [137.23898267745972, 134.08246445655823], 'accuracy': [0.3383004430124849, 0.3407168747482884], 'auc': [0.5877396505618926, 0.620088227094536]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 0.50

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 0.82

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 0.56

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 0.74

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 0.45

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 0.88

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 0.91
[92mINFO [0m:      Sent reply
01/23/2025 14:57:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:57:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:57:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9bbc4a60-9dbe-48c9-979a-d8843a5f8aca
01/23/2025 14:57:37:INFO:Received: train message 9bbc4a60-9dbe-48c9-979a-d8843a5f8aca
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:00:05:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:16:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:16:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 677df5cf-8f62-4ece-b2ca-91d9ab9abcf6
01/23/2025 15:16:35:INFO:Received: evaluate message 677df5cf-8f62-4ece-b2ca-91d9ab9abcf6
[92mINFO [0m:      Sent reply
01/23/2025 15:20:23:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:21:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:21:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b34f4997-6cdf-4134-8c50-15ab7c1705e0
01/23/2025 15:21:12:INFO:Received: train message b34f4997-6cdf-4134-8c50-15ab7c1705e0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:23:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:40:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:40:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f38d4108-f082-45db-8dd9-ab243e029640
01/23/2025 15:40:15:INFO:Received: evaluate message f38d4108-f082-45db-8dd9-ab243e029640
[92mINFO [0m:      Sent reply
01/23/2025 15:44:00:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:44:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:44:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8800a5cf-d85f-4e14-b55e-0a4a8ba20ff0
01/23/2025 15:44:35:INFO:Received: train message 8800a5cf-d85f-4e14-b55e-0a4a8ba20ff0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:45:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:03:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:03:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d9cc6c2a-cbf6-4a21-a56f-e437793870b2
01/23/2025 16:03:52:INFO:Received: evaluate message d9cc6c2a-cbf6-4a21-a56f-e437793870b2
[92mINFO [0m:      Sent reply
01/23/2025 16:07:51:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:08:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:08:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 14dbf6c3-18fe-4261-88e5-f37572b574b3
01/23/2025 16:08:14:INFO:Received: train message 14dbf6c3-18fe-4261-88e5-f37572b574b3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:10:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:27:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:27:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0f519048-6752-4680-8235-4a822fab5845
01/23/2025 16:27:15:INFO:Received: evaluate message 0f519048-6752-4680-8235-4a822fab5845
[92mINFO [0m:      Sent reply
01/23/2025 16:31:20:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:31:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:31:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ee3d8c7f-c0bd-423c-9929-204fc84a08ee
01/23/2025 16:31:50:INFO:Received: train message ee3d8c7f-c0bd-423c-9929-204fc84a08ee
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:34:22:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:51:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:51:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 24311baa-1218-49b8-98f2-705fbd0a1e52
01/23/2025 16:51:01:INFO:Received: evaluate message 24311baa-1218-49b8-98f2-705fbd0a1e52
[92mINFO [0m:      Sent reply
01/23/2025 16:54:56:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:55:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:55:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f4272ada-509c-4b21-bde0-8db2b1ce94cb
01/23/2025 16:55:25:INFO:Received: train message f4272ada-509c-4b21-bde0-8db2b1ce94cb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:57:48:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:14:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:14:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d72de13c-4dd2-4021-9e6d-e19a924b198d
01/23/2025 17:14:22:INFO:Received: evaluate message d72de13c-4dd2-4021-9e6d-e19a924b198d

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 0.34

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 0.73

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 0.82

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 0.89

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 0.85

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.4008262321352959
Epsilon = 10.00 and Loss = 0.49
[92mINFO [0m:      Sent reply
01/23/2025 17:18:15:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:19:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:19:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1e807477-ffe3-41f6-9512-8b29ea661cd2
01/23/2025 17:19:03:INFO:Received: train message 1e807477-ffe3-41f6-9512-8b29ea661cd2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:21:28:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:38:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:38:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a9b26296-1c0d-4b60-a351-e5925b0e277b
01/23/2025 17:38:20:INFO:Received: evaluate message a9b26296-1c0d-4b60-a351-e5925b0e277b
[92mINFO [0m:      Sent reply
01/23/2025 17:42:14:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:42:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:42:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 199b5c4c-6b24-47ad-906e-43eb86d5eb27
01/23/2025 17:42:30:INFO:Received: train message 199b5c4c-6b24-47ad-906e-43eb86d5eb27
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:44:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:01:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:01:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9217709c-1d02-4bbd-b7f2-8895fc6364f7
01/23/2025 18:01:45:INFO:Received: evaluate message 9217709c-1d02-4bbd-b7f2-8895fc6364f7
[92mINFO [0m:      Sent reply
01/23/2025 18:05:54:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:06:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:06:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cffed2f6-3a3e-46a3-b7dc-2d7cb30bdda2
01/23/2025 18:06:10:INFO:Received: train message cffed2f6-3a3e-46a3-b7dc-2d7cb30bdda2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:07:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:25:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:25:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fd3e30a0-8577-4d6a-9a77-0350e85a8251
01/23/2025 18:25:22:INFO:Received: evaluate message fd3e30a0-8577-4d6a-9a77-0350e85a8251
[92mINFO [0m:      Sent reply
01/23/2025 18:29:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:29:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:29:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message aa6ed1b0-f3f9-42a3-9cb8-19d3fe3da449
01/23/2025 18:29:51:INFO:Received: train message aa6ed1b0-f3f9-42a3-9cb8-19d3fe3da449
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:32:15:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:48:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:48:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 230cca26-4c47-4679-b468-710963710f42
01/23/2025 18:48:54:INFO:Received: evaluate message 230cca26-4c47-4679-b468-710963710f42
[92mINFO [0m:      Sent reply
01/23/2025 18:52:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:53:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:53:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9c05585f-f378-4d7f-912e-7b31109e5821
01/23/2025 18:53:27:INFO:Received: train message 9c05585f-f378-4d7f-912e-7b31109e5821
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:55:57:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:12:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:12:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ba953ed6-04ee-4ff6-87cf-ac4ca8c260db
01/23/2025 19:12:27:INFO:Received: evaluate message ba953ed6-04ee-4ff6-87cf-ac4ca8c260db

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.37410448332627616
Epsilon = 10.00 and Loss = 1.13

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.34738273451725643
Epsilon = 10.00 and Loss = 0.69

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.32066098570823676
Epsilon = 10.00 and Loss = 0.51

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.293939236899217
Epsilon = 10.00 and Loss = 0.70

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.2672174880901973
Epsilon = 10.00 and Loss = 0.51
[92mINFO [0m:      Sent reply
01/23/2025 19:16:32:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:16:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:16:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9aad6ffe-2954-457c-a05a-52ebef2bed8a
01/23/2025 19:16:49:INFO:Received: train message 9aad6ffe-2954-457c-a05a-52ebef2bed8a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:19:02:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:35:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:35:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b880d420-0cd3-40f7-8f39-95bc4cc9f42f
01/23/2025 19:35:58:INFO:Received: evaluate message b880d420-0cd3-40f7-8f39-95bc4cc9f42f
[92mINFO [0m:      Sent reply
01/23/2025 19:40:02:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:40:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:40:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0c24d0f5-9469-4b67-8b63-4dda1d79dcec
01/23/2025 19:40:15:INFO:Received: train message 0c24d0f5-9469-4b67-8b63-4dda1d79dcec
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:41:14:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:59:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:59:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fefd15cf-b4f7-470e-ab9a-804b91eaf91c
01/23/2025 19:59:31:INFO:Received: evaluate message fefd15cf-b4f7-470e-ab9a-804b91eaf91c
[92mINFO [0m:      Sent reply
01/23/2025 20:03:32:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:03:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:03:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b7e6b9fb-a70a-4015-be77-9f8940c7fe8c
01/23/2025 20:03:59:INFO:Received: train message b7e6b9fb-a70a-4015-be77-9f8940c7fe8c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:06:25:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:23:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:23:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 524154f8-59ad-44ef-9b1c-d67e2c880990
01/23/2025 20:23:17:INFO:Received: evaluate message 524154f8-59ad-44ef-9b1c-d67e2c880990
[92mINFO [0m:      Sent reply
01/23/2025 20:27:12:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:27:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:27:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 472c6e5f-95cc-44fb-9960-b3477a85d9a8
01/23/2025 20:27:36:INFO:Received: train message 472c6e5f-95cc-44fb-9960-b3477a85d9a8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:30:03:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:46:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:46:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ee22b2f5-0324-403b-bc36-740968b549a3
01/23/2025 20:46:47:INFO:Received: evaluate message ee22b2f5-0324-403b-bc36-740968b549a3

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.2404957392811775
Epsilon = 10.00 and Loss = 0.35

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.2137739904721578
Epsilon = 10.00 and Loss = 0.77

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.18705224166313808
Epsilon = 10.00 and Loss = 0.72

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.16033049285411838
Epsilon = 10.00 and Loss = 0.65
[92mINFO [0m:      Sent reply
01/23/2025 20:50:44:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:50:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:50:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5ebd88a6-64a9-4b54-979d-854953b79b9e
01/23/2025 20:50:55:INFO:Received: train message 5ebd88a6-64a9-4b54-979d-854953b79b9e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:51:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:10:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:10:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d917056f-e41c-45e9-a7a9-8de880f7f30f
01/23/2025 21:10:26:INFO:Received: evaluate message d917056f-e41c-45e9-a7a9-8de880f7f30f
[92mINFO [0m:      Sent reply
01/23/2025 21:14:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:15:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:15:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7c6638ab-5a25-4b02-bad3-f7d19c0ec660
01/23/2025 21:15:05:INFO:Received: train message 7c6638ab-5a25-4b02-bad3-f7d19c0ec660
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:17:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:34:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:34:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5a6d76cb-bdee-4d29-ac1a-66cffffaf316
01/23/2025 21:34:40:INFO:Received: evaluate message 5a6d76cb-bdee-4d29-ac1a-66cffffaf316
[92mINFO [0m:      Sent reply
01/23/2025 21:38:57:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:39:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:39:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 828bb1e3-16e4-4de3-bfd5-ed413a23c634
01/23/2025 21:39:30:INFO:Received: train message 828bb1e3-16e4-4de3-bfd5-ed413a23c634
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:42:08:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:05:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:05:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 37b17343-4688-4013-b78b-6bf8e74e21d8
01/23/2025 22:05:06:INFO:Received: evaluate message 37b17343-4688-4013-b78b-6bf8e74e21d8
[92mINFO [0m:      Sent reply
01/23/2025 22:09:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:10:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:10:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ce179be8-bc33-49af-9b26-0f1c18367979
01/23/2025 22:10:34:INFO:Received: train message ce179be8-bc33-49af-9b26-0f1c18367979

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.13360874404509865
Epsilon = 10.00 and Loss = 0.59

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.10688699523607892
Epsilon = 10.00 and Loss = 0.45

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.08016524642705916
Epsilon = 10.00 and Loss = 0.34

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:13:25:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:37:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:37:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4d180514-cd8c-4027-8bbd-3392c299384d
01/23/2025 22:37:40:INFO:Received: evaluate message 4d180514-cd8c-4027-8bbd-3392c299384d
[92mINFO [0m:      Sent reply
01/23/2025 22:42:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:43:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:43:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 38185d9f-546d-43b6-9c51-89ecd45d22c3
01/23/2025 22:43:25:INFO:Received: train message 38185d9f-546d-43b6-9c51-89ecd45d22c3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:46:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:15:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:15:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ce9d9441-7f87-42e3-991c-e139747fdb70
01/23/2025 23:15:24:INFO:Received: evaluate message ce9d9441-7f87-42e3-991c-e139747fdb70
[92mINFO [0m:      Sent reply
01/23/2025 23:20:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:20:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:20:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bf6e0020-fb64-410d-a045-e9e8d20b2bad
01/23/2025 23:20:44:INFO:Received: train message bf6e0020-fb64-410d-a045-e9e8d20b2bad
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 23:23:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:48:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:48:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 19cc880d-1fd7-41e5-9a8a-fec74237368c
01/23/2025 23:48:56:INFO:Received: evaluate message 19cc880d-1fd7-41e5-9a8a-fec74237368c
[92mINFO [0m:      Sent reply
01/23/2025 23:53:48:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:53:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:53:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message a3afc226-f40f-491f-a076-72eed84d8442
01/23/2025 23:53:49:INFO:Received: reconnect message a3afc226-f40f-491f-a076-72eed84d8442
01/23/2025 23:53:49:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/23/2025 23:53:49:INFO:Disconnect and shut down
4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.05344349761803944
Epsilon = 10.00 and Loss = 0.61

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.02672174880901972
Epsilon = 10.00 and Loss = 0.77

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808]}

Base Noise Multiplier Received:  0.495758056640625
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [5.061040878295898, 21.359228134155273, 0.8561860918998718, 1.3016425371170044, 0.786310076713562, 1.7844135761260986, 0.5180856585502625, 0.39919161796569824]
Noise Multiplier after list and tensor:  4.008262321352959
Noise Multiplier after Epsilon Scaling:  0.4008262321352959
Noise Multiplier after Convergence: 0.0
Epsilon = 10.00 and Loss = 0.79

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529, 116.09026396274567], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993, 0.5372533225936368], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808, 0.731616827854312]}



Final client history:
{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529, 116.09026396274567], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993, 0.5372533225936368], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808, 0.731616827854312]}

