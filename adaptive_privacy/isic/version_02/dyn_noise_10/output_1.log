nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.1 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/27/2025 11:32:26:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 11:32:26:DEBUG:ChannelConnectivity.IDLE
01/27/2025 11:32:26:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 11:32:26:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 11:33:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:33:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fb9eee8b-9b73-4985-8623-426c6c58ca17
01/27/2025 11:33:00:INFO:Received: train message fb9eee8b-9b73-4985-8623-426c6c58ca17
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 12:06:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:07:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:07:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b7e66ac7-0e24-4f96-86eb-13f03ac8260c
01/27/2025 12:07:37:INFO:Received: evaluate message b7e66ac7-0e24-4f96-86eb-13f03ac8260c
[92mINFO [0m:      Sent reply
01/27/2025 12:12:46:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:13:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:13:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 77e9b24f-1a1b-4bcc-b677-c34427533be9
01/27/2025 12:13:13:INFO:Received: train message 77e9b24f-1a1b-4bcc-b677-c34427533be9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 12:48:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:49:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:49:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f4094679-fead-4b26-b788-2a5743e8249a
01/27/2025 12:49:20:INFO:Received: evaluate message f4094679-fead-4b26-b788-2a5743e8249a
[92mINFO [0m:      Sent reply
01/27/2025 12:54:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:55:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:55:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 940608af-cdaa-4e34-9a94-945556a7ba3b
01/27/2025 12:55:15:INFO:Received: train message 940608af-cdaa-4e34-9a94-945556a7ba3b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 13:31:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:32:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:32:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d32cdd0f-0a97-4115-af00-a4af1a3fe6cf
01/27/2025 13:32:01:INFO:Received: evaluate message d32cdd0f-0a97-4115-af00-a4af1a3fe6cf
[92mINFO [0m:      Sent reply
01/27/2025 13:36:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:37:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:37:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a458798a-ee20-4182-89e5-9494dd70af54
01/27/2025 13:37:50:INFO:Received: train message a458798a-ee20-4182-89e5-9494dd70af54
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 14:11:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:12:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:12:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4998b4ea-ab29-4468-9e75-dce95d37b47e
01/27/2025 14:12:29:INFO:Received: evaluate message 4998b4ea-ab29-4468-9e75-dce95d37b47e
[92mINFO [0m:      Sent reply
01/27/2025 14:17:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:18:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:18:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message af7e1b6c-60f7-4ba1-a4af-def439f1bd61
01/27/2025 14:18:04:INFO:Received: train message af7e1b6c-60f7-4ba1-a4af-def439f1bd61
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 14:52:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:53:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:53:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 020d1bc7-17df-48cc-9d17-4897569e982e
01/27/2025 14:53:03:INFO:Received: evaluate message 020d1bc7-17df-48cc-9d17-4897569e982e
[92mINFO [0m:      Sent reply
01/27/2025 14:57:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:58:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:58:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 435b7147-77ac-4750-926d-d7904cb6ebdf
01/27/2025 14:58:11:INFO:Received: train message 435b7147-77ac-4750-926d-d7904cb6ebdf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 15:34:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 15:35:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 15:35:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3be158c2-18d2-4db8-8111-96bfbc32c773
01/27/2025 15:35:27:INFO:Received: evaluate message 3be158c2-18d2-4db8-8111-96bfbc32c773
[92mINFO [0m:      Sent reply
01/27/2025 15:40:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 15:40:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 15:40:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message abeb1242-97ba-4a34-aab2-57f44fc04394
01/27/2025 15:40:58:INFO:Received: train message abeb1242-97ba-4a34-aab2-57f44fc04394
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 16:14:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:15:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:15:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eeb98ad9-45e8-4f74-b2b0-cedf2dace465
01/27/2025 16:15:08:INFO:Received: evaluate message eeb98ad9-45e8-4f74-b2b0-cedf2dace465
[92mINFO [0m:      Sent reply
01/27/2025 16:19:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:20:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:20:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 69364b58-b242-4558-be29-df233cd2c6f7
01/27/2025 16:20:29:INFO:Received: train message 69364b58-b242-4558-be29-df233cd2c6f7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 16:54:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:54:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:54:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2526af90-589d-49d4-9847-6c0d5cdd64e5
01/27/2025 16:54:58:INFO:Received: evaluate message 2526af90-589d-49d4-9847-6c0d5cdd64e5
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 10, target_epsilon: 10, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102], 'accuracy': [0.19693918646798228], 'auc': [0.4459623197507399]}

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543], 'accuracy': [0.19693918646798228, 0.23238018525976642], 'auc': [0.4459623197507399, 0.45312056473212875]}

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294]}

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659]}

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755]}

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456]}

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124]}

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)

[92mINFO [0m:      Sent reply
01/27/2025 16:59:51:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:00:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:00:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 16767768-7d1b-4142-9ffb-95d071ade23d
01/27/2025 17:00:26:INFO:Received: train message 16767768-7d1b-4142-9ffb-95d071ade23d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 17:32:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:33:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:33:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2ea8dcb4-36a4-4b71-98ca-66431caae38d
01/27/2025 17:33:07:INFO:Received: evaluate message 2ea8dcb4-36a4-4b71-98ca-66431caae38d
[92mINFO [0m:      Sent reply
01/27/2025 17:37:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:38:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:38:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message acd29951-87d5-4195-9e0e-1b744889ba26
01/27/2025 17:38:37:INFO:Received: train message acd29951-87d5-4195-9e0e-1b744889ba26
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 18:17:28:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:18:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:18:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7c4b07b2-c5d8-4659-a080-1aba6c242e1a
01/27/2025 18:18:04:INFO:Received: evaluate message 7c4b07b2-c5d8-4659-a080-1aba6c242e1a
[92mINFO [0m:      Sent reply
01/27/2025 18:23:41:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:24:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:24:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 66b8096e-df14-41bd-934d-49570f4ee122
01/27/2025 18:24:04:INFO:Received: train message 66b8096e-df14-41bd-934d-49570f4ee122
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 19:06:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:07:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:07:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b6d46889-0eab-4ec6-af24-5a99749590c4
01/27/2025 19:07:06:INFO:Received: evaluate message b6d46889-0eab-4ec6-af24-5a99749590c4
[92mINFO [0m:      Sent reply
01/27/2025 19:12:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:13:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:13:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 98ce7c38-a51f-499d-a878-40d83dcded73
01/27/2025 19:13:07:INFO:Received: train message 98ce7c38-a51f-499d-a878-40d83dcded73
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 19:58:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:59:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:59:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 53a4901f-8df6-4d22-be48-0145231ee767
01/27/2025 19:59:35:INFO:Received: evaluate message 53a4901f-8df6-4d22-be48-0145231ee767
[92mINFO [0m:      Sent reply
01/27/2025 20:05:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:05:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:05:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 764ecc2e-2968-46bf-84c1-fa3211711c41
01/27/2025 20:05:49:INFO:Received: train message 764ecc2e-2968-46bf-84c1-fa3211711c41
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 20:43:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:43:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:43:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f5a94561-3381-4e32-ba26-a080ce303d51
01/27/2025 20:43:45:INFO:Received: evaluate message f5a94561-3381-4e32-ba26-a080ce303d51
[92mINFO [0m:      Sent reply
01/27/2025 20:48:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:49:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:49:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2e8e6600-54dc-42c5-8218-ecbac057812f
01/27/2025 20:49:01:INFO:Received: train message 2e8e6600-54dc-42c5-8218-ecbac057812f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 21:22:04:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 21:22:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 21:22:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e621b1c2-3e13-493c-abda-48c24da4d6ca
01/27/2025 21:22:33:INFO:Received: evaluate message e621b1c2-3e13-493c-abda-48c24da4d6ca

{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655]}

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221]}

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779]}

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098, 1.765155115148688], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824, 0.34393878372935965], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779, 0.5177511222367319]}

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098, 1.765155115148688, 1.75747421649114], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824, 0.34393878372935965, 0.34313330648409185], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779, 0.5177511222367319, 0.5246326183305553]}

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098, 1.765155115148688, 1.75747421649114, 1.7514289996818915], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824, 0.34393878372935965, 0.34313330648409185, 0.34313330648409185], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779, 0.5177511222367319, 0.5246326183305553, 0.5310733000304378]}

Base Noise Multiplier Received:  0.8197021484375
Data Scaling Factor: 0.3920097903754293 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.06218395754694939, 0.09299822896718979, 0.060015834867954254, 0.019780416041612625, 0.028124617412686348, 0.006554558873176575, 0.008348643779754639, 0.017487607896327972]
Noise Multiplier after list and tensor:  0.03693673317320645
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)

[92mINFO [0m:      Sent reply
01/27/2025 21:27:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 21:27:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 21:27:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 24ef943e-9e3e-4bff-a129-0971cbdb5d67
01/27/2025 21:27:46:INFO:Received: train message 24ef943e-9e3e-4bff-a129-0971cbdb5d67
