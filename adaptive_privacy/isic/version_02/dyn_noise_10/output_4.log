nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.1 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/27/2025 11:23:56:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 11:23:56:DEBUG:ChannelConnectivity.IDLE
01/27/2025 11:23:56:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 11:23:56:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 11:33:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:33:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2ac59885-b104-4daa-b595-5ae6654fe200
01/27/2025 11:33:00:INFO:Received: train message 2ac59885-b104-4daa-b595-5ae6654fe200
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 11:44:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:07:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:07:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b3434d77-a1ed-4ca2-a1d7-219566b31430
01/27/2025 12:07:38:INFO:Received: evaluate message b3434d77-a1ed-4ca2-a1d7-219566b31430
[92mINFO [0m:      Sent reply
01/27/2025 12:12:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:13:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:13:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e4920fc6-6c5b-41e3-bd00-99cb36d81063
01/27/2025 12:13:26:INFO:Received: train message e4920fc6-6c5b-41e3-bd00-99cb36d81063
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 12:25:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:49:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:49:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 002930ed-1f04-4523-95f5-dc5b5e902378
01/27/2025 12:49:30:INFO:Received: evaluate message 002930ed-1f04-4523-95f5-dc5b5e902378
[92mINFO [0m:      Sent reply
01/27/2025 12:54:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:55:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:55:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ea4a7cf2-8ca0-40f5-8a2b-65c85cdb51e0
01/27/2025 12:55:11:INFO:Received: train message ea4a7cf2-8ca0-40f5-8a2b-65c85cdb51e0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 13:06:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:32:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:32:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f84b9a07-8fc8-422f-8f89-f539eacbda44
01/27/2025 13:32:24:INFO:Received: evaluate message f84b9a07-8fc8-422f-8f89-f539eacbda44
[92mINFO [0m:      Sent reply
01/27/2025 13:37:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:37:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:37:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c4d7ec37-34bd-4dc0-80ed-47e1a5be269c
01/27/2025 13:37:50:INFO:Received: train message c4d7ec37-34bd-4dc0-80ed-47e1a5be269c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 13:49:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:12:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:12:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4c5d2e18-0474-4fc2-b737-d9dd4cb66dbb
01/27/2025 14:12:14:INFO:Received: evaluate message 4c5d2e18-0474-4fc2-b737-d9dd4cb66dbb
[92mINFO [0m:      Sent reply
01/27/2025 14:17:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:18:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:18:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5fa59124-3229-43f4-bbb2-882b0fe00de3
01/27/2025 14:18:25:INFO:Received: train message 5fa59124-3229-43f4-bbb2-882b0fe00de3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 14:29:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:52:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:52:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 23c7fafd-d67a-4fd8-b463-c20747ec64df
01/27/2025 14:52:40:INFO:Received: evaluate message 23c7fafd-d67a-4fd8-b463-c20747ec64df
[92mINFO [0m:      Sent reply
01/27/2025 14:57:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:58:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:58:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7991865e-1915-4ad5-bdf2-fc70df98d4e7
01/27/2025 14:58:24:INFO:Received: train message 7991865e-1915-4ad5-bdf2-fc70df98d4e7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 15:10:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 15:35:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 15:35:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3d7dd2a1-8e41-4df9-bf7a-71d4be7930f7
01/27/2025 15:35:27:INFO:Received: evaluate message 3d7dd2a1-8e41-4df9-bf7a-71d4be7930f7
[92mINFO [0m:      Sent reply
01/27/2025 15:40:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 15:40:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 15:40:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8d1f3c7e-0e1e-43c5-9c79-da08f793b356
01/27/2025 15:40:46:INFO:Received: train message 8d1f3c7e-0e1e-43c5-9c79-da08f793b356
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 15:52:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:14:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:14:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 87c65579-a28a-4f63-a4b0-16ca21eb2576
01/27/2025 16:14:59:INFO:Received: evaluate message 87c65579-a28a-4f63-a4b0-16ca21eb2576
[92mINFO [0m:      Sent reply
01/27/2025 16:19:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:20:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:20:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 16efa3d8-711d-4c7b-95a2-d0cf9c5a9a80
01/27/2025 16:20:07:INFO:Received: train message 16efa3d8-711d-4c7b-95a2-d0cf9c5a9a80
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 16:31:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:54:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:54:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e58bea88-7b8c-4958-897f-7c7d066e96b2
01/27/2025 16:54:58:INFO:Received: evaluate message e58bea88-7b8c-4958-897f-7c7d066e96b2
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 10, target_epsilon: 10, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102], 'accuracy': [0.19693918646798228], 'auc': [0.4459623197507399]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543], 'accuracy': [0.19693918646798228, 0.23238018525976642], 'auc': [0.4459623197507399, 0.45312056473212875]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)

[92mINFO [0m:      Sent reply
01/27/2025 16:59:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:00:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:00:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5c8a59e1-0609-452c-aebf-fa9178826232
01/27/2025 17:00:22:INFO:Received: train message 5c8a59e1-0609-452c-aebf-fa9178826232
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 17:11:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:33:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:33:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5c783fc8-a20a-48f7-8282-8bde404e186e
01/27/2025 17:33:12:INFO:Received: evaluate message 5c783fc8-a20a-48f7-8282-8bde404e186e
[92mINFO [0m:      Sent reply
01/27/2025 17:37:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:38:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:38:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e5345db5-e369-4a9b-8f8c-4b8531730eba
01/27/2025 17:38:33:INFO:Received: train message e5345db5-e369-4a9b-8f8c-4b8531730eba
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 17:49:45:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:17:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:17:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5559078-f247-4b0d-b0d4-eea29a06c41b
01/27/2025 18:17:51:INFO:Received: evaluate message b5559078-f247-4b0d-b0d4-eea29a06c41b
[92mINFO [0m:      Sent reply
01/27/2025 18:23:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:24:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:24:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 78937cac-c8c9-42a9-a842-d342659632bd
01/27/2025 18:24:11:INFO:Received: train message 78937cac-c8c9-42a9-a842-d342659632bd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 18:37:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:07:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:07:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 95cb661b-8027-429c-a42b-829170d9c734
01/27/2025 19:07:07:INFO:Received: evaluate message 95cb661b-8027-429c-a42b-829170d9c734
[92mINFO [0m:      Sent reply
01/27/2025 19:12:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:13:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:13:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c8f6305d-e88b-49f5-bc20-025e68fa7562
01/27/2025 19:13:14:INFO:Received: train message c8f6305d-e88b-49f5-bc20-025e68fa7562
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 19:26:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:59:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:59:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a39e77c9-a235-4bad-a082-38a90b8075fe
01/27/2025 19:59:28:INFO:Received: evaluate message a39e77c9-a235-4bad-a082-38a90b8075fe
[92mINFO [0m:      Sent reply
01/27/2025 20:05:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:05:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:05:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e6fb01b7-12a6-4003-9dd2-e9c380a72d18
01/27/2025 20:05:56:INFO:Received: train message e6fb01b7-12a6-4003-9dd2-e9c380a72d18
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 20:19:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:43:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:43:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message acaa87da-1943-452e-934a-4d5e4454aa09
01/27/2025 20:43:32:INFO:Received: evaluate message acaa87da-1943-452e-934a-4d5e4454aa09
[92mINFO [0m:      Sent reply
01/27/2025 20:48:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:48:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:48:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e6c90de2-daac-41fe-a8f2-1f10da630e06
01/27/2025 20:48:51:INFO:Received: train message e6c90de2-daac-41fe-a8f2-1f10da630e06
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 20:59:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 21:22:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 21:22:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 07f715dd-93da-41ac-af3f-23f2a90ec359
01/27/2025 21:22:35:INFO:Received: evaluate message 07f715dd-93da-41ac-af3f-23f2a90ec359

{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098, 1.765155115148688], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824, 0.34393878372935965], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779, 0.5177511222367319]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098, 1.765155115148688, 1.75747421649114], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824, 0.34393878372935965, 0.34313330648409185], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779, 0.5177511222367319, 0.5246326183305553]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098, 1.765155115148688, 1.75747421649114, 1.7514289996818915], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824, 0.34393878372935965, 0.34313330648409185, 0.34313330648409185], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779, 0.5177511222367319, 0.5246326183305553, 0.5310733000304378]}

Base Noise Multiplier Received:  0.621337890625
Data Scaling Factor: 0.07133551774505546 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005783265922218561, 0.014499098993837833, 0.004910085815936327, 0.0030885818414390087, 0.008211377076804638, 0.0014982587890699506, 0.0005142497830092907, 0.0033270157873630524]
Noise Multiplier after list and tensor:  0.005228991751209833
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)

[92mINFO [0m:      Sent reply
01/27/2025 21:27:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 21:28:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 21:28:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c2861ad1-f073-444c-acd3-aa7e117f4d56
01/27/2025 21:28:05:INFO:Received: train message c2861ad1-f073-444c-acd3-aa7e117f4d56
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 21:39:31:INFO:Sent reply
