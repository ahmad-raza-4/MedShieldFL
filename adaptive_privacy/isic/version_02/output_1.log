nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/version_02/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/20/2025 10:42:04:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/20/2025 10:42:04:DEBUG:ChannelConnectivity.IDLE
01/20/2025 10:42:04:DEBUG:ChannelConnectivity.CONNECTING
01/20/2025 10:42:04:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/20/2025 10:42:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 10:42:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5a00349b-95c8-4cb5-b450-0b293ca02ba3
01/20/2025 10:42:10:INFO:Received: evaluate message 5a00349b-95c8-4cb5-b450-0b293ca02ba3
[92mINFO [0m:      Sent reply
01/20/2025 10:43:35:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 10:43:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 10:43:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 70076243-7def-43eb-bb49-1a3d5cc87847
01/20/2025 10:43:41:INFO:Received: train message 70076243-7def-43eb-bb49-1a3d5cc87847
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/20/2025 10:45:11:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 10:45:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 10:45:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b7ca8bcc-6b11-40d6-8dfe-bb0c2618d535
01/20/2025 10:45:21:INFO:Received: evaluate message b7ca8bcc-6b11-40d6-8dfe-bb0c2618d535
[92mINFO [0m:      Sent reply
01/20/2025 10:46:52:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 10:46:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 10:46:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e3997411-781b-4982-9f7f-149fe61c0348
01/20/2025 10:46:58:INFO:Received: train message e3997411-781b-4982-9f7f-149fe61c0348
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/20/2025 10:48:25:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 10:48:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 10:48:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 847b991d-5944-4149-940d-2f5ea7b693f7
01/20/2025 10:48:36:INFO:Received: evaluate message 847b991d-5944-4149-940d-2f5ea7b693f7
[92mINFO [0m:      Sent reply
01/20/2025 10:50:08:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 10:50:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 10:50:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2f441895-1048-4d1d-b5de-2eff109650ab
01/20/2025 10:50:15:INFO:Received: train message 2f441895-1048-4d1d-b5de-2eff109650ab
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/20/2025 10:51:41:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 10:51:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 10:51:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9398f9ca-9731-4514-8d31-f47ca5d07f1b
01/20/2025 10:51:51:INFO:Received: evaluate message 9398f9ca-9731-4514-8d31-f47ca5d07f1b
[92mINFO [0m:      Sent reply
01/20/2025 10:53:22:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 10:53:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 10:53:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2ad2297d-a575-4ca5-891c-5ffcd1590c60
01/20/2025 10:53:28:INFO:Received: train message 2ad2297d-a575-4ca5-891c-5ffcd1590c60
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/20/2025 10:54:55:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 10:55:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 10:55:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a2bdbc5e-e917-491f-a4b9-14871d4029f4
01/20/2025 10:55:07:INFO:Received: evaluate message a2bdbc5e-e917-491f-a4b9-14871d4029f4
[92mINFO [0m:      Sent reply
01/20/2025 10:56:37:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 10:56:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 10:56:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4638d275-c443-4e92-a553-cb539242b41f
01/20/2025 10:56:43:INFO:Received: train message 4638d275-c443-4e92-a553-cb539242b41f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/20/2025 10:57:55:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 10:58:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 10:58:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c98de2b2-70df-44ea-900e-375bdb7d3922
01/20/2025 10:58:07:INFO:Received: evaluate message c98de2b2-70df-44ea-900e-375bdb7d3922
[92mINFO [0m:      Sent reply
01/20/2025 10:59:19:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 10:59:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 10:59:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f1349841-e92a-4ca9-a334-2006fb519058
01/20/2025 10:59:25:INFO:Received: train message f1349841-e92a-4ca9-a334-2006fb519058
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/20/2025 11:00:22:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 11:00:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 11:00:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1d779b19-a9af-4400-86a1-209ddd2b16bd
01/20/2025 11:00:34:INFO:Received: evaluate message 1d779b19-a9af-4400-86a1-209ddd2b16bd
[92mINFO [0m:      Sent reply
01/20/2025 11:01:46:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 11:01:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 11:01:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0f0012b5-3b79-44c7-8a39-740b079e09b6
01/20/2025 11:01:52:INFO:Received: train message 0f0012b5-3b79-44c7-8a39-740b079e09b6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/20/2025 11:02:44:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 11:02:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 11:02:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a20c5011-379f-4615-845d-361f7acf9d63
01/20/2025 11:02:57:INFO:Received: evaluate message a20c5011-379f-4615-845d-361f7acf9d63
[92mINFO [0m:      Sent reply
01/20/2025 11:03:53:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 11:03:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 11:03:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 32b5ba93-800b-4180-979b-c3d8f74cdfe9
01/20/2025 11:03:59:INFO:Received: train message 32b5ba93-800b-4180-979b-c3d8f74cdfe9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/20/2025 11:04:50:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 11:05:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 11:05:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8c071601-8f1c-441b-8132-8288acc704de
01/20/2025 11:05:01:INFO:Received: evaluate message 8c071601-8f1c-441b-8132-8288acc704de
[92mINFO [0m:      Sent reply
01/20/2025 11:05:57:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 11:06:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 11:06:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9b1dc2aa-915a-46b0-84bb-b11022aad69a
01/20/2025 11:06:03:INFO:Received: train message 9b1dc2aa-915a-46b0-84bb-b11022aad69a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/20/2025 11:06:57:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 11:07:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 11:07:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 33d1626c-7c11-4f0b-b6a6-ab36e5369d16
01/20/2025 11:07:09:INFO:Received: evaluate message 33d1626c-7c11-4f0b-b6a6-ab36e5369d16
[92mINFO [0m:      Sent reply
01/20/2025 11:08:07:INFO:Sent reply
[92mINFO [0m:      
01/20/2025 11:08:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/20/2025 11:08:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 1c2c7628-e66b-42d8-afcf-d63fa63ac799
01/20/2025 11:08:07:INFO:Received: reconnect message 1c2c7628-e66b-42d8-afcf-d63fa63ac799
01/20/2025 11:08:07:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/20/2025 11:08:07:INFO:Disconnect and shut down
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/version_02', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/version_02']

{'loss': [182.00785517692566], 'accuracy': [0.02738622633910592], 'auc': [0.49856963389248926]}

Sample rate in Fit
 0.04885496183206107
BaseNM 1.787109375
noise multiplier 1.3066721642389894
Noise multiplier before  adjustment: 1.3066721642389894
Noise multiplier before convergence adjustment: 1.3066721642389894
Updated noise multiplier after convergence adjustment: 1.3066721642389894
Epsilon = 1.63

{'loss': [182.00785517692566, 176.59093403816223], 'accuracy': [0.02738622633910592, 0.03946838501812324], 'auc': [0.49856963389248926, 0.5004551587844493]}

Sample rate in Fit
 0.04885496183206107
BaseNM 1.787109375
noise multiplier 1.3066721642389894
Noise multiplier before  adjustment: 1.3066721642389894
Noise multiplier before convergence adjustment: 1.3066721642389894
Updated noise multiplier after convergence adjustment: 1.3066721642389894
Epsilon = 1.63

{'loss': [182.00785517692566, 176.59093403816223, 172.17751932144165], 'accuracy': [0.02738622633910592, 0.03946838501812324, 0.0648409182440596], 'auc': [0.49856963389248926, 0.5004551587844493, 0.5015879744565964]}

Sample rate in Fit
 0.04885496183206107
BaseNM 1.787109375
noise multiplier 1.3066721642389894
Noise multiplier before  adjustment: 1.3066721642389894
Noise multiplier before convergence adjustment: 1.3066721642389894
Updated noise multiplier after convergence adjustment: 1.3066721642389894
Epsilon = 1.63

{'loss': [182.00785517692566, 176.59093403816223, 172.17751932144165, 168.13567519187927], 'accuracy': [0.02738622633910592, 0.03946838501812324, 0.0648409182440596, 0.1103503826016915], 'auc': [0.49856963389248926, 0.5004551587844493, 0.5015879744565964, 0.503583038264758]}

Sample rate in Fit
 0.04885496183206107
BaseNM 1.787109375
noise multiplier 1.3066721642389894
Noise multiplier before  adjustment: 1.3066721642389894
Noise multiplier before convergence adjustment: 1.3066721642389894
Updated noise multiplier after convergence adjustment: 1.3066721642389894
Epsilon = 1.63

{'loss': [182.00785517692566, 176.59093403816223, 172.17751932144165, 168.13567519187927, 164.6749004125595], 'accuracy': [0.02738622633910592, 0.03946838501812324, 0.0648409182440596, 0.1103503826016915, 0.1707611759967781], 'auc': [0.49856963389248926, 0.5004551587844493, 0.5015879744565964, 0.503583038264758, 0.5052341540757489]}

Sample rate in Fit
 0.04885496183206107
BaseNM 1.787109375
noise multiplier 1.3066721642389894
Noise multiplier before  adjustment: 1.3066721642389894
Noise multiplier before convergence adjustment: 1.3066721642389894
Updated noise multiplier after convergence adjustment: 1.3066721642389894
Epsilon = 1.63

{'loss': [182.00785517692566, 176.59093403816223, 172.17751932144165, 168.13567519187927, 164.6749004125595, 161.70956361293793], 'accuracy': [0.02738622633910592, 0.03946838501812324, 0.0648409182440596, 0.1103503826016915, 0.1707611759967781, 0.2360048328634716], 'auc': [0.49856963389248926, 0.5004551587844493, 0.5015879744565964, 0.503583038264758, 0.5052341540757489, 0.5074520673915099]}

Sample rate in Fit
 0.04885496183206107
BaseNM 1.787109375
noise multiplier 1.3066721642389894
Noise multiplier before  adjustment: 1.3066721642389894
Noise multiplier before convergence adjustment: 1.3066721642389894
Updated noise multiplier after convergence adjustment: 1.3066721642389894
Epsilon = 1.63

{'loss': [182.00785517692566, 176.59093403816223, 172.17751932144165, 168.13567519187927, 164.6749004125595, 161.70956361293793, 159.06345093250275], 'accuracy': [0.02738622633910592, 0.03946838501812324, 0.0648409182440596, 0.1103503826016915, 0.1707611759967781, 0.2360048328634716, 0.2815142972211035], 'auc': [0.49856963389248926, 0.5004551587844493, 0.5015879744565964, 0.503583038264758, 0.5052341540757489, 0.5074520673915099, 0.5084789364448844]}

Sample rate in Fit
 0.04885496183206107
BaseNM 1.787109375
noise multiplier 1.3066721642389894
Noise multiplier before  adjustment: 1.3066721642389894
Noise multiplier before convergence adjustment: 1.3066721642389894
Updated noise multiplier after convergence adjustment: 1.3066721642389894
Epsilon = 1.63

{'loss': [182.00785517692566, 176.59093403816223, 172.17751932144165, 168.13567519187927, 164.6749004125595, 161.70956361293793, 159.06345093250275, 156.9385472536087], 'accuracy': [0.02738622633910592, 0.03946838501812324, 0.0648409182440596, 0.1103503826016915, 0.1707611759967781, 0.2360048328634716, 0.2815142972211035, 0.31292790978654855], 'auc': [0.49856963389248926, 0.5004551587844493, 0.5015879744565964, 0.503583038264758, 0.5052341540757489, 0.5074520673915099, 0.5084789364448844, 0.5110915411561034]}

Sample rate in Fit
 0.04885496183206107
BaseNM 1.787109375
noise multiplier 1.3066721642389894
Noise multiplier before  adjustment: 1.3066721642389894
Noise multiplier before convergence adjustment: 1.3066721642389894
Updated noise multiplier after convergence adjustment: 1.3066721642389894
Epsilon = 1.63

{'loss': [182.00785517692566, 176.59093403816223, 172.17751932144165, 168.13567519187927, 164.6749004125595, 161.70956361293793, 159.06345093250275, 156.9385472536087, 155.199538230896], 'accuracy': [0.02738622633910592, 0.03946838501812324, 0.0648409182440596, 0.1103503826016915, 0.1707611759967781, 0.2360048328634716, 0.2815142972211035, 0.31292790978654855, 0.3217881594844946], 'auc': [0.49856963389248926, 0.5004551587844493, 0.5015879744565964, 0.503583038264758, 0.5052341540757489, 0.5074520673915099, 0.5084789364448844, 0.5110915411561034, 0.5132614211452565]}

Sample rate in Fit
 0.04885496183206107
BaseNM 1.787109375
noise multiplier 1.3066721642389894
Noise multiplier before  adjustment: 1.3066721642389894
Noise multiplier before convergence adjustment: 1.3066721642389894
Updated noise multiplier after convergence adjustment: 1.3066721642389894
Epsilon = 1.63

{'loss': [182.00785517692566, 176.59093403816223, 172.17751932144165, 168.13567519187927, 164.6749004125595, 161.70956361293793, 159.06345093250275, 156.9385472536087, 155.199538230896, 153.8139888048172], 'accuracy': [0.02738622633910592, 0.03946838501812324, 0.0648409182440596, 0.1103503826016915, 0.1707611759967781, 0.2360048328634716, 0.2815142972211035, 0.31292790978654855, 0.3217881594844946, 0.3282319774466371], 'auc': [0.49856963389248926, 0.5004551587844493, 0.5015879744565964, 0.503583038264758, 0.5052341540757489, 0.5074520673915099, 0.5084789364448844, 0.5110915411561034, 0.5132614211452565, 0.5147408533056672]}



Final client history:
{'loss': [182.00785517692566, 176.59093403816223, 172.17751932144165, 168.13567519187927, 164.6749004125595, 161.70956361293793, 159.06345093250275, 156.9385472536087, 155.199538230896, 153.8139888048172], 'accuracy': [0.02738622633910592, 0.03946838501812324, 0.0648409182440596, 0.1103503826016915, 0.1707611759967781, 0.2360048328634716, 0.2815142972211035, 0.31292790978654855, 0.3217881594844946, 0.3282319774466371], 'auc': [0.49856963389248926, 0.5004551587844493, 0.5015879744565964, 0.503583038264758, 0.5052341540757489, 0.5074520673915099, 0.5084789364448844, 0.5110915411561034, 0.5132614211452565, 0.5147408533056672]}

