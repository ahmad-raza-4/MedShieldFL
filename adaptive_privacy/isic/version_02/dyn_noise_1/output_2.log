nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/23/2025 11:28:14:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/23/2025 11:28:14:DEBUG:ChannelConnectivity.IDLE
01/23/2025 11:28:14:DEBUG:ChannelConnectivity.CONNECTING
01/23/2025 11:28:14:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/23/2025 11:31:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:31:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 82861f37-3a42-4bea-abfb-fbf5de65064f
01/23/2025 11:31:14:INFO:Received: train message 82861f37-3a42-4bea-abfb-fbf5de65064f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:44:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:50:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:50:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message acea64d9-e9d1-4869-be93-f132034eb890
01/23/2025 11:50:39:INFO:Received: evaluate message acea64d9-e9d1-4869-be93-f132034eb890
[92mINFO [0m:      Sent reply
01/23/2025 11:54:41:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:55:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:55:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a6481836-058b-4315-9da9-31e4b403ac85
01/23/2025 11:55:02:INFO:Received: train message a6481836-058b-4315-9da9-31e4b403ac85
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:07:47:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:14:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:14:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bc26694d-cdf9-4afe-b0f6-6ea39cdcaf36
01/23/2025 12:14:20:INFO:Received: evaluate message bc26694d-cdf9-4afe-b0f6-6ea39cdcaf36
[92mINFO [0m:      Sent reply
01/23/2025 12:18:32:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:18:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:18:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a3cc8bb9-9cd0-4b65-967d-598348c8cfdb
01/23/2025 12:18:41:INFO:Received: train message a3cc8bb9-9cd0-4b65-967d-598348c8cfdb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:31:15:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:38:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:38:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7264d58c-59d2-4111-936a-2b7fbe07dd61
01/23/2025 12:38:01:INFO:Received: evaluate message 7264d58c-59d2-4111-936a-2b7fbe07dd61
[92mINFO [0m:      Sent reply
01/23/2025 12:41:54:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:42:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:42:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fc471b18-480e-4a9c-9854-ade3024eaaff
01/23/2025 12:42:24:INFO:Received: train message fc471b18-480e-4a9c-9854-ade3024eaaff
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:55:04:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:01:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:01:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b2c7842f-cc3e-4f68-96de-2f1b9a93d2d5
01/23/2025 13:01:37:INFO:Received: evaluate message b2c7842f-cc3e-4f68-96de-2f1b9a93d2d5
[92mINFO [0m:      Sent reply
01/23/2025 13:05:38:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:06:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:06:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 95884bde-22a7-4473-9001-2d10c41e3b8f
01/23/2025 13:06:02:INFO:Received: train message 95884bde-22a7-4473-9001-2d10c41e3b8f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:18:57:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:25:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:25:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 741e9b58-2c8e-4122-b093-82db3a0876b9
01/23/2025 13:25:20:INFO:Received: evaluate message 741e9b58-2c8e-4122-b093-82db3a0876b9
[92mINFO [0m:      Sent reply
01/23/2025 13:29:19:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:30:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:30:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6b922084-af7a-4847-bb58-5e08c63b606c
01/23/2025 13:30:01:INFO:Received: train message 6b922084-af7a-4847-bb58-5e08c63b606c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:42:58:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:49:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:49:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 85965845-d4be-4776-8d1f-fbebd2250856
01/23/2025 13:49:19:INFO:Received: evaluate message 85965845-d4be-4776-8d1f-fbebd2250856
[92mINFO [0m:      Sent reply
01/23/2025 13:53:22:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:53:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:53:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9dcf195d-fe1d-4953-a883-90f07a6610f3
01/23/2025 13:53:47:INFO:Received: train message 9dcf195d-fe1d-4953-a883-90f07a6610f3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:06:19:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:12:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:12:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 27658a25-1f83-4b64-b4b8-2ba40d64dafa
01/23/2025 14:12:46:INFO:Received: evaluate message 27658a25-1f83-4b64-b4b8-2ba40d64dafa
[92mINFO [0m:      Sent reply
01/23/2025 14:16:52:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:17:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:17:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9cfdefe3-2ae2-4148-b691-7055e6d49a33
01/23/2025 14:17:28:INFO:Received: train message 9cfdefe3-2ae2-4148-b691-7055e6d49a33
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:30:12:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:36:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:36:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dc3952be-b592-4ecb-aec4-3245fb3b6c1a
01/23/2025 14:36:37:INFO:Received: evaluate message dc3952be-b592-4ecb-aec4-3245fb3b6c1a
[92mINFO [0m:      Sent reply
01/23/2025 14:40:34:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:41:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:41:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9a5da263-c54b-47aa-b1c2-fcbe68c1aa70
01/23/2025 14:41:07:INFO:Received: train message 9a5da263-c54b-47aa-b1c2-fcbe68c1aa70
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:53:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:00:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:00:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7aad5a95-138d-45e4-989c-4e1f3801ba33
01/23/2025 15:00:19:INFO:Received: evaluate message 7aad5a95-138d-45e4-989c-4e1f3801ba33
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.42

{'loss': [137.2596139907837], 'accuracy': [0.3383004430124849], 'auc': [0.5868727978587345]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.49

{'loss': [137.2596139907837, 133.6919516324997], 'accuracy': [0.3383004430124849, 0.3407168747482884], 'auc': [0.5868727978587345, 0.6194824557606924]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.03

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.15

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.44

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.02

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.17

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.02

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.64
[92mINFO [0m:      Sent reply
01/23/2025 15:04:22:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:04:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:04:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 381fba06-dc6d-4132-9254-c3f724f6465f
01/23/2025 15:04:50:INFO:Received: train message 381fba06-dc6d-4132-9254-c3f724f6465f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:17:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:24:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:24:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cde4b342-6d4c-4b60-a4fd-b6e1193c6866
01/23/2025 15:24:02:INFO:Received: evaluate message cde4b342-6d4c-4b60-a4fd-b6e1193c6866
[92mINFO [0m:      Sent reply
01/23/2025 15:27:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:28:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:28:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7fcde6bc-bc67-4eb4-b417-052895649587
01/23/2025 15:28:41:INFO:Received: train message 7fcde6bc-bc67-4eb4-b417-052895649587
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:41:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:48:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:48:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4a50f87d-327c-4bf4-b32a-3f297068b94c
01/23/2025 15:48:01:INFO:Received: evaluate message 4a50f87d-327c-4bf4-b32a-3f297068b94c
[92mINFO [0m:      Sent reply
01/23/2025 15:52:00:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:52:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:52:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c86ca964-5f81-4843-9892-3b37a2c9af28
01/23/2025 15:52:16:INFO:Received: train message c86ca964-5f81-4843-9892-3b37a2c9af28
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:05:06:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:11:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:11:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a6a7ad4b-469b-4cc8-b7bc-6f94c3b86323
01/23/2025 16:11:23:INFO:Received: evaluate message a6a7ad4b-469b-4cc8-b7bc-6f94c3b86323
[92mINFO [0m:      Sent reply
01/23/2025 16:15:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:15:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:15:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ce22d058-a892-4322-bbe0-144637b3ae8c
01/23/2025 16:15:53:INFO:Received: train message ce22d058-a892-4322-bbe0-144637b3ae8c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:28:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:34:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:34:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e1f942f8-f787-4fcd-b6c3-183d9825ff30
01/23/2025 16:34:53:INFO:Received: evaluate message e1f942f8-f787-4fcd-b6c3-183d9825ff30
[92mINFO [0m:      Sent reply
01/23/2025 16:38:48:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:39:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:39:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1ac81161-d00e-4e77-99a0-5d5edb861ac9
01/23/2025 16:39:29:INFO:Received: train message 1ac81161-d00e-4e77-99a0-5d5edb861ac9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:52:15:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:58:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:58:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1700a5e5-4ef1-4bde-8298-f893213303d0
01/23/2025 16:58:41:INFO:Received: evaluate message 1700a5e5-4ef1-4bde-8298-f893213303d0
[92mINFO [0m:      Sent reply
01/23/2025 17:02:47:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:03:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:03:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message af28fff3-fe81-4be6-a234-3b7026737757
01/23/2025 17:03:17:INFO:Received: train message af28fff3-fe81-4be6-a234-3b7026737757
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:16:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:22:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:22:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8b076cf4-5ffe-46db-ad93-9876ebf62dfa
01/23/2025 17:22:34:INFO:Received: evaluate message 8b076cf4-5ffe-46db-ad93-9876ebf62dfa

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.35

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.21

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.19

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.38

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.23

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.661095928400755
Epsilon = 1.00 and Loss = 0.13
[92mINFO [0m:      Sent reply
01/23/2025 17:26:34:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:26:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:26:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7cde5d1c-28fd-4d71-b912-10d9a93052eb
01/23/2025 17:26:51:INFO:Received: train message 7cde5d1c-28fd-4d71-b912-10d9a93052eb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:39:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:46:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:46:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e9f1cc03-d3b4-4ff5-ac54-81dad72d5dae
01/23/2025 17:46:16:INFO:Received: evaluate message e9f1cc03-d3b4-4ff5-ac54-81dad72d5dae
[92mINFO [0m:      Sent reply
01/23/2025 17:50:12:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:50:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:50:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f02e1463-7c07-4c2a-9e9c-ca63ce05757b
01/23/2025 17:50:44:INFO:Received: train message f02e1463-7c07-4c2a-9e9c-ca63ce05757b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:03:34:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:09:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:09:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a8078b02-b9e2-48ad-8339-6cc37eed6a6f
01/23/2025 18:09:59:INFO:Received: evaluate message a8078b02-b9e2-48ad-8339-6cc37eed6a6f
[92mINFO [0m:      Sent reply
01/23/2025 18:14:04:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:14:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:14:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cf96d7bd-0560-43ae-b999-81704509310f
01/23/2025 18:14:18:INFO:Received: train message cf96d7bd-0560-43ae-b999-81704509310f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:27:01:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:33:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:33:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1be72d8e-7b50-469d-afe8-137e7d8be02f
01/23/2025 18:33:29:INFO:Received: evaluate message 1be72d8e-7b50-469d-afe8-137e7d8be02f
[92mINFO [0m:      Sent reply
01/23/2025 18:37:14:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:38:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:38:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 067947d0-9e42-47ac-8611-ff89d2f97891
01/23/2025 18:38:14:INFO:Received: train message 067947d0-9e42-47ac-8611-ff89d2f97891
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:50:49:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:57:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:57:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 449991e5-6eb9-49c6-8e1f-82273c2b5f6d
01/23/2025 18:57:06:INFO:Received: evaluate message 449991e5-6eb9-49c6-8e1f-82273c2b5f6d
[92mINFO [0m:      Sent reply
01/23/2025 19:00:53:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:01:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:01:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3f52a1bf-6618-4077-8aef-9170dce4b658
01/23/2025 19:01:48:INFO:Received: train message 3f52a1bf-6618-4077-8aef-9170dce4b658
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:14:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:21:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:21:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3ea5c4f6-9dc7-4cfd-8840-5c01d5892752
01/23/2025 19:21:02:INFO:Received: evaluate message 3ea5c4f6-9dc7-4cfd-8840-5c01d5892752

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.483689533174038
Epsilon = 1.00 and Loss = 0.08

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.306283137947321
Epsilon = 1.00 and Loss = 0.28

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 2.128876742720604
Epsilon = 1.00 and Loss = 0.48

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 1.951470347493887
Epsilon = 1.00 and Loss = 0.33

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 1.7740639522671702
Epsilon = 1.00 and Loss = 0.76
[92mINFO [0m:      Sent reply
01/23/2025 19:25:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:25:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:25:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9b839804-899c-4511-a85e-bf045c6aa2ab
01/23/2025 19:25:36:INFO:Received: train message 9b839804-899c-4511-a85e-bf045c6aa2ab
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:38:18:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:44:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:44:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6eaa9d67-3f94-461d-9f3e-c3d449af7651
01/23/2025 19:44:37:INFO:Received: evaluate message 6eaa9d67-3f94-461d-9f3e-c3d449af7651
[92mINFO [0m:      Sent reply
01/23/2025 19:48:29:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:49:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:49:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 195700f3-a8aa-4cff-b112-2ad2f76f480c
01/23/2025 19:49:06:INFO:Received: train message 195700f3-a8aa-4cff-b112-2ad2f76f480c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:01:47:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:08:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:08:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aa791b33-86ed-4462-adba-43c5490cdadb
01/23/2025 20:08:13:INFO:Received: evaluate message aa791b33-86ed-4462-adba-43c5490cdadb
[92mINFO [0m:      Sent reply
01/23/2025 20:11:32:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:12:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:12:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 76fb8ef1-6c10-43a0-a488-e30d4823ae7d
01/23/2025 20:12:45:INFO:Received: train message 76fb8ef1-6c10-43a0-a488-e30d4823ae7d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:25:38:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:32:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:32:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b9eecf21-e565-44e7-a095-e52648a20090
01/23/2025 20:32:13:INFO:Received: evaluate message b9eecf21-e565-44e7-a095-e52648a20090
[92mINFO [0m:      Sent reply
01/23/2025 20:36:25:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:36:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:36:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8b9d4519-b448-412b-9b29-3510521b7460
01/23/2025 20:36:47:INFO:Received: train message 8b9d4519-b448-412b-9b29-3510521b7460
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:49:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:56:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:56:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 76882064-9fe5-41e7-8c49-07975c67ef99
01/23/2025 20:56:09:INFO:Received: evaluate message 76882064-9fe5-41e7-8c49-07975c67ef99

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 1.5966575570404529
Epsilon = 1.00 and Loss = 0.52

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 1.419251161813736
Epsilon = 1.00 and Loss = 0.28

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 1.241844766587019
Epsilon = 1.00 and Loss = 0.34

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 1.064438371360302
Epsilon = 1.00 and Loss = 0.74
[92mINFO [0m:      Sent reply
01/23/2025 21:00:18:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:00:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:00:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2381e8b0-d06a-4e02-a060-80eae06174f0
01/23/2025 21:00:29:INFO:Received: train message 2381e8b0-d06a-4e02-a060-80eae06174f0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:13:09:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:20:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:20:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5a1dc8f6-f755-4e58-92f3-c274f05fee6f
01/23/2025 21:20:03:INFO:Received: evaluate message 5a1dc8f6-f755-4e58-92f3-c274f05fee6f
[92mINFO [0m:      Sent reply
01/23/2025 21:24:02:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:24:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:24:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4a52e32d-1c0a-4922-bfd2-3b436eb563ab
01/23/2025 21:24:49:INFO:Received: train message 4a52e32d-1c0a-4922-bfd2-3b436eb563ab
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:37:31:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:44:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:44:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 67c68a90-9ce9-4bb0-bfc9-7d1414445a82
01/23/2025 21:44:19:INFO:Received: evaluate message 67c68a90-9ce9-4bb0-bfc9-7d1414445a82
[92mINFO [0m:      Sent reply
01/23/2025 21:48:33:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:49:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:49:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 00a2feec-dd15-488d-9e70-96ef771e79ac
01/23/2025 21:49:08:INFO:Received: train message 00a2feec-dd15-488d-9e70-96ef771e79ac
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:01:41:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:08:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:08:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 198fbc27-932b-4943-8f8a-7002ebb3f660
01/23/2025 22:08:31:INFO:Received: evaluate message 198fbc27-932b-4943-8f8a-7002ebb3f660
[92mINFO [0m:      Sent reply
01/23/2025 22:12:49:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:13:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:13:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d55ec174-ba5d-4663-942b-2184738aab6b
01/23/2025 22:13:18:INFO:Received: train message d55ec174-ba5d-4663-942b-2184738aab6b

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 0.8870319761335851
Epsilon = 1.00 and Loss = 0.42

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 0.7096255809068681
Epsilon = 1.00 and Loss = 0.03

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 0.5322191856801509
Epsilon = 1.00 and Loss = 0.23

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:25:56:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:32:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:32:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a5bf41a9-769a-4702-8060-afc5cd98b274
01/23/2025 22:32:28:INFO:Received: evaluate message a5bf41a9-769a-4702-8060-afc5cd98b274
[92mINFO [0m:      Sent reply
01/23/2025 22:36:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:37:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:37:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c65cb525-55c8-4c8d-b17e-b1630bf9ab1e
01/23/2025 22:37:08:INFO:Received: train message c65cb525-55c8-4c8d-b17e-b1630bf9ab1e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:49:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:56:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:56:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0894f71e-a2bd-4ec7-8ba8-03fb740dca71
01/23/2025 22:56:39:INFO:Received: evaluate message 0894f71e-a2bd-4ec7-8ba8-03fb740dca71
[92mINFO [0m:      Sent reply
01/23/2025 23:00:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:01:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:01:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b06632ac-2fe3-4d89-a8a1-bf188141e314
01/23/2025 23:01:07:INFO:Received: train message b06632ac-2fe3-4d89-a8a1-bf188141e314
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 23:13:48:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:20:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:20:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3b4e7e7a-00b1-48ac-b030-d31b38a5b9ce
01/23/2025 23:20:53:INFO:Received: evaluate message 3b4e7e7a-00b1-48ac-b030-d31b38a5b9ce
[92mINFO [0m:      Sent reply
01/23/2025 23:25:00:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:25:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:25:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 2d170bae-c0cf-4547-b1a7-df12c4a59113
01/23/2025 23:25:00:INFO:Received: reconnect message 2d170bae-c0cf-4547-b1a7-df12c4a59113
01/23/2025 23:25:00:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/23/2025 23:25:00:INFO:Disconnect and shut down
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 0.3548127904534339
Epsilon = 1.00 and Loss = 0.02

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 0.17740639522671695
Epsilon = 1.00 and Loss = 0.13

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142]}

Base Noise Multiplier Received:  2.6953125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.21474826335906982, 17.364336013793945, 0.38096344470977783, 0.2870998680591583, 0.8825726509094238, 0.4453169107437134, 0.7748465538024902, 0.9388837218284607]
Noise Multiplier after list and tensor:  2.661095928400755
Noise Multiplier after Epsilon Scaling:  2.661095928400755
Noise Multiplier after Convergence: 0.0
Epsilon = 1.00 and Loss = 0.45

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995, 116.7615122795105], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315, 0.5368505839710028], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142, 0.7311505973177221]}



Final client history:
{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995, 116.7615122795105], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315, 0.5368505839710028], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142, 0.7311505973177221]}

