nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/23/2025 11:30:46:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/23/2025 11:30:46:DEBUG:ChannelConnectivity.IDLE
01/23/2025 11:30:46:DEBUG:ChannelConnectivity.CONNECTING
01/23/2025 11:30:46:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/23/2025 11:30:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:30:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8881ce82-5582-4052-9ecb-91c1ed8de2b5
01/23/2025 11:30:55:INFO:Received: train message 8881ce82-5582-4052-9ecb-91c1ed8de2b5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:50:05:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:50:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:50:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 934d7b4c-d085-401d-9148-bac680e4f494
01/23/2025 11:50:39:INFO:Received: evaluate message 934d7b4c-d085-401d-9148-bac680e4f494
[92mINFO [0m:      Sent reply
01/23/2025 11:54:46:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:55:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:55:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9bcfb1cf-9924-4e28-86ec-c18ab9b3b9db
01/23/2025 11:55:09:INFO:Received: train message 9bcfb1cf-9924-4e28-86ec-c18ab9b3b9db
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:13:51:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:14:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:14:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1b6fc5d9-5e19-4da1-a342-e0fe549db870
01/23/2025 12:14:18:INFO:Received: evaluate message 1b6fc5d9-5e19-4da1-a342-e0fe549db870
[92mINFO [0m:      Sent reply
01/23/2025 12:18:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:18:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:18:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f09c2cab-5cf3-45af-a937-305a18681d76
01/23/2025 12:18:58:INFO:Received: train message f09c2cab-5cf3-45af-a937-305a18681d76
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:37:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:37:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:37:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a69a6cb3-8242-4eb5-8f8d-140faf83cf69
01/23/2025 12:37:45:INFO:Received: evaluate message a69a6cb3-8242-4eb5-8f8d-140faf83cf69
[92mINFO [0m:      Sent reply
01/23/2025 12:41:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:42:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:42:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 19066297-ea2a-4c69-b2f4-cfb8b53ba88f
01/23/2025 12:42:20:INFO:Received: train message 19066297-ea2a-4c69-b2f4-cfb8b53ba88f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:01:01:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:01:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:01:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e2005ace-864a-4b12-9fff-ec2d33f90e1f
01/23/2025 13:01:33:INFO:Received: evaluate message e2005ace-864a-4b12-9fff-ec2d33f90e1f
[92mINFO [0m:      Sent reply
01/23/2025 13:05:38:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:06:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:06:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 76188c9d-2f14-4cd6-87d3-29ebd0f81122
01/23/2025 13:06:02:INFO:Received: train message 76188c9d-2f14-4cd6-87d3-29ebd0f81122
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:24:58:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:25:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:25:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 50e23371-4903-46f5-b5ee-2d4d8ce6cffa
01/23/2025 13:25:28:INFO:Received: evaluate message 50e23371-4903-46f5-b5ee-2d4d8ce6cffa
[92mINFO [0m:      Sent reply
01/23/2025 13:29:30:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:29:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:29:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 67c696cd-8fb1-4b9c-887c-b49e22a3d821
01/23/2025 13:29:46:INFO:Received: train message 67c696cd-8fb1-4b9c-887c-b49e22a3d821
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:48:44:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:49:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:49:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 571da7cb-0e27-45ee-95c8-78e230081306
01/23/2025 13:49:19:INFO:Received: evaluate message 571da7cb-0e27-45ee-95c8-78e230081306
[92mINFO [0m:      Sent reply
01/23/2025 13:53:22:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:53:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:53:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 66e21e4c-f514-4a96-8c45-d88dfe572c2a
01/23/2025 13:53:49:INFO:Received: train message 66e21e4c-f514-4a96-8c45-d88dfe572c2a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:12:23:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:12:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:12:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 97ae2beb-008b-4205-86fc-3988396ccf9c
01/23/2025 14:12:54:INFO:Received: evaluate message 97ae2beb-008b-4205-86fc-3988396ccf9c
[92mINFO [0m:      Sent reply
01/23/2025 14:17:02:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:17:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:17:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 23d9452e-583a-4360-9c31-c406d78b2c50
01/23/2025 14:17:29:INFO:Received: train message 23d9452e-583a-4360-9c31-c406d78b2c50
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:36:08:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:36:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:36:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 689c342b-d836-4f73-b733-6c2a1a25e134
01/23/2025 14:36:33:INFO:Received: evaluate message 689c342b-d836-4f73-b733-6c2a1a25e134
[92mINFO [0m:      Sent reply
01/23/2025 14:40:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:41:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:41:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a6807e69-8150-4d11-b468-87c639e59fff
01/23/2025 14:41:08:INFO:Received: train message a6807e69-8150-4d11-b468-87c639e59fff
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:59:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:00:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:00:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 635199b4-30d1-4d3f-8f2f-45283399233d
01/23/2025 15:00:17:INFO:Received: evaluate message 635199b4-30d1-4d3f-8f2f-45283399233d
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.65

{'loss': [137.2596139907837], 'accuracy': [0.3383004430124849], 'auc': [0.5868727978587345]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.86

{'loss': [137.2596139907837, 133.6919516324997], 'accuracy': [0.3383004430124849, 0.3407168747482884], 'auc': [0.5868727978587345, 0.6194824557606924]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.18

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.68

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.43

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.28

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.71

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.63

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.72
[92mINFO [0m:      Sent reply
01/23/2025 15:04:18:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:04:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:04:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dfe05e94-705f-4149-80c9-b4fe7e241d6e
01/23/2025 15:04:50:INFO:Received: train message dfe05e94-705f-4149-80c9-b4fe7e241d6e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:23:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:24:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:24:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 885fb3ad-8d0c-41b5-b532-98c314e9dbe1
01/23/2025 15:24:09:INFO:Received: evaluate message 885fb3ad-8d0c-41b5-b532-98c314e9dbe1
[92mINFO [0m:      Sent reply
01/23/2025 15:28:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:28:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:28:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6da829b0-33df-441c-8da9-51dceecd412c
01/23/2025 15:28:32:INFO:Received: train message 6da829b0-33df-441c-8da9-51dceecd412c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:47:30:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:47:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:47:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3c12787d-adaf-4939-9f56-9d24a19e722a
01/23/2025 15:47:48:INFO:Received: evaluate message 3c12787d-adaf-4939-9f56-9d24a19e722a
[92mINFO [0m:      Sent reply
01/23/2025 15:51:41:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:52:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:52:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b7f61686-6a08-4cb0-abb2-09cc8bdedf19
01/23/2025 15:52:14:INFO:Received: train message b7f61686-6a08-4cb0-abb2-09cc8bdedf19
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:11:03:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:11:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:11:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6220384d-ddab-4f4f-951c-637be6eef227
01/23/2025 16:11:16:INFO:Received: evaluate message 6220384d-ddab-4f4f-951c-637be6eef227
[92mINFO [0m:      Sent reply
01/23/2025 16:14:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:15:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:15:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b7d50806-d7aa-4e8e-a6c1-5496dc5388e9
01/23/2025 16:15:47:INFO:Received: train message b7d50806-d7aa-4e8e-a6c1-5496dc5388e9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:34:32:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:34:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:34:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7a2965ee-f46b-4bb1-a4cf-2ea94757096f
01/23/2025 16:34:49:INFO:Received: evaluate message 7a2965ee-f46b-4bb1-a4cf-2ea94757096f
[92mINFO [0m:      Sent reply
01/23/2025 16:38:28:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:39:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:39:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2fed1ae1-b6f1-4df1-af44-63b1a88996e5
01/23/2025 16:39:19:INFO:Received: train message 2fed1ae1-b6f1-4df1-af44-63b1a88996e5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:58:09:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:58:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:58:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e1c2f12e-dda8-459e-b9a0-1335be3fba9e
01/23/2025 16:58:39:INFO:Received: evaluate message e1c2f12e-dda8-459e-b9a0-1335be3fba9e
[92mINFO [0m:      Sent reply
01/23/2025 17:02:46:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:02:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:02:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cdff7ee5-9b2b-45f2-b69d-e3717ef6800e
01/23/2025 17:02:59:INFO:Received: train message cdff7ee5-9b2b-45f2-b69d-e3717ef6800e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:22:01:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:22:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:22:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2ef12a5d-f4e2-45c8-9ddc-317903096bca
01/23/2025 17:22:17:INFO:Received: evaluate message 2ef12a5d-f4e2-45c8-9ddc-317903096bca

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.47

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.59

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.39

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.42

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 2.08

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2936260104179382
Epsilon = 1.00 and Loss = 1.43
[92mINFO [0m:      Sent reply
01/23/2025 17:26:04:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:27:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:27:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fe2c07e4-7234-4b8e-8abd-5b6ce207d660
01/23/2025 17:27:02:INFO:Received: train message fe2c07e4-7234-4b8e-8abd-5b6ce207d660
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:45:41:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:46:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:46:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 028e88b9-999f-4354-a7f8-82cc2c7b17b7
01/23/2025 17:46:12:INFO:Received: evaluate message 028e88b9-999f-4354-a7f8-82cc2c7b17b7
[92mINFO [0m:      Sent reply
01/23/2025 17:50:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:50:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:50:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3787f1ad-bc09-4fcc-aaf9-2c1a025a21b7
01/23/2025 17:50:31:INFO:Received: train message 3787f1ad-bc09-4fcc-aaf9-2c1a025a21b7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:09:24:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:09:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:09:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6804bd10-ba4f-487e-957a-aa593a820f04
01/23/2025 18:09:55:INFO:Received: evaluate message 6804bd10-ba4f-487e-957a-aa593a820f04
[92mINFO [0m:      Sent reply
01/23/2025 18:14:00:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:14:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:14:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 18d61647-7fed-4aae-9cad-cfff20257e7d
01/23/2025 18:14:34:INFO:Received: train message 18d61647-7fed-4aae-9cad-cfff20257e7d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:33:07:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:33:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:33:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d2cde482-9ecf-46e7-b5a2-cb106ccd4f70
01/23/2025 18:33:41:INFO:Received: evaluate message d2cde482-9ecf-46e7-b5a2-cb106ccd4f70
[92mINFO [0m:      Sent reply
01/23/2025 18:37:44:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:38:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:38:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 00cc94f0-6deb-423d-8225-56e3e266f6a9
01/23/2025 18:38:01:INFO:Received: train message 00cc94f0-6deb-423d-8225-56e3e266f6a9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:56:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:57:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:57:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 87353d2f-e712-4417-9aff-ca769ad36d70
01/23/2025 18:57:05:INFO:Received: evaluate message 87353d2f-e712-4417-9aff-ca769ad36d70
[92mINFO [0m:      Sent reply
01/23/2025 19:00:53:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:01:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:01:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1e321106-2ee6-431a-8286-6f94f306e4c4
01/23/2025 19:01:31:INFO:Received: train message 1e321106-2ee6-431a-8286-6f94f306e4c4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:20:30:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:21:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:21:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 75a098fa-f5dd-457b-bb1c-b7991415ae17
01/23/2025 19:21:01:INFO:Received: evaluate message 75a098fa-f5dd-457b-bb1c-b7991415ae17

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.2073842763900757
Epsilon = 1.00 and Loss = 1.74

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.1211425423622132
Epsilon = 1.00 and Loss = 1.77

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 1.0349008083343507
Epsilon = 1.00 and Loss = 1.35

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 0.9486590743064881
Epsilon = 1.00 and Loss = 1.76

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 0.8624173402786256
Epsilon = 1.00 and Loss = 1.42
[92mINFO [0m:      Sent reply
01/23/2025 19:25:12:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:25:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:25:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dbd0f657-fd70-4fb5-91dc-c876b6a44014
01/23/2025 19:25:42:INFO:Received: train message dbd0f657-fd70-4fb5-91dc-c876b6a44014
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:44:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:44:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:44:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 861041d2-38f4-4598-bca4-a2c8272d6f2f
01/23/2025 19:44:50:INFO:Received: evaluate message 861041d2-38f4-4598-bca4-a2c8272d6f2f
[92mINFO [0m:      Sent reply
01/23/2025 19:48:51:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:49:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:49:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5e609e34-d3f7-49c2-b1f9-4c1c5e676184
01/23/2025 19:49:20:INFO:Received: train message 5e609e34-d3f7-49c2-b1f9-4c1c5e676184
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:07:53:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:08:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:08:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e6990717-0cfa-40b6-970f-96948d7b16b6
01/23/2025 20:08:28:INFO:Received: evaluate message e6990717-0cfa-40b6-970f-96948d7b16b6
[92mINFO [0m:      Sent reply
01/23/2025 20:12:25:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:12:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:12:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 569ceef5-a8f2-407a-a8d0-97745782277a
01/23/2025 20:12:53:INFO:Received: train message 569ceef5-a8f2-407a-a8d0-97745782277a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:31:39:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:32:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:32:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 02dcbd2d-ca16-42c6-a22a-59380e375365
01/23/2025 20:32:13:INFO:Received: evaluate message 02dcbd2d-ca16-42c6-a22a-59380e375365
[92mINFO [0m:      Sent reply
01/23/2025 20:36:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:36:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:36:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7b5020f6-8526-4f46-99bc-f42a41430d38
01/23/2025 20:36:41:INFO:Received: train message 7b5020f6-8526-4f46-99bc-f42a41430d38
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:55:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:56:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:56:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8b8c5ec1-32c2-4d49-be2f-fe1e340e55ea
01/23/2025 20:56:02:INFO:Received: evaluate message 8b8c5ec1-32c2-4d49-be2f-fe1e340e55ea

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 0.7761756062507629
Epsilon = 1.00 and Loss = 1.90

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 0.6899338722229004
Epsilon = 1.00 and Loss = 1.35

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 0.6036921381950379
Epsilon = 1.00 and Loss = 1.66

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 0.5174504041671754
Epsilon = 1.00 and Loss = 1.39
[92mINFO [0m:      Sent reply
01/23/2025 20:59:58:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:00:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:00:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1827badf-8caa-4018-a01b-da0b9bac7b90
01/23/2025 21:00:52:INFO:Received: train message 1827badf-8caa-4018-a01b-da0b9bac7b90
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:19:43:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:20:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:20:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 754b7ca9-feee-4bee-85cc-09693124d423
01/23/2025 21:20:21:INFO:Received: evaluate message 754b7ca9-feee-4bee-85cc-09693124d423
[92mINFO [0m:      Sent reply
01/23/2025 21:24:20:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:24:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:24:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8a059d25-804e-49b6-adfc-6c6d272d8793
01/23/2025 21:24:36:INFO:Received: train message 8a059d25-804e-49b6-adfc-6c6d272d8793
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:43:51:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:44:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:44:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 90011ab8-dc85-47ad-a06c-6c37062efd2c
01/23/2025 21:44:30:INFO:Received: evaluate message 90011ab8-dc85-47ad-a06c-6c37062efd2c
[92mINFO [0m:      Sent reply
01/23/2025 21:48:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:49:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:49:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c01bb88f-c6c0-4ba5-99dd-2509c3f752b6
01/23/2025 21:49:11:INFO:Received: train message c01bb88f-c6c0-4ba5-99dd-2509c3f752b6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:07:55:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:08:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:08:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f91b4085-42c3-4470-b6cb-823d4da5053f
01/23/2025 22:08:31:INFO:Received: evaluate message f91b4085-42c3-4470-b6cb-823d4da5053f
[92mINFO [0m:      Sent reply
01/23/2025 22:12:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:13:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:13:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c59eebbe-e25e-4a62-aa1f-cd529e366d71
01/23/2025 22:13:26:INFO:Received: train message c59eebbe-e25e-4a62-aa1f-cd529e366d71

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 0.4312086701393128
Epsilon = 1.00 and Loss = 1.35

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 0.34496693611145024
Epsilon = 1.00 and Loss = 1.12

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 0.25872520208358757
Epsilon = 1.00 and Loss = 1.86

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:31:58:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:32:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:32:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9f98ebb8-8fc8-4822-bc64-f166e157c675
01/23/2025 22:32:18:INFO:Received: evaluate message 9f98ebb8-8fc8-4822-bc64-f166e157c675
[92mINFO [0m:      Sent reply
01/23/2025 22:36:06:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:37:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:37:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 285ac2d6-cf3c-45c6-8e28-6cd598d1667e
01/23/2025 22:37:10:INFO:Received: train message 285ac2d6-cf3c-45c6-8e28-6cd598d1667e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:55:58:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:56:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:56:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 13f02da3-7aa7-49ab-b77a-abab45ec85c8
01/23/2025 22:56:19:INFO:Received: evaluate message 13f02da3-7aa7-49ab-b77a-abab45ec85c8
[92mINFO [0m:      Sent reply
01/23/2025 23:00:08:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:01:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:01:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 456d6572-ce59-4f12-b2bf-f221d3cd6809
01/23/2025 23:01:11:INFO:Received: train message 456d6572-ce59-4f12-b2bf-f221d3cd6809
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 23:20:10:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:20:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:20:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5ea51308-4900-47fb-98de-7124e59e04c4
01/23/2025 23:20:51:INFO:Received: evaluate message 5ea51308-4900-47fb-98de-7124e59e04c4
[92mINFO [0m:      Sent reply
01/23/2025 23:24:56:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:25:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:25:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 764e5790-c853-4849-8fc6-71de5ba07518
01/23/2025 23:25:00:INFO:Received: reconnect message 764e5790-c853-4849-8fc6-71de5ba07518
01/23/2025 23:25:01:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/23/2025 23:25:01:INFO:Disconnect and shut down
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 0.17248346805572506
Epsilon = 1.00 and Loss = 1.30

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 0.08624173402786253
Epsilon = 1.00 and Loss = 1.57

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142]}

Base Noise Multiplier Received:  4.3359375
Data Scaling Factor: 2.550956696878147 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [2.0933446884155273, 3.170933246612549, 2.1151139736175537, 0.748146653175354, 0.9417634010314941, 0.4032469689846039, 0.46833479404449463, 0.4081243574619293]
Noise Multiplier after list and tensor:  1.2936260104179382
Noise Multiplier after Epsilon Scaling:  1.2936260104179382
Noise Multiplier after Convergence: 0.0
Epsilon = 1.00 and Loss = 1.19

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995, 116.7615122795105], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315, 0.5368505839710028], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142, 0.7311505973177221]}



Final client history:
{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995, 116.7615122795105], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315, 0.5368505839710028], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142, 0.7311505973177221]}

