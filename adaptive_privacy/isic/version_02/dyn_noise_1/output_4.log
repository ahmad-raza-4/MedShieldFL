nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/23/2025 11:27:11:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/23/2025 11:27:11:DEBUG:ChannelConnectivity.IDLE
01/23/2025 11:27:11:DEBUG:ChannelConnectivity.CONNECTING
01/23/2025 11:27:11:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/23/2025 11:31:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:31:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5d559aa9-4923-4bff-b04e-2c1173bc5f08
01/23/2025 11:31:07:INFO:Received: train message 5d559aa9-4923-4bff-b04e-2c1173bc5f08
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:40:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:50:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:50:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 27657121-4ede-47d6-9442-d306ef696f19
01/23/2025 11:50:35:INFO:Received: evaluate message 27657121-4ede-47d6-9442-d306ef696f19
[92mINFO [0m:      Sent reply
01/23/2025 11:54:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:55:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:55:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 25f80ffc-f192-40d8-9048-6a9051bf9d45
01/23/2025 11:55:07:INFO:Received: train message 25f80ffc-f192-40d8-9048-6a9051bf9d45
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:03:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:14:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:14:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f6ed2c31-28d4-40e3-b4ba-e0b692a6e819
01/23/2025 12:14:24:INFO:Received: evaluate message f6ed2c31-28d4-40e3-b4ba-e0b692a6e819
[92mINFO [0m:      Sent reply
01/23/2025 12:18:34:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:19:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:19:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f5df3d7d-f838-4ace-a4dc-c2689e6c0328
01/23/2025 12:19:03:INFO:Received: train message f5df3d7d-f838-4ace-a4dc-c2689e6c0328
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:28:09:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:37:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:37:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 864fc12e-c65b-42aa-b2f6-4d336e2c648f
01/23/2025 12:37:42:INFO:Received: evaluate message 864fc12e-c65b-42aa-b2f6-4d336e2c648f
[92mINFO [0m:      Sent reply
01/23/2025 12:41:08:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:42:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:42:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1d1622d8-6e9b-481d-826c-c12bf6de771a
01/23/2025 12:42:30:INFO:Received: train message 1d1622d8-6e9b-481d-826c-c12bf6de771a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:51:32:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:01:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:01:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f47fe0f1-844d-4d38-90dc-91aacdeb7c58
01/23/2025 13:01:29:INFO:Received: evaluate message f47fe0f1-844d-4d38-90dc-91aacdeb7c58
[92mINFO [0m:      Sent reply
01/23/2025 13:05:34:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:06:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:06:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a1ff06e4-a04c-4520-867a-9814fb25bce3
01/23/2025 13:06:02:INFO:Received: train message a1ff06e4-a04c-4520-867a-9814fb25bce3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:15:07:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:25:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:25:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7a785bf5-5e05-4ae6-a6e7-65f9f29a9611
01/23/2025 13:25:29:INFO:Received: evaluate message 7a785bf5-5e05-4ae6-a6e7-65f9f29a9611
[92mINFO [0m:      Sent reply
01/23/2025 13:29:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:29:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:29:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3d8234e3-7346-4b11-bcc3-b744767de2af
01/23/2025 13:29:57:INFO:Received: train message 3d8234e3-7346-4b11-bcc3-b744767de2af
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:38:57:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:49:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:49:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 302564f0-5208-479a-b258-2bd96625ac56
01/23/2025 13:49:04:INFO:Received: evaluate message 302564f0-5208-479a-b258-2bd96625ac56
[92mINFO [0m:      Sent reply
01/23/2025 13:53:04:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:53:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:53:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 668d395e-ebca-471e-9a4a-4700b1d8b6f7
01/23/2025 13:53:42:INFO:Received: train message 668d395e-ebca-471e-9a4a-4700b1d8b6f7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:02:25:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:12:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:12:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 806fafa3-297c-47da-8bd9-4650c4c13016
01/23/2025 14:12:49:INFO:Received: evaluate message 806fafa3-297c-47da-8bd9-4650c4c13016
[92mINFO [0m:      Sent reply
01/23/2025 14:16:58:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:17:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:17:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 685aa7a5-fe65-4b95-a02b-6fb1789e54a5
01/23/2025 14:17:11:INFO:Received: train message 685aa7a5-fe65-4b95-a02b-6fb1789e54a5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:25:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:36:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:36:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8cec39fc-53b7-42bc-8563-f9c3f57ae937
01/23/2025 14:36:40:INFO:Received: evaluate message 8cec39fc-53b7-42bc-8563-f9c3f57ae937
[92mINFO [0m:      Sent reply
01/23/2025 14:40:38:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:40:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:40:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f6543d6d-44b0-4a90-8336-963646656c0f
01/23/2025 14:40:59:INFO:Received: train message f6543d6d-44b0-4a90-8336-963646656c0f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:49:51:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:00:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:00:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b203cac0-a081-47eb-9bac-8af092352643
01/23/2025 15:00:18:INFO:Received: evaluate message b203cac0-a081-47eb-9bac-8af092352643
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 2.11

{'loss': [137.2596139907837], 'accuracy': [0.3383004430124849], 'auc': [0.5868727978587345]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 2.00

{'loss': [137.2596139907837, 133.6919516324997], 'accuracy': [0.3383004430124849, 0.3407168747482884], 'auc': [0.5868727978587345, 0.6194824557606924]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 1.92

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 2.15

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 1.61

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 2.71

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 2.41

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 2.65

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 2.36
[92mINFO [0m:      Sent reply
01/23/2025 15:04:22:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:04:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:04:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 204c614e-3e4e-47a4-ae85-21850367b142
01/23/2025 15:04:37:INFO:Received: train message 204c614e-3e4e-47a4-ae85-21850367b142
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:13:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:24:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:24:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 675fbdd1-0404-48f3-9b0a-63fcb956d4cd
01/23/2025 15:24:09:INFO:Received: evaluate message 675fbdd1-0404-48f3-9b0a-63fcb956d4cd
[92mINFO [0m:      Sent reply
01/23/2025 15:28:12:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:28:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:28:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 033a7382-fda9-41cd-9230-1f5a8edbe334
01/23/2025 15:28:41:INFO:Received: train message 033a7382-fda9-41cd-9230-1f5a8edbe334
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:37:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:48:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:48:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 66fb8c20-9469-455d-ba4f-629b3d5cb36d
01/23/2025 15:48:01:INFO:Received: evaluate message 66fb8c20-9469-455d-ba4f-629b3d5cb36d
[92mINFO [0m:      Sent reply
01/23/2025 15:52:00:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:52:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:52:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 825f0342-1ae5-4ffe-ad5d-1433b35e6fb7
01/23/2025 15:52:29:INFO:Received: train message 825f0342-1ae5-4ffe-ad5d-1433b35e6fb7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:01:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:11:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:11:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 41efce65-fb68-4f35-ac39-0dfedd5f7dde
01/23/2025 16:11:36:INFO:Received: evaluate message 41efce65-fb68-4f35-ac39-0dfedd5f7dde
[92mINFO [0m:      Sent reply
01/23/2025 16:15:32:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:16:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:16:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cbe806ae-ff5e-4659-b556-029aa88cecad
01/23/2025 16:16:01:INFO:Received: train message cbe806ae-ff5e-4659-b556-029aa88cecad
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:24:48:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:34:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:34:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 95b656be-02d3-42c9-a071-5a1d83cb31cc
01/23/2025 16:34:58:INFO:Received: evaluate message 95b656be-02d3-42c9-a071-5a1d83cb31cc
[92mINFO [0m:      Sent reply
01/23/2025 16:39:00:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:39:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:39:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 135e8933-921b-43b6-8ae3-44b31b59058f
01/23/2025 16:39:14:INFO:Received: train message 135e8933-921b-43b6-8ae3-44b31b59058f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:47:39:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:58:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:58:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 555f8be5-453a-4c37-8f93-4d2058b88f5a
01/23/2025 16:58:39:INFO:Received: evaluate message 555f8be5-453a-4c37-8f93-4d2058b88f5a
[92mINFO [0m:      Sent reply
01/23/2025 17:02:44:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:03:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:03:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f4900308-616a-40b3-b93e-816766ef0d35
01/23/2025 17:03:19:INFO:Received: train message f4900308-616a-40b3-b93e-816766ef0d35
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:12:22:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:22:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:22:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4facac2f-25ec-4011-97a2-e54d1987affd
01/23/2025 17:22:31:INFO:Received: evaluate message 4facac2f-25ec-4011-97a2-e54d1987affd

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 2.73

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 2.39

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 1.85

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 2.06

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 2.43

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.349246308207512
Epsilon = 1.00 and Loss = 2.55
[92mINFO [0m:      Sent reply
01/23/2025 17:26:30:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:26:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:26:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c2f87f8f-3dc2-4b9d-b771-319b2091535a
01/23/2025 17:26:51:INFO:Received: train message c2f87f8f-3dc2-4b9d-b771-319b2091535a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:35:48:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:46:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:46:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 257fa37f-284f-414b-b8fe-064b9b653c1e
01/23/2025 17:46:05:INFO:Received: evaluate message 257fa37f-284f-414b-b8fe-064b9b653c1e
[92mINFO [0m:      Sent reply
01/23/2025 17:49:56:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:50:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:50:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8e624d86-5caa-4a3e-913a-e2ca0e628cbd
01/23/2025 17:50:42:INFO:Received: train message 8e624d86-5caa-4a3e-913a-e2ca0e628cbd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:59:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:09:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:09:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f8cfceee-6242-405b-8091-203cb585ab5f
01/23/2025 18:09:57:INFO:Received: evaluate message f8cfceee-6242-405b-8091-203cb585ab5f
[92mINFO [0m:      Sent reply
01/23/2025 18:14:02:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:14:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:14:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 52959f9f-f4b2-4c2e-8307-4b2d1ca94536
01/23/2025 18:14:32:INFO:Received: train message 52959f9f-f4b2-4c2e-8307-4b2d1ca94536
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:23:24:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:33:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:33:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 58588d91-5caf-49bf-8287-78043eb05022
01/23/2025 18:33:26:INFO:Received: evaluate message 58588d91-5caf-49bf-8287-78043eb05022
[92mINFO [0m:      Sent reply
01/23/2025 18:37:00:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:38:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:38:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 578e2763-30c7-42b3-9fef-34ee5f6790a9
01/23/2025 18:38:08:INFO:Received: train message 578e2763-30c7-42b3-9fef-34ee5f6790a9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:46:55:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:57:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:57:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c3be9b04-9db4-4a15-8d0c-96db0caf695c
01/23/2025 18:57:20:INFO:Received: evaluate message c3be9b04-9db4-4a15-8d0c-96db0caf695c
[92mINFO [0m:      Sent reply
01/23/2025 19:01:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:01:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:01:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1310fa9e-005b-4aea-b6a5-d3502eb558eb
01/23/2025 19:01:44:INFO:Received: train message 1310fa9e-005b-4aea-b6a5-d3502eb558eb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:10:46:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:20:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:20:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ae144b28-fea2-48f5-ab03-16090044c65f
01/23/2025 19:20:57:INFO:Received: evaluate message ae144b28-fea2-48f5-ab03-16090044c65f

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 3.1259632209936776
Epsilon = 1.00 and Loss = 1.91

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 2.902680133779844
Epsilon = 1.00 and Loss = 1.84

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 2.6793970465660095
Epsilon = 1.00 and Loss = 2.05

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 2.4561139593521757
Epsilon = 1.00 and Loss = 2.26

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 2.2328308721383414
Epsilon = 1.00 and Loss = 2.07
[92mINFO [0m:      Sent reply
01/23/2025 19:24:51:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:25:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:25:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 19fa438e-2551-43f5-bd59-62cdd38531c1
01/23/2025 19:25:29:INFO:Received: train message 19fa438e-2551-43f5-bd59-62cdd38531c1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:34:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:44:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:44:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 84024ecb-c090-4bba-9b2b-38cf1a8f0588
01/23/2025 19:44:44:INFO:Received: evaluate message 84024ecb-c090-4bba-9b2b-38cf1a8f0588
[92mINFO [0m:      Sent reply
01/23/2025 19:48:43:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:49:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:49:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3f33530c-5b2b-4aa5-ae20-d6dd6894a14b
01/23/2025 19:49:21:INFO:Received: train message 3f33530c-5b2b-4aa5-ae20-d6dd6894a14b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:58:18:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:08:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:08:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 47bce6ee-7a0a-4a0e-900c-932e5356ba47
01/23/2025 20:08:30:INFO:Received: evaluate message 47bce6ee-7a0a-4a0e-900c-932e5356ba47
[92mINFO [0m:      Sent reply
01/23/2025 20:12:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:12:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:12:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 88fbea83-9c07-4da0-8215-a84041bf7c5f
01/23/2025 20:12:48:INFO:Received: train message 88fbea83-9c07-4da0-8215-a84041bf7c5f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:21:56:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:32:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:32:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c88b4ac7-378e-405f-8987-8fe45f3ffe21
01/23/2025 20:32:12:INFO:Received: evaluate message c88b4ac7-378e-405f-8987-8fe45f3ffe21
[92mINFO [0m:      Sent reply
01/23/2025 20:36:23:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:36:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:36:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 57bd4846-d10f-4649-9911-30ad59ae1cd0
01/23/2025 20:36:56:INFO:Received: train message 57bd4846-d10f-4649-9911-30ad59ae1cd0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:45:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:56:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:56:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bf7b1195-cfb5-4d43-9030-909f67d046bf
01/23/2025 20:56:05:INFO:Received: evaluate message bf7b1195-cfb5-4d43-9030-909f67d046bf

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 2.009547784924507
Epsilon = 1.00 and Loss = 2.69

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 1.786264697710673
Epsilon = 1.00 and Loss = 1.86

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 1.5629816104968388
Epsilon = 1.00 and Loss = 2.44

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 1.3396985232830048
Epsilon = 1.00 and Loss = 1.64
[92mINFO [0m:      Sent reply
01/23/2025 21:00:12:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:00:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:00:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 42aebc26-00ed-450f-8a85-b676cf5273f0
01/23/2025 21:00:41:INFO:Received: train message 42aebc26-00ed-450f-8a85-b676cf5273f0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:09:34:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:20:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:20:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6ae128e6-5e46-4575-8c64-0bf53c641b2d
01/23/2025 21:20:01:INFO:Received: evaluate message 6ae128e6-5e46-4575-8c64-0bf53c641b2d
[92mINFO [0m:      Sent reply
01/23/2025 21:23:53:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:24:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:24:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b6f1781c-e424-4c2a-8cc6-ae8426de3554
01/23/2025 21:24:55:INFO:Received: train message b6f1781c-e424-4c2a-8cc6-ae8426de3554
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:33:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:44:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:44:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d5934c8c-4bca-431f-beea-d60d35085f19
01/23/2025 21:44:29:INFO:Received: evaluate message d5934c8c-4bca-431f-beea-d60d35085f19
[92mINFO [0m:      Sent reply
01/23/2025 21:48:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:48:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:48:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 494a170f-35d5-4fd0-b473-669fd51651b8
01/23/2025 21:48:48:INFO:Received: train message 494a170f-35d5-4fd0-b473-669fd51651b8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:56:58:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:08:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:08:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f80c0084-b418-419d-b680-4ca131c32e39
01/23/2025 22:08:25:INFO:Received: evaluate message f80c0084-b418-419d-b680-4ca131c32e39
[92mINFO [0m:      Sent reply
01/23/2025 22:12:44:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:13:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:13:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 118ea5da-28ef-47dd-b384-0e9ce93baabc
01/23/2025 22:13:09:INFO:Received: train message 118ea5da-28ef-47dd-b384-0e9ce93baabc

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 1.1164154360691707
Epsilon = 1.00 and Loss = 2.12

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 0.8931323488553367
Epsilon = 1.00 and Loss = 1.83

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 0.6698492616415023
Epsilon = 1.00 and Loss = 2.07

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:21:47:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:32:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:32:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dee44605-fc2b-463d-acf6-8a2340c69209
01/23/2025 22:32:26:INFO:Received: evaluate message dee44605-fc2b-463d-acf6-8a2340c69209
[92mINFO [0m:      Sent reply
01/23/2025 22:36:30:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:36:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:36:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message aa913f8d-4ebb-488b-abba-e9d06a3fd407
01/23/2025 22:36:54:INFO:Received: train message aa913f8d-4ebb-488b-abba-e9d06a3fd407
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:45:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:56:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:56:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e07db135-a8df-47a9-b0ba-9e79135e7397
01/23/2025 22:56:24:INFO:Received: evaluate message e07db135-a8df-47a9-b0ba-9e79135e7397
[92mINFO [0m:      Sent reply
01/23/2025 23:00:33:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:01:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:01:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c56daad0-cc5f-4f44-83e6-bec0f15524df
01/23/2025 23:01:11:INFO:Received: train message c56daad0-cc5f-4f44-83e6-bec0f15524df
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 23:10:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:20:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:20:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6f8fd47d-cf7c-4472-acc7-f370aa48593f
01/23/2025 23:20:33:INFO:Received: evaluate message 6f8fd47d-cf7c-4472-acc7-f370aa48593f
[92mINFO [0m:      Sent reply
01/23/2025 23:24:49:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:25:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:25:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 9f6e3044-e141-4d62-8327-e2c2f9982354
01/23/2025 23:25:00:INFO:Received: reconnect message 9f6e3044-e141-4d62-8327-e2c2f9982354
01/23/2025 23:25:00:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/23/2025 23:25:00:INFO:Disconnect and shut down
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 0.44656617442766816
Epsilon = 1.00 and Loss = 1.86

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 0.22328308721383408
Epsilon = 1.00 and Loss = 1.97

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142]}

Base Noise Multiplier Received:  2.109375
Data Scaling Factor: 14.018262313226343 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [4.055187225341797, 8.802675247192383, 3.3082034587860107, 1.3586992025375366, 5.835646152496338, 0.746344268321991, 0.409929096698761, 2.2772858142852783]
Noise Multiplier after list and tensor:  3.349246308207512
Noise Multiplier after Epsilon Scaling:  3.349246308207512
Noise Multiplier after Convergence: 0.0
Epsilon = 1.00 and Loss = 1.94

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995, 116.7615122795105], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315, 0.5368505839710028], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142, 0.7311505973177221]}



Final client history:
{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995, 116.7615122795105], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315, 0.5368505839710028], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142, 0.7311505973177221]}

