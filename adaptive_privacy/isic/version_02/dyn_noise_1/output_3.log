nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/23/2025 11:28:16:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/23/2025 11:28:16:DEBUG:ChannelConnectivity.IDLE
01/23/2025 11:28:16:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/23/2025 11:31:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:31:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4c74cf0b-d0ff-4216-a385-7b804333574a
01/23/2025 11:31:07:INFO:Received: train message 4c74cf0b-d0ff-4216-a385-7b804333574a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:43:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:50:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:50:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message df633a80-7965-4122-b5fa-a0d080e4b2bc
01/23/2025 11:50:35:INFO:Received: evaluate message df633a80-7965-4122-b5fa-a0d080e4b2bc
[92mINFO [0m:      Sent reply
01/23/2025 11:54:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:55:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:55:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 63c7a90b-d3ab-48b1-a6af-a3e6531dcab6
01/23/2025 11:55:14:INFO:Received: train message 63c7a90b-d3ab-48b1-a6af-a3e6531dcab6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:06:55:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:14:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:14:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message db4c53fc-50dd-407d-a38c-9cb9afcaa8bf
01/23/2025 12:14:18:INFO:Received: evaluate message db4c53fc-50dd-407d-a38c-9cb9afcaa8bf
[92mINFO [0m:      Sent reply
01/23/2025 12:18:23:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:18:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:18:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cb5a8fde-cb8d-4a3d-95e5-6f3b8f511a67
01/23/2025 12:18:50:INFO:Received: train message cb5a8fde-cb8d-4a3d-95e5-6f3b8f511a67
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:30:31:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:38:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:38:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 72998e29-f8f1-418c-a04d-265ca01a763b
01/23/2025 12:38:02:INFO:Received: evaluate message 72998e29-f8f1-418c-a04d-265ca01a763b
[92mINFO [0m:      Sent reply
01/23/2025 12:42:01:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:42:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:42:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 64361304-091c-4374-bb08-7c180d7e2a18
01/23/2025 12:42:14:INFO:Received: train message 64361304-091c-4374-bb08-7c180d7e2a18
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:53:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:01:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:01:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message af2eb75b-a117-4b8f-b6de-7dc910b482e8
01/23/2025 13:01:23:INFO:Received: evaluate message af2eb75b-a117-4b8f-b6de-7dc910b482e8
[92mINFO [0m:      Sent reply
01/23/2025 13:04:56:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:06:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:06:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 70aeb5f7-82bb-4341-a488-93219a995751
01/23/2025 13:06:07:INFO:Received: train message 70aeb5f7-82bb-4341-a488-93219a995751
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:18:07:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:25:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:25:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e5559bc1-71c9-45bb-9cae-0f024266db2f
01/23/2025 13:25:16:INFO:Received: evaluate message e5559bc1-71c9-45bb-9cae-0f024266db2f
[92mINFO [0m:      Sent reply
01/23/2025 13:29:02:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:30:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:30:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5cbc39ad-4a54-4552-9ec9-a3ef5bf582c2
01/23/2025 13:30:00:INFO:Received: train message 5cbc39ad-4a54-4552-9ec9-a3ef5bf582c2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:42:03:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:49:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:49:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b6e7c932-46e8-4b1e-b47d-c15f53b30fce
01/23/2025 13:49:04:INFO:Received: evaluate message b6e7c932-46e8-4b1e-b47d-c15f53b30fce
[92mINFO [0m:      Sent reply
01/23/2025 13:53:08:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:53:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:53:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a962adc7-ee52-46a4-9199-57edafbabb56
01/23/2025 13:53:50:INFO:Received: train message a962adc7-ee52-46a4-9199-57edafbabb56
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:05:23:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:12:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:12:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5e6699ca-a250-466c-9544-4ac1b7041a74
01/23/2025 14:12:57:INFO:Received: evaluate message 5e6699ca-a250-466c-9544-4ac1b7041a74
[92mINFO [0m:      Sent reply
01/23/2025 14:17:02:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:17:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:17:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8a0a1b77-9d68-49a0-a6d2-aa1e37d1e6ca
01/23/2025 14:17:32:INFO:Received: train message 8a0a1b77-9d68-49a0-a6d2-aa1e37d1e6ca
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:29:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:36:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:36:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b46bab05-5d4b-4dd0-a97f-844378d250af
01/23/2025 14:36:37:INFO:Received: evaluate message b46bab05-5d4b-4dd0-a97f-844378d250af
[92mINFO [0m:      Sent reply
01/23/2025 14:40:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:40:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:40:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8d9f3f03-6492-4f4f-9aaa-beac20a47f81
01/23/2025 14:40:53:INFO:Received: train message 8d9f3f03-6492-4f4f-9aaa-beac20a47f81
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:52:22:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:00:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:00:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 15a615a2-256d-4c2e-b8ac-f112e3d92cca
01/23/2025 15:00:17:INFO:Received: evaluate message 15a615a2-256d-4c2e-b8ac-f112e3d92cca
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 1.77

{'loss': [137.2596139907837], 'accuracy': [0.3383004430124849], 'auc': [0.5868727978587345]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 1.56

{'loss': [137.2596139907837, 133.6919516324997], 'accuracy': [0.3383004430124849, 0.3407168747482884], 'auc': [0.5868727978587345, 0.6194824557606924]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 1.52

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 1.90

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 1.71

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 1.51

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 0.98

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 1.95

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 1.82
[92mINFO [0m:      Sent reply
01/23/2025 15:04:21:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:04:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:04:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e0a302b6-a681-4d91-9711-9f3a46f160ff
01/23/2025 15:04:52:INFO:Received: train message e0a302b6-a681-4d91-9711-9f3a46f160ff
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:16:38:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:24:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:24:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 929ba56a-efd1-4b97-9325-dfaf70a3cbf3
01/23/2025 15:24:06:INFO:Received: evaluate message 929ba56a-efd1-4b97-9325-dfaf70a3cbf3
[92mINFO [0m:      Sent reply
01/23/2025 15:28:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:28:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:28:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3deb63d2-5dbd-486d-a515-0706725031e1
01/23/2025 15:28:47:INFO:Received: train message 3deb63d2-5dbd-486d-a515-0706725031e1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:40:41:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:48:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:48:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8428d1fd-6697-4f28-998d-bd08890fcc3e
01/23/2025 15:48:03:INFO:Received: evaluate message 8428d1fd-6697-4f28-998d-bd08890fcc3e
[92mINFO [0m:      Sent reply
01/23/2025 15:52:01:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:52:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:52:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message af16c807-4201-4945-a767-d9abf8cde4b7
01/23/2025 15:52:24:INFO:Received: train message af16c807-4201-4945-a767-d9abf8cde4b7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:04:20:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:11:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:11:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7af5ba5c-148c-44b4-9dbd-08376c155d4a
01/23/2025 16:11:32:INFO:Received: evaluate message 7af5ba5c-148c-44b4-9dbd-08376c155d4a
[92mINFO [0m:      Sent reply
01/23/2025 16:15:30:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:16:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:16:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 869d8302-b3d7-4b14-ba73-ed91174f3450
01/23/2025 16:16:00:INFO:Received: train message 869d8302-b3d7-4b14-ba73-ed91174f3450
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:27:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:35:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:35:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1fd38cd5-684b-4f3d-b8c1-70718254f68c
01/23/2025 16:35:05:INFO:Received: evaluate message 1fd38cd5-684b-4f3d-b8c1-70718254f68c
[92mINFO [0m:      Sent reply
01/23/2025 16:39:04:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:39:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:39:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f37837be-219b-4d9b-92a8-445e8adc0a93
01/23/2025 16:39:29:INFO:Received: train message f37837be-219b-4d9b-92a8-445e8adc0a93
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:51:19:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:58:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:58:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2d047756-789d-4088-aea9-5af6826eca90
01/23/2025 16:58:40:INFO:Received: evaluate message 2d047756-789d-4088-aea9-5af6826eca90
[92mINFO [0m:      Sent reply
01/23/2025 17:02:48:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:03:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:03:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0ed86eab-7b76-4b25-a238-f0d853369460
01/23/2025 17:03:13:INFO:Received: train message 0ed86eab-7b76-4b25-a238-f0d853369460
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:15:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:22:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:22:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 64d30c73-abdd-4cea-8a58-65282a0b0a3c
01/23/2025 17:22:34:INFO:Received: evaluate message 64d30c73-abdd-4cea-8a58-65282a0b0a3c

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 1.55

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 1.96

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 2.04

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 2.00

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 1.45

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.4693489857017994
Epsilon = 1.00 and Loss = 1.98
[92mINFO [0m:      Sent reply
01/23/2025 17:26:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:26:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:26:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8dee5c53-0897-4d7b-960a-f4498867e6cd
01/23/2025 17:26:55:INFO:Received: train message 8dee5c53-0897-4d7b-960a-f4498867e6cd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:38:52:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:46:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:46:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8ddc73a9-b66e-42b1-80d1-2459a18a56ab
01/23/2025 17:46:00:INFO:Received: evaluate message 8ddc73a9-b66e-42b1-80d1-2459a18a56ab
[92mINFO [0m:      Sent reply
01/23/2025 17:49:42:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:50:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:50:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fee513a8-7829-4729-a6a0-6f69d3c81464
01/23/2025 17:50:45:INFO:Received: train message fee513a8-7829-4729-a6a0-6f69d3c81464
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:02:31:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:09:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:09:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 55e464f3-dec4-4c88-b7a1-ddce6787adae
01/23/2025 18:09:57:INFO:Received: evaluate message 55e464f3-dec4-4c88-b7a1-ddce6787adae
[92mINFO [0m:      Sent reply
01/23/2025 18:14:02:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:14:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:14:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a3c3d01d-ffa1-40a3-98ae-db74846417d0
01/23/2025 18:14:19:INFO:Received: train message a3c3d01d-ffa1-40a3-98ae-db74846417d0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:25:58:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:33:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:33:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5b497c4a-15b6-48b8-9f4b-da6999cc242c
01/23/2025 18:33:41:INFO:Received: evaluate message 5b497c4a-15b6-48b8-9f4b-da6999cc242c
[92mINFO [0m:      Sent reply
01/23/2025 18:37:43:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:38:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:38:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 14d99206-48ca-474c-b00e-d695b7300caa
01/23/2025 18:38:13:INFO:Received: train message 14d99206-48ca-474c-b00e-d695b7300caa
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:49:55:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:57:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:57:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 317faace-a6ac-4af8-9de9-4123758d6df4
01/23/2025 18:57:20:INFO:Received: evaluate message 317faace-a6ac-4af8-9de9-4123758d6df4
[92mINFO [0m:      Sent reply
01/23/2025 19:01:21:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:01:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:01:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c62bdd5c-74cd-4962-9cf5-8ef7fa7c133e
01/23/2025 19:01:51:INFO:Received: train message c62bdd5c-74cd-4962-9cf5-8ef7fa7c133e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:13:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:21:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:21:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7c1e3415-ddcc-418d-94c9-11771d46807e
01/23/2025 19:21:04:INFO:Received: evaluate message 7c1e3415-ddcc-418d-94c9-11771d46807e

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.304725719988346
Epsilon = 1.00 and Loss = 1.49

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 2.140102454274893
Epsilon = 1.00 and Loss = 1.76

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 1.9754791885614396
Epsilon = 1.00 and Loss = 1.52

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 1.8108559228479864
Epsilon = 1.00 and Loss = 1.42

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 1.6462326571345332
Epsilon = 1.00 and Loss = 1.55
[92mINFO [0m:      Sent reply
01/23/2025 19:25:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:25:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:25:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 82e66be9-ceb6-4c79-b239-dac7076ce24b
01/23/2025 19:25:44:INFO:Received: train message 82e66be9-ceb6-4c79-b239-dac7076ce24b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:37:23:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:44:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:44:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 06720c59-073b-471d-9743-d13a8baffed2
01/23/2025 19:44:50:INFO:Received: evaluate message 06720c59-073b-471d-9743-d13a8baffed2
[92mINFO [0m:      Sent reply
01/23/2025 19:48:48:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:49:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:49:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 338bd621-bfa1-44d5-afdd-979e52a57564
01/23/2025 19:49:11:INFO:Received: train message 338bd621-bfa1-44d5-afdd-979e52a57564
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:00:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:08:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:08:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 91c4aede-6ed8-4f6f-9421-eb907f898a66
01/23/2025 20:08:28:INFO:Received: evaluate message 91c4aede-6ed8-4f6f-9421-eb907f898a66
[92mINFO [0m:      Sent reply
01/23/2025 20:12:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:12:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:12:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fda1d243-bc94-4cf5-8390-7dd33ece9d3c
01/23/2025 20:12:53:INFO:Received: train message fda1d243-bc94-4cf5-8390-7dd33ece9d3c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:24:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:32:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:32:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 92a0e07c-7bb3-4823-abca-fcd00388d3ae
01/23/2025 20:32:12:INFO:Received: evaluate message 92a0e07c-7bb3-4823-abca-fcd00388d3ae
[92mINFO [0m:      Sent reply
01/23/2025 20:36:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:36:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:36:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 02c3201b-a925-4aac-a9a8-9dc8b9da5c62
01/23/2025 20:36:56:INFO:Received: train message 02c3201b-a925-4aac-a9a8-9dc8b9da5c62
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:48:53:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:56:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:56:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7e341931-7feb-4084-adc2-9839338d50f2
01/23/2025 20:56:13:INFO:Received: evaluate message 7e341931-7feb-4084-adc2-9839338d50f2

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 1.4816093914210795
Epsilon = 1.00 and Loss = 1.73

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 1.3169861257076263
Epsilon = 1.00 and Loss = 1.41

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 1.152362859994173
Epsilon = 1.00 and Loss = 1.62

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 0.9877395942807198
Epsilon = 1.00 and Loss = 1.73
[92mINFO [0m:      Sent reply
01/23/2025 21:00:20:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:00:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:00:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6df4f74e-0dfd-45f2-b405-5d4d42ccc5c7
01/23/2025 21:00:51:INFO:Received: train message 6df4f74e-0dfd-45f2-b405-5d4d42ccc5c7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:12:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:20:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:20:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e1e0d6ea-0990-4b9b-9eff-bc6bc3d1b985
01/23/2025 21:20:20:INFO:Received: evaluate message e1e0d6ea-0990-4b9b-9eff-bc6bc3d1b985
[92mINFO [0m:      Sent reply
01/23/2025 21:24:21:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:24:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:24:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f1cb4f32-26f5-4fb5-86d2-f5569f5f51dc
01/23/2025 21:24:36:INFO:Received: train message f1cb4f32-26f5-4fb5-86d2-f5569f5f51dc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:36:05:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:44:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:44:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dcc10a39-2559-4084-9754-37c83965c962
01/23/2025 21:44:29:INFO:Received: evaluate message dcc10a39-2559-4084-9754-37c83965c962
[92mINFO [0m:      Sent reply
01/23/2025 21:48:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:49:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:49:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b665c0a9-7b72-48b4-be29-1f93cb527161
01/23/2025 21:49:01:INFO:Received: train message b665c0a9-7b72-48b4-be29-1f93cb527161
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:00:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:08:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:08:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e13aa191-7bba-4c49-b199-3df6ba465fc9
01/23/2025 22:08:31:INFO:Received: evaluate message e13aa191-7bba-4c49-b199-3df6ba465fc9
[92mINFO [0m:      Sent reply
01/23/2025 22:12:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:13:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:13:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 660b39af-5250-4e88-b204-1c8d732e6035
01/23/2025 22:13:23:INFO:Received: train message 660b39af-5250-4e88-b204-1c8d732e6035

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 0.8231163285672666
Epsilon = 1.00 and Loss = 2.19

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 0.6584930628538133
Epsilon = 1.00 and Loss = 1.72

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 0.4938697971403598
Epsilon = 1.00 and Loss = 1.34

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:25:03:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:32:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:32:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 66ab67af-8ca2-4cca-802a-45b8c02a265c
01/23/2025 22:32:18:INFO:Received: evaluate message 66ab67af-8ca2-4cca-802a-45b8c02a265c
[92mINFO [0m:      Sent reply
01/23/2025 22:36:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:37:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:37:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4284d12f-ca68-44ce-98e1-018f41bacd0f
01/23/2025 22:37:07:INFO:Received: train message 4284d12f-ca68-44ce-98e1-018f41bacd0f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:48:47:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:56:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:56:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9daf6276-6709-4904-9b4a-21d2313191d3
01/23/2025 22:56:31:INFO:Received: evaluate message 9daf6276-6709-4904-9b4a-21d2313191d3
[92mINFO [0m:      Sent reply
01/23/2025 23:00:33:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:00:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:00:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message faa58226-e012-4fb8-a003-27e9cb12658c
01/23/2025 23:00:58:INFO:Received: train message faa58226-e012-4fb8-a003-27e9cb12658c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 23:12:18:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:20:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:20:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ad01446a-e330-4038-b62d-8369d1ba5809
01/23/2025 23:20:48:INFO:Received: evaluate message ad01446a-e330-4038-b62d-8369d1ba5809
[92mINFO [0m:      Sent reply
01/23/2025 23:24:56:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:25:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:25:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message bfef1114-026b-47d9-b8d0-5e911c8fe59b
01/23/2025 23:25:00:INFO:Received: reconnect message bfef1114-026b-47d9-b8d0-5e911c8fe59b
01/23/2025 23:25:01:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/23/2025 23:25:01:INFO:Disconnect and shut down
2.4693489857017994
Noise Multiplier after Convergence: 0.3292465314269065
Epsilon = 1.00 and Loss = 1.31

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 0.16462326571345326
Epsilon = 1.00 and Loss = 1.23

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142]}

Base Noise Multiplier Received:  2.5
Data Scaling Factor: 9.4132292827945 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [3.7964088916778564, 9.296133995056152, 1.5147727727890015, 0.5244988203048706, 2.885880947113037, 0.4831946790218353, 0.6087186336517334, 0.6451831459999084]
Noise Multiplier after list and tensor:  2.4693489857017994
Noise Multiplier after Epsilon Scaling:  2.4693489857017994
Noise Multiplier after Convergence: 0.0
Epsilon = 1.00 and Loss = 1.66

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995, 116.7615122795105], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315, 0.5368505839710028], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142, 0.7311505973177221]}



Final client history:
{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995, 116.7615122795105], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315, 0.5368505839710028], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142, 0.7311505973177221]}

