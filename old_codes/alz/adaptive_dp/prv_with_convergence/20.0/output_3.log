nohup: ignoring input
02/05/2025 10:04:49:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:04:49:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:04:49:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778689.193836 1878385 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:05:36:INFO:
[92mINFO [0m:      Received: train message e864dac6-ef25-4cf2-bf43-5154112217d5
02/05/2025 10:05:36:INFO:Received: train message e864dac6-ef25-4cf2-bf43-5154112217d5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:06:17:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:07:34:INFO:
[92mINFO [0m:      Received: evaluate message af91ed28-a9dc-4cef-a716-7acd8720a2b6
02/05/2025 10:07:34:INFO:Received: evaluate message af91ed28-a9dc-4cef-a716-7acd8720a2b6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:07:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:08:18:INFO:
[92mINFO [0m:      Received: train message a30d20d4-30f9-4a3c-82ea-fc7a77b30855
02/05/2025 10:08:18:INFO:Received: train message a30d20d4-30f9-4a3c-82ea-fc7a77b30855
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:08:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:38:INFO:
[92mINFO [0m:      Received: evaluate message 7975903c-e2c4-4d36-ab6c-b285e56a9137
02/05/2025 10:09:38:INFO:Received: evaluate message 7975903c-e2c4-4d36-ab6c-b285e56a9137
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:09:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:10:35:INFO:
[92mINFO [0m:      Received: train message c54c3b36-bd84-43ab-abb6-488651d6fffd
02/05/2025 10:10:35:INFO:Received: train message c54c3b36-bd84-43ab-abb6-488651d6fffd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:11:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:12:24:INFO:
[92mINFO [0m:      Received: evaluate message 16002869-00ce-40f0-8f1b-35dad0404684
02/05/2025 10:12:24:INFO:Received: evaluate message 16002869-00ce-40f0-8f1b-35dad0404684
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:12:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:01:INFO:
[92mINFO [0m:      Received: train message c6b8aa84-b60c-4107-a881-c592c8256702
02/05/2025 10:13:01:INFO:Received: train message c6b8aa84-b60c-4107-a881-c592c8256702
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:13:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:14:38:INFO:
[92mINFO [0m:      Received: evaluate message d69693bc-b665-4ce9-b254-d94c8e117eb2
02/05/2025 10:14:38:INFO:Received: evaluate message d69693bc-b665-4ce9-b254-d94c8e117eb2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:14:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:47:INFO:
[92mINFO [0m:      Received: train message 3cc269cc-1e89-4cf5-97c2-a79cfea629fb
02/05/2025 10:15:47:INFO:Received: train message 3cc269cc-1e89-4cf5-97c2-a79cfea629fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:16:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:17:10:INFO:
[92mINFO [0m:      Received: evaluate message 2f745d6d-2bcf-44d3-9b0a-9e42474fbb6d
02/05/2025 10:17:10:INFO:Received: evaluate message 2f745d6d-2bcf-44d3-9b0a-9e42474fbb6d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:17:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:17:53:INFO:
[92mINFO [0m:      Received: train message 09fbe871-529a-46eb-a475-01a644f33ab4
02/05/2025 10:17:53:INFO:Received: train message 09fbe871-529a-46eb-a475-01a644f33ab4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:18:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:20:59:INFO:
[92mINFO [0m:      Received: evaluate message ac82762b-988b-44f9-8565-adbf9d779d17
02/05/2025 10:20:59:INFO:Received: evaluate message ac82762b-988b-44f9-8565-adbf9d779d17
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:21:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:35:INFO:
[92mINFO [0m:      Received: train message 8480f7d3-be44-48e2-89f2-5b996523605d
02/05/2025 10:21:35:INFO:Received: train message 8480f7d3-be44-48e2-89f2-5b996523605d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:22:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:23:38:INFO:
[92mINFO [0m:      Received: evaluate message f395c9e6-8425-4999-bf5c-c4c83e1364f1
02/05/2025 10:23:38:INFO:Received: evaluate message f395c9e6-8425-4999-bf5c-c4c83e1364f1
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814], 'accuracy': [0.5175918686473807], 'auc': [0.714457107011283], 'precision': [0.4117264737633955], 'recall': [0.5175918686473807], 'f1': [0.42093829387716764]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771], 'accuracy': [0.5175918686473807, 0.5363565285379203], 'auc': [0.714457107011283, 0.7407919892413165], 'precision': [0.4117264737633955, 0.4429501750516807], 'recall': [0.5175918686473807, 0.5363565285379203], 'f1': [0.42093829387716764, 0.4824075932878562]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:23:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:23:INFO:
[92mINFO [0m:      Received: train message 3fb342c1-68a8-4a49-be84-f64963ba9a80
02/05/2025 10:24:23:INFO:Received: train message 3fb342c1-68a8-4a49-be84-f64963ba9a80
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:25:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:09:INFO:
[92mINFO [0m:      Received: evaluate message 1be3b566-ca46-4bc4-953a-5b6eda567777
02/05/2025 10:26:09:INFO:Received: evaluate message 1be3b566-ca46-4bc4-953a-5b6eda567777
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:26:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:52:INFO:
[92mINFO [0m:      Received: train message de7c4313-6684-4dd4-a943-0e245a36ec8d
02/05/2025 10:26:52:INFO:Received: train message de7c4313-6684-4dd4-a943-0e245a36ec8d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:27:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:28:40:INFO:
[92mINFO [0m:      Received: evaluate message e3879a8e-26c4-4b87-a58e-277ba2a95286
02/05/2025 10:28:40:INFO:Received: evaluate message e3879a8e-26c4-4b87-a58e-277ba2a95286
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:28:44:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:22:INFO:
[92mINFO [0m:      Received: train message 3864e602-fecd-458d-8ced-0b442e949d1a
02/05/2025 10:29:22:INFO:Received: train message 3864e602-fecd-458d-8ced-0b442e949d1a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:30:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:31:11:INFO:
[92mINFO [0m:      Received: evaluate message a563dbaf-0700-482d-a210-6a2e5b35620f
02/05/2025 10:31:11:INFO:Received: evaluate message a563dbaf-0700-482d-a210-6a2e5b35620f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:31:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:31:47:INFO:
[92mINFO [0m:      Received: train message 5e473bd0-e659-452b-a02b-57d1c8bb9018
02/05/2025 10:31:47:INFO:Received: train message 5e473bd0-e659-452b-a02b-57d1c8bb9018
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:32:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:33:29:INFO:
[92mINFO [0m:      Received: evaluate message b135bfef-de0e-4b21-b74b-79a9d4b31400
02/05/2025 10:33:29:INFO:Received: evaluate message b135bfef-de0e-4b21-b74b-79a9d4b31400
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:33:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:07:INFO:
[92mINFO [0m:      Received: train message 4b6fd899-adad-4773-a288-c05d03187c02
02/05/2025 10:34:07:INFO:Received: train message 4b6fd899-adad-4773-a288-c05d03187c02

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:34:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:36:11:INFO:
[92mINFO [0m:      Received: evaluate message 4f970d86-b677-4664-b2e4-4b9993a2dbc3
02/05/2025 10:36:11:INFO:Received: evaluate message 4f970d86-b677-4664-b2e4-4b9993a2dbc3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:36:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:36:51:INFO:
[92mINFO [0m:      Received: train message 839b7dfa-02e5-46b4-ac75-e73b6bfd6ffe
02/05/2025 10:36:51:INFO:Received: train message 839b7dfa-02e5-46b4-ac75-e73b6bfd6ffe
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:37:31:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:38:29:INFO:
[92mINFO [0m:      Received: evaluate message 2f44502b-5271-4d5d-9f0b-d63044ef92e9
02/05/2025 10:38:29:INFO:Received: evaluate message 2f44502b-5271-4d5d-9f0b-d63044ef92e9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:38:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:38:53:INFO:
[92mINFO [0m:      Received: train message b1c96a65-6914-4a69-bacb-cb760fce3e3f
02/05/2025 10:38:53:INFO:Received: train message b1c96a65-6914-4a69-bacb-cb760fce3e3f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:39:29:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:41:03:INFO:
[92mINFO [0m:      Received: evaluate message e9f75a0f-52e4-410d-8115-adb09b5f561b
02/05/2025 10:41:03:INFO:Received: evaluate message e9f75a0f-52e4-410d-8115-adb09b5f561b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:41:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:41:27:INFO:
[92mINFO [0m:      Received: train message 1d3e295f-f10c-41a3-974a-24f4851b4cb1
02/05/2025 10:41:27:INFO:Received: train message 1d3e295f-f10c-41a3-974a-24f4851b4cb1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:42:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:34:INFO:
[92mINFO [0m:      Received: evaluate message 0a861067-a606-4ae8-939a-113f8395c047
02/05/2025 10:43:34:INFO:Received: evaluate message 0a861067-a606-4ae8-939a-113f8395c047
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:43:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:57:INFO:
[92mINFO [0m:      Received: train message c5a396bd-9cc8-4994-9320-e81e0644106a
02/05/2025 10:43:57:INFO:Received: train message c5a396bd-9cc8-4994-9320-e81e0644106a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:44:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:46:06:INFO:
[92mINFO [0m:      Received: evaluate message bdda431e-4b8d-405a-901f-966c97f9b1ef
02/05/2025 10:46:06:INFO:Received: evaluate message bdda431e-4b8d-405a-901f-966c97f9b1ef
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:46:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:46:39:INFO:
[92mINFO [0m:      Received: train message 1dfa280f-ca70-4e03-8c54-557c46171c6d
02/05/2025 10:46:39:INFO:Received: train message 1dfa280f-ca70-4e03-8c54-557c46171c6d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:47:22:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:49:05:INFO:
[92mINFO [0m:      Received: evaluate message 8463098a-bc49-4194-81f8-676cabda5ea5
02/05/2025 10:49:05:INFO:Received: evaluate message 8463098a-bc49-4194-81f8-676cabda5ea5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:49:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:49:45:INFO:
[92mINFO [0m:      Received: train message 659677dc-0647-4944-9681-f9ef037395e5
02/05/2025 10:49:45:INFO:Received: train message 659677dc-0647-4944-9681-f9ef037395e5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:50:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:52:09:INFO:
[92mINFO [0m:      Received: evaluate message 3a379b4b-6814-4b3a-aaa2-48198a3b34cc
02/05/2025 10:52:09:INFO:Received: evaluate message 3a379b4b-6814-4b3a-aaa2-48198a3b34cc

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:52:16:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:53:16:INFO:
[92mINFO [0m:      Received: train message 9b46b988-ad7b-4aa9-943d-64dc9824bb23
02/05/2025 10:53:16:INFO:Received: train message 9b46b988-ad7b-4aa9-943d-64dc9824bb23
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:54:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:55:26:INFO:
[92mINFO [0m:      Received: evaluate message bf2d5944-40bc-47b1-a896-e6c1298ba311
02/05/2025 10:55:26:INFO:Received: evaluate message bf2d5944-40bc-47b1-a896-e6c1298ba311
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:55:32:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:56:54:INFO:
[92mINFO [0m:      Received: train message 0546849a-eb87-4934-9623-0b2b983a87d2
02/05/2025 10:56:54:INFO:Received: train message 0546849a-eb87-4934-9623-0b2b983a87d2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:57:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:59:32:INFO:
[92mINFO [0m:      Received: evaluate message 0c1e728a-7b87-433f-bab4-b9ad561ebe3b
02/05/2025 10:59:32:INFO:Received: evaluate message 0c1e728a-7b87-433f-bab4-b9ad561ebe3b

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:59:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:00:48:INFO:
[92mINFO [0m:      Received: train message 63306c94-6823-42b3-99d7-854e6e895376
02/05/2025 11:00:48:INFO:Received: train message 63306c94-6823-42b3-99d7-854e6e895376
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:01:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:03:48:INFO:
[92mINFO [0m:      Received: evaluate message f287b027-2d22-4fcd-9afe-e85490541867
02/05/2025 11:03:48:INFO:Received: evaluate message f287b027-2d22-4fcd-9afe-e85490541867
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:03:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:58:INFO:
[92mINFO [0m:      Received: train message d156be5f-632e-4619-ab36-b3921c50e392
02/05/2025 11:04:58:INFO:Received: train message d156be5f-632e-4619-ab36-b3921c50e392
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:06:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:08:04:INFO:
[92mINFO [0m:      Received: evaluate message 00784d9e-95ce-44e8-8af0-8de82efb503a
02/05/2025 11:08:04:INFO:Received: evaluate message 00784d9e-95ce-44e8-8af0-8de82efb503a

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:09:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:10:11:INFO:
[92mINFO [0m:      Received: train message 89d71f33-f926-440a-868b-ccd60ef040a8
02/05/2025 11:10:11:INFO:Received: train message 89d71f33-f926-440a-868b-ccd60ef040a8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:11:29:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:12:43:INFO:
[92mINFO [0m:      Received: evaluate message 74f85df6-2844-4181-b2c7-5849163f788d
02/05/2025 11:12:43:INFO:Received: evaluate message 74f85df6-2844-4181-b2c7-5849163f788d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:12:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:14:09:INFO:
[92mINFO [0m:      Received: train message e8950447-b038-4058-8bed-b896ee7cb11e
02/05/2025 11:14:09:INFO:Received: train message e8950447-b038-4058-8bed-b896ee7cb11e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:15:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:17:03:INFO:
[92mINFO [0m:      Received: evaluate message 10e89e50-b547-4347-ba85-427f9ca36929
02/05/2025 11:17:03:INFO:Received: evaluate message 10e89e50-b547-4347-ba85-427f9ca36929

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:17:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:48:INFO:
[92mINFO [0m:      Received: train message 34e5d751-9fb9-454f-b05a-0b59ae31ce1f
02/05/2025 11:18:48:INFO:Received: train message 34e5d751-9fb9-454f-b05a-0b59ae31ce1f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:19:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:20:58:INFO:
[92mINFO [0m:      Received: evaluate message 832a6f44-77ad-4d74-afbd-aeca1a4034f8
02/05/2025 11:20:58:INFO:Received: evaluate message 832a6f44-77ad-4d74-afbd-aeca1a4034f8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:21:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:49:INFO:
[92mINFO [0m:      Received: train message 0b85a60f-afd9-4eb4-90bd-7abacfd457bf
02/05/2025 11:21:49:INFO:Received: train message 0b85a60f-afd9-4eb4-90bd-7abacfd457bf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:22:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:23:40:INFO:
[92mINFO [0m:      Received: evaluate message 0e5ae47c-89cd-4071-88bb-969a403a0838
02/05/2025 11:23:40:INFO:Received: evaluate message 0e5ae47c-89cd-4071-88bb-969a403a0838

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:23:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:24:30:INFO:
[92mINFO [0m:      Received: train message 4516cb1c-69fa-4ba1-9584-c0650ff54e76
02/05/2025 11:24:30:INFO:Received: train message 4516cb1c-69fa-4ba1-9584-c0650ff54e76
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:25:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:33:INFO:
[92mINFO [0m:      Received: evaluate message 4f9c7f3b-38b6-4785-b00e-6b6c3b3f514c
02/05/2025 11:26:33:INFO:Received: evaluate message 4f9c7f3b-38b6-4785-b00e-6b6c3b3f514c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:26:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:27:07:INFO:
[92mINFO [0m:      Received: train message fa7f819e-8827-4050-9b2a-72b9ca7d64cf
02/05/2025 11:27:07:INFO:Received: train message fa7f819e-8827-4050-9b2a-72b9ca7d64cf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:27:53:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:07:INFO:
[92mINFO [0m:      Received: evaluate message 6d680695-8fa0-467b-ad3b-ffc08cd27606
02/05/2025 11:29:07:INFO:Received: evaluate message 6d680695-8fa0-467b-ad3b-ffc08cd27606

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:29:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:30:08:INFO:
[92mINFO [0m:      Received: train message 88f4c22e-adbe-4ec0-940e-e913e9b350e4
02/05/2025 11:30:08:INFO:Received: train message 88f4c22e-adbe-4ec0-940e-e913e9b350e4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:30:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:31:44:INFO:
[92mINFO [0m:      Received: evaluate message 4da7925d-d3a0-492b-8490-1620be0922bb
02/05/2025 11:31:44:INFO:Received: evaluate message 4da7925d-d3a0-492b-8490-1620be0922bb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:31:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:32:54:INFO:
[92mINFO [0m:      Received: train message dec2082b-9a58-44ec-8bd1-791a079140a5
02/05/2025 11:32:54:INFO:Received: train message dec2082b-9a58-44ec-8bd1-791a079140a5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:33:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:34:40:INFO:
[92mINFO [0m:      Received: evaluate message 2e66771f-99fb-4c01-af16-ccf3ea9d6c58
02/05/2025 11:34:40:INFO:Received: evaluate message 2e66771f-99fb-4c01-af16-ccf3ea9d6c58

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609, 1.06856795481353], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411, 0.8014472007956828], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894, 0.5738852152204384], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604, 0.5213103198308118]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:34:44:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:34:55:INFO:
[92mINFO [0m:      Received: reconnect message 835e7702-b4d5-4290-aab7-3931831f0dad
02/05/2025 11:34:55:INFO:Received: reconnect message 835e7702-b4d5-4290-aab7-3931831f0dad
02/05/2025 11:34:55:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:34:55:INFO:Disconnect and shut down

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609, 1.06856795481353, 1.0267752834928512], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411, 0.8014472007956828, 0.8011279792333296], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894, 0.5738852152204384, 0.5912726894244645], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604, 0.5213103198308118, 0.5506124270879513]}



Final client history:
{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609, 1.06856795481353, 1.0267752834928512], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411, 0.8014472007956828, 0.8011279792333296], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894, 0.5738852152204384, 0.5912726894244645], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604, 0.5213103198308118, 0.5506124270879513]}

