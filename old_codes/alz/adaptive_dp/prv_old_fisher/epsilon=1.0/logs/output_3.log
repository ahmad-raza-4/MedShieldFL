nohup: ignoring input
01/27/2025 00:41:12:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 00:41:12:DEBUG:ChannelConnectivity.IDLE
01/27/2025 00:41:12:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 00:41:12:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 00:41:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:41:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message c5824ee6-ffec-4e9d-a318-554c5a2f8067
01/27/2025 00:41:12:INFO:Received: get_parameters message c5824ee6-ffec-4e9d-a318-554c5a2f8067
[92mINFO [0m:      Sent reply
01/27/2025 00:41:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:42:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:42:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a3245c67-6d07-4820-b408-c7339059d467
01/27/2025 00:42:08:INFO:Received: train message a3245c67-6d07-4820-b408-c7339059d467
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:42:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:43:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:43:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 82b743ed-9411-4589-9a3a-a7dcec873ad8
01/27/2025 00:43:41:INFO:Received: evaluate message 82b743ed-9411-4589-9a3a-a7dcec873ad8
[92mINFO [0m:      Sent reply
01/27/2025 00:43:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:44:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:44:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ec9624f0-d37a-47d9-804d-acd1a799fdc2
01/27/2025 00:44:01:INFO:Received: train message ec9624f0-d37a-47d9-804d-acd1a799fdc2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:44:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:45:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:45:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b918fa88-c247-4c54-a78b-2c2d10b780b1
01/27/2025 00:45:42:INFO:Received: evaluate message b918fa88-c247-4c54-a78b-2c2d10b780b1
[92mINFO [0m:      Sent reply
01/27/2025 00:45:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:46:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:46:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f85a5100-1e90-4b77-9432-b27f1cda7d83
01/27/2025 00:46:23:INFO:Received: train message f85a5100-1e90-4b77-9432-b27f1cda7d83
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:46:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:47:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:47:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 852e545f-b447-456b-a544-f5d16af71952
01/27/2025 00:47:18:INFO:Received: evaluate message 852e545f-b447-456b-a544-f5d16af71952
[92mINFO [0m:      Sent reply
01/27/2025 00:47:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:47:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:47:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6b56c80a-9936-4a47-b436-73e42c0f04d4
01/27/2025 00:47:55:INFO:Received: train message 6b56c80a-9936-4a47-b436-73e42c0f04d4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:48:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:49:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:49:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fdd59f8b-69d1-4133-9046-414a98b0bea0
01/27/2025 00:49:02:INFO:Received: evaluate message fdd59f8b-69d1-4133-9046-414a98b0bea0
[92mINFO [0m:      Sent reply
01/27/2025 00:49:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:49:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:49:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8284141d-17b5-47cb-8837-e7225066e161
01/27/2025 00:49:19:INFO:Received: train message 8284141d-17b5-47cb-8837-e7225066e161
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:49:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:50:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:50:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6d84147c-7751-4c96-84ce-d7c58a994df2
01/27/2025 00:50:34:INFO:Received: evaluate message 6d84147c-7751-4c96-84ce-d7c58a994df2
[92mINFO [0m:      Sent reply
01/27/2025 00:50:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:50:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:50:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 78abed5f-980f-4d54-b235-fbbe34765c0a
01/27/2025 00:50:59:INFO:Received: train message 78abed5f-980f-4d54-b235-fbbe34765c0a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:51:09:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:52:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:52:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aa98435f-7018-419f-808f-472aec8d8859
01/27/2025 00:52:19:INFO:Received: evaluate message aa98435f-7018-419f-808f-472aec8d8859
[92mINFO [0m:      Sent reply
01/27/2025 00:52:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:52:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:52:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message adb23869-227c-484a-800b-1e4fb100e6ee
01/27/2025 00:52:51:INFO:Received: train message adb23869-227c-484a-800b-1e4fb100e6ee
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:53:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:53:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:53:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bd186da8-0673-47c9-909d-44f6d1be2802
01/27/2025 00:53:54:INFO:Received: evaluate message bd186da8-0673-47c9-909d-44f6d1be2802
[92mINFO [0m:      Sent reply
01/27/2025 00:53:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:54:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:54:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1b17cdb9-30d0-471b-8ad2-dedd961f9b21
01/27/2025 00:54:14:INFO:Received: train message 1b17cdb9-30d0-471b-8ad2-dedd961f9b21
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406], 'accuracy': [0.5160281469898358], 'auc': [0.7097259125281268]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157], 'accuracy': [0.5160281469898358, 0.506645817044566], 'auc': [0.7097259125281268, 0.7279838714938965]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:54:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:55:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:55:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1f2b9562-7982-417d-9d16-29f7b894c3d0
01/27/2025 00:55:34:INFO:Received: evaluate message 1f2b9562-7982-417d-9d16-29f7b894c3d0
[92mINFO [0m:      Sent reply
01/27/2025 00:55:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:56:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:56:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 931682e9-bd97-49e8-b9a3-2bdc64b6b2a1
01/27/2025 00:56:16:INFO:Received: train message 931682e9-bd97-49e8-b9a3-2bdc64b6b2a1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:56:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:57:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:57:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dc4b8f5e-dc41-4060-a654-220f04ffe724
01/27/2025 00:57:06:INFO:Received: evaluate message dc4b8f5e-dc41-4060-a654-220f04ffe724
[92mINFO [0m:      Sent reply
01/27/2025 00:57:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:57:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:57:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0619bd8a-cabc-4e10-95d1-9810040ba477
01/27/2025 00:57:51:INFO:Received: train message 0619bd8a-cabc-4e10-95d1-9810040ba477
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:58:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:58:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:58:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 24a097c2-85c5-498c-8d06-6da2f27d4e33
01/27/2025 00:58:35:INFO:Received: evaluate message 24a097c2-85c5-498c-8d06-6da2f27d4e33
[92mINFO [0m:      Sent reply
01/27/2025 00:58:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 00:59:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 00:59:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 33abc8b4-4592-4b83-85ff-4c369ca8d94e
01/27/2025 00:59:23:INFO:Received: train message 33abc8b4-4592-4b83-85ff-4c369ca8d94e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 00:59:37:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:00:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:00:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9f53cade-7c97-4c89-a57f-27750a354c72
01/27/2025 01:00:15:INFO:Received: evaluate message 9f53cade-7c97-4c89-a57f-27750a354c72
[92mINFO [0m:      Sent reply
01/27/2025 01:00:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:00:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:00:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 76b98f14-9a72-4413-8e54-d3d126f48c74
01/27/2025 01:00:51:INFO:Received: train message 76b98f14-9a72-4413-8e54-d3d126f48c74
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:01:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:01:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:01:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8d9d167c-7159-472d-84e4-259e7bac0fa8
01/27/2025 01:01:36:INFO:Received: evaluate message 8d9d167c-7159-472d-84e4-259e7bac0fa8
[92mINFO [0m:      Sent reply
01/27/2025 01:01:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:02:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:02:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 658ec310-6810-47b0-920d-eec8266b3c9b
01/27/2025 01:02:19:INFO:Received: train message 658ec310-6810-47b0-920d-eec8266b3c9b
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:02:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:03:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:03:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bd2d6ddc-49a9-4377-a3f4-53532597563d
01/27/2025 01:03:03:INFO:Received: evaluate message bd2d6ddc-49a9-4377-a3f4-53532597563d
[92mINFO [0m:      Sent reply
01/27/2025 01:03:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:03:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:03:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 29d0d4ba-7567-4f44-a4ca-9bb01818765f
01/27/2025 01:03:48:INFO:Received: train message 29d0d4ba-7567-4f44-a4ca-9bb01818765f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:03:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:04:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:04:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bd56686f-3420-4a81-8baa-6256bd51648f
01/27/2025 01:04:29:INFO:Received: evaluate message bd56686f-3420-4a81-8baa-6256bd51648f
[92mINFO [0m:      Sent reply
01/27/2025 01:04:31:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:04:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:04:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message af16d875-1dc4-4305-8eed-f67e725aa2c7
01/27/2025 01:04:41:INFO:Received: train message af16d875-1dc4-4305-8eed-f67e725aa2c7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:04:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:05:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:05:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 82615243-60fd-4fb3-b75e-b9ea6b465f88
01/27/2025 01:05:46:INFO:Received: evaluate message 82615243-60fd-4fb3-b75e-b9ea6b465f88
[92mINFO [0m:      Sent reply
01/27/2025 01:05:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:06:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:06:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 93a3cb8b-e806-4901-b0c6-224d90fca973
01/27/2025 01:06:16:INFO:Received: train message 93a3cb8b-e806-4901-b0c6-224d90fca973
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:06:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:07:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:07:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8f0c723e-df65-490b-9629-84887c7f01c9
01/27/2025 01:07:15:INFO:Received: evaluate message 8f0c723e-df65-490b-9629-84887c7f01c9
[92mINFO [0m:      Sent reply
01/27/2025 01:07:17:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:07:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:07:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d61aa8be-615e-4918-ab5e-fd2ecb95c01d
01/27/2025 01:07:57:INFO:Received: train message d61aa8be-615e-4918-ab5e-fd2ecb95c01d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:08:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:08:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:08:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6ddc2567-b820-44d0-ab8f-b7235209a5bc
01/27/2025 01:08:43:INFO:Received: evaluate message 6ddc2567-b820-44d0-ab8f-b7235209a5bc
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:08:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:09:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:09:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4a41c460-82fc-42e7-95a0-01f229815d06
01/27/2025 01:09:26:INFO:Received: train message 4a41c460-82fc-42e7-95a0-01f229815d06
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:09:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:10:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:10:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ce5ed3b4-ca0a-46e0-8059-3342fc87cea0
01/27/2025 01:10:16:INFO:Received: evaluate message ce5ed3b4-ca0a-46e0-8059-3342fc87cea0
[92mINFO [0m:      Sent reply
01/27/2025 01:10:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:10:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:10:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0eb21717-c7e9-4e78-b8fd-c45ee964442b
01/27/2025 01:10:45:INFO:Received: train message 0eb21717-c7e9-4e78-b8fd-c45ee964442b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:10:55:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:11:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:11:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d7a033a9-d392-4b05-b24b-ec3baf9ee433
01/27/2025 01:11:31:INFO:Received: evaluate message d7a033a9-d392-4b05-b24b-ec3baf9ee433
[92mINFO [0m:      Sent reply
01/27/2025 01:11:33:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:11:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:11:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8f407680-dd6b-4197-9a92-e0dce600644e
01/27/2025 01:11:58:INFO:Received: train message 8f407680-dd6b-4197-9a92-e0dce600644e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:12:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:13:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:13:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2d3afd1c-2f3b-4b01-ae67-aaff001befde
01/27/2025 01:13:40:INFO:Received: evaluate message 2d3afd1c-2f3b-4b01-ae67-aaff001befde
[92mINFO [0m:      Sent reply
01/27/2025 01:13:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:14:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:14:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f7caf62f-34c5-4b60-95aa-cc07c512ddd4
01/27/2025 01:14:32:INFO:Received: train message f7caf62f-34c5-4b60-95aa-cc07c512ddd4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:14:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:15:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:15:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5e403687-54bb-4bdd-b5cb-c2c323c673dd
01/27/2025 01:15:18:INFO:Received: evaluate message 5e403687-54bb-4bdd-b5cb-c2c323c673dd

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:15:19:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:15:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:15:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d589ecff-89c8-425f-b6d9-3d7f0d8925cb
01/27/2025 01:15:52:INFO:Received: train message d589ecff-89c8-425f-b6d9-3d7f0d8925cb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:16:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:16:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:16:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c0130a96-6c5d-4ffa-897f-46bb26e861c4
01/27/2025 01:16:32:INFO:Received: evaluate message c0130a96-6c5d-4ffa-897f-46bb26e861c4
[92mINFO [0m:      Sent reply
01/27/2025 01:16:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:17:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:17:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3f496b03-234c-4fc3-8f7b-24ba0e689e25
01/27/2025 01:17:09:INFO:Received: train message 3f496b03-234c-4fc3-8f7b-24ba0e689e25
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:17:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:18:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:18:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9a482b17-3ca1-44e4-a445-e45a6e25ee59
01/27/2025 01:18:00:INFO:Received: evaluate message 9a482b17-3ca1-44e4-a445-e45a6e25ee59
[92mINFO [0m:      Sent reply
01/27/2025 01:18:02:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:18:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:18:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e688280b-2d76-4f56-b814-d19ed59c13e3
01/27/2025 01:18:27:INFO:Received: train message e688280b-2d76-4f56-b814-d19ed59c13e3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:18:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:19:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:19:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message efda430b-1c57-46e6-8ad9-c965a9a5b355
01/27/2025 01:19:11:INFO:Received: evaluate message efda430b-1c57-46e6-8ad9-c965a9a5b355
[92mINFO [0m:      Sent reply
01/27/2025 01:19:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:19:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:19:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f1a5c6b9-de3b-431c-846c-6aee83a2cd66
01/27/2025 01:19:57:INFO:Received: train message f1a5c6b9-de3b-431c-846c-6aee83a2cd66

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:20:07:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:20:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:20:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a8861420-5abf-4a5b-9352-5dcbdfbf4da0
01/27/2025 01:20:48:INFO:Received: evaluate message a8861420-5abf-4a5b-9352-5dcbdfbf4da0
[92mINFO [0m:      Sent reply
01/27/2025 01:20:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:21:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:21:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a66a5853-d39d-481b-8cfd-4438275f0eec
01/27/2025 01:21:21:INFO:Received: train message a66a5853-d39d-481b-8cfd-4438275f0eec
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:21:30:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:22:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:22:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c9c8b845-e7c6-4dc6-a179-bc28c2dbeb19
01/27/2025 01:22:13:INFO:Received: evaluate message c9c8b845-e7c6-4dc6-a179-bc28c2dbeb19
[92mINFO [0m:      Sent reply
01/27/2025 01:22:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:22:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:22:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0d50c4ad-1452-4422-9173-78a0037a71b5
01/27/2025 01:22:25:INFO:Received: train message 0d50c4ad-1452-4422-9173-78a0037a71b5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:22:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:23:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:23:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7cc32a38-a6c8-455b-a5fd-91a711bf5a23
01/27/2025 01:23:36:INFO:Received: evaluate message 7cc32a38-a6c8-455b-a5fd-91a711bf5a23
[92mINFO [0m:      Sent reply
01/27/2025 01:23:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:24:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:24:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d86cdfe2-2370-4b76-8856-67b0b5932438
01/27/2025 01:24:12:INFO:Received: train message d86cdfe2-2370-4b76-8856-67b0b5932438
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:24:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:25:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:25:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c4f22d40-0c10-4c7e-a5d3-5d5494129c14
01/27/2025 01:25:03:INFO:Received: evaluate message c4f22d40-0c10-4c7e-a5d3-5d5494129c14
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/27/2025 01:25:06:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:25:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:25:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1527ce37-9c8a-48fb-be62-2efee7504185
01/27/2025 01:25:34:INFO:Received: train message 1527ce37-9c8a-48fb-be62-2efee7504185
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:25:44:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:26:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:26:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 35eaaf71-cf0e-4ad5-ad00-698fd2212d09
01/27/2025 01:26:11:INFO:Received: evaluate message 35eaaf71-cf0e-4ad5-ad00-698fd2212d09
[92mINFO [0m:      Sent reply
01/27/2025 01:26:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:26:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:26:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 94deda69-99c3-4899-9b82-a315261df47a
01/27/2025 01:26:56:INFO:Received: train message 94deda69-99c3-4899-9b82-a315261df47a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 01:27:05:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:27:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:27:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 766cd424-d4d5-4a45-8359-04068ab3a096
01/27/2025 01:27:46:INFO:Received: evaluate message 766cd424-d4d5-4a45-8359-04068ab3a096
[92mINFO [0m:      Sent reply
01/27/2025 01:27:48:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 01:27:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 01:27:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message f00fc01e-e4eb-4f57-ae20-38f14ca7aa5c
01/27/2025 01:27:53:INFO:Received: reconnect message f00fc01e-e4eb-4f57-ae20-38f14ca7aa5c
01/27/2025 01:27:53:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/27/2025 01:27:53:INFO:Disconnect and shut down

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.333984375
Data Scaling Factor: 0.09203125 where Client Data Size: 589
Noise Multiplier after Fisher Scaling:  [0.049313921481370926, 0.01503819040954113, 0.09402269870042801, 0.017986468970775604]
Noise Multiplier after list and tensor:  0.04409031989052892
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 589, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695, 1.0624548865072982], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791, 0.5809225957779516], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168, 0.7911837794938563]}



Final client history:
{'loss': [1.0611879024028406, 1.162706578775157, 1.1313222250591692, 1.136262853598576, 1.1076660168366659, 1.100428231998381, 1.0958970649527608, 1.1248102733360035, 1.1048002171274085, 1.1181965500605675, 1.0492936816879135, 1.1033950619067505, 1.0755901108783517, 1.0728753176026273, 1.086773152599305, 1.0736395522800621, 1.1040426218668122, 1.082525068656647, 1.095809344578013, 1.1431053045487571, 1.087065784934911, 1.0710626358255173, 1.096062311397296, 1.0807578243530758, 1.0705676345362898, 1.1241889742783404, 1.0717769030764104, 1.063861373908078, 1.0340014355922695, 1.0624548865072982], 'accuracy': [0.5160281469898358, 0.506645817044566, 0.5207193119624707, 0.5324472243940579, 0.5340109460516028, 0.5402658326817826, 0.5433932759968726, 0.5355746677091477, 0.5504300234558248, 0.5433932759968726, 0.5598123534010946, 0.5512118842845973, 0.5590304925723222, 0.562157935887412, 0.5613760750586395, 0.5613760750586395, 0.5582486317435497, 0.5605942142298671, 0.5590304925723222, 0.5535574667709148, 0.5668491008600469, 0.5699765441751369, 0.5645035183737295, 0.5715402658326818, 0.5715402658326818, 0.5613760750586395, 0.5731039874902267, 0.5793588741204065, 0.5801407349491791, 0.5809225957779516], 'auc': [0.7097259125281268, 0.7279838714938965, 0.7378186912057271, 0.7445223681021075, 0.7499881378102775, 0.7542057000438297, 0.7582000593996208, 0.7609890379174776, 0.7636235081215835, 0.766128972959482, 0.7701997293662082, 0.7712283267308313, 0.7733383040594235, 0.7738913571335515, 0.7754539871744209, 0.7769745723607147, 0.7781970616529721, 0.7792542405702306, 0.7793191818649445, 0.7798566791434193, 0.7825472019616573, 0.7829999272562769, 0.7832839939234117, 0.7841454009443916, 0.7857045452281983, 0.7856270023859705, 0.7887194619961024, 0.789703908460549, 0.7897155122731168, 0.7911837794938563]}

