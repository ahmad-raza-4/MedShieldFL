nohup: ignoring input
01/26/2025 22:40:12:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/26/2025 22:40:12:DEBUG:ChannelConnectivity.IDLE
01/26/2025 22:40:12:DEBUG:ChannelConnectivity.CONNECTING
01/26/2025 22:40:12:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/26/2025 22:40:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:40:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bf7a7c6b-2597-474a-be7a-d9513f0b7966
01/26/2025 22:40:48:INFO:Received: train message bf7a7c6b-2597-474a-be7a-d9513f0b7966
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:41:21:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:42:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:42:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message efe10457-7991-4fad-94ef-d45e1d13f7e3
01/26/2025 22:42:08:INFO:Received: evaluate message efe10457-7991-4fad-94ef-d45e1d13f7e3
[92mINFO [0m:      Sent reply
01/26/2025 22:42:14:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:42:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:42:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message af9051ce-abd3-46de-a13d-964a7aafdd76
01/26/2025 22:42:48:INFO:Received: train message af9051ce-abd3-46de-a13d-964a7aafdd76
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:43:21:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:44:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:44:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ec92d56c-2df7-4c4d-afe3-ff479b37d51d
01/26/2025 22:44:03:INFO:Received: evaluate message ec92d56c-2df7-4c4d-afe3-ff479b37d51d
[92mINFO [0m:      Sent reply
01/26/2025 22:44:07:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:44:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:44:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 539c732b-36fd-4331-822e-8946ae6f1937
01/26/2025 22:44:37:INFO:Received: train message 539c732b-36fd-4331-822e-8946ae6f1937
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:45:05:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:46:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:46:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 47a5696b-6a9d-46ea-94eb-2c0b3b3ee325
01/26/2025 22:46:12:INFO:Received: evaluate message 47a5696b-6a9d-46ea-94eb-2c0b3b3ee325
[92mINFO [0m:      Sent reply
01/26/2025 22:46:17:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:46:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:46:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message acab8cd3-1f36-427d-ab92-99d36d6a13a3
01/26/2025 22:46:55:INFO:Received: train message acab8cd3-1f36-427d-ab92-99d36d6a13a3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:47:27:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:48:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:48:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 14eb6167-f5bd-43b2-b8d6-b7dc6ee43952
01/26/2025 22:48:11:INFO:Received: evaluate message 14eb6167-f5bd-43b2-b8d6-b7dc6ee43952
[92mINFO [0m:      Sent reply
01/26/2025 22:48:16:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:48:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:48:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message df0f2789-55a8-499a-9b00-cbeb38631242
01/26/2025 22:48:30:INFO:Received: train message df0f2789-55a8-499a-9b00-cbeb38631242
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:48:57:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:49:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:49:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 94707930-6cee-468d-84cb-780643324c1a
01/26/2025 22:49:58:INFO:Received: evaluate message 94707930-6cee-468d-84cb-780643324c1a
[92mINFO [0m:      Sent reply
01/26/2025 22:50:03:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:50:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:50:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3a7ed62d-174d-4ee5-b163-492acd2352cc
01/26/2025 22:50:47:INFO:Received: train message 3a7ed62d-174d-4ee5-b163-492acd2352cc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:51:22:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:51:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:51:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7a42d5ec-91be-4320-92d1-0ee087d7485c
01/26/2025 22:51:49:INFO:Received: evaluate message 7a42d5ec-91be-4320-92d1-0ee087d7485c
[92mINFO [0m:      Sent reply
01/26/2025 22:51:52:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:52:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:52:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 436ded93-e381-4e17-be2b-0516b0a807ec
01/26/2025 22:52:29:INFO:Received: train message 436ded93-e381-4e17-be2b-0516b0a807ec
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:52:59:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:54:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:54:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9ad960fd-5031-4714-8139-33a524ea012c
01/26/2025 22:54:20:INFO:Received: evaluate message 9ad960fd-5031-4714-8139-33a524ea012c
[92mINFO [0m:      Sent reply
01/26/2025 22:54:26:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:54:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:54:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d74a5fe7-138e-403c-b6ce-a52b0d6c28f3
01/26/2025 22:54:56:INFO:Received: train message d74a5fe7-138e-403c-b6ce-a52b0d6c28f3
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30, target_epsilon: 30, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544], 'accuracy': [0.5199374511336982], 'auc': [0.7254494785945842]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378], 'accuracy': [0.5199374511336982, 0.5238467552775606], 'auc': [0.7254494785945842, 0.7453794430136471]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653]}

/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:55:33:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:56:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:56:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7fb72a0f-f9c4-41a3-927d-44e1fde654f7
01/26/2025 22:56:39:INFO:Received: evaluate message 7fb72a0f-f9c4-41a3-927d-44e1fde654f7
[92mINFO [0m:      Sent reply
01/26/2025 22:56:43:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:57:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:57:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b78a7525-0d07-4764-9190-65f7c4a8a765
01/26/2025 22:57:22:INFO:Received: train message b78a7525-0d07-4764-9190-65f7c4a8a765
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:57:57:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:58:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:58:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d7012871-04d1-4899-a4e4-d3d409dfa040
01/26/2025 22:58:46:INFO:Received: evaluate message d7012871-04d1-4899-a4e4-d3d409dfa040
[92mINFO [0m:      Sent reply
01/26/2025 22:58:50:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:59:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:59:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9de2f009-9e43-4c74-9e4a-a1f78783401f
01/26/2025 22:59:36:INFO:Received: train message 9de2f009-9e43-4c74-9e4a-a1f78783401f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:00:10:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:00:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:00:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2d51e651-1b27-4375-a27e-60b5cd97ae7f
01/26/2025 23:00:45:INFO:Received: evaluate message 2d51e651-1b27-4375-a27e-60b5cd97ae7f
[92mINFO [0m:      Sent reply
01/26/2025 23:00:48:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:01:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:01:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 132379b4-498c-49d4-8cd5-a7096d30767c
01/26/2025 23:01:29:INFO:Received: train message 132379b4-498c-49d4-8cd5-a7096d30767c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:02:02:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:03:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:03:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6c9c956d-9db9-410c-b94d-30f3721481b8
01/26/2025 23:03:10:INFO:Received: evaluate message 6c9c956d-9db9-410c-b94d-30f3721481b8
[92mINFO [0m:      Sent reply
01/26/2025 23:03:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:03:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:03:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message df38c45a-9853-41db-b935-6f9c2f44ceb2
01/26/2025 23:03:58:INFO:Received: train message df38c45a-9853-41db-b935-6f9c2f44ceb2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:04:30:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:05:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:05:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f9a62c14-b8a7-4d2f-a8e1-217d713be809
01/26/2025 23:05:10:INFO:Received: evaluate message f9a62c14-b8a7-4d2f-a8e1-217d713be809
[92mINFO [0m:      Sent reply
01/26/2025 23:05:13:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:05:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:05:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 64b1cd8d-b784-424b-a453-2f3220079e2d
01/26/2025 23:05:59:INFO:Received: train message 64b1cd8d-b784-424b-a453-2f3220079e2d
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:06:31:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:07:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:07:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 109f515f-78b6-4fb6-a298-543134b56417
01/26/2025 23:07:09:INFO:Received: evaluate message 109f515f-78b6-4fb6-a298-543134b56417
[92mINFO [0m:      Sent reply
01/26/2025 23:07:12:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:07:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:07:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0a904163-436b-463e-87f2-49eac554d419
01/26/2025 23:07:50:INFO:Received: train message 0a904163-436b-463e-87f2-49eac554d419
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:08:20:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:09:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:09:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b9bb9062-3598-47b7-a0b8-112e078f27f3
01/26/2025 23:09:01:INFO:Received: evaluate message b9bb9062-3598-47b7-a0b8-112e078f27f3
[92mINFO [0m:      Sent reply
01/26/2025 23:09:05:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:09:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:09:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a89b14b8-9e28-45f0-8124-6e0c171a3c36
01/26/2025 23:09:52:INFO:Received: train message a89b14b8-9e28-45f0-8124-6e0c171a3c36
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:10:25:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:11:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:11:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5b767b90-87cf-4fe9-9568-3ffedcd31f33
01/26/2025 23:11:01:INFO:Received: evaluate message 5b767b90-87cf-4fe9-9568-3ffedcd31f33
[92mINFO [0m:      Sent reply
01/26/2025 23:11:05:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:11:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:11:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fce41e2b-c9cd-4e1f-8af5-ba96ddf58490
01/26/2025 23:11:33:INFO:Received: train message fce41e2b-c9cd-4e1f-8af5-ba96ddf58490
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:12:00:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:13:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:13:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fee1f382-0268-4bc5-b45c-69cbc85e5239
01/26/2025 23:13:07:INFO:Received: evaluate message fee1f382-0268-4bc5-b45c-69cbc85e5239
[92mINFO [0m:      Sent reply
01/26/2025 23:13:09:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:14:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:14:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 80a1ef6f-4bc0-4a1e-a552-3e7870b3a11f
01/26/2025 23:14:01:INFO:Received: train message 80a1ef6f-4bc0-4a1e-a552-3e7870b3a11f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:14:31:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:15:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:15:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 070e999a-1144-4ef4-98a4-78cd8937a7a8
01/26/2025 23:15:31:INFO:Received: evaluate message 070e999a-1144-4ef4-98a4-78cd8937a7a8
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:15:35:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:16:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:16:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8229e400-5706-4f43-9bfa-77817d57ab04
01/26/2025 23:16:14:INFO:Received: train message 8229e400-5706-4f43-9bfa-77817d57ab04
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:16:51:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:18:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:18:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 624bdef1-19d6-4adb-b9a4-9ddf35d22333
01/26/2025 23:18:01:INFO:Received: evaluate message 624bdef1-19d6-4adb-b9a4-9ddf35d22333
[92mINFO [0m:      Sent reply
01/26/2025 23:18:04:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:18:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:18:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a7c30775-5d93-4077-aa33-05242d5fb71b
01/26/2025 23:18:30:INFO:Received: train message a7c30775-5d93-4077-aa33-05242d5fb71b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:18:55:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:20:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:20:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e2474226-2895-474e-a101-97e0a9e47d4e
01/26/2025 23:20:27:INFO:Received: evaluate message e2474226-2895-474e-a101-97e0a9e47d4e
[92mINFO [0m:      Sent reply
01/26/2025 23:20:31:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:21:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:21:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 369f271d-7460-4fff-b8b8-48c84f930921
01/26/2025 23:21:01:INFO:Received: train message 369f271d-7460-4fff-b8b8-48c84f930921
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:21:31:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:22:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:22:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 69e85925-a3e0-4b78-8c26-af77eb84f3b7
01/26/2025 23:22:47:INFO:Received: evaluate message 69e85925-a3e0-4b78-8c26-af77eb84f3b7
[92mINFO [0m:      Sent reply
01/26/2025 23:22:53:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:23:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:23:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4b46800c-48c3-4ae2-9f3e-94d3eee122c0
01/26/2025 23:23:44:INFO:Received: train message 4b46800c-48c3-4ae2-9f3e-94d3eee122c0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:24:22:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:25:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:25:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a8761775-636d-4a95-b770-52599d96e6c7
01/26/2025 23:25:07:INFO:Received: evaluate message a8761775-636d-4a95-b770-52599d96e6c7

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:25:09:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:25:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:25:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1715a66e-c792-4a50-85fc-e82ca5d0f305
01/26/2025 23:25:51:INFO:Received: train message 1715a66e-c792-4a50-85fc-e82ca5d0f305
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:26:24:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:26:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:26:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0b5a40c8-81d0-4e2b-b7cd-9efe0ddef0f0
01/26/2025 23:26:48:INFO:Received: evaluate message 0b5a40c8-81d0-4e2b-b7cd-9efe0ddef0f0
[92mINFO [0m:      Sent reply
01/26/2025 23:26:50:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:27:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:27:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6cd9ed8d-5618-4566-896b-63f35d425d2e
01/26/2025 23:27:34:INFO:Received: train message 6cd9ed8d-5618-4566-896b-63f35d425d2e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:28:05:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:28:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:28:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 321254b3-4f25-4ee1-bbc6-f3f54453d369
01/26/2025 23:28:53:INFO:Received: evaluate message 321254b3-4f25-4ee1-bbc6-f3f54453d369
[92mINFO [0m:      Sent reply
01/26/2025 23:28:57:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:29:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:29:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5dccdf10-fac9-4385-ba3e-d628dbf9853c
01/26/2025 23:29:16:INFO:Received: train message 5dccdf10-fac9-4385-ba3e-d628dbf9853c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:29:44:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:30:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:30:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e6819723-2813-4c46-9f71-f8437c74041f
01/26/2025 23:30:48:INFO:Received: evaluate message e6819723-2813-4c46-9f71-f8437c74041f
[92mINFO [0m:      Sent reply
01/26/2025 23:30:52:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:31:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:31:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8b2791f0-2f26-4bee-9b1d-a62214603eb9
01/26/2025 23:31:14:INFO:Received: train message 8b2791f0-2f26-4bee-9b1d-a62214603eb9

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:31:45:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:32:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:32:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4e928edb-fe83-4091-8158-ff3056ac5383
01/26/2025 23:32:28:INFO:Received: evaluate message 4e928edb-fe83-4091-8158-ff3056ac5383
[92mINFO [0m:      Sent reply
01/26/2025 23:32:32:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:33:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:33:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b210b192-7bf3-4329-a1ec-e639f9f849b3
01/26/2025 23:33:14:INFO:Received: train message b210b192-7bf3-4329-a1ec-e639f9f849b3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:33:56:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:34:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:34:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d39a5fa7-b330-49ae-bd81-273dc8ff4e2f
01/26/2025 23:34:27:INFO:Received: evaluate message d39a5fa7-b330-49ae-bd81-273dc8ff4e2f
[92mINFO [0m:      Sent reply
01/26/2025 23:34:31:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:34:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:34:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 216054e8-3ca0-4302-ad7f-ac34fdad6387
01/26/2025 23:34:59:INFO:Received: train message 216054e8-3ca0-4302-ad7f-ac34fdad6387
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:35:29:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:36:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:36:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2ad6eb96-c6a0-4a04-a5c9-35da868b2251
01/26/2025 23:36:09:INFO:Received: evaluate message 2ad6eb96-c6a0-4a04-a5c9-35da868b2251
[92mINFO [0m:      Sent reply
01/26/2025 23:36:14:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:36:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:36:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0dc486ab-e129-4a7c-adf6-50075fd56aee
01/26/2025 23:36:52:INFO:Received: train message 0dc486ab-e129-4a7c-adf6-50075fd56aee
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:37:26:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:38:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:38:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 598be9aa-4ec8-44e8-a285-fd5842ca3db9
01/26/2025 23:38:03:INFO:Received: evaluate message 598be9aa-4ec8-44e8-a285-fd5842ca3db9
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:38:06:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:38:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:38:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 47599d59-9531-4cdb-ac5f-d7b7a3ee09c8
01/26/2025 23:38:36:INFO:Received: train message 47599d59-9531-4cdb-ac5f-d7b7a3ee09c8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:39:04:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:40:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:40:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 756b6079-b820-4bff-8c4d-63978e01a674
01/26/2025 23:40:16:INFO:Received: evaluate message 756b6079-b820-4bff-8c4d-63978e01a674
[92mINFO [0m:      Sent reply
01/26/2025 23:40:21:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:40:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:40:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 904499ba-8a69-466f-a3e4-96219d537360
01/26/2025 23:40:58:INFO:Received: train message 904499ba-8a69-466f-a3e4-96219d537360
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:41:33:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:42:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:42:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 49ef06c1-771e-4cc5-a1df-26617d49eff9
01/26/2025 23:42:19:INFO:Received: evaluate message 49ef06c1-771e-4cc5-a1df-26617d49eff9
[92mINFO [0m:      Sent reply
01/26/2025 23:42:22:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:42:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:42:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message b43d1bdb-0991-4710-916d-2a8dfbd219ce
01/26/2025 23:42:42:INFO:Received: reconnect message b43d1bdb-0991-4710-916d-2a8dfbd219ce
01/26/2025 23:42:42:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/26/2025 23:42:42:INFO:Disconnect and shut down

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.41473388671875
Data Scaling Factor: 0.17 where Client Data Size: 1088
Noise Multiplier after Fisher Scaling:  [0.0072173671796917915, 0.004477553069591522, 0.011578495614230633, 0.036820657551288605]
Noise Multiplier after list and tensor:  0.015023518353700638
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1088, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504, 1.0184915054580772], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836, 0.5918686473807663], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188, 0.8098659294281917]}



Final client history:
{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504, 1.0184915054580772], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836, 0.5918686473807663], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188, 0.8098659294281917]}

