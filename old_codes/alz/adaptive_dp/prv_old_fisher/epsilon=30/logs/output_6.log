nohup: ignoring input
01/26/2025 22:40:08:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/26/2025 22:40:08:DEBUG:ChannelConnectivity.IDLE
01/26/2025 22:40:08:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/26/2025 22:40:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:40:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6bd1948d-51b5-4a6b-8ce9-b40dfed4a2b3
01/26/2025 22:40:46:INFO:Received: train message 6bd1948d-51b5-4a6b-8ce9-b40dfed4a2b3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:41:14:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:42:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:42:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aa4699f3-f5b5-465f-ad0b-5f741d9a47d3
01/26/2025 22:42:08:INFO:Received: evaluate message aa4699f3-f5b5-465f-ad0b-5f741d9a47d3
[92mINFO [0m:      Sent reply
01/26/2025 22:42:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:42:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:42:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4b4dd515-0cbd-45e4-bab0-66a587e36df6
01/26/2025 22:42:38:INFO:Received: train message 4b4dd515-0cbd-45e4-bab0-66a587e36df6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:43:00:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:43:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:43:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b04a83ef-c177-47d1-b2cf-a9012b84e0e5
01/26/2025 22:43:58:INFO:Received: evaluate message b04a83ef-c177-47d1-b2cf-a9012b84e0e5
[92mINFO [0m:      Sent reply
01/26/2025 22:44:02:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:44:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:44:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a9d1e066-b05e-4d33-a45f-0416637e9c71
01/26/2025 22:44:37:INFO:Received: train message a9d1e066-b05e-4d33-a45f-0416637e9c71
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:45:00:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:46:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:46:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 62481d8c-de0a-43bd-80f7-11bfc68c08e5
01/26/2025 22:46:12:INFO:Received: evaluate message 62481d8c-de0a-43bd-80f7-11bfc68c08e5
[92mINFO [0m:      Sent reply
01/26/2025 22:46:17:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:46:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:46:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 90df8259-ce70-4f3a-9766-e7d5c0cddc53
01/26/2025 22:46:33:INFO:Received: train message 90df8259-ce70-4f3a-9766-e7d5c0cddc53
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:46:52:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:48:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:48:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4c17b2eb-ac18-4f54-b1da-dc64c8eef708
01/26/2025 22:48:09:INFO:Received: evaluate message 4c17b2eb-ac18-4f54-b1da-dc64c8eef708
[92mINFO [0m:      Sent reply
01/26/2025 22:48:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:48:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:48:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f788764d-2597-41ae-807f-670c20454de2
01/26/2025 22:48:48:INFO:Received: train message f788764d-2597-41ae-807f-670c20454de2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:49:10:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:50:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:50:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f0f8b11f-680f-4397-8a4e-f1005ea7eadd
01/26/2025 22:50:12:INFO:Received: evaluate message f0f8b11f-680f-4397-8a4e-f1005ea7eadd
[92mINFO [0m:      Sent reply
01/26/2025 22:50:16:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:50:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:50:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 50bc4deb-b049-4dcf-8537-dac150b770b3
01/26/2025 22:50:51:INFO:Received: train message 50bc4deb-b049-4dcf-8537-dac150b770b3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:51:18:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:52:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:52:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 05f6b389-6f3b-4383-b299-d040b133c1dd
01/26/2025 22:52:12:INFO:Received: evaluate message 05f6b389-6f3b-4383-b299-d040b133c1dd
[92mINFO [0m:      Sent reply
01/26/2025 22:52:18:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:52:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:52:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fb219286-0017-43d6-8803-87c22294f152
01/26/2025 22:52:49:INFO:Received: train message fb219286-0017-43d6-8803-87c22294f152
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:53:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:54:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:54:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 85b67875-bc10-41a7-8ce7-a4bf3d40578c
01/26/2025 22:54:23:INFO:Received: evaluate message 85b67875-bc10-41a7-8ce7-a4bf3d40578c
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30, target_epsilon: 30, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544], 'accuracy': [0.5199374511336982], 'auc': [0.7254494785945842]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378], 'accuracy': [0.5199374511336982, 0.5238467552775606], 'auc': [0.7254494785945842, 0.7453794430136471]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653]}
[92mINFO [0m:      Sent reply
01/26/2025 22:54:28:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:54:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:54:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1dd04014-a874-4e1a-8370-cccb4f96de7f
01/26/2025 22:54:51:INFO:Received: train message 1dd04014-a874-4e1a-8370-cccb4f96de7f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:55:18:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:56:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:56:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 19c73077-c72f-4d6d-bc43-06ae08229dcd
01/26/2025 22:56:23:INFO:Received: evaluate message 19c73077-c72f-4d6d-bc43-06ae08229dcd
[92mINFO [0m:      Sent reply
01/26/2025 22:56:26:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:56:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:56:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 55198958-9514-4fdc-9012-6aa5198d8176
01/26/2025 22:56:54:INFO:Received: train message 55198958-9514-4fdc-9012-6aa5198d8176
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:57:18:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:58:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:58:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c9fa2d49-0eb4-422e-b8fa-9765d6a60618
01/26/2025 22:58:52:INFO:Received: evaluate message c9fa2d49-0eb4-422e-b8fa-9765d6a60618
[92mINFO [0m:      Sent reply
01/26/2025 22:58:58:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 22:59:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 22:59:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e2004d98-7405-4f45-814a-60ec4afe4378
01/26/2025 22:59:30:INFO:Received: train message e2004d98-7405-4f45-814a-60ec4afe4378
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 22:59:57:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:01:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:01:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 198c6296-6417-433a-87c1-a8262fede375
01/26/2025 23:01:00:INFO:Received: evaluate message 198c6296-6417-433a-87c1-a8262fede375
[92mINFO [0m:      Sent reply
01/26/2025 23:01:04:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:01:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:01:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bb275034-dc56-4aee-8ed4-3a778c1e11e7
01/26/2025 23:01:33:INFO:Received: train message bb275034-dc56-4aee-8ed4-3a778c1e11e7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:02:01:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:03:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:03:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1489df02-6912-4437-9799-cffeb945a4d8
01/26/2025 23:03:10:INFO:Received: evaluate message 1489df02-6912-4437-9799-cffeb945a4d8
[92mINFO [0m:      Sent reply
01/26/2025 23:03:15:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:04:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:04:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8c19eec4-f169-4390-8b92-981669e40856
01/26/2025 23:04:00:INFO:Received: train message 8c19eec4-f169-4390-8b92-981669e40856
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:04:26:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:05:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:05:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3ac00051-1a54-437b-8158-ae0c673857c5
01/26/2025 23:05:16:INFO:Received: evaluate message 3ac00051-1a54-437b-8158-ae0c673857c5
[92mINFO [0m:      Sent reply
01/26/2025 23:05:21:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:05:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:05:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ff22a264-9eda-4757-ab64-d5c47ad9b0d0
01/26/2025 23:05:49:INFO:Received: train message ff22a264-9eda-4757-ab64-d5c47ad9b0d0

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:06:13:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:06:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:06:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 29f282a5-1f90-40d7-a543-bffb0f903ca0
01/26/2025 23:06:59:INFO:Received: evaluate message 29f282a5-1f90-40d7-a543-bffb0f903ca0
[92mINFO [0m:      Sent reply
01/26/2025 23:07:02:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:07:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:07:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eef51d8a-c880-4b04-846a-9a5b0fb47baf
01/26/2025 23:07:34:INFO:Received: train message eef51d8a-c880-4b04-846a-9a5b0fb47baf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:07:57:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:09:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:09:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 441e163c-dc41-4061-a7c0-4019e81a5af2
01/26/2025 23:09:12:INFO:Received: evaluate message 441e163c-dc41-4061-a7c0-4019e81a5af2
[92mINFO [0m:      Sent reply
01/26/2025 23:09:17:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:09:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:09:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 792d3a1e-6954-41ee-bce2-c11d4a72bda3
01/26/2025 23:09:54:INFO:Received: train message 792d3a1e-6954-41ee-bce2-c11d4a72bda3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:10:20:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:11:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:11:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1c009b12-4753-4ebb-bc8f-3719ae068ab5
01/26/2025 23:11:16:INFO:Received: evaluate message 1c009b12-4753-4ebb-bc8f-3719ae068ab5
[92mINFO [0m:      Sent reply
01/26/2025 23:11:20:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:11:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:11:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1cff2219-dd97-4e79-90ec-4b76bb43f130
01/26/2025 23:11:44:INFO:Received: train message 1cff2219-dd97-4e79-90ec-4b76bb43f130
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:12:08:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:13:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:13:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 551740b0-fcf2-49fb-8e39-6ba8eeaa00c3
01/26/2025 23:13:19:INFO:Received: evaluate message 551740b0-fcf2-49fb-8e39-6ba8eeaa00c3
[92mINFO [0m:      Sent reply
01/26/2025 23:13:24:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:13:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:13:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message beb583c4-8415-493e-a82f-bd83c8d7eb2a
01/26/2025 23:13:52:INFO:Received: train message beb583c4-8415-493e-a82f-bd83c8d7eb2a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:14:16:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:15:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:15:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 98366c9d-0e49-485d-bad2-3e71c526088a
01/26/2025 23:15:21:INFO:Received: evaluate message 98366c9d-0e49-485d-bad2-3e71c526088a
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:15:24:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:16:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:16:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6bf09cac-3650-41e1-9040-a194c398a3b4
01/26/2025 23:16:08:INFO:Received: train message 6bf09cac-3650-41e1-9040-a194c398a3b4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:16:35:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:17:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:17:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8feeed0f-a82c-4893-bb9f-e1268ee83bda
01/26/2025 23:17:57:INFO:Received: evaluate message 8feeed0f-a82c-4893-bb9f-e1268ee83bda
[92mINFO [0m:      Sent reply
01/26/2025 23:17:59:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:18:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:18:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 864f2f55-8b95-4898-aadc-5a6a11a8c414
01/26/2025 23:18:55:INFO:Received: train message 864f2f55-8b95-4898-aadc-5a6a11a8c414
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:19:19:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:20:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:20:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message da196cdf-2a15-4dd3-9909-e9ed37a8ace1
01/26/2025 23:20:23:INFO:Received: evaluate message da196cdf-2a15-4dd3-9909-e9ed37a8ace1
[92mINFO [0m:      Sent reply
01/26/2025 23:20:28:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:21:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:21:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e966a7d8-3345-46b0-8ce8-7ca9c5069b27
01/26/2025 23:21:05:INFO:Received: train message e966a7d8-3345-46b0-8ce8-7ca9c5069b27
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:21:27:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:22:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:22:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6dc7ff81-05a2-4d94-a438-c2faacf203f6
01/26/2025 23:22:29:INFO:Received: evaluate message 6dc7ff81-05a2-4d94-a438-c2faacf203f6
[92mINFO [0m:      Sent reply
01/26/2025 23:22:31:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:23:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:23:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0cd0ab17-5306-4cc0-9bec-79c7109deee2
01/26/2025 23:23:46:INFO:Received: train message 0cd0ab17-5306-4cc0-9bec-79c7109deee2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:24:16:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:25:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:25:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 24e63ca8-23b0-4f4b-9ea6-dd6d494d74b0
01/26/2025 23:25:00:INFO:Received: evaluate message 24e63ca8-23b0-4f4b-9ea6-dd6d494d74b0

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:25:04:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:25:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:25:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a1d24039-9798-46e8-a710-e84a56731112
01/26/2025 23:25:43:INFO:Received: train message a1d24039-9798-46e8-a710-e84a56731112
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:26:05:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:27:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:27:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d144f88c-b00d-4ba9-ad8d-657fa0e4a595
01/26/2025 23:27:00:INFO:Received: evaluate message d144f88c-b00d-4ba9-ad8d-657fa0e4a595
[92mINFO [0m:      Sent reply
01/26/2025 23:27:03:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:27:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:27:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0e96e62d-1e98-464b-b3db-77c25a2fe115
01/26/2025 23:27:30:INFO:Received: train message 0e96e62d-1e98-464b-b3db-77c25a2fe115
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:27:52:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:28:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:28:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7fab8f78-a1e8-49af-bc60-b6eb12cf7fdb
01/26/2025 23:28:55:INFO:Received: evaluate message 7fab8f78-a1e8-49af-bc60-b6eb12cf7fdb
[92mINFO [0m:      Sent reply
01/26/2025 23:29:00:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:29:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:29:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 471b0f08-b2d3-46e8-8bf4-0f941ef350b3
01/26/2025 23:29:18:INFO:Received: train message 471b0f08-b2d3-46e8-8bf4-0f941ef350b3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:29:37:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:30:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:30:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b7a6b2a0-a5d6-4e37-b4af-d88575a29f39
01/26/2025 23:30:38:INFO:Received: evaluate message b7a6b2a0-a5d6-4e37-b4af-d88575a29f39
[92mINFO [0m:      Sent reply
01/26/2025 23:30:42:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:31:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:31:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7f7cf73e-730e-497f-bdbd-97d61a526675
01/26/2025 23:31:19:INFO:Received: train message 7f7cf73e-730e-497f-bdbd-97d61a526675

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:31:44:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:32:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:32:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f6bca3a0-0246-4377-b309-93d3533a578b
01/26/2025 23:32:31:INFO:Received: evaluate message f6bca3a0-0246-4377-b309-93d3533a578b
[92mINFO [0m:      Sent reply
01/26/2025 23:32:35:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:33:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:33:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 047ccfee-e260-4a27-be4b-03d558a5e122
01/26/2025 23:33:05:INFO:Received: train message 047ccfee-e260-4a27-be4b-03d558a5e122
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:33:34:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:34:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:34:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 82704ac3-7ed6-46da-ad13-ff4185e7cc96
01/26/2025 23:34:39:INFO:Received: evaluate message 82704ac3-7ed6-46da-ad13-ff4185e7cc96
[92mINFO [0m:      Sent reply
01/26/2025 23:34:43:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:35:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:35:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8f426adb-a3a6-4850-9f65-1c8aeaccb15d
01/26/2025 23:35:17:INFO:Received: train message 8f426adb-a3a6-4850-9f65-1c8aeaccb15d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:35:42:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:36:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:36:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 98ceea29-96fa-4e98-a2cf-cf4920f28792
01/26/2025 23:36:09:INFO:Received: evaluate message 98ceea29-96fa-4e98-a2cf-cf4920f28792
[92mINFO [0m:      Sent reply
01/26/2025 23:36:13:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:36:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:36:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ad671b0-33e3-4b3b-a215-81cbaf1589bb
01/26/2025 23:36:53:INFO:Received: train message 8ad671b0-33e3-4b3b-a215-81cbaf1589bb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:37:17:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:38:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:38:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f6797e8c-89fc-486b-b892-97a948cfa9f8
01/26/2025 23:38:15:INFO:Received: evaluate message f6797e8c-89fc-486b-b892-97a948cfa9f8
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/26/2025 23:38:20:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:38:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:38:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f183b482-0250-4a5f-b3ec-38e441a893fc
01/26/2025 23:38:37:INFO:Received: train message f183b482-0250-4a5f-b3ec-38e441a893fc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:38:58:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:40:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:40:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9b5544f1-0262-49e0-97c1-4b1646eaf45e
01/26/2025 23:40:15:INFO:Received: evaluate message 9b5544f1-0262-49e0-97c1-4b1646eaf45e
[92mINFO [0m:      Sent reply
01/26/2025 23:40:19:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:40:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:40:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 95623452-7d23-49b0-96b5-a59ecf9ae55d
01/26/2025 23:40:44:INFO:Received: train message 95623452-7d23-49b0-96b5-a59ecf9ae55d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/26/2025 23:41:08:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:42:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:42:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f0213c0c-2693-4dab-922a-e0c440ad2e4a
01/26/2025 23:42:36:INFO:Received: evaluate message f0213c0c-2693-4dab-922a-e0c440ad2e4a
[92mINFO [0m:      Sent reply
01/26/2025 23:42:40:INFO:Sent reply
[92mINFO [0m:      
01/26/2025 23:42:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/26/2025 23:42:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 11b51fdf-b732-4367-9a81-6613c83568c8
01/26/2025 23:42:41:INFO:Received: reconnect message 11b51fdf-b732-4367-9a81-6613c83568c8
01/26/2025 23:42:42:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/26/2025 23:42:42:INFO:Disconnect and shut down

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3961181640625
Data Scaling Factor: 0.09234375 where Client Data Size: 591
Noise Multiplier after Fisher Scaling:  [0.0028213486075401306, 0.0023722767364233732, 0.015483713708817959, 0.00542535912245512]
Noise Multiplier after list and tensor:  0.006525674543809146
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504, 1.0184915054580772], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836, 0.5918686473807663], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188, 0.8098659294281917]}



Final client history:
{'loss': [1.0719002894445544, 1.1476353154320378, 1.1481956942553069, 1.0951668860112624, 1.0691930930347906, 1.0886689859782466, 1.0882667891675368, 1.0830622525584241, 1.084133233662412, 1.0823156458032235, 1.0176229257430762, 1.0593435624505134, 1.0433047446447765, 1.045424162642484, 1.0524802610126671, 1.049043425505193, 1.0759795546252062, 1.0288015880838235, 1.0487176290333877, 1.1139122723041799, 1.0474102944951211, 1.047719383165182, 1.0484611136639277, 1.048891198830832, 1.0007799031960023, 1.0954404740337285, 1.03982552753192, 1.0157480147595142, 1.0122437177457504, 1.0184915054580772], 'accuracy': [0.5199374511336982, 0.5238467552775606, 0.5308835027365129, 0.5402658326817826, 0.5496481626270524, 0.5598123534010946, 0.5559030492572322, 0.5535574667709148, 0.562157935887412, 0.5559030492572322, 0.5676309616888194, 0.5707584050039093, 0.5762314308053167, 0.5715402658326818, 0.5793588741204065, 0.5824863174354965, 0.5731039874902267, 0.5824863174354965, 0.5840500390930414, 0.5684128225175918, 0.5762314308053167, 0.5793588741204065, 0.5871774824081314, 0.5895230648944488, 0.5910867865519938, 0.5770132916340891, 0.5895230648944488, 0.5903049257232212, 0.5942142298670836, 0.5918686473807663], 'auc': [0.7254494785945842, 0.7453794430136471, 0.7554186829719838, 0.7648633569083153, 0.771435295516314, 0.775034223005478, 0.7783384145186653, 0.7811152963135419, 0.7825739553187089, 0.7855194483601196, 0.7898890962097116, 0.791594534130833, 0.7920399010904827, 0.79151254894495, 0.7928993830792918, 0.7947371674277951, 0.7961892617712503, 0.7979383825037141, 0.7973727725069698, 0.7981555984241595, 0.8014518137442952, 0.8019051266863099, 0.8021128538155167, 0.8029633087981671, 0.8061220399301571, 0.8059527572031768, 0.8076795834762531, 0.8088779384832196, 0.8079091487395188, 0.8098659294281917]}

