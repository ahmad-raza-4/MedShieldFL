nohup: ignoring input
02/05/2025 03:50:52:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 03:50:52:DEBUG:ChannelConnectivity.IDLE
02/05/2025 03:50:52:DEBUG:ChannelConnectivity.CONNECTING
02/05/2025 03:50:52:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
02/05/2025 03:50:52:INFO:
[92mINFO [0m:      Received: get_parameters message 4d7def8b-6378-42eb-9c7d-5a7ac217033d
02/05/2025 03:50:52:INFO:Received: get_parameters message 4d7def8b-6378-42eb-9c7d-5a7ac217033d
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738756252.368056 3092222 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      Sent reply
02/05/2025 03:50:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 03:51:07:INFO:
[92mINFO [0m:      Received: train message 32a1d0c7-2d4f-4935-a5f8-efd69d4ddf06
02/05/2025 03:51:07:INFO:Received: train message 32a1d0c7-2d4f-4935-a5f8-efd69d4ddf06
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 03:52:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 03:52:23:INFO:
[92mINFO [0m:      Received: evaluate message e5bcffd3-790c-4b2c-b4bc-6f6b506e8d21
02/05/2025 03:52:23:INFO:Received: evaluate message e5bcffd3-790c-4b2c-b4bc-6f6b506e8d21
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 03:52:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 03:52:37:INFO:
[92mINFO [0m:      Received: train message a7a75e60-0a64-405b-af8f-488d4ea8e231
02/05/2025 03:52:37:INFO:Received: train message a7a75e60-0a64-405b-af8f-488d4ea8e231
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 03:53:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 03:53:54:INFO:
[92mINFO [0m:      Received: evaluate message 1339dda0-152b-4ac1-a240-4552c6d4140b
02/05/2025 03:53:54:INFO:Received: evaluate message 1339dda0-152b-4ac1-a240-4552c6d4140b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 03:53:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 03:54:07:INFO:
[92mINFO [0m:      Received: train message c45da74e-cabe-4672-95b1-425f63694901
02/05/2025 03:54:07:INFO:Received: train message c45da74e-cabe-4672-95b1-425f63694901
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 03:55:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 03:55:20:INFO:
[92mINFO [0m:      Received: evaluate message 8cc24ba1-d976-4404-a4fe-4de63f203c10
02/05/2025 03:55:20:INFO:Received: evaluate message 8cc24ba1-d976-4404-a4fe-4de63f203c10
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 03:55:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 03:55:33:INFO:
[92mINFO [0m:      Received: train message 037a52e4-6a63-4fc7-8de7-e3ef810c7d9e
02/05/2025 03:55:33:INFO:Received: train message 037a52e4-6a63-4fc7-8de7-e3ef810c7d9e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 03:56:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 03:56:50:INFO:
[92mINFO [0m:      Received: evaluate message 681a3de5-c609-4d5f-8591-2150f21a2e19
02/05/2025 03:56:50:INFO:Received: evaluate message 681a3de5-c609-4d5f-8591-2150f21a2e19
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 03:56:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 03:57:02:INFO:
[92mINFO [0m:      Received: train message 14457441-6deb-4097-9a71-d115ecc332c7
02/05/2025 03:57:02:INFO:Received: train message 14457441-6deb-4097-9a71-d115ecc332c7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 03:58:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 03:58:20:INFO:
[92mINFO [0m:      Received: evaluate message b1fb0007-8d1b-40f5-aae7-2922d436fcc9
02/05/2025 03:58:20:INFO:Received: evaluate message b1fb0007-8d1b-40f5-aae7-2922d436fcc9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 03:58:26:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 03:58:32:INFO:
[92mINFO [0m:      Received: train message f58589c5-ca4a-43e3-94d9-111c67ceaba3
02/05/2025 03:58:32:INFO:Received: train message f58589c5-ca4a-43e3-94d9-111c67ceaba3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 03:59:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 03:59:50:INFO:
[92mINFO [0m:      Received: evaluate message 5a22406b-16d2-43d4-be44-34d36e92daca
02/05/2025 03:59:50:INFO:Received: evaluate message 5a22406b-16d2-43d4-be44-34d36e92daca
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 03:59:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:00:02:INFO:
[92mINFO [0m:      Received: train message 6455dc54-85d5-4965-9e01-84418e2d6853
02/05/2025 04:00:02:INFO:Received: train message 6455dc54-85d5-4965-9e01-84418e2d6853
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:01:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:01:18:INFO:
[92mINFO [0m:      Received: evaluate message f9e0ad10-e8f6-4aab-9834-90f66fda8f47
02/05/2025 04:01:18:INFO:Received: evaluate message f9e0ad10-e8f6-4aab-9834-90f66fda8f47
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30, target_epsilon: 30, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.9519776633945733], 'accuracy': [0.49257232212666147], 'auc': [0.5248385070801708], 'precision': [0.5355357114391173], 'recall': [0.49257232212666147], 'f1': [0.44772422124192734]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.9519776633945733, 2.3501022547669854], 'accuracy': [0.49257232212666147, 0.47771696637998434], 'auc': [0.5248385070801708, 0.5573441855808725], 'precision': [0.5355357114391173, 0.55729896055403], 'recall': [0.49257232212666147, 0.47771696637998434], 'f1': [0.44772422124192734, 0.4217446313569452]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.9519776633945733, 2.3501022547669854, 1.8452071867259896], 'accuracy': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285], 'auc': [0.5248385070801708, 0.5573441855808725, 0.6444153371122376], 'precision': [0.5355357114391173, 0.55729896055403, 0.5255656561972999], 'recall': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285], 'f1': [0.44772422124192734, 0.4217446313569452, 0.47243292306536355]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.9519776633945733, 2.3501022547669854, 1.8452071867259896, 1.9883617678070273], 'accuracy': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459], 'auc': [0.5248385070801708, 0.5573441855808725, 0.6444153371122376, 0.644848894085722], 'precision': [0.5355357114391173, 0.55729896055403, 0.5255656561972999, 0.5505402912899241], 'recall': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459], 'f1': [0.44772422124192734, 0.4217446313569452, 0.47243292306536355, 0.47044509469248286]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.9519776633945733, 2.3501022547669854, 1.8452071867259896, 1.9883617678070273, 1.708335176611245], 'accuracy': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645], 'auc': [0.5248385070801708, 0.5573441855808725, 0.6444153371122376, 0.644848894085722, 0.7085255190985233], 'precision': [0.5355357114391173, 0.55729896055403, 0.5255656561972999, 0.5505402912899241, 0.526770778147169], 'recall': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645], 'f1': [0.44772422124192734, 0.4217446313569452, 0.47243292306536355, 0.47044509469248286, 0.5091104675299821]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.9519776633945733, 2.3501022547669854, 1.8452071867259896, 1.9883617678070273, 1.708335176611245, 1.8698665045953244], 'accuracy': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645, 0.5230648944487881], 'auc': [0.5248385070801708, 0.5573441855808725, 0.6444153371122376, 0.644848894085722, 0.7085255190985233, 0.6937598165234484], 'precision': [0.5355357114391173, 0.55729896055403, 0.5255656561972999, 0.5505402912899241, 0.526770778147169, 0.5428720026872438], 'recall': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645, 0.5230648944487881], 'f1': [0.44772422124192734, 0.4217446313569452, 0.47243292306536355, 0.47044509469248286, 0.5091104675299821, 0.4840584548700751]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:01:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:01:31:INFO:
[92mINFO [0m:      Received: train message 29642e00-a493-4a47-8f59-31253b16fcb3
02/05/2025 04:01:31:INFO:Received: train message 29642e00-a493-4a47-8f59-31253b16fcb3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:02:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:02:46:INFO:
[92mINFO [0m:      Received: evaluate message 4e3f8774-2555-4800-aa09-e28fb79e2cfc
02/05/2025 04:02:46:INFO:Received: evaluate message 4e3f8774-2555-4800-aa09-e28fb79e2cfc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:02:52:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:02:59:INFO:
[92mINFO [0m:      Received: train message 9a9ca823-3c51-4ecb-943e-9e6a5f971138
02/05/2025 04:02:59:INFO:Received: train message 9a9ca823-3c51-4ecb-943e-9e6a5f971138
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:04:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:04:17:INFO:
[92mINFO [0m:      Received: evaluate message 0b6b55a1-f77c-49d3-91ed-ed0c0d5dc9d1
02/05/2025 04:04:17:INFO:Received: evaluate message 0b6b55a1-f77c-49d3-91ed-ed0c0d5dc9d1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:04:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:04:31:INFO:
[92mINFO [0m:      Received: train message 6e32a4b7-f7e7-4b32-8c74-8512930eed62
02/05/2025 04:04:31:INFO:Received: train message 6e32a4b7-f7e7-4b32-8c74-8512930eed62
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 04:05:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:05:45:INFO:
[92mINFO [0m:      Received: evaluate message 11d07539-200e-497d-bc03-774a236541c9
02/05/2025 04:05:45:INFO:Received: evaluate message 11d07539-200e-497d-bc03-774a236541c9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 04:05:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 04:05:50:INFO:
[92mINFO [0m:      Received: reconnect message 7c36a7d7-96aa-41ae-8f10-2521eab45743
02/05/2025 04:05:50:INFO:Received: reconnect message 7c36a7d7-96aa-41ae-8f10-2521eab45743
02/05/2025 04:05:50:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 04:05:50:INFO:Disconnect and shut down

{'loss': [1.9519776633945733, 2.3501022547669854, 1.8452071867259896, 1.9883617678070273, 1.708335176611245, 1.8698665045953244, 1.8507918117216922], 'accuracy': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645, 0.5230648944487881, 0.5238467552775606], 'auc': [0.5248385070801708, 0.5573441855808725, 0.6444153371122376, 0.644848894085722, 0.7085255190985233, 0.6937598165234484, 0.6968110593999833], 'precision': [0.5355357114391173, 0.55729896055403, 0.5255656561972999, 0.5505402912899241, 0.526770778147169, 0.5428720026872438, 0.5505704342618958], 'recall': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645, 0.5230648944487881, 0.5238467552775606], 'f1': [0.44772422124192734, 0.4217446313569452, 0.47243292306536355, 0.47044509469248286, 0.5091104675299821, 0.4840584548700751, 0.4833230881486298]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.9519776633945733, 2.3501022547669854, 1.8452071867259896, 1.9883617678070273, 1.708335176611245, 1.8698665045953244, 1.8507918117216922, 1.7056863882445654], 'accuracy': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645, 0.5230648944487881, 0.5238467552775606, 0.5371383893666928], 'auc': [0.5248385070801708, 0.5573441855808725, 0.6444153371122376, 0.644848894085722, 0.7085255190985233, 0.6937598165234484, 0.6968110593999833, 0.7207217789107684], 'precision': [0.5355357114391173, 0.55729896055403, 0.5255656561972999, 0.5505402912899241, 0.526770778147169, 0.5428720026872438, 0.5505704342618958, 0.5385446111703862], 'recall': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645, 0.5230648944487881, 0.5238467552775606, 0.5371383893666928], 'f1': [0.44772422124192734, 0.4217446313569452, 0.47243292306536355, 0.47044509469248286, 0.5091104675299821, 0.4840584548700751, 0.4833230881486298, 0.5007714558367545]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.9519776633945733, 2.3501022547669854, 1.8452071867259896, 1.9883617678070273, 1.708335176611245, 1.8698665045953244, 1.8507918117216922, 1.7056863882445654, 1.7839896120567915], 'accuracy': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645, 0.5230648944487881, 0.5238467552775606, 0.5371383893666928, 0.5355746677091477], 'auc': [0.5248385070801708, 0.5573441855808725, 0.6444153371122376, 0.644848894085722, 0.7085255190985233, 0.6937598165234484, 0.6968110593999833, 0.7207217789107684, 0.7195213213163856], 'precision': [0.5355357114391173, 0.55729896055403, 0.5255656561972999, 0.5505402912899241, 0.526770778147169, 0.5428720026872438, 0.5505704342618958, 0.5385446111703862, 0.5569066223301979], 'recall': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645, 0.5230648944487881, 0.5238467552775606, 0.5371383893666928, 0.5355746677091477], 'f1': [0.44772422124192734, 0.4217446313569452, 0.47243292306536355, 0.47044509469248286, 0.5091104675299821, 0.4840584548700751, 0.4833230881486298, 0.5007714558367545, 0.49646422519711053]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.9519776633945733, 2.3501022547669854, 1.8452071867259896, 1.9883617678070273, 1.708335176611245, 1.8698665045953244, 1.8507918117216922, 1.7056863882445654, 1.7839896120567915, 1.8746066548518598], 'accuracy': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645, 0.5230648944487881, 0.5238467552775606, 0.5371383893666928, 0.5355746677091477, 0.5238467552775606], 'auc': [0.5248385070801708, 0.5573441855808725, 0.6444153371122376, 0.644848894085722, 0.7085255190985233, 0.6937598165234484, 0.6968110593999833, 0.7207217789107684, 0.7195213213163856, 0.7069139750643644], 'precision': [0.5355357114391173, 0.55729896055403, 0.5255656561972999, 0.5505402912899241, 0.526770778147169, 0.5428720026872438, 0.5505704342618958, 0.5385446111703862, 0.5569066223301979, 0.5506210989864027], 'recall': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645, 0.5230648944487881, 0.5238467552775606, 0.5371383893666928, 0.5355746677091477, 0.5238467552775606], 'f1': [0.44772422124192734, 0.4217446313569452, 0.47243292306536355, 0.47044509469248286, 0.5091104675299821, 0.4840584548700751, 0.4833230881486298, 0.5007714558367545, 0.49646422519711053, 0.4836762591742541]}



Final client history:
{'loss': [1.9519776633945733, 2.3501022547669854, 1.8452071867259896, 1.9883617678070273, 1.708335176611245, 1.8698665045953244, 1.8507918117216922, 1.7056863882445654, 1.7839896120567915, 1.8746066548518598], 'accuracy': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645, 0.5230648944487881, 0.5238467552775606, 0.5371383893666928, 0.5355746677091477, 0.5238467552775606], 'auc': [0.5248385070801708, 0.5573441855808725, 0.6444153371122376, 0.644848894085722, 0.7085255190985233, 0.6937598165234484, 0.6968110593999833, 0.7207217789107684, 0.7195213213163856, 0.7069139750643644], 'precision': [0.5355357114391173, 0.55729896055403, 0.5255656561972999, 0.5505402912899241, 0.526770778147169, 0.5428720026872438, 0.5505704342618958, 0.5385446111703862, 0.5569066223301979, 0.5506210989864027], 'recall': [0.49257232212666147, 0.47771696637998434, 0.5105551211884285, 0.5129007036747459, 0.544175136825645, 0.5230648944487881, 0.5238467552775606, 0.5371383893666928, 0.5355746677091477, 0.5238467552775606], 'f1': [0.44772422124192734, 0.4217446313569452, 0.47243292306536355, 0.47044509469248286, 0.5091104675299821, 0.4840584548700751, 0.4833230881486298, 0.5007714558367545, 0.49646422519711053, 0.4836762591742541]}

