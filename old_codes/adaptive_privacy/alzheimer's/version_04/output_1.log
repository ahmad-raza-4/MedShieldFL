nohup: ignoring input
02/05/2025 06:54:21:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 06:54:21:DEBUG:ChannelConnectivity.IDLE
02/05/2025 06:54:21:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
02/05/2025 06:54:21:INFO:
[92mINFO [0m:      Received: get_parameters message f69aad95-ee29-414a-bba5-00f13c1ff975
02/05/2025 06:54:21:INFO:Received: get_parameters message f69aad95-ee29-414a-bba5-00f13c1ff975
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738767261.713677 2607078 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      Sent reply
02/05/2025 06:54:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 06:54:36:INFO:
[92mINFO [0m:      Received: train message 78b5492d-6ba6-461a-9f73-906d2a25324a
02/05/2025 06:54:36:INFO:Received: train message 78b5492d-6ba6-461a-9f73-906d2a25324a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 06:55:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 06:55:24:INFO:
[92mINFO [0m:      Received: evaluate message e8eda83d-a862-4a8a-8d99-4a8cb94ba11c
02/05/2025 06:55:24:INFO:Received: evaluate message e8eda83d-a862-4a8a-8d99-4a8cb94ba11c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 06:55:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 06:55:35:INFO:
[92mINFO [0m:      Received: train message 4bcf32a4-b469-4a46-bffe-933747d6b657
02/05/2025 06:55:35:INFO:Received: train message 4bcf32a4-b469-4a46-bffe-933747d6b657
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 06:56:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 06:56:22:INFO:
[92mINFO [0m:      Received: evaluate message d42a34ac-40f3-469e-a224-d0b7ffc2249b
02/05/2025 06:56:22:INFO:Received: evaluate message d42a34ac-40f3-469e-a224-d0b7ffc2249b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 06:56:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 06:56:31:INFO:
[92mINFO [0m:      Received: train message d56758f2-9a30-4369-977a-2d304d335163
02/05/2025 06:56:31:INFO:Received: train message d56758f2-9a30-4369-977a-2d304d335163
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 06:57:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 06:57:16:INFO:
[92mINFO [0m:      Received: evaluate message 1449edcb-e9e9-4543-9b7f-ee6a45666137
02/05/2025 06:57:16:INFO:Received: evaluate message 1449edcb-e9e9-4543-9b7f-ee6a45666137
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 06:57:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 06:57:26:INFO:
[92mINFO [0m:      Received: train message a486fcd7-1733-44c8-bd84-99b53ceaa888
02/05/2025 06:57:26:INFO:Received: train message a486fcd7-1733-44c8-bd84-99b53ceaa888
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 06:57:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 06:58:11:INFO:
[92mINFO [0m:      Received: evaluate message 17e66043-f45e-469a-a715-60b830dd2cd7
02/05/2025 06:58:11:INFO:Received: evaluate message 17e66043-f45e-469a-a715-60b830dd2cd7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 06:58:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 06:58:21:INFO:
[92mINFO [0m:      Received: train message 460c4fca-a123-4f67-a8ed-2151e445e40e
02/05/2025 06:58:21:INFO:Received: train message 460c4fca-a123-4f67-a8ed-2151e445e40e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 06:58:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 06:59:08:INFO:
[92mINFO [0m:      Received: evaluate message 51841aa6-b780-45d3-a038-cf246dca1e41
02/05/2025 06:59:08:INFO:Received: evaluate message 51841aa6-b780-45d3-a038-cf246dca1e41
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 06:59:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 06:59:18:INFO:
[92mINFO [0m:      Received: train message 10a796ca-fa1b-4bf5-b66c-a23598d042ff
02/05/2025 06:59:18:INFO:Received: train message 10a796ca-fa1b-4bf5-b66c-a23598d042ff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 06:59:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 07:00:03:INFO:
[92mINFO [0m:      Received: evaluate message 78f81da6-0a44-4954-bc08-11d53a33f5ae
02/05/2025 07:00:03:INFO:Received: evaluate message 78f81da6-0a44-4954-bc08-11d53a33f5ae
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 07:00:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 07:00:12:INFO:
[92mINFO [0m:      Received: train message f2eb0e59-73e3-468c-a920-6d8f88a79b5d
02/05/2025 07:00:12:INFO:Received: train message f2eb0e59-73e3-468c-a920-6d8f88a79b5d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 07:00:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 07:00:53:INFO:
[92mINFO [0m:      Received: evaluate message 32808244-cbeb-4340-b611-89110b0ac5cc
02/05/2025 07:00:53:INFO:Received: evaluate message 32808244-cbeb-4340-b611-89110b0ac5cc
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954], 'accuracy': [0.35105551211884284], 'auc': [0.49385709274345885], 'precision': [0.6231785640434563], 'recall': [0.35105551211884284], 'f1': [0.1833951127195922]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646], 'accuracy': [0.35105551211884284, 0.43471462079749806], 'auc': [0.49385709274345885, 0.4926623554484254], 'precision': [0.6231785640434563, 0.56764227853351], 'recall': [0.35105551211884284, 0.43471462079749806], 'f1': [0.1833951127195922, 0.3510492981299551]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126, 1.8211777950749908], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582, 0.5721949151975435], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837, 0.527881748466725], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713, 0.47150202919337764]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 07:00:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 07:01:03:INFO:
[92mINFO [0m:      Received: train message a492b325-075f-447b-99a4-b12b8aa37108
02/05/2025 07:01:03:INFO:Received: train message a492b325-075f-447b-99a4-b12b8aa37108
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 07:01:29:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 07:01:44:INFO:
[92mINFO [0m:      Received: evaluate message 3dc01674-9a2f-4b84-8f1a-4428497ac036
02/05/2025 07:01:44:INFO:Received: evaluate message 3dc01674-9a2f-4b84-8f1a-4428497ac036
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 07:01:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 07:01:54:INFO:
[92mINFO [0m:      Received: train message 4157105d-f41a-4138-987d-ec835804cf0a
02/05/2025 07:01:54:INFO:Received: train message 4157105d-f41a-4138-987d-ec835804cf0a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 07:02:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 07:02:37:INFO:
[92mINFO [0m:      Received: evaluate message e48ee56f-5f46-4ce3-8203-ba384aad969a
02/05/2025 07:02:37:INFO:Received: evaluate message e48ee56f-5f46-4ce3-8203-ba384aad969a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 07:02:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 07:02:47:INFO:
[92mINFO [0m:      Received: train message 7164c09f-4cc6-4519-9454-2ce6ebe8c73f
02/05/2025 07:02:47:INFO:Received: train message 7164c09f-4cc6-4519-9454-2ce6ebe8c73f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 07:03:17:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 07:03:29:INFO:
[92mINFO [0m:      Received: evaluate message f3aa8dfa-d067-47fa-9eed-162e344f2d25
02/05/2025 07:03:29:INFO:Received: evaluate message f3aa8dfa-d067-47fa-9eed-162e344f2d25
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 07:03:31:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 07:03:32:INFO:
[92mINFO [0m:      Received: reconnect message a134c4f9-63e6-487e-88c7-e85470448e20
02/05/2025 07:03:32:INFO:Received: reconnect message a134c4f9-63e6-487e-88c7-e85470448e20
02/05/2025 07:03:32:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 07:03:32:INFO:Disconnect and shut down

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126, 1.8211777950749908, 1.9451654829183904], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582, 0.5721949151975435, 0.5678565402090624], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837, 0.527881748466725, 0.534388193394705], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713, 0.47150202919337764, 0.4680520134848537]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126, 1.8211777950749908, 1.9451654829183904, 1.910481638041346], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582, 0.5721949151975435, 0.5678565402090624, 0.5816478990426055], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837, 0.527881748466725, 0.534388193394705, 0.5317417441177757], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713, 0.47150202919337764, 0.4680520134848537, 0.4718829233473253]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126, 1.8211777950749908, 1.9451654829183904, 1.910481638041346, 1.8484912142914847], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009, 0.508209538702111], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582, 0.5721949151975435, 0.5678565402090624, 0.5816478990426055, 0.6061525975384168], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837, 0.527881748466725, 0.534388193394705, 0.5317417441177757, 0.5248556730657858], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009, 0.508209538702111], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713, 0.47150202919337764, 0.4680520134848537, 0.4718829233473253, 0.4693398156061687]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126, 1.8211777950749908, 1.9451654829183904, 1.910481638041346, 1.8484912142914847, 2.0151613874359464], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009, 0.508209538702111, 0.5058639562157936], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582, 0.5721949151975435, 0.5678565402090624, 0.5816478990426055, 0.6061525975384168, 0.5724589607873449], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837, 0.527881748466725, 0.534388193394705, 0.5317417441177757, 0.5248556730657858, 0.5497248083323522], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009, 0.508209538702111, 0.5058639562157936], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713, 0.47150202919337764, 0.4680520134848537, 0.4718829233473253, 0.4693398156061687, 0.4615842965514985]}



Final client history:
{'loss': [2.069351186254954, 2.0442438714258646, 1.7544340877979412, 1.9294812782631898, 1.7563575291256126, 1.8211777950749908, 1.9451654829183904, 1.910481638041346, 1.8484912142914847, 2.0151613874359464], 'accuracy': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009, 0.508209538702111, 0.5058639562157936], 'auc': [0.49385709274345885, 0.4926623554484254, 0.5330024534029894, 0.5166291617471737, 0.5677970344891582, 0.5721949151975435, 0.5678565402090624, 0.5816478990426055, 0.6061525975384168, 0.5724589607873449], 'precision': [0.6231785640434563, 0.56764227853351, 0.527405362671161, 0.526549624364188, 0.5251446469402837, 0.527881748466725, 0.534388193394705, 0.5317417441177757, 0.5248556730657858, 0.5497248083323522], 'recall': [0.35105551211884284, 0.43471462079749806, 0.5027365129007036, 0.5011727912431587, 0.5191555903049258, 0.5105551211884285, 0.508209538702111, 0.5113369820172009, 0.508209538702111, 0.5058639562157936], 'f1': [0.1833951127195922, 0.3510492981299551, 0.4621083019664243, 0.46072378886340254, 0.48196355583240713, 0.47150202919337764, 0.4680520134848537, 0.4718829233473253, 0.4693398156061687, 0.4615842965514985]}

