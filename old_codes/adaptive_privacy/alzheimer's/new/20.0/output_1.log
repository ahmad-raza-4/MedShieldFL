nohup: ignoring input
02/05/2025 10:04:58:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:04:58:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:04:58:DEBUG:ChannelConnectivity.CONNECTING
02/05/2025 10:04:58:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778698.605633 1886183 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:05:36:INFO:
[92mINFO [0m:      Received: train message 021cfe0d-2f2b-43f5-b3db-7d3f4e6d7a57
02/05/2025 10:05:36:INFO:Received: train message 021cfe0d-2f2b-43f5-b3db-7d3f4e6d7a57
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:06:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:07:32:INFO:
[92mINFO [0m:      Received: evaluate message 3574bdcc-c4f6-4960-b671-4df6ca6c4aca
02/05/2025 10:07:32:INFO:Received: evaluate message 3574bdcc-c4f6-4960-b671-4df6ca6c4aca
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:07:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:08:03:INFO:
[92mINFO [0m:      Received: train message 028c286a-11a4-4fc0-abbe-0f86f5486ec4
02/05/2025 10:08:03:INFO:Received: train message 028c286a-11a4-4fc0-abbe-0f86f5486ec4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:09:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:51:INFO:
[92mINFO [0m:      Received: evaluate message 35ced7b5-ba57-498f-9e2e-ef4e0541e243
02/05/2025 10:09:51:INFO:Received: evaluate message 35ced7b5-ba57-498f-9e2e-ef4e0541e243
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:09:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:10:40:INFO:
[92mINFO [0m:      Received: train message e1e02b28-ebfb-4ef1-b48e-323b45c71f79
02/05/2025 10:10:40:INFO:Received: train message e1e02b28-ebfb-4ef1-b48e-323b45c71f79
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:11:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:12:29:INFO:
[92mINFO [0m:      Received: evaluate message 1c9a20ac-e1c7-46c5-9bbb-3d38ea92a308
02/05/2025 10:12:29:INFO:Received: evaluate message 1c9a20ac-e1c7-46c5-9bbb-3d38ea92a308
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:12:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:09:INFO:
[92mINFO [0m:      Received: train message b3c37804-0520-469e-ba57-148c839e2d21
02/05/2025 10:13:09:INFO:Received: train message b3c37804-0520-469e-ba57-148c839e2d21
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:14:14:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:01:INFO:
[92mINFO [0m:      Received: evaluate message b38b511c-517a-4089-be24-13127c833fcf
02/05/2025 10:15:01:INFO:Received: evaluate message b38b511c-517a-4089-be24-13127c833fcf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:15:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:49:INFO:
[92mINFO [0m:      Received: train message 8a051980-b9eb-4b84-89f8-c26bc6133355
02/05/2025 10:15:49:INFO:Received: train message 8a051980-b9eb-4b84-89f8-c26bc6133355
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:16:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:17:37:INFO:
[92mINFO [0m:      Received: evaluate message 2b5a821a-94b6-4b1d-91dc-3e4f76650773
02/05/2025 10:17:37:INFO:Received: evaluate message 2b5a821a-94b6-4b1d-91dc-3e4f76650773
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:17:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:18:INFO:
[92mINFO [0m:      Received: train message 8dfa246e-4fec-4b89-88b8-0102d8e011ad
02/05/2025 10:18:18:INFO:Received: train message 8dfa246e-4fec-4b89-88b8-0102d8e011ad
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:19:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:19:49:INFO:
[92mINFO [0m:      Received: evaluate message f12a0ae2-7ff3-4b0c-a7c2-9435d63d6794
02/05/2025 10:19:49:INFO:Received: evaluate message f12a0ae2-7ff3-4b0c-a7c2-9435d63d6794
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:19:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:21:49:INFO:
[92mINFO [0m:      Received: train message c0f8d978-5ef5-4dec-93fe-0476e9b69a1a
02/05/2025 10:21:49:INFO:Received: train message c0f8d978-5ef5-4dec-93fe-0476e9b69a1a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:22:51:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:23:21:INFO:
[92mINFO [0m:      Received: evaluate message bbf35ad5-7eb1-4b2e-abfd-8c934502c53d
02/05/2025 10:23:21:INFO:Received: evaluate message bbf35ad5-7eb1-4b2e-abfd-8c934502c53d
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 20.0, target_epsilon: 20.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814], 'accuracy': [0.5175918686473807], 'auc': [0.714457107011283], 'precision': [0.4117264737633955], 'recall': [0.5175918686473807], 'f1': [0.42093829387716764]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771], 'accuracy': [0.5175918686473807, 0.5363565285379203], 'auc': [0.714457107011283, 0.7407919892413165], 'precision': [0.4117264737633955, 0.4429501750516807], 'recall': [0.5175918686473807, 0.5363565285379203], 'f1': [0.42093829387716764, 0.4824075932878562]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:23:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:10:INFO:
[92mINFO [0m:      Received: train message bbf9bfeb-7e2e-4466-b1e7-0a3d655e9ee3
02/05/2025 10:24:10:INFO:Received: train message bbf9bfeb-7e2e-4466-b1e7-0a3d655e9ee3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:25:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:01:INFO:
[92mINFO [0m:      Received: evaluate message 75792499-3536-4fac-86ae-2cd41974abd0
02/05/2025 10:26:01:INFO:Received: evaluate message 75792499-3536-4fac-86ae-2cd41974abd0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:26:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:26:50:INFO:
[92mINFO [0m:      Received: train message 986b25a4-a827-4580-91d5-40a78c42b3d1
02/05/2025 10:26:50:INFO:Received: train message 986b25a4-a827-4580-91d5-40a78c42b3d1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:27:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:28:27:INFO:
[92mINFO [0m:      Received: evaluate message 0f7274cd-a3e2-4fa7-9a2b-adf9bc00c70f
02/05/2025 10:28:27:INFO:Received: evaluate message 0f7274cd-a3e2-4fa7-9a2b-adf9bc00c70f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:28:32:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:20:INFO:
[92mINFO [0m:      Received: train message a202fe6f-873a-4ef9-9f21-d086c3ff3d64
02/05/2025 10:29:20:INFO:Received: train message a202fe6f-873a-4ef9-9f21-d086c3ff3d64
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:30:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:31:06:INFO:
[92mINFO [0m:      Received: evaluate message f90f87fd-043f-4c6f-9eee-53f520227504
02/05/2025 10:31:06:INFO:Received: evaluate message f90f87fd-043f-4c6f-9eee-53f520227504
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:31:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:31:32:INFO:
[92mINFO [0m:      Received: train message 18b56aee-80b7-47d8-9b35-e4472a4f7e54
02/05/2025 10:31:32:INFO:Received: train message 18b56aee-80b7-47d8-9b35-e4472a4f7e54
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:32:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:33:41:INFO:
[92mINFO [0m:      Received: evaluate message 9a45d524-ccf1-419f-ae76-77e3c6afc08c
02/05/2025 10:33:41:INFO:Received: evaluate message 9a45d524-ccf1-419f-ae76-77e3c6afc08c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:33:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:05:INFO:
[92mINFO [0m:      Received: train message 1826775d-0d46-4a50-b47a-19679367a25a
02/05/2025 10:34:05:INFO:Received: train message 1826775d-0d46-4a50-b47a-19679367a25a

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:35:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:46:INFO:
[92mINFO [0m:      Received: evaluate message b4021590-a153-48ea-9aa8-e7834455de34
02/05/2025 10:35:46:INFO:Received: evaluate message b4021590-a153-48ea-9aa8-e7834455de34
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:35:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:36:31:INFO:
[92mINFO [0m:      Received: train message 1559a0c0-58a0-4a0c-b170-86ce7f4e8688
02/05/2025 10:36:31:INFO:Received: train message 1559a0c0-58a0-4a0c-b170-86ce7f4e8688
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:37:29:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:38:32:INFO:
[92mINFO [0m:      Received: evaluate message 22830923-b1c5-40ef-9812-abdde6b26963
02/05/2025 10:38:32:INFO:Received: evaluate message 22830923-b1c5-40ef-9812-abdde6b26963
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:38:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:05:INFO:
[92mINFO [0m:      Received: train message 4cda47c4-4f46-42a7-88ad-7c1f5f388b2a
02/05/2025 10:39:05:INFO:Received: train message 4cda47c4-4f46-42a7-88ad-7c1f5f388b2a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:40:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:47:INFO:
[92mINFO [0m:      Received: evaluate message 230ef123-ad6e-4515-8e9a-36ecd57a0d4a
02/05/2025 10:40:47:INFO:Received: evaluate message 230ef123-ad6e-4515-8e9a-36ecd57a0d4a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:40:52:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:41:42:INFO:
[92mINFO [0m:      Received: train message ed05e25f-8395-4d24-a4e2-a6cf500aefc4
02/05/2025 10:41:42:INFO:Received: train message ed05e25f-8395-4d24-a4e2-a6cf500aefc4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:42:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:19:INFO:
[92mINFO [0m:      Received: evaluate message 7a0696cf-6767-4984-a83d-0581f0cc8cf5
02/05/2025 10:43:19:INFO:Received: evaluate message 7a0696cf-6767-4984-a83d-0581f0cc8cf5
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:43:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:44:13:INFO:
[92mINFO [0m:      Received: train message 00a95670-4aab-4906-9178-ca63c0dce67c
02/05/2025 10:44:13:INFO:Received: train message 00a95670-4aab-4906-9178-ca63c0dce67c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:45:17:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:46:06:INFO:
[92mINFO [0m:      Received: evaluate message eeeea074-714b-448e-8775-5fcaf451864f
02/05/2025 10:46:06:INFO:Received: evaluate message eeeea074-714b-448e-8775-5fcaf451864f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:46:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:46:54:INFO:
[92mINFO [0m:      Received: train message 2f7283d5-d2bc-45a3-b184-8d0c14a2297d
02/05/2025 10:46:54:INFO:Received: train message 2f7283d5-d2bc-45a3-b184-8d0c14a2297d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:48:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:48:50:INFO:
[92mINFO [0m:      Received: evaluate message c6314894-7b3b-4aaa-8cd3-017aa49654a1
02/05/2025 10:48:50:INFO:Received: evaluate message c6314894-7b3b-4aaa-8cd3-017aa49654a1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:48:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:49:51:INFO:
[92mINFO [0m:      Received: train message f04821a1-9656-4ae9-9519-db1878e3da04
02/05/2025 10:49:51:INFO:Received: train message f04821a1-9656-4ae9-9519-db1878e3da04
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:50:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:51:36:INFO:
[92mINFO [0m:      Received: evaluate message 0358be94-c2f0-4842-a6d1-a705e7f58fa1
02/05/2025 10:51:36:INFO:Received: evaluate message 0358be94-c2f0-4842-a6d1-a705e7f58fa1

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:51:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:52:55:INFO:
[92mINFO [0m:      Received: train message 6d3cd21e-ff98-4ea9-9686-13618a72064e
02/05/2025 10:52:55:INFO:Received: train message 6d3cd21e-ff98-4ea9-9686-13618a72064e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:54:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:55:40:INFO:
[92mINFO [0m:      Received: evaluate message bd25ca9c-0437-4919-9b9a-c49a31f11224
02/05/2025 10:55:40:INFO:Received: evaluate message bd25ca9c-0437-4919-9b9a-c49a31f11224
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:55:45:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:57:05:INFO:
[92mINFO [0m:      Received: train message 2996de91-d013-4d23-9a19-4a19d543a194
02/05/2025 10:57:06:INFO:Received: train message 2996de91-d013-4d23-9a19-4a19d543a194
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:58:22:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:59:51:INFO:
[92mINFO [0m:      Received: evaluate message ed0e6ba3-59d0-4f38-a7c9-a702d7cbe6e1
02/05/2025 10:59:51:INFO:Received: evaluate message ed0e6ba3-59d0-4f38-a7c9-a702d7cbe6e1

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:59:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:00:54:INFO:
[92mINFO [0m:      Received: train message 0b53a88c-224c-4066-b080-08d4d6038eaf
02/05/2025 11:00:54:INFO:Received: train message 0b53a88c-224c-4066-b080-08d4d6038eaf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:02:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:03:06:INFO:
[92mINFO [0m:      Received: evaluate message b7694c4e-b469-4f37-b4b1-fd119146c87e
02/05/2025 11:03:06:INFO:Received: evaluate message b7694c4e-b469-4f37-b4b1-fd119146c87e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:03:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:51:INFO:
[92mINFO [0m:      Received: train message 0a3ba5d8-0f5e-4a6b-bf36-04a4397e9e1e
02/05/2025 11:04:51:INFO:Received: train message 0a3ba5d8-0f5e-4a6b-bf36-04a4397e9e1e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:06:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:08:08:INFO:
[92mINFO [0m:      Received: evaluate message 998dc5c6-629f-4612-81d1-a070fb5292fb
02/05/2025 11:08:08:INFO:Received: evaluate message 998dc5c6-629f-4612-81d1-a070fb5292fb

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:08:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:10:08:INFO:
[92mINFO [0m:      Received: train message c7d9de98-038d-4e02-93bf-f020566d676c
02/05/2025 11:10:08:INFO:Received: train message c7d9de98-038d-4e02-93bf-f020566d676c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:11:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:13:13:INFO:
[92mINFO [0m:      Received: evaluate message 5c944ebe-cadb-4a0f-99ae-e725ded4188e
02/05/2025 11:13:13:INFO:Received: evaluate message 5c944ebe-cadb-4a0f-99ae-e725ded4188e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:13:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:14:09:INFO:
[92mINFO [0m:      Received: train message 0c1ffde6-eb78-4050-9fc8-8e9365dd6753
02/05/2025 11:14:09:INFO:Received: train message 0c1ffde6-eb78-4050-9fc8-8e9365dd6753
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:15:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:17:28:INFO:
[92mINFO [0m:      Received: evaluate message 5c487622-935f-4497-835c-f645e6d65dad
02/05/2025 11:17:28:INFO:Received: evaluate message 5c487622-935f-4497-835c-f645e6d65dad

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:17:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:43:INFO:
[92mINFO [0m:      Received: train message 80727d00-bac6-4dc6-97df-8495ca1b87a0
02/05/2025 11:18:43:INFO:Received: train message 80727d00-bac6-4dc6-97df-8495ca1b87a0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:19:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:20:59:INFO:
[92mINFO [0m:      Received: evaluate message 8c9e2a0a-9e3f-4f04-adab-9694860d7eb1
02/05/2025 11:20:59:INFO:Received: evaluate message 8c9e2a0a-9e3f-4f04-adab-9694860d7eb1
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:21:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:30:INFO:
[92mINFO [0m:      Received: train message b363d774-ea6d-49ff-931b-953eb1fbfe37
02/05/2025 11:21:30:INFO:Received: train message b363d774-ea6d-49ff-931b-953eb1fbfe37
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:22:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:23:32:INFO:
[92mINFO [0m:      Received: evaluate message 2ae733e2-0114-4845-a2c1-90df297b7df3
02/05/2025 11:23:32:INFO:Received: evaluate message 2ae733e2-0114-4845-a2c1-90df297b7df3

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:23:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:24:10:INFO:
[92mINFO [0m:      Received: train message bd6ca6bf-d002-4f24-b898-fb154c393018
02/05/2025 11:24:10:INFO:Received: train message bd6ca6bf-d002-4f24-b898-fb154c393018
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:25:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:25:INFO:
[92mINFO [0m:      Received: evaluate message d32106f1-3847-42c7-b493-16c363877aa9
02/05/2025 11:26:25:INFO:Received: evaluate message d32106f1-3847-42c7-b493-16c363877aa9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:26:30:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:27:08:INFO:
[92mINFO [0m:      Received: train message b58a69f0-ec99-4b13-84e5-cc93a7b507f2
02/05/2025 11:27:08:INFO:Received: train message b58a69f0-ec99-4b13-84e5-cc93a7b507f2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:28:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:07:INFO:
[92mINFO [0m:      Received: evaluate message c5150cc3-a8dc-44d7-8b3c-cb34d557f4cb
02/05/2025 11:29:07:INFO:Received: evaluate message c5150cc3-a8dc-44d7-8b3c-cb34d557f4cb

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:29:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:45:INFO:
[92mINFO [0m:      Received: train message d6491bb1-d622-4476-b30b-960009e70617
02/05/2025 11:29:45:INFO:Received: train message d6491bb1-d622-4476-b30b-960009e70617
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:30:55:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:32:07:INFO:
[92mINFO [0m:      Received: evaluate message a887f641-1259-461d-9857-53913f715233
02/05/2025 11:32:07:INFO:Received: evaluate message a887f641-1259-461d-9857-53913f715233
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:32:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:32:49:INFO:
[92mINFO [0m:      Received: train message 6d3b0e28-6113-431e-b042-9797e9f77332
02/05/2025 11:32:49:INFO:Received: train message 6d3b0e28-6113-431e-b042-9797e9f77332
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:33:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:34:42:INFO:
[92mINFO [0m:      Received: evaluate message 660fce56-1252-47cd-9e1c-97e61453854c
02/05/2025 11:34:42:INFO:Received: evaluate message 660fce56-1252-47cd-9e1c-97e61453854c

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609, 1.06856795481353], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411, 0.8014472007956828], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894, 0.5738852152204384], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604, 0.5213103198308118]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:34:47:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:34:56:INFO:
[92mINFO [0m:      Received: reconnect message b437b78a-e8e4-4cd4-9a78-d9aeba91f90d
02/05/2025 11:34:56:INFO:Received: reconnect message b437b78a-e8e4-4cd4-9a78-d9aeba91f90d
02/05/2025 11:34:56:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:34:56:INFO:Disconnect and shut down

{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609, 1.06856795481353, 1.0267752834928512], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411, 0.8014472007956828, 0.8011279792333296], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894, 0.5738852152204384, 0.5912726894244645], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604, 0.5213103198308118, 0.5506124270879513]}



Final client history:
{'loss': [1.0837007416142814, 1.043205380672771, 1.1355634092632172, 1.1057051776255173, 1.1453039260279676, 1.0685648075577987, 1.1002541530682204, 1.0969796594555625, 1.1176141393287187, 1.083074988332738, 1.1617764976455236, 1.089933113906419, 1.099846787383726, 1.100461448981857, 1.0738126552132912, 1.0672176450933675, 1.100981664899926, 1.0518108773641608, 1.0608476461890715, 1.0819784754696444, 1.038023872940311, 1.0768852568492935, 1.069915803080895, 1.0555098961348455, 1.0602724756273654, 1.0511923531613563, 1.0532370351831646, 1.0443571652165609, 1.06856795481353, 1.0267752834928512], 'accuracy': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'auc': [0.714457107011283, 0.7407919892413165, 0.7521896412620028, 0.7589031379024227, 0.7623792700712373, 0.7677941084447658, 0.7698406197522176, 0.7738830791467564, 0.7759469567848631, 0.7775314691391562, 0.7785968346002009, 0.7818292445889371, 0.783319758391369, 0.7847556783363081, 0.7872858517278245, 0.787691180545923, 0.7884253995904047, 0.7902236080150578, 0.7900795914554773, 0.7921463780439142, 0.7932584734664857, 0.7940361877609079, 0.7946575006266114, 0.7951758674471235, 0.7967483318716694, 0.7971762040484442, 0.7988305523799076, 0.7997313221280411, 0.8014472007956828, 0.8011279792333296], 'precision': [0.4117264737633955, 0.4429501750516807, 0.43213044534800177, 0.44904392515208313, 0.435431437037429, 0.46451387916162123, 0.45893211092171043, 0.5959344526305321, 0.5918263219521416, 0.6062798474453706, 0.5860461315920721, 0.5809808571124357, 0.6107371686292368, 0.6134811433818737, 0.5929851437090023, 0.6011188425748236, 0.5872135840074293, 0.5973934010956745, 0.6003333028792696, 0.5820432301007545, 0.6067100131212719, 0.6038884903048661, 0.5898287162293437, 0.5951773485680159, 0.5821855569654759, 0.598875317049299, 0.5908483054722141, 0.6090078544403894, 0.5738852152204384, 0.5912726894244645], 'recall': [0.5175918686473807, 0.5363565285379203, 0.5340109460516028, 0.5449569976544175, 0.5355746677091477, 0.5527756059421423, 0.5566849100860047, 0.5535574667709148, 0.5512118842845973, 0.562157935887412, 0.547302580140735, 0.5645035183737295, 0.5676309616888194, 0.5684128225175918, 0.5754495699765442, 0.5785770132916341, 0.5707584050039093, 0.5809225957779516, 0.5762314308053167, 0.5762314308053167, 0.5801407349491791, 0.5871774824081314, 0.5793588741204065, 0.584831899921814, 0.5793588741204065, 0.5895230648944488, 0.5863956215793589, 0.5863956215793589, 0.5746677091477717, 0.5942142298670836], 'f1': [0.42093829387716764, 0.4824075932878562, 0.4632187645147562, 0.48813364991003466, 0.4671627453556531, 0.5043215844601998, 0.4977721718957445, 0.4962790890954825, 0.48816571454443464, 0.508012016339495, 0.47549865471817865, 0.5070091137147035, 0.5122833395754048, 0.5157376840611355, 0.5220548131999179, 0.5268304608977814, 0.5152614607076919, 0.533482181022095, 0.5294987743827624, 0.5239093203811733, 0.5358362435223645, 0.5334013466258085, 0.528169325733427, 0.5362623859925574, 0.5305659288693126, 0.5434087533402058, 0.5368746302066135, 0.5366918896698604, 0.5213103198308118, 0.5506124270879513]}

