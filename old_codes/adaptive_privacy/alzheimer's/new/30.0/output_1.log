nohup: ignoring input
02/05/2025 10:03:28:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:03:28:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:03:28:DEBUG:ChannelConnectivity.CONNECTING
02/05/2025 10:03:28:DEBUG:ChannelConnectivity.READY
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778608.723144 1804199 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      
02/05/2025 10:03:52:INFO:
[92mINFO [0m:      Received: train message fd009a89-4dbf-4514-877d-e28437faf9e5
02/05/2025 10:03:52:INFO:Received: train message fd009a89-4dbf-4514-877d-e28437faf9e5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:04:57:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:04:INFO:
[92mINFO [0m:      Received: evaluate message 79326f95-8bce-4a7c-81a8-194d002115e3
02/05/2025 10:06:04:INFO:Received: evaluate message 79326f95-8bce-4a7c-81a8-194d002115e3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:06:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:42:INFO:
[92mINFO [0m:      Received: train message 7fc04091-ac86-468f-ab96-7110f4509cff
02/05/2025 10:06:42:INFO:Received: train message 7fc04091-ac86-468f-ab96-7110f4509cff
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:07:42:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:08:19:INFO:
[92mINFO [0m:      Received: evaluate message b511aaae-5fd2-432e-b666-c514cceb7f1a
02/05/2025 10:08:19:INFO:Received: evaluate message b511aaae-5fd2-432e-b666-c514cceb7f1a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:08:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:09:INFO:
[92mINFO [0m:      Received: train message 8c24b8a1-e68c-4c1e-a411-da4d50ccceab
02/05/2025 10:09:09:INFO:Received: train message 8c24b8a1-e68c-4c1e-a411-da4d50ccceab
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:10:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:03:INFO:
[92mINFO [0m:      Received: evaluate message 4d7370b0-b1a3-467d-83ed-d35075fdc5d0
02/05/2025 10:11:03:INFO:Received: evaluate message 4d7370b0-b1a3-467d-83ed-d35075fdc5d0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:11:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:37:INFO:
[92mINFO [0m:      Received: train message de599e13-6f2c-4597-8970-2ea607e4cc34
02/05/2025 10:11:37:INFO:Received: train message de599e13-6f2c-4597-8970-2ea607e4cc34
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:12:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:30:INFO:
[92mINFO [0m:      Received: evaluate message 880f0c77-764a-4d23-bb9c-e56da6a7604f
02/05/2025 10:13:30:INFO:Received: evaluate message 880f0c77-764a-4d23-bb9c-e56da6a7604f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:13:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:14:04:INFO:
[92mINFO [0m:      Received: train message 927e28a3-116f-4943-9424-fc1f7cce7dc4
02/05/2025 10:14:04:INFO:Received: train message 927e28a3-116f-4943-9424-fc1f7cce7dc4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:15:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:15:51:INFO:
[92mINFO [0m:      Received: evaluate message b122305b-39c5-4008-adf5-48773cea55d4
02/05/2025 10:15:51:INFO:Received: evaluate message b122305b-39c5-4008-adf5-48773cea55d4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:15:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:31:INFO:
[92mINFO [0m:      Received: train message 5d3b5423-c6b7-41cf-a350-a32aa761b592
02/05/2025 10:16:31:INFO:Received: train message 5d3b5423-c6b7-41cf-a350-a32aa761b592
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:17:29:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:18:INFO:
[92mINFO [0m:      Received: evaluate message 06e8e4b8-8274-433d-853f-724a46811599
02/05/2025 10:18:18:INFO:Received: evaluate message 06e8e4b8-8274-433d-853f-724a46811599
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:18:22:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:19:16:INFO:
[92mINFO [0m:      Received: train message 58ea09e2-0ed5-44a7-9d84-250d0672c5c9
02/05/2025 10:19:16:INFO:Received: train message 58ea09e2-0ed5-44a7-9d84-250d0672c5c9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:21:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:22:03:INFO:
[92mINFO [0m:      Received: evaluate message 72eec9ee-06dd-457f-9335-9c4290f3d8d7
02/05/2025 10:22:03:INFO:Received: evaluate message 72eec9ee-06dd-457f-9335-9c4290f3d8d7
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986], 'accuracy': [0.5160281469898358], 'auc': [0.7174808607527126], 'precision': [0.4096278307389395], 'recall': [0.5160281469898358], 'f1': [0.42340290242033957]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763], 'accuracy': [0.5160281469898358, 0.5371383893666928], 'auc': [0.7174808607527126, 0.743196921722251], 'precision': [0.4096278307389395, 0.4446497303781511], 'recall': [0.5160281469898358, 0.5371383893666928], 'f1': [0.42340290242033957, 0.48416854046252017]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:22:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:22:46:INFO:
[92mINFO [0m:      Received: train message 4720853b-f0ce-43cc-8a65-0d2f80732b68
02/05/2025 10:22:46:INFO:Received: train message 4720853b-f0ce-43cc-8a65-0d2f80732b68
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:23:44:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:25:INFO:
[92mINFO [0m:      Received: evaluate message 164814ff-e11a-47ed-a4df-72a6572a9b51
02/05/2025 10:24:25:INFO:Received: evaluate message 164814ff-e11a-47ed-a4df-72a6572a9b51
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:24:29:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:25:17:INFO:
[92mINFO [0m:      Received: train message 2d19bb03-1feb-49e4-a0f8-abef547d50b2
02/05/2025 10:25:17:INFO:Received: train message 2d19bb03-1feb-49e4-a0f8-abef547d50b2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:26:17:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:10:INFO:
[92mINFO [0m:      Received: evaluate message 8dbb27fb-e7aa-4fca-a738-bc92d3d82122
02/05/2025 10:27:10:INFO:Received: evaluate message 8dbb27fb-e7aa-4fca-a738-bc92d3d82122
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:27:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:31:INFO:
[92mINFO [0m:      Received: train message c8614c16-d6ee-4ff7-9ecc-9cb00da33361
02/05/2025 10:27:31:INFO:Received: train message c8614c16-d6ee-4ff7-9ecc-9cb00da33361
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:28:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:30:INFO:
[92mINFO [0m:      Received: evaluate message 012c9062-0b35-4c81-beeb-df4dadffd582
02/05/2025 10:29:30:INFO:Received: evaluate message 012c9062-0b35-4c81-beeb-df4dadffd582
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:29:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:30:05:INFO:
[92mINFO [0m:      Received: train message 550d401d-7e3d-4d6d-946b-4f4d65fcfd7f
02/05/2025 10:30:05:INFO:Received: train message 550d401d-7e3d-4d6d-946b-4f4d65fcfd7f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:31:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:31:52:INFO:
[92mINFO [0m:      Received: evaluate message 42557fc7-fc26-42ed-9dab-e734d74f3a76
02/05/2025 10:31:52:INFO:Received: evaluate message 42557fc7-fc26-42ed-9dab-e734d74f3a76
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:31:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:31:INFO:
[92mINFO [0m:      Received: train message b9cd3afd-c4a1-48a6-856a-885e84e3060e
02/05/2025 10:32:31:INFO:Received: train message b9cd3afd-c4a1-48a6-856a-885e84e3060e

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:33:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:14:INFO:
[92mINFO [0m:      Received: evaluate message cffe4578-6715-4da7-8b9e-01d8caae2fec
02/05/2025 10:34:14:INFO:Received: evaluate message cffe4578-6715-4da7-8b9e-01d8caae2fec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:34:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:25:INFO:
[92mINFO [0m:      Received: train message 596dbee5-9109-4113-9fd3-35fb2e60e9ec
02/05/2025 10:35:25:INFO:Received: train message 596dbee5-9109-4113-9fd3-35fb2e60e9ec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:36:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:36:54:INFO:
[92mINFO [0m:      Received: evaluate message fbb27a35-672c-433d-98ae-1ea99295361a
02/05/2025 10:36:54:INFO:Received: evaluate message fbb27a35-672c-433d-98ae-1ea99295361a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:37:00:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:37:41:INFO:
[92mINFO [0m:      Received: train message 065c514e-5de0-41a5-b6b0-91d2d8c291f3
02/05/2025 10:37:41:INFO:Received: train message 065c514e-5de0-41a5-b6b0-91d2d8c291f3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:38:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:49:INFO:
[92mINFO [0m:      Received: evaluate message 9e5ae2a8-ce15-44ef-8602-c02f6b01fc4c
02/05/2025 10:39:49:INFO:Received: evaluate message 9e5ae2a8-ce15-44ef-8602-c02f6b01fc4c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:39:53:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:25:INFO:
[92mINFO [0m:      Received: train message d7a44423-7a65-4834-8264-075b94b46f3f
02/05/2025 10:40:25:INFO:Received: train message d7a44423-7a65-4834-8264-075b94b46f3f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:41:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:42:01:INFO:
[92mINFO [0m:      Received: evaluate message 99bcfd9a-69a0-49c5-88dd-ad756d15032f
02/05/2025 10:42:01:INFO:Received: evaluate message 99bcfd9a-69a0-49c5-88dd-ad756d15032f
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:42:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:06:INFO:
[92mINFO [0m:      Received: train message 9c6d1f1f-41d8-4bf5-9619-31f3677cb72a
02/05/2025 10:43:06:INFO:Received: train message 9c6d1f1f-41d8-4bf5-9619-31f3677cb72a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:44:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:44:32:INFO:
[92mINFO [0m:      Received: evaluate message 97aac08c-2dce-4a48-949d-c9878b2eed15
02/05/2025 10:44:32:INFO:Received: evaluate message 97aac08c-2dce-4a48-949d-c9878b2eed15
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:44:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:21:INFO:
[92mINFO [0m:      Received: train message ad6a9269-4702-4f1c-b951-3103f5da1358
02/05/2025 10:45:21:INFO:Received: train message ad6a9269-4702-4f1c-b951-3103f5da1358
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:46:18:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:47:31:INFO:
[92mINFO [0m:      Received: evaluate message ecc99b22-4df3-435f-b610-baab686d7c2d
02/05/2025 10:47:31:INFO:Received: evaluate message ecc99b22-4df3-435f-b610-baab686d7c2d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:47:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:48:42:INFO:
[92mINFO [0m:      Received: train message 9f038a52-d162-499b-ad6d-f1d54b0c4dc2
02/05/2025 10:48:42:INFO:Received: train message 9f038a52-d162-499b-ad6d-f1d54b0c4dc2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:49:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:50:41:INFO:
[92mINFO [0m:      Received: evaluate message 6a673ffa-df05-4976-875d-d22132abd544
02/05/2025 10:50:41:INFO:Received: evaluate message 6a673ffa-df05-4976-875d-d22132abd544

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:50:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:51:09:INFO:
[92mINFO [0m:      Received: train message d41df87d-5bec-4ce4-a447-ca44736e5647
02/05/2025 10:51:09:INFO:Received: train message d41df87d-5bec-4ce4-a447-ca44736e5647
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:52:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:53:43:INFO:
[92mINFO [0m:      Received: evaluate message 0c79b5a9-53a7-457c-99f7-4c348ddc4dec
02/05/2025 10:53:43:INFO:Received: evaluate message 0c79b5a9-53a7-457c-99f7-4c348ddc4dec
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:53:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:54:48:INFO:
[92mINFO [0m:      Received: train message 5fdf6005-8857-48f7-b0c3-0422cef53004
02/05/2025 10:54:48:INFO:Received: train message 5fdf6005-8857-48f7-b0c3-0422cef53004
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:56:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:57:19:INFO:
[92mINFO [0m:      Received: evaluate message c87eba9d-9b94-4671-b4ad-6432d9d969f4
02/05/2025 10:57:19:INFO:Received: evaluate message c87eba9d-9b94-4671-b4ad-6432d9d969f4

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:57:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:58:46:INFO:
[92mINFO [0m:      Received: train message 785988c1-2679-4aab-884c-2741f3cd021e
02/05/2025 10:58:46:INFO:Received: train message 785988c1-2679-4aab-884c-2741f3cd021e
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:00:16:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:01:28:INFO:
[92mINFO [0m:      Received: evaluate message 49138741-9e20-4222-acf7-7cae97fa8cbc
02/05/2025 11:01:28:INFO:Received: evaluate message 49138741-9e20-4222-acf7-7cae97fa8cbc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:01:36:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:02:24:INFO:
[92mINFO [0m:      Received: train message c4270798-2aea-4220-85ec-0a75da293d22
02/05/2025 11:02:24:INFO:Received: train message c4270798-2aea-4220-85ec-0a75da293d22
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:03:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:29:INFO:
[92mINFO [0m:      Received: evaluate message feedcbf2-c6a3-48e8-9c25-fd231c99b93a
02/05/2025 11:04:29:INFO:Received: evaluate message feedcbf2-c6a3-48e8-9c25-fd231c99b93a

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:04:34:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:06:26:INFO:
[92mINFO [0m:      Received: train message 426a1f6e-d2c1-45af-b172-b3ab413575bc
02/05/2025 11:06:26:INFO:Received: train message 426a1f6e-d2c1-45af-b172-b3ab413575bc
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:07:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:09:12:INFO:
[92mINFO [0m:      Received: evaluate message 276f3f5b-92cb-4493-8e4c-9ef728ae4c44
02/05/2025 11:09:12:INFO:Received: evaluate message 276f3f5b-92cb-4493-8e4c-9ef728ae4c44
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:09:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:10:13:INFO:
[92mINFO [0m:      Received: train message 61fa10ad-59bc-4f3c-96f4-34cc57153dcf
02/05/2025 11:10:13:INFO:Received: train message 61fa10ad-59bc-4f3c-96f4-34cc57153dcf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:11:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:13:09:INFO:
[92mINFO [0m:      Received: evaluate message 64304ecd-b090-46a4-a012-d90a607cc280
02/05/2025 11:13:09:INFO:Received: evaluate message 64304ecd-b090-46a4-a012-d90a607cc280

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:13:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:14:30:INFO:
[92mINFO [0m:      Received: train message 7e370460-11e4-4af0-971e-9f692d6b62f4
02/05/2025 11:14:30:INFO:Received: train message 7e370460-11e4-4af0-971e-9f692d6b62f4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:15:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:16:57:INFO:
[92mINFO [0m:      Received: evaluate message 98553eb3-d562-4b7c-b8d5-366732ca15d4
02/05/2025 11:16:57:INFO:Received: evaluate message 98553eb3-d562-4b7c-b8d5-366732ca15d4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:17:03:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:28:INFO:
[92mINFO [0m:      Received: train message e7065cdc-ae27-4df2-a9de-6d92806be9f5
02/05/2025 11:18:28:INFO:Received: train message e7065cdc-ae27-4df2-a9de-6d92806be9f5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:19:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:20:54:INFO:
[92mINFO [0m:      Received: evaluate message 33984dae-956d-4abe-b599-054bda3762b0
02/05/2025 11:20:54:INFO:Received: evaluate message 33984dae-956d-4abe-b599-054bda3762b0

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:20:59:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:36:INFO:
[92mINFO [0m:      Received: train message f16b4881-be27-4d23-94b4-c5ad8c97a7a7
02/05/2025 11:21:36:INFO:Received: train message f16b4881-be27-4d23-94b4-c5ad8c97a7a7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:22:53:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:23:44:INFO:
[92mINFO [0m:      Received: evaluate message 036122f3-d7e4-4f53-804b-d52dd21a8ff2
02/05/2025 11:23:44:INFO:Received: evaluate message 036122f3-d7e4-4f53-804b-d52dd21a8ff2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:23:49:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:24:33:INFO:
[92mINFO [0m:      Received: train message 25794191-8895-4b5c-9f51-c12d3960157d
02/05/2025 11:24:33:INFO:Received: train message 25794191-8895-4b5c-9f51-c12d3960157d
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:25:44:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:36:INFO:
[92mINFO [0m:      Received: evaluate message efa2674e-350a-47af-8204-1a8c5a28e45b
02/05/2025 11:26:36:INFO:Received: evaluate message efa2674e-350a-47af-8204-1a8c5a28e45b

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:26:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:27:22:INFO:
[92mINFO [0m:      Received: train message 373af72c-d109-4982-9387-5fc5db13fba9
02/05/2025 11:27:22:INFO:Received: train message 373af72c-d109-4982-9387-5fc5db13fba9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:28:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:12:INFO:
[92mINFO [0m:      Received: evaluate message e873961a-933e-43c9-9228-585b535b13db
02/05/2025 11:29:12:INFO:Received: evaluate message e873961a-933e-43c9-9228-585b535b13db
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:29:17:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:56:INFO:
[92mINFO [0m:      Received: train message 944d77e6-6c84-4ab0-ac0f-5fca933aa6e8
02/05/2025 11:29:56:INFO:Received: train message 944d77e6-6c84-4ab0-ac0f-5fca933aa6e8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:31:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:32:01:INFO:
[92mINFO [0m:      Received: evaluate message 301284b7-cf81-40fc-bf5c-b60bfd11ae2d
02/05/2025 11:32:01:INFO:Received: evaluate message 301284b7-cf81-40fc-bf5c-b60bfd11ae2d

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175, 1.0642800787820585], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442, 0.8032596909451641], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128, 0.5765473316940177], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563, 0.5233120793052078]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1806, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:32:06:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:32:07:INFO:
[92mINFO [0m:      Received: reconnect message 7b2ac336-6b9e-4706-868f-6116feb1f399
02/05/2025 11:32:07:INFO:Received: reconnect message 7b2ac336-6b9e-4706-868f-6116feb1f399
02/05/2025 11:32:07:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:32:07:INFO:Disconnect and shut down
