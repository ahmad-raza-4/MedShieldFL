nohup: ignoring input
02/05/2025 10:03:20:DEBUG:Opened insecure gRPC connection (no certificates were passed)
02/05/2025 10:03:20:DEBUG:ChannelConnectivity.IDLE
02/05/2025 10:03:20:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
02/05/2025 10:03:20:INFO:
[92mINFO [0m:      Received: get_parameters message 5f80d42d-c68b-4248-9099-6fa0c945269c
02/05/2025 10:03:20:INFO:Received: get_parameters message 5f80d42d-c68b-4248-9099-6fa0c945269c
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738778600.327051 1794936 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers
[92mINFO [0m:      Sent reply
02/05/2025 10:03:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:04:03:INFO:
[92mINFO [0m:      Received: train message 3c97ce84-9d72-4528-9001-4d1400a1f6ce
02/05/2025 10:04:03:INFO:Received: train message 3c97ce84-9d72-4528-9001-4d1400a1f6ce
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:04:41:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:05:52:INFO:
[92mINFO [0m:      Received: evaluate message de774444-ce66-435e-af25-0a044b975be9
02/05/2025 10:05:52:INFO:Received: evaluate message de774444-ce66-435e-af25-0a044b975be9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:05:58:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:06:46:INFO:
[92mINFO [0m:      Received: train message 4182524d-b350-4a0a-9609-c9b4821bd781
02/05/2025 10:06:46:INFO:Received: train message 4182524d-b350-4a0a-9609-c9b4821bd781
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:07:23:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:08:22:INFO:
[92mINFO [0m:      Received: evaluate message c7c126bc-374b-4f1c-8ec2-90757849e1a4
02/05/2025 10:08:22:INFO:Received: evaluate message c7c126bc-374b-4f1c-8ec2-90757849e1a4
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:08:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:09:21:INFO:
[92mINFO [0m:      Received: train message 35621832-0f65-4e74-bd51-6a019cc0332a
02/05/2025 10:09:21:INFO:Received: train message 35621832-0f65-4e74-bd51-6a019cc0332a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:10:01:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:03:INFO:
[92mINFO [0m:      Received: evaluate message 25fa2c29-aebe-4e79-ab1d-9fff017f5cf3
02/05/2025 10:11:03:INFO:Received: evaluate message 25fa2c29-aebe-4e79-ab1d-9fff017f5cf3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:11:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:11:44:INFO:
[92mINFO [0m:      Received: train message 79eb8176-74db-4fa7-8bc6-e925ad6714f6
02/05/2025 10:11:44:INFO:Received: train message 79eb8176-74db-4fa7-8bc6-e925ad6714f6
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:12:21:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:13:15:INFO:
[92mINFO [0m:      Received: evaluate message 70454af1-3e3a-4c52-81a1-5fc67ed11e60
02/05/2025 10:13:15:INFO:Received: evaluate message 70454af1-3e3a-4c52-81a1-5fc67ed11e60
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:13:21:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:14:04:INFO:
[92mINFO [0m:      Received: train message d2afd8d3-96c5-4d14-9797-0e8e90a01411
02/05/2025 10:14:04:INFO:Received: train message d2afd8d3-96c5-4d14-9797-0e8e90a01411
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:14:40:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:00:INFO:
[92mINFO [0m:      Received: evaluate message 4ff016fc-98da-4257-b5e7-984c860489dd
02/05/2025 10:16:00:INFO:Received: evaluate message 4ff016fc-98da-4257-b5e7-984c860489dd
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:16:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:16:45:INFO:
[92mINFO [0m:      Received: train message 926e918b-7c8e-434d-8a58-0f718a74b379
02/05/2025 10:16:45:INFO:Received: train message 926e918b-7c8e-434d-8a58-0f718a74b379
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:17:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:18:08:INFO:
[92mINFO [0m:      Received: evaluate message 3401f1b7-e2af-4066-82df-e8a5931996a9
02/05/2025 10:18:08:INFO:Received: evaluate message 3401f1b7-e2af-4066-82df-e8a5931996a9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:18:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:19:09:INFO:
[92mINFO [0m:      Received: train message 9128f8bb-e4a5-4763-800d-5922fcdb9c1a
02/05/2025 10:19:09:INFO:Received: train message 9128f8bb-e4a5-4763-800d-5922fcdb9c1a
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:19:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:22:03:INFO:
[92mINFO [0m:      Received: evaluate message e414e1ae-c806-405e-b4ef-d0d352d08fcc
02/05/2025 10:22:03:INFO:Received: evaluate message e414e1ae-c806-405e-b4ef-d0d352d08fcc
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 6400, num_classes: 4

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1a: Client Initialized
Step 1b: Recomputing FIM for epoch 1
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986], 'accuracy': [0.5160281469898358], 'auc': [0.7174808607527126], 'precision': [0.4096278307389395], 'recall': [0.5160281469898358], 'f1': [0.42340290242033957]}

Step 1b: Recomputing FIM for epoch 2
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763], 'accuracy': [0.5160281469898358, 0.5371383893666928], 'auc': [0.7174808607527126, 0.743196921722251], 'precision': [0.4096278307389395, 0.4446497303781511], 'recall': [0.5160281469898358, 0.5371383893666928], 'f1': [0.42340290242033957, 0.48416854046252017]}

Step 1b: Recomputing FIM for epoch 3
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665]}

Step 1b: Recomputing FIM for epoch 4
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255]}

Step 1b: Recomputing FIM for epoch 5
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093]}

Step 1b: Recomputing FIM for epoch 6
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021]}

Step 1b: Recomputing FIM for epoch 7
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:22:08:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:22:24:INFO:
[92mINFO [0m:      Received: train message 089984f3-f6c6-42b2-a0a9-6340f4131721
02/05/2025 10:22:24:INFO:Received: train message 089984f3-f6c6-42b2-a0a9-6340f4131721
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:22:56:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:24:29:INFO:
[92mINFO [0m:      Received: evaluate message 48b49f3d-1fd6-4091-ab8f-efe1ee9d2e11
02/05/2025 10:24:29:INFO:Received: evaluate message 48b49f3d-1fd6-4091-ab8f-efe1ee9d2e11
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:24:35:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:25:09:INFO:
[92mINFO [0m:      Received: train message f11ab9aa-7551-475f-9773-e4bf0b0bfd81
02/05/2025 10:25:09:INFO:Received: train message f11ab9aa-7551-475f-9773-e4bf0b0bfd81
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:25:48:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:06:INFO:
[92mINFO [0m:      Received: evaluate message 8e25e631-9fdf-4cc5-a243-cc014ae765b8
02/05/2025 10:27:06:INFO:Received: evaluate message 8e25e631-9fdf-4cc5-a243-cc014ae765b8
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:27:11:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:27:36:INFO:
[92mINFO [0m:      Received: train message d9f05f96-c8b4-4828-89b2-ff0509ef8b70
02/05/2025 10:27:36:INFO:Received: train message d9f05f96-c8b4-4828-89b2-ff0509ef8b70
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:28:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:24:INFO:
[92mINFO [0m:      Received: evaluate message ceee4463-9b6c-4243-b814-216dc23a28b0
02/05/2025 10:29:24:INFO:Received: evaluate message ceee4463-9b6c-4243-b814-216dc23a28b0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:29:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:29:57:INFO:
[92mINFO [0m:      Received: train message e0efd3fd-2251-4398-93fb-f9db61a9591f
02/05/2025 10:29:57:INFO:Received: train message e0efd3fd-2251-4398-93fb-f9db61a9591f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:30:31:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:04:INFO:
[92mINFO [0m:      Received: evaluate message a41d4ef1-1aef-49c2-a925-8a6140dcd858
02/05/2025 10:32:04:INFO:Received: evaluate message a41d4ef1-1aef-49c2-a925-8a6140dcd858
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:32:09:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:32:22:INFO:
[92mINFO [0m:      Received: train message 1b7b4593-dc9d-48ae-bbca-58d64fa69970
02/05/2025 10:32:22:INFO:Received: train message 1b7b4593-dc9d-48ae-bbca-58d64fa69970

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439]}

Step 1b: Recomputing FIM for epoch 8
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995]}

Step 1b: Recomputing FIM for epoch 9
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436]}

Step 1b: Recomputing FIM for epoch 10
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803]}

Step 1b: Recomputing FIM for epoch 11
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797]}

/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:32:53:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:34:32:INFO:
[92mINFO [0m:      Received: evaluate message 4d7d5f82-3a5b-42c2-88d6-f43876ef5d90
02/05/2025 10:34:32:INFO:Received: evaluate message 4d7d5f82-3a5b-42c2-88d6-f43876ef5d90
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:34:37:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:35:10:INFO:
[92mINFO [0m:      Received: train message a08ac3d6-c07f-4082-952c-d2abcb533a9f
02/05/2025 10:35:10:INFO:Received: train message a08ac3d6-c07f-4082-952c-d2abcb533a9f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:35:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:37:16:INFO:
[92mINFO [0m:      Received: evaluate message 80f7bbd3-9331-4c63-8a8a-4e2842e44019
02/05/2025 10:37:16:INFO:Received: evaluate message 80f7bbd3-9331-4c63-8a8a-4e2842e44019
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:37:20:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:38:00:INFO:
[92mINFO [0m:      Received: train message 5b544e36-8243-47d1-b2fa-98c4ec3a98e2
02/05/2025 10:38:00:INFO:Received: train message 5b544e36-8243-47d1-b2fa-98c4ec3a98e2
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:38:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:39:24:INFO:
[92mINFO [0m:      Received: evaluate message 16dcb2ac-4cfa-46f9-aec5-1cb7a54d6659
02/05/2025 10:39:24:INFO:Received: evaluate message 16dcb2ac-4cfa-46f9-aec5-1cb7a54d6659
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:39:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:40:32:INFO:
[92mINFO [0m:      Received: train message 8a170a7b-9343-469f-8553-009b39ce932f
02/05/2025 10:40:32:INFO:Received: train message 8a170a7b-9343-469f-8553-009b39ce932f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:41:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:42:20:INFO:
[92mINFO [0m:      Received: evaluate message 5f6cf89a-0c34-4620-a23b-3338fdfb4e4c
02/05/2025 10:42:20:INFO:Received: evaluate message 5f6cf89a-0c34-4620-a23b-3338fdfb4e4c
Step 1b: Recomputing FIM for epoch 12
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974]}

Step 1b: Recomputing FIM for epoch 13
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089]}

Step 1b: Recomputing FIM for epoch 14
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874]}

Step 1b: Recomputing FIM for epoch 15
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:42:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:43:05:INFO:
[92mINFO [0m:      Received: train message 774cff9f-441c-4bea-ad40-9610c5ea7124
02/05/2025 10:43:05:INFO:Received: train message 774cff9f-441c-4bea-ad40-9610c5ea7124
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:43:43:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:44:58:INFO:
[92mINFO [0m:      Received: evaluate message e6969b2b-7627-4a22-8b99-9def7c1a78b3
02/05/2025 10:44:58:INFO:Received: evaluate message e6969b2b-7627-4a22-8b99-9def7c1a78b3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:45:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:45:41:INFO:
[92mINFO [0m:      Received: train message 222add0b-5fcc-4236-bab0-4b5a37c3fa6b
02/05/2025 10:45:41:INFO:Received: train message 222add0b-5fcc-4236-bab0-4b5a37c3fa6b
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:46:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:47:44:INFO:
[92mINFO [0m:      Received: evaluate message 645b6ede-9393-44f6-a01f-7c73baae3fa0
02/05/2025 10:47:44:INFO:Received: evaluate message 645b6ede-9393-44f6-a01f-7c73baae3fa0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:47:50:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:48:43:INFO:
[92mINFO [0m:      Received: train message d7ec3c4e-1cc6-4e94-9715-498729262cd5
02/05/2025 10:48:43:INFO:Received: train message d7ec3c4e-1cc6-4e94-9715-498729262cd5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:49:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:50:41:INFO:
[92mINFO [0m:      Received: evaluate message 7a42aecd-922a-4755-a5fa-4e6f4562fab5
02/05/2025 10:50:41:INFO:Received: evaluate message 7a42aecd-922a-4755-a5fa-4e6f4562fab5

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702]}

Step 1b: Recomputing FIM for epoch 16
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207]}

Step 1b: Recomputing FIM for epoch 17
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588]}

Step 1b: Recomputing FIM for epoch 18
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:50:46:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:51:26:INFO:
[92mINFO [0m:      Received: train message 642da6b3-76d9-48d6-8d9a-47799fdfd7fb
02/05/2025 10:51:26:INFO:Received: train message 642da6b3-76d9-48d6-8d9a-47799fdfd7fb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:52:15:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:53:47:INFO:
[92mINFO [0m:      Received: evaluate message 6df0b8f5-a9d7-4163-aa71-9ce86115cc75
02/05/2025 10:53:47:INFO:Received: evaluate message 6df0b8f5-a9d7-4163-aa71-9ce86115cc75
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:53:53:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:54:31:INFO:
[92mINFO [0m:      Received: train message 194c315d-5ffc-4f62-8ea6-bc13b8733fd9
02/05/2025 10:54:31:INFO:Received: train message 194c315d-5ffc-4f62-8ea6-bc13b8733fd9
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:55:25:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:57:21:INFO:
[92mINFO [0m:      Received: evaluate message 13a81ab3-323d-4798-900b-a9704bc3f67a
02/05/2025 10:57:21:INFO:Received: evaluate message 13a81ab3-323d-4798-900b-a9704bc3f67a

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922]}

Step 1b: Recomputing FIM for epoch 19
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271]}

Step 1b: Recomputing FIM for epoch 20
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 10:57:39:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 10:58:46:INFO:
[92mINFO [0m:      Received: train message 8e5cec19-dc3e-4919-a32c-d0170175b29c
02/05/2025 10:58:46:INFO:Received: train message 8e5cec19-dc3e-4919-a32c-d0170175b29c
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 10:59:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:01:07:INFO:
[92mINFO [0m:      Received: evaluate message aec59007-a0ff-4539-b722-782dc593afe0
02/05/2025 11:01:07:INFO:Received: evaluate message aec59007-a0ff-4539-b722-782dc593afe0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:01:17:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:02:34:INFO:
[92mINFO [0m:      Received: train message 25cd3297-407e-425c-a6eb-60bee55537c5
02/05/2025 11:02:34:INFO:Received: train message 25cd3297-407e-425c-a6eb-60bee55537c5
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:03:27:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:04:57:INFO:
[92mINFO [0m:      Received: evaluate message bb555287-6654-441a-9b90-018668c48a31
02/05/2025 11:04:57:INFO:Received: evaluate message bb555287-6654-441a-9b90-018668c48a31

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173]}

Step 1b: Recomputing FIM for epoch 21
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692]}

Step 1b: Recomputing FIM for epoch 22
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:05:05:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:06:23:INFO:
[92mINFO [0m:      Received: train message b15a3079-2309-48d0-a745-c13faafb3ddf
02/05/2025 11:06:23:INFO:Received: train message b15a3079-2309-48d0-a745-c13faafb3ddf
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:07:10:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:09:13:INFO:
[92mINFO [0m:      Received: evaluate message a7c114db-cd55-4ef8-b10e-04e00697b83f
02/05/2025 11:09:13:INFO:Received: evaluate message a7c114db-cd55-4ef8-b10e-04e00697b83f
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:09:19:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:10:33:INFO:
[92mINFO [0m:      Received: train message 98cf497d-a0b2-483b-90d1-2e9c85ea7085
02/05/2025 11:10:33:INFO:Received: train message 98cf497d-a0b2-483b-90d1-2e9c85ea7085
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:11:24:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:12:56:INFO:
[92mINFO [0m:      Received: evaluate message b9fb0576-52ab-4883-a820-a3d2a59892f4
02/05/2025 11:12:56:INFO:Received: evaluate message b9fb0576-52ab-4883-a820-a3d2a59892f4

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129]}

Step 1b: Recomputing FIM for epoch 23
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608]}

Step 1b: Recomputing FIM for epoch 24
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:13:02:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:13:51:INFO:
[92mINFO [0m:      Received: train message 2752e529-0c7f-48b5-a603-5bfef99b5b86
02/05/2025 11:13:51:INFO:Received: train message 2752e529-0c7f-48b5-a603-5bfef99b5b86
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:14:33:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:16:58:INFO:
[92mINFO [0m:      Received: evaluate message e9bf90db-7360-47e1-bf98-86f7aba8d2d0
02/05/2025 11:16:58:INFO:Received: evaluate message e9bf90db-7360-47e1-bf98-86f7aba8d2d0
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:17:13:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:18:47:INFO:
[92mINFO [0m:      Received: train message 86d76e79-a3f8-43c2-9151-37fabfae9100
02/05/2025 11:18:47:INFO:Received: train message 86d76e79-a3f8-43c2-9151-37fabfae9100
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:19:38:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:20:49:INFO:
[92mINFO [0m:      Received: evaluate message 464e152b-2239-4c13-96f1-ec555a56236e
02/05/2025 11:20:49:INFO:Received: evaluate message 464e152b-2239-4c13-96f1-ec555a56236e

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475]}

Step 1b: Recomputing FIM for epoch 25
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585]}

Step 1b: Recomputing FIM for epoch 26
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:20:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:21:27:INFO:
[92mINFO [0m:      Received: train message b4335db8-6403-4633-9445-0261f65754eb
02/05/2025 11:21:27:INFO:Received: train message b4335db8-6403-4633-9445-0261f65754eb
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:22:16:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:23:27:INFO:
[92mINFO [0m:      Received: evaluate message fecd3850-8086-4500-b55a-afc68fa93614
02/05/2025 11:23:27:INFO:Received: evaluate message fecd3850-8086-4500-b55a-afc68fa93614
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:23:31:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:24:38:INFO:
[92mINFO [0m:      Received: train message 32c1b9c3-0f0f-433c-b167-e3e201a852b7
02/05/2025 11:24:38:INFO:Received: train message 32c1b9c3-0f0f-433c-b167-e3e201a852b7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:25:28:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:26:26:INFO:
[92mINFO [0m:      Received: evaluate message 35f35292-eb81-46c0-b4c0-0ae47d1ca1c5
02/05/2025 11:26:26:INFO:Received: evaluate message 35f35292-eb81-46c0-b4c0-0ae47d1ca1c5

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897]}

Step 1b: Recomputing FIM for epoch 27
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352]}

Step 1b: Recomputing FIM for epoch 28
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:26:32:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:27:19:INFO:
[92mINFO [0m:      Received: train message e7f17988-7124-4dfd-a195-fc376d0c7722
02/05/2025 11:27:19:INFO:Received: train message e7f17988-7124-4dfd-a195-fc376d0c7722
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:28:07:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:29:07:INFO:
[92mINFO [0m:      Received: evaluate message c8066ceb-b027-461c-aa89-131e6827d8d3
02/05/2025 11:29:07:INFO:Received: evaluate message c8066ceb-b027-461c-aa89-131e6827d8d3
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:29:12:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:30:08:INFO:
[92mINFO [0m:      Received: train message 96ee5172-1408-4945-bdd8-18656c18c1b7
02/05/2025 11:30:08:INFO:Received: train message 96ee5172-1408-4945-bdd8-18656c18c1b7
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
02/05/2025 11:30:54:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:31:58:INFO:
[92mINFO [0m:      Received: evaluate message 0d6bce3e-b769-46a4-9809-59f97f8e332d
02/05/2025 11:31:58:INFO:Received: evaluate message 0d6bce3e-b769-46a4-9809-59f97f8e332d

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563]}

Step 1b: Recomputing FIM for epoch 29
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175, 1.0642800787820585], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442, 0.8032596909451641], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128, 0.5765473316940177], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563, 0.5233120793052078]}

Step 1b: Recomputing FIM for epoch 30
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 591, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[92mINFO [0m:      Sent reply
02/05/2025 11:32:04:INFO:Sent reply
[92mINFO [0m:      
02/05/2025 11:32:06:INFO:
[92mINFO [0m:      Received: reconnect message a7e5bdbe-d58e-4908-b283-9742bef7b428
02/05/2025 11:32:06:INFO:Received: reconnect message a7e5bdbe-d58e-4908-b283-9742bef7b428
02/05/2025 11:32:06:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
02/05/2025 11:32:06:INFO:Disconnect and shut down

{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175, 1.0642800787820585, 1.0229751927205788], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442, 0.5942142298670836], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442, 0.8032596909451641, 0.8030471862751019], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128, 0.5765473316940177, 0.5915168783026465], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442, 0.5942142298670836], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563, 0.5233120793052078, 0.5527371062811647]}



Final client history:
{'loss': [1.0781317562493986, 1.0396617486478763, 1.1302045759882566, 1.1010379650520103, 1.1441289632929965, 1.0660528576029196, 1.0943268989099946, 1.09047675785187, 1.1113358030009772, 1.079707012128047, 1.1585062881108836, 1.086857287980692, 1.0960701268757573, 1.0969787921823497, 1.0682740604346204, 1.0644603270026647, 1.0965471822270385, 1.0475284179921631, 1.0558368249084915, 1.0795805363379203, 1.0334370847415701, 1.0729833360385672, 1.0645720765775213, 1.0512909733102693, 1.0547589066440561, 1.0459468113993928, 1.0509916467998437, 1.0402478661921175, 1.0642800787820585, 1.0229751927205788], 'accuracy': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442, 0.5942142298670836], 'auc': [0.7174808607527126, 0.743196921722251, 0.7542212666906339, 0.7610551555667565, 0.764293092039422, 0.7694207263560737, 0.7716762839572426, 0.7756595711833625, 0.778030604033989, 0.7794183927401199, 0.7803217284707833, 0.7835454627309733, 0.7849499128178068, 0.7865746840394147, 0.7892868124121256, 0.7896417413005548, 0.7900090852469398, 0.7917851104971613, 0.7916996106172305, 0.7936529803185322, 0.7946568244204151, 0.7956553534540435, 0.7961937983740468, 0.7967152972505088, 0.7984755002471218, 0.7988375838525364, 0.8003673744802164, 0.8014219355101442, 0.8032596909451641, 0.8030471862751019], 'precision': [0.4096278307389395, 0.4446497303781511, 0.4341749170777527, 0.45071076009013833, 0.4358950843399806, 0.46586630815322133, 0.45845241129337605, 0.5993294002002474, 0.5951116445029571, 0.537716454699062, 0.5875685983821168, 0.5845947243404725, 0.5430080058354136, 0.6142736177060594, 0.59866899725491, 0.6015159111155269, 0.5887906834601023, 0.5976226412286977, 0.6076060730420754, 0.5831674785004183, 0.5955476659020549, 0.5995372648230018, 0.5925752436936306, 0.5972377436535812, 0.5773865638046831, 0.5944407633336243, 0.5900646254910092, 0.6096217652537128, 0.5765473316940177, 0.5915168783026465], 'recall': [0.5160281469898358, 0.5371383893666928, 0.5355746677091477, 0.5465207193119624, 0.5363565285379203, 0.5543393275996873, 0.5559030492572322, 0.5566849100860047, 0.5543393275996873, 0.5629397967161845, 0.5488663017982799, 0.565285379202502, 0.5699765441751369, 0.5691946833463644, 0.5762314308053167, 0.5777951524628616, 0.5723221266614542, 0.5809225957779516, 0.581704456606724, 0.5738858483189992, 0.5801407349491791, 0.5824863174354965, 0.5809225957779516, 0.5840500390930414, 0.5801407349491791, 0.5903049257232212, 0.5832681782642689, 0.5871774824081314, 0.5754495699765442, 0.5942142298670836], 'f1': [0.42340290242033957, 0.48416854046252017, 0.46611708770062665, 0.48996858247128255, 0.46733505885986093, 0.5057703343186021, 0.4975875316477439, 0.4997723287528995, 0.49235487143150436, 0.509359553561803, 0.476638044457797, 0.5083344542121974, 0.5142227188140089, 0.5165248158617874, 0.5241844590699702, 0.5261357378318207, 0.517103997725588, 0.5336844603684922, 0.5358061924976271, 0.5224015165989173, 0.5368344806898692, 0.5289370140785129, 0.5310108164943608, 0.5365138782173475, 0.5319008769423585, 0.544920490627897, 0.5343239724879352, 0.5373070023159563, 0.5233120793052078, 0.5527371062811647]}

