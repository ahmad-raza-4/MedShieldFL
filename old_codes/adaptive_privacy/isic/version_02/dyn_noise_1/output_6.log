nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/23/2025 11:25:34:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/23/2025 11:25:34:DEBUG:ChannelConnectivity.IDLE
01/23/2025 11:25:34:DEBUG:ChannelConnectivity.CONNECTING
01/23/2025 11:25:34:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/23/2025 11:25:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:25:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: get_parameters message ba38c5dd-093f-4ab1-97d8-156e194a7b96
01/23/2025 11:25:34:INFO:Received: get_parameters message ba38c5dd-093f-4ab1-97d8-156e194a7b96
[92mINFO [0m:      Sent reply
01/23/2025 11:25:39:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:31:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:31:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7892b213-0819-4b98-8248-45c13b3811bc
01/23/2025 11:31:08:INFO:Received: train message 7892b213-0819-4b98-8248-45c13b3811bc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:33:30:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:50:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:50:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c599085a-4f67-452b-a528-bcd4a95e4d70
01/23/2025 11:50:35:INFO:Received: evaluate message c599085a-4f67-452b-a528-bcd4a95e4d70
[92mINFO [0m:      Sent reply
01/23/2025 11:54:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:54:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:54:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dc9fe988-4973-4cfd-8a23-50fc79de0cde
01/23/2025 11:54:59:INFO:Received: train message dc9fe988-4973-4cfd-8a23-50fc79de0cde
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:56:46:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:14:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:14:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 878b2145-3e34-43f4-bfa7-2e34e6c25357
01/23/2025 12:14:21:INFO:Received: evaluate message 878b2145-3e34-43f4-bfa7-2e34e6c25357
[92mINFO [0m:      Sent reply
01/23/2025 12:18:30:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:19:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:19:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2bd38ea6-4bd5-4b30-86c7-1261d6502ad3
01/23/2025 12:19:02:INFO:Received: train message 2bd38ea6-4bd5-4b30-86c7-1261d6502ad3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:21:25:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:38:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:38:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7d7670d7-b9d0-4915-bee2-3c4355b87754
01/23/2025 12:38:02:INFO:Received: evaluate message 7d7670d7-b9d0-4915-bee2-3c4355b87754
[92mINFO [0m:      Sent reply
01/23/2025 12:42:01:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:42:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:42:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ab3d34f0-39e3-46ff-8236-559ea5b40b8c
01/23/2025 12:42:28:INFO:Received: train message ab3d34f0-39e3-46ff-8236-559ea5b40b8c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:44:47:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:01:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:01:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b1a831c5-74b6-4ba1-82f9-e9e093de5c2d
01/23/2025 13:01:29:INFO:Received: evaluate message b1a831c5-74b6-4ba1-82f9-e9e093de5c2d
[92mINFO [0m:      Sent reply
01/23/2025 13:05:30:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:06:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:06:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ffa9bab1-f940-4aaa-8aac-df1a03fc4226
01/23/2025 13:06:02:INFO:Received: train message ffa9bab1-f940-4aaa-8aac-df1a03fc4226
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:08:21:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:25:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:25:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 054f210e-844f-4675-b01a-908657a8cff4
01/23/2025 13:25:32:INFO:Received: evaluate message 054f210e-844f-4675-b01a-908657a8cff4
[92mINFO [0m:      Sent reply
01/23/2025 13:29:31:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:29:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:29:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd016ddc-f583-4f41-8812-2af14c1cd7bb
01/23/2025 13:29:55:INFO:Received: train message cd016ddc-f583-4f41-8812-2af14c1cd7bb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:32:16:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:49:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:49:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 40eedb9e-de96-4466-a4d2-dd1e70aae304
01/23/2025 13:49:18:INFO:Received: evaluate message 40eedb9e-de96-4466-a4d2-dd1e70aae304
[92mINFO [0m:      Sent reply
01/23/2025 13:53:21:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:53:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:53:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd7c8a29-f260-4cbb-bb12-d26aa261790f
01/23/2025 13:53:52:INFO:Received: train message cd7c8a29-f260-4cbb-bb12-d26aa261790f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:56:18:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:12:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:12:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 644f8872-0dfc-4515-916a-ae12277fec2c
01/23/2025 14:12:46:INFO:Received: evaluate message 644f8872-0dfc-4515-916a-ae12277fec2c
[92mINFO [0m:      Sent reply
01/23/2025 14:16:57:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:17:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:17:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5de777de-baa0-4f85-b15d-c945ce9c0033
01/23/2025 14:17:16:INFO:Received: train message 5de777de-baa0-4f85-b15d-c945ce9c0033
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:19:15:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:36:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:36:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d630793f-caf5-4b8f-9eb4-257d17391ca2
01/23/2025 14:36:24:INFO:Received: evaluate message d630793f-caf5-4b8f-9eb4-257d17391ca2
[92mINFO [0m:      Sent reply
01/23/2025 14:39:39:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:41:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:41:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a348814b-35f4-47a0-b475-b21f6e8e1384
01/23/2025 14:41:07:INFO:Received: train message a348814b-35f4-47a0-b475-b21f6e8e1384
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:43:28:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:00:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:00:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f3fc7639-7719-455b-b64e-4fb1cd7b97fd
01/23/2025 15:00:07:INFO:Received: evaluate message f3fc7639-7719-455b-b64e-4fb1cd7b97fd
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_1']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 1, target_epsilon: 1, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 2.08

{'loss': [137.2596139907837], 'accuracy': [0.3383004430124849], 'auc': [0.5868727978587345]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 1.15

{'loss': [137.2596139907837, 133.6919516324997], 'accuracy': [0.3383004430124849, 0.3407168747482884], 'auc': [0.5868727978587345, 0.6194824557606924]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 0.51

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 0.79

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 0.50

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 0.72

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 0.45

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 0.86

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 0.91
[92mINFO [0m:      Sent reply
01/23/2025 15:03:29:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:04:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:04:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1728bc6d-c5e0-4954-9fdf-9a6131e8023e
01/23/2025 15:04:45:INFO:Received: train message 1728bc6d-c5e0-4954-9fdf-9a6131e8023e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:07:01:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:24:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:24:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 56b7a858-3eb2-48ba-8b61-1bf32262eaf1
01/23/2025 15:24:07:INFO:Received: evaluate message 56b7a858-3eb2-48ba-8b61-1bf32262eaf1
[92mINFO [0m:      Sent reply
01/23/2025 15:28:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:28:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:28:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0ddc88e3-9b50-412d-be04-279517fd706d
01/23/2025 15:28:46:INFO:Received: train message 0ddc88e3-9b50-412d-be04-279517fd706d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:31:10:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:47:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:47:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d31e4c8a-9524-4995-b921-a7f54fefa308
01/23/2025 15:47:45:INFO:Received: evaluate message d31e4c8a-9524-4995-b921-a7f54fefa308
[92mINFO [0m:      Sent reply
01/23/2025 15:51:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:52:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:52:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ae10aae1-afb9-4733-9653-ad9e2392ac4a
01/23/2025 15:52:29:INFO:Received: train message ae10aae1-afb9-4733-9653-ad9e2392ac4a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:54:52:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:11:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:11:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5866dcba-ceb6-4ce5-89a9-50e13377d17c
01/23/2025 16:11:36:INFO:Received: evaluate message 5866dcba-ceb6-4ce5-89a9-50e13377d17c
[92mINFO [0m:      Sent reply
01/23/2025 16:15:31:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:15:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:15:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ce21d561-40da-4576-913f-3e6d7238ddbd
01/23/2025 16:15:44:INFO:Received: train message ce21d561-40da-4576-913f-3e6d7238ddbd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:17:22:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:35:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:35:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2d4b7b92-b3a2-4a3b-93f3-40e44b901377
01/23/2025 16:35:04:INFO:Received: evaluate message 2d4b7b92-b3a2-4a3b-93f3-40e44b901377
[92mINFO [0m:      Sent reply
01/23/2025 16:39:04:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:39:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:39:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 80196139-1542-49b1-981f-abe84cdd48ad
01/23/2025 16:39:35:INFO:Received: train message 80196139-1542-49b1-981f-abe84cdd48ad
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:42:00:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:58:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:58:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fb7ec13a-33a6-4d78-9c01-c65daf22abd3
01/23/2025 16:58:33:INFO:Received: evaluate message fb7ec13a-33a6-4d78-9c01-c65daf22abd3
[92mINFO [0m:      Sent reply
01/23/2025 17:02:15:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:03:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:03:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eeff51d1-8327-4f5d-834c-f751ffdcaf6c
01/23/2025 17:03:13:INFO:Received: train message eeff51d1-8327-4f5d-834c-f751ffdcaf6c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:05:31:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:22:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:22:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ae376696-83c6-4ca4-be84-230cec8288ea
01/23/2025 17:22:27:INFO:Received: evaluate message ae376696-83c6-4ca4-be84-230cec8288ea

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 0.34

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 0.73

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 0.87

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 0.92

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 0.93

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 9.211869925260544
Epsilon = 1.00 and Loss = 0.47
[92mINFO [0m:      Sent reply
01/23/2025 17:26:32:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:27:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:27:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 52d47c85-34c5-43b3-823a-83fa902c3f41
01/23/2025 17:27:04:INFO:Received: train message 52d47c85-34c5-43b3-823a-83fa902c3f41
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:29:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:46:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:46:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 956ca861-2c69-4340-935b-f468d6f27b39
01/23/2025 17:46:14:INFO:Received: evaluate message 956ca861-2c69-4340-935b-f468d6f27b39
[92mINFO [0m:      Sent reply
01/23/2025 17:50:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:50:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:50:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a7a4a22f-7e6c-484c-8857-959ce2390f58
01/23/2025 17:50:47:INFO:Received: train message a7a4a22f-7e6c-484c-8857-959ce2390f58
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:53:09:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:09:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:09:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a7977e9d-b3ce-4c7f-80c6-f04b02b88e18
01/23/2025 18:09:58:INFO:Received: evaluate message a7977e9d-b3ce-4c7f-80c6-f04b02b88e18
[92mINFO [0m:      Sent reply
01/23/2025 18:14:04:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:14:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:14:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 316af9b1-24c6-4279-a85d-01c38b97162a
01/23/2025 18:14:19:INFO:Received: train message 316af9b1-24c6-4279-a85d-01c38b97162a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:16:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:33:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:33:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c9d20d04-eea1-428a-907e-e9536b37f4e1
01/23/2025 18:33:39:INFO:Received: evaluate message c9d20d04-eea1-428a-907e-e9536b37f4e1
[92mINFO [0m:      Sent reply
01/23/2025 18:37:43:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:37:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:37:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 083ed36d-c651-4e6a-81ae-f9476a50ed32
01/23/2025 18:37:57:INFO:Received: train message 083ed36d-c651-4e6a-81ae-f9476a50ed32
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:39:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:57:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:57:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6fec647d-1a39-46e0-80a4-1a84f28b29e6
01/23/2025 18:57:13:INFO:Received: evaluate message 6fec647d-1a39-46e0-80a4-1a84f28b29e6
[92mINFO [0m:      Sent reply
01/23/2025 19:01:17:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:01:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:01:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c0f1aac6-daa1-405e-8172-98e45d9a5455
01/23/2025 19:01:42:INFO:Received: train message c0f1aac6-daa1-405e-8172-98e45d9a5455
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:04:02:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:21:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:21:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c756ef13-6c8d-4eb5-8163-bf17c7706d03
01/23/2025 19:21:01:INFO:Received: evaluate message c756ef13-6c8d-4eb5-8163-bf17c7706d03

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 8.597745263576508
Epsilon = 1.00 and Loss = 1.08

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 7.983620601892472
Epsilon = 1.00 and Loss = 0.66

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 7.369495940208435
Epsilon = 1.00 and Loss = 0.49

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 6.755371278524399
Epsilon = 1.00 and Loss = 0.81

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 6.141246616840363
Epsilon = 1.00 and Loss = 0.52
[92mINFO [0m:      Sent reply
01/23/2025 19:25:08:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:25:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:25:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5ef26cc5-feb7-4a25-8d65-09866df6f59d
01/23/2025 19:25:34:INFO:Received: train message 5ef26cc5-feb7-4a25-8d65-09866df6f59d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:27:55:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:44:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:44:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6482eb2b-fa8d-42d5-bdf5-820e87284f2b
01/23/2025 19:44:52:INFO:Received: evaluate message 6482eb2b-fa8d-42d5-bdf5-820e87284f2b
[92mINFO [0m:      Sent reply
01/23/2025 19:48:51:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:49:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:49:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2efcf17d-1696-4c47-a863-f186d733416e
01/23/2025 19:49:16:INFO:Received: train message 2efcf17d-1696-4c47-a863-f186d733416e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:51:38:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:08:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:08:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ee6c2be6-47a9-4401-b7e2-0d4f1c0ef3d4
01/23/2025 20:08:28:INFO:Received: evaluate message ee6c2be6-47a9-4401-b7e2-0d4f1c0ef3d4
[92mINFO [0m:      Sent reply
01/23/2025 20:12:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:12:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:12:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ad7f6c04-e155-4b31-af2c-7d7a21b72b8f
01/23/2025 20:12:47:INFO:Received: train message ad7f6c04-e155-4b31-af2c-7d7a21b72b8f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:15:00:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:32:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:32:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 90095097-d21b-4d75-8e7d-a55a08da70ba
01/23/2025 20:32:09:INFO:Received: evaluate message 90095097-d21b-4d75-8e7d-a55a08da70ba
[92mINFO [0m:      Sent reply
01/23/2025 20:36:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:36:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:36:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 70261751-9d39-49fa-aa5c-3190f1b91116
01/23/2025 20:36:46:INFO:Received: train message 70261751-9d39-49fa-aa5c-3190f1b91116
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:39:09:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:56:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:56:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e6add9b5-eafb-4ec0-ae9d-8de5c3ddf771
01/23/2025 20:56:10:INFO:Received: evaluate message e6add9b5-eafb-4ec0-ae9d-8de5c3ddf771

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 5.527121955156326
Epsilon = 1.00 and Loss = 0.38

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 4.91299729347229
Epsilon = 1.00 and Loss = 0.78

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 4.298872631788254
Epsilon = 1.00 and Loss = 0.65

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 3.6847479701042176
Epsilon = 1.00 and Loss = 0.66
[92mINFO [0m:      Sent reply
01/23/2025 21:00:19:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:00:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:00:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message abf5bba7-6900-41cc-a4af-a43444e40eaf
01/23/2025 21:00:33:INFO:Received: train message abf5bba7-6900-41cc-a4af-a43444e40eaf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:02:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:20:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:20:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e5218df6-016e-40dd-816f-0c8ef1fcf949
01/23/2025 21:20:21:INFO:Received: evaluate message e5218df6-016e-40dd-816f-0c8ef1fcf949
[92mINFO [0m:      Sent reply
01/23/2025 21:24:20:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:24:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:24:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e939aba0-e7b9-4926-9f72-5e3600fc90cb
01/23/2025 21:24:54:INFO:Received: train message e939aba0-e7b9-4926-9f72-5e3600fc90cb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:27:23:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:44:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:44:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d71d9b6b-80b9-4be0-a627-06e894aa118a
01/23/2025 21:44:17:INFO:Received: evaluate message d71d9b6b-80b9-4be0-a627-06e894aa118a
[92mINFO [0m:      Sent reply
01/23/2025 21:48:20:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:49:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:49:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1cf1b507-4612-4676-b5ac-4adc92388fbe
01/23/2025 21:49:06:INFO:Received: train message 1cf1b507-4612-4676-b5ac-4adc92388fbe
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:51:28:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:08:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:08:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 94ab712b-02a2-4a68-a7d4-d3df51a208a8
01/23/2025 22:08:36:INFO:Received: evaluate message 94ab712b-02a2-4a68-a7d4-d3df51a208a8
[92mINFO [0m:      Sent reply
01/23/2025 22:12:43:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:13:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:13:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0846323f-1bf2-4f4d-a992-5f046ea3a825
01/23/2025 22:13:01:INFO:Received: train message 0846323f-1bf2-4f4d-a992-5f046ea3a825

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 3.0706233084201817
Epsilon = 1.00 and Loss = 0.64

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 2.4564986467361454
Epsilon = 1.00 and Loss = 0.44

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 1.8423739850521084
Epsilon = 1.00 and Loss = 0.39

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:14:18:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:32:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:32:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f24f8efb-7c41-4d9e-a185-f6fd1aaa9be0
01/23/2025 22:32:32:INFO:Received: evaluate message f24f8efb-7c41-4d9e-a185-f6fd1aaa9be0
[92mINFO [0m:      Sent reply
01/23/2025 22:36:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:37:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:37:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b33d01bc-f94a-4ac2-b83b-82903699ebb4
01/23/2025 22:37:00:INFO:Received: train message b33d01bc-f94a-4ac2-b83b-82903699ebb4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:39:05:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:56:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:56:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1d398d51-3d73-4d6f-b1d2-51e721fc6ad9
01/23/2025 22:56:37:INFO:Received: evaluate message 1d398d51-3d73-4d6f-b1d2-51e721fc6ad9
[92mINFO [0m:      Sent reply
01/23/2025 23:00:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:01:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:01:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 09ab805b-ec1e-4616-bc6a-72fb3fc5fa96
01/23/2025 23:01:14:INFO:Received: train message 09ab805b-ec1e-4616-bc6a-72fb3fc5fa96
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 23:03:39:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:20:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:20:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ac1c6907-b40f-4835-abb7-337227ce009b
01/23/2025 23:20:38:INFO:Received: evaluate message ac1c6907-b40f-4835-abb7-337227ce009b
[92mINFO [0m:      Sent reply
01/23/2025 23:24:44:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:25:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:25:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 547cccc8-e78e-46eb-a50d-e1ef5f6e19ce
01/23/2025 23:25:00:INFO:Received: reconnect message 547cccc8-e78e-46eb-a50d-e1ef5f6e19ce
01/23/2025 23:25:00:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/23/2025 23:25:00:INFO:Disconnect and shut down
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 1.2282493233680722
Epsilon = 1.00 and Loss = 0.73

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 0.6141246616840361
Epsilon = 1.00 and Loss = 0.76

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142]}

Base Noise Multiplier Received:  1.1328125
Data Scaling Factor: 72.16809116809117 where Client Data Size: 351
Noise Multiplier after Fisher Scaling:  [11.569578170776367, 49.108238220214844, 2.0604610443115234, 3.0083892345428467, 1.8213592767715454, 4.077914237976074, 1.1791132688522339, 0.869905948638916]
Noise Multiplier after list and tensor:  9.211869925260544
Noise Multiplier after Epsilon Scaling:  9.211869925260544
Noise Multiplier after Convergence: 0.0
Epsilon = 1.00 and Loss = 0.78

{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995, 116.7615122795105], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315, 0.5368505839710028], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142, 0.7311505973177221]}



Final client history:
{'loss': [137.2596139907837, 133.6919516324997, 136.09523212909698, 138.6259878873825, 139.713250041008, 139.4705867767334, 137.49091362953186, 135.83193266391754, 132.71509325504303, 130.50868046283722, 128.72331547737122, 127.73188984394073, 126.37969863414764, 125.48011147975922, 124.28206539154053, 123.43013620376587, 122.3192925453186, 121.99394845962524, 121.36356103420258, 120.53682029247284, 120.03408372402191, 119.36390566825867, 118.71580564975739, 118.17940270900726, 117.86651146411896, 117.7983648777008, 117.46852028369904, 116.91642844676971, 116.7303866147995, 116.7615122795105], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3499798630688683, 0.3568264196536448, 0.37575513491743856, 0.39468385018123237, 0.41723721304873135, 0.4333467579540878, 0.4482480869915425, 0.4571083366894885, 0.4679822795006041, 0.47643979057591623, 0.48409182440596055, 0.4881192106322996, 0.49174385823600486, 0.49536850583971004, 0.5014095851792187, 0.5058397100281917, 0.5114780507450665, 0.5159081755940395, 0.5175191300845751, 0.5203383004430124, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5324204591220298, 0.5336286749899315, 0.5368505839710028], 'auc': [0.5868727978587345, 0.6194824557606924, 0.6332747268286469, 0.6409784505427432, 0.6475128342621772, 0.6521635288666228, 0.6577039549239638, 0.6629549860977864, 0.6687035869251049, 0.6726062551192823, 0.6778951726604219, 0.6821951189999591, 0.6862314148221464, 0.6903174656640896, 0.6935635788905234, 0.697246263995306, 0.7006663096721644, 0.7032523867222171, 0.7059213245135924, 0.7094335125819758, 0.7118719042376389, 0.7144465959612978, 0.7169617778359723, 0.7195841148130424, 0.7220698200809929, 0.7238531010048092, 0.7259609180441174, 0.7282000937855511, 0.7298981633751142, 0.7311505973177221]}

