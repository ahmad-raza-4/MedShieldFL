nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.1 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/27/2025 11:21:50:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 11:21:50:DEBUG:ChannelConnectivity.IDLE
01/27/2025 11:21:50:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 11:32:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:32:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7a71fcf1-7f54-47c9-97f6-4e0d7d5cb492
01/27/2025 11:32:55:INFO:Received: train message 7a71fcf1-7f54-47c9-97f6-4e0d7d5cb492
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 11:37:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:07:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:07:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fd88f957-59cb-4fd7-ab5a-1fa1272bcdd4
01/27/2025 12:07:42:INFO:Received: evaluate message fd88f957-59cb-4fd7-ab5a-1fa1272bcdd4
[92mINFO [0m:      Sent reply
01/27/2025 12:12:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:13:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:13:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0e7464c5-5dfa-4cf5-ab88-8c9141a18862
01/27/2025 12:13:13:INFO:Received: train message 0e7464c5-5dfa-4cf5-ab88-8c9141a18862
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 12:18:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:49:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:49:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 78af04ef-b256-49cf-9ffc-4717f9db0fc6
01/27/2025 12:49:26:INFO:Received: evaluate message 78af04ef-b256-49cf-9ffc-4717f9db0fc6
[92mINFO [0m:      Sent reply
01/27/2025 12:54:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:55:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:55:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c19b11ea-31df-4241-ac8e-6b966073fa81
01/27/2025 12:55:02:INFO:Received: train message c19b11ea-31df-4241-ac8e-6b966073fa81
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 12:59:47:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:32:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:32:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 28ea5de8-c01d-426a-9003-676bc5b65e6d
01/27/2025 13:32:18:INFO:Received: evaluate message 28ea5de8-c01d-426a-9003-676bc5b65e6d
[92mINFO [0m:      Sent reply
01/27/2025 13:37:11:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:37:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:37:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 909fb1f2-72e8-4c13-babd-ef5edc249f7c
01/27/2025 13:37:40:INFO:Received: train message 909fb1f2-72e8-4c13-babd-ef5edc249f7c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 13:42:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:12:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:12:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 95aa6108-fb6d-42b5-b516-95defc0a2fa7
01/27/2025 14:12:39:INFO:Received: evaluate message 95aa6108-fb6d-42b5-b516-95defc0a2fa7
[92mINFO [0m:      Sent reply
01/27/2025 14:17:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:17:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:17:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cdfcaf10-c2f0-4fa2-986b-3b84bbdfa60f
01/27/2025 14:17:56:INFO:Received: train message cdfcaf10-c2f0-4fa2-986b-3b84bbdfa60f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 14:22:52:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:52:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:52:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b971fb08-c064-4849-b908-ffb3ec192807
01/27/2025 14:52:41:INFO:Received: evaluate message b971fb08-c064-4849-b908-ffb3ec192807
[92mINFO [0m:      Sent reply
01/27/2025 14:57:25:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:58:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:58:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 494ee4a7-c5ad-417e-ac1f-78e5fd664598
01/27/2025 14:58:28:INFO:Received: train message 494ee4a7-c5ad-417e-ac1f-78e5fd664598
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 15:03:42:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 15:35:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 15:35:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b6b6e6a3-cbd4-4181-85b7-8cf05c02a212
01/27/2025 15:35:18:INFO:Received: evaluate message b6b6e6a3-cbd4-4181-85b7-8cf05c02a212
[92mINFO [0m:      Sent reply
01/27/2025 15:40:10:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 15:40:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 15:40:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 044a5112-deb8-42cc-b15a-d87d66450356
01/27/2025 15:40:52:INFO:Received: train message 044a5112-deb8-42cc-b15a-d87d66450356
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 15:45:58:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:15:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:15:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d1016ddd-1504-4ecd-8518-3b6686283689
01/27/2025 16:15:07:INFO:Received: evaluate message d1016ddd-1504-4ecd-8518-3b6686283689
[92mINFO [0m:      Sent reply
01/27/2025 16:19:54:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:20:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:20:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 065b8320-13bf-4c97-b0d7-b71fb1a5a41c
01/27/2025 16:20:21:INFO:Received: train message 065b8320-13bf-4c97-b0d7-b71fb1a5a41c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 16:25:08:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:55:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:55:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c2dc6823-4529-4c15-81ff-b8a32575191c
01/27/2025 16:55:02:INFO:Received: evaluate message c2dc6823-4529-4c15-81ff-b8a32575191c
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 10, target_epsilon: 10, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102], 'accuracy': [0.19693918646798228], 'auc': [0.4459623197507399]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543], 'accuracy': [0.19693918646798228, 0.23238018525976642], 'auc': [0.4459623197507399, 0.45312056473212875]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)

[92mINFO [0m:      Sent reply
01/27/2025 16:59:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:00:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:00:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4c04d713-a037-4574-bae2-180311c85474
01/27/2025 17:00:25:INFO:Received: train message 4c04d713-a037-4574-bae2-180311c85474
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 17:05:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:32:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:32:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a7dd923e-f1f9-4443-afa5-a1236d4b508e
01/27/2025 17:32:54:INFO:Received: evaluate message a7dd923e-f1f9-4443-afa5-a1236d4b508e
[92mINFO [0m:      Sent reply
01/27/2025 17:37:27:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:38:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:38:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7d6d0634-9d0d-4894-a760-f40519c5bc2c
01/27/2025 17:38:29:INFO:Received: train message 7d6d0634-9d0d-4894-a760-f40519c5bc2c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 17:43:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:18:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:18:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3dea415c-97f8-4d67-8236-86a10cf02ab0
01/27/2025 18:18:05:INFO:Received: evaluate message 3dea415c-97f8-4d67-8236-86a10cf02ab0
[92mINFO [0m:      Sent reply
01/27/2025 18:23:43:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:23:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:23:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 726f60cc-f94f-4532-be5a-cfd992fa9eae
01/27/2025 18:23:55:INFO:Received: train message 726f60cc-f94f-4532-be5a-cfd992fa9eae
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 18:29:13:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:06:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:06:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d618ba12-3ea3-4dd9-94a5-38d0a522db71
01/27/2025 19:06:58:INFO:Received: evaluate message d618ba12-3ea3-4dd9-94a5-38d0a522db71
[92mINFO [0m:      Sent reply
01/27/2025 19:12:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:13:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:13:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 95534c37-75b6-4fff-9eb6-a7eea7a36091
01/27/2025 19:13:17:INFO:Received: train message 95534c37-75b6-4fff-9eb6-a7eea7a36091
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 19:18:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:59:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:59:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e0948d2d-595d-4fe7-8454-00b0d6e8037a
01/27/2025 19:59:32:INFO:Received: evaluate message e0948d2d-595d-4fe7-8454-00b0d6e8037a
[92mINFO [0m:      Sent reply
01/27/2025 20:05:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:05:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:05:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3d1fb0bc-7c86-41ee-84c1-7fc63e33f314
01/27/2025 20:05:56:INFO:Received: train message 3d1fb0bc-7c86-41ee-84c1-7fc63e33f314
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 20:11:32:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:43:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:43:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 274cdc27-cda8-45d5-9975-bb39f699866c
01/27/2025 20:43:36:INFO:Received: evaluate message 274cdc27-cda8-45d5-9975-bb39f699866c
[92mINFO [0m:      Sent reply
01/27/2025 20:48:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:49:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:49:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e2d07834-9edf-4ebb-b0b2-342f0da23688
01/27/2025 20:49:03:INFO:Received: train message e2d07834-9edf-4ebb-b0b2-342f0da23688
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 20:53:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 21:22:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 21:22:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e4221146-2496-4bc4-804d-d98b787aa607
01/27/2025 21:22:39:INFO:Received: evaluate message e4221146-2496-4bc4-804d-d98b787aa607

{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098, 1.765155115148688], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824, 0.34393878372935965], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779, 0.5177511222367319]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098, 1.765155115148688, 1.75747421649114], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824, 0.34393878372935965, 0.34313330648409185], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779, 0.5177511222367319, 0.5246326183305553]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098, 1.765155115148688, 1.75747421649114, 1.7514289996818915], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824, 0.34393878372935965, 0.34313330648409185, 0.34313330648409185], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779, 0.5177511222367319, 0.5246326183305553, 0.5310733000304378]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 0.025857644783072124 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.0031183441169559956, 0.005603444762527943, 0.00015354919014498591, 0.0005257779266685247, 0.0026407118421047926, 0.0005628828075714409, 0.00027857438544742763, 0.00012702475942205638]
Noise Multiplier after list and tensor:  0.0016262887238553958
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)

[92mINFO [0m:      Sent reply
01/27/2025 21:27:29:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 21:28:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 21:28:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 287cd454-4516-4237-9af2-9b51e2a1e8c8
01/27/2025 21:28:02:INFO:Received: train message 287cd454-4516-4237-9af2-9b51e2a1e8c8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 21:32:56:INFO:Sent reply
