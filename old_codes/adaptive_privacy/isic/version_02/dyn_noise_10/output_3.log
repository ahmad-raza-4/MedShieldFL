nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.1 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/27/2025 11:25:32:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/27/2025 11:25:32:DEBUG:ChannelConnectivity.IDLE
01/27/2025 11:25:32:DEBUG:ChannelConnectivity.CONNECTING
01/27/2025 11:25:32:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/27/2025 11:32:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 11:32:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 36a16937-5e9d-44db-bdc9-69eb29e12ce6
01/27/2025 11:32:55:INFO:Received: train message 36a16937-5e9d-44db-bdc9-69eb29e12ce6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 11:48:41:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:07:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:07:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e5607e6d-a885-4353-b6bf-04f1cd35bdb7
01/27/2025 12:07:25:INFO:Received: evaluate message e5607e6d-a885-4353-b6bf-04f1cd35bdb7
[92mINFO [0m:      Sent reply
01/27/2025 12:12:21:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:13:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:13:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 607f3f19-f2d6-48b4-a881-c12a23336ea7
01/27/2025 12:13:03:INFO:Received: train message 607f3f19-f2d6-48b4-a881-c12a23336ea7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 12:28:36:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:49:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:49:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f40825be-491e-4746-b588-cb981316d7ec
01/27/2025 12:49:14:INFO:Received: evaluate message f40825be-491e-4746-b588-cb981316d7ec
[92mINFO [0m:      Sent reply
01/27/2025 12:54:14:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 12:54:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 12:54:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d0403b49-5c5d-4562-a28d-4c82dc4fb20c
01/27/2025 12:54:58:INFO:Received: train message d0403b49-5c5d-4562-a28d-4c82dc4fb20c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 13:10:22:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:32:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:32:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e538a0cb-8d9d-4782-beab-4ac1f7d4b4fe
01/27/2025 13:32:01:INFO:Received: evaluate message e538a0cb-8d9d-4782-beab-4ac1f7d4b4fe
[92mINFO [0m:      Sent reply
01/27/2025 13:36:56:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 13:37:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 13:37:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 238c67f9-1ba4-40fc-a1a3-7c86c7dbbe57
01/27/2025 13:37:54:INFO:Received: train message 238c67f9-1ba4-40fc-a1a3-7c86c7dbbe57
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 13:53:16:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:12:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:12:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a96a891d-44c7-41d3-80e2-daa971743e08
01/27/2025 14:12:30:INFO:Received: evaluate message a96a891d-44c7-41d3-80e2-daa971743e08
[92mINFO [0m:      Sent reply
01/27/2025 14:17:39:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:18:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:18:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 138bc4d6-ea83-467b-9499-bdb95d574f64
01/27/2025 14:18:25:INFO:Received: train message 138bc4d6-ea83-467b-9499-bdb95d574f64
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 14:34:23:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:53:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:53:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 80a10b0a-b528-422e-b0a0-02dbed22700c
01/27/2025 14:53:01:INFO:Received: evaluate message 80a10b0a-b528-422e-b0a0-02dbed22700c
[92mINFO [0m:      Sent reply
01/27/2025 14:57:53:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 14:58:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 14:58:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 54aa6b12-f39f-46e8-abbc-e0d99eec8b0f
01/27/2025 14:58:26:INFO:Received: train message 54aa6b12-f39f-46e8-abbc-e0d99eec8b0f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 15:14:20:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 15:35:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 15:35:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3ef187e8-703f-4e20-89da-eab5779da6d8
01/27/2025 15:35:20:INFO:Received: evaluate message 3ef187e8-703f-4e20-89da-eab5779da6d8
[92mINFO [0m:      Sent reply
01/27/2025 15:40:15:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 15:40:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 15:40:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cdaeacb5-e6ff-4be7-b47c-f63db8c3a836
01/27/2025 15:40:58:INFO:Received: train message cdaeacb5-e6ff-4be7-b47c-f63db8c3a836
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 15:55:57:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:14:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:14:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 64b6eada-946c-46a8-b97e-2190cc684c56
01/27/2025 16:14:46:INFO:Received: evaluate message 64b6eada-946c-46a8-b97e-2190cc684c56
[92mINFO [0m:      Sent reply
01/27/2025 16:19:18:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:20:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:20:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d3b9b1a7-0d9d-42eb-be8f-70685a60a790
01/27/2025 16:20:28:INFO:Received: train message d3b9b1a7-0d9d-42eb-be8f-70685a60a790
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 16:35:34:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 16:54:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 16:54:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 23fff703-afe5-48ce-8444-2030a5b4e66c
01/27/2025 16:54:58:INFO:Received: evaluate message 23fff703-afe5-48ce-8444-2030a5b4e66c
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 10, target_epsilon: 10, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102], 'accuracy': [0.19693918646798228], 'auc': [0.4459623197507399]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543], 'accuracy': [0.19693918646798228, 0.23238018525976642], 'auc': [0.4459623197507399, 0.45312056473212875]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)

[92mINFO [0m:      Sent reply
01/27/2025 16:59:49:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:00:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:00:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4922824c-042b-48f3-9706-a4783e0bab79
01/27/2025 17:00:10:INFO:Received: train message 4922824c-042b-48f3-9706-a4783e0bab79
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 17:15:00:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:33:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:33:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0cff6b8f-89f0-4bf8-bcf3-827e4e985328
01/27/2025 17:33:04:INFO:Received: evaluate message 0cff6b8f-89f0-4bf8-bcf3-827e4e985328
[92mINFO [0m:      Sent reply
01/27/2025 17:37:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 17:38:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 17:38:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ef2d80bc-4ef8-4e5c-a812-ee6b09783271
01/27/2025 17:38:13:INFO:Received: train message ef2d80bc-4ef8-4e5c-a812-ee6b09783271
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 17:53:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:18:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:18:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9b309c28-3a67-4beb-9c29-1fe21723d5c1
01/27/2025 18:18:02:INFO:Received: evaluate message 9b309c28-3a67-4beb-9c29-1fe21723d5c1
[92mINFO [0m:      Sent reply
01/27/2025 18:23:38:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 18:24:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 18:24:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0829e039-0e55-4408-aaed-e3d0e9d66b86
01/27/2025 18:24:15:INFO:Received: train message 0829e039-0e55-4408-aaed-e3d0e9d66b86
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 18:42:03:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:07:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:07:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5c5e6ed7-e1a6-48dd-bbd3-bd622ca3fdc8
01/27/2025 19:07:04:INFO:Received: evaluate message 5c5e6ed7-e1a6-48dd-bbd3-bd622ca3fdc8
[92mINFO [0m:      Sent reply
01/27/2025 19:12:40:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:13:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:13:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 84a4d50f-cd48-4f65-90e5-e252f5532243
01/27/2025 19:13:07:INFO:Received: train message 84a4d50f-cd48-4f65-90e5-e252f5532243
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 19:30:50:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 19:59:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 19:59:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 62f114b9-52cb-4340-8dd2-9aaec29a7d3e
01/27/2025 19:59:32:INFO:Received: evaluate message 62f114b9-52cb-4340-8dd2-9aaec29a7d3e
[92mINFO [0m:      Sent reply
01/27/2025 20:05:12:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:05:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:05:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd1f4a7a-20bf-4055-b7bc-17d18b206215
01/27/2025 20:05:34:INFO:Received: train message cd1f4a7a-20bf-4055-b7bc-17d18b206215
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 20:23:24:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:43:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:43:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c5490744-b25d-4a65-94cc-5c220ab989ef
01/27/2025 20:43:34:INFO:Received: evaluate message c5490744-b25d-4a65-94cc-5c220ab989ef
[92mINFO [0m:      Sent reply
01/27/2025 20:48:26:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 20:49:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 20:49:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 888e14a4-2000-4978-bf52-95042f5ef271
01/27/2025 20:49:07:INFO:Received: train message 888e14a4-2000-4978-bf52-95042f5ef271
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 21:04:04:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 21:22:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 21:22:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1e50e2e9-3a35-464d-95d8-89325fd1eded
01/27/2025 21:22:24:INFO:Received: evaluate message 1e50e2e9-3a35-464d-95d8-89325fd1eded

{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098, 1.765155115148688], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824, 0.34393878372935965], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779, 0.5177511222367319]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098, 1.765155115148688, 1.75747421649114], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824, 0.34393878372935965, 0.34313330648409185], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779, 0.5177511222367319, 0.5246326183305553]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)


{'loss': [2.0637545787275102, 2.0087720752482543, 1.9602251271302786, 1.9178086867795276, 1.8821397151766504, 1.8517365428261636, 1.8266429125278456, 1.8059340058148892, 1.7893866446053746, 1.7758301356796098, 1.765155115148688, 1.75747421649114, 1.7514289996818915], 'accuracy': [0.19693918646798228, 0.23238018525976642, 0.2605718888441402, 0.29842931937172773, 0.3221908981071285, 0.3387031816351188, 0.3407168747482884, 0.3411196133709223, 0.34313330648409185, 0.342327829238824, 0.34393878372935965, 0.34313330648409185, 0.34313330648409185], 'auc': [0.4459623197507399, 0.45312056473212875, 0.4602570017876294, 0.4675412872273659, 0.47475328936944755, 0.48197536384117456, 0.48932848347250124, 0.49660635135622655, 0.5039251400382221, 0.5108443173051779, 0.5177511222367319, 0.5246326183305553, 0.5310733000304378]}

Base Noise Multiplier Received:  0.66375732421875
Data Scaling Factor: 0.10623346887213296 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.01243721041828394, 0.030587119981646538, 0.004207799211144447, 0.0009142957860603929, 0.010416004806756973, 0.0020632585510611534, 0.002323855645954609, 0.0018798827659338713]
Noise Multiplier after list and tensor:  0.00810367839585524
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)

[92mINFO [0m:      Sent reply
01/27/2025 21:27:01:INFO:Sent reply
[92mINFO [0m:      
01/27/2025 21:27:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/27/2025 21:27:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ce03d2de-a177-4c06-a5e2-ca771aeecc02
01/27/2025 21:27:48:INFO:Received: train message ce03d2de-a177-4c06-a5e2-ca771aeecc02
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/27/2025 21:42:55:INFO:Sent reply
