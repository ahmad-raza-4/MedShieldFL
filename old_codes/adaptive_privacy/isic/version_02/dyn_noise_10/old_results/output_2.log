nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/23/2025 11:22:10:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/23/2025 11:22:10:DEBUG:ChannelConnectivity.IDLE
01/23/2025 11:22:10:DEBUG:ChannelConnectivity.CONNECTING
01/23/2025 11:22:10:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/23/2025 11:24:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:24:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cc57f6c9-78fc-4e2d-a660-1c0f47b38524
01/23/2025 11:24:32:INFO:Received: train message cc57f6c9-78fc-4e2d-a660-1c0f47b38524
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:37:19:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:43:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:43:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c862ce3d-cd94-49eb-a3de-2a7862d26a60
01/23/2025 11:43:51:INFO:Received: evaluate message c862ce3d-cd94-49eb-a3de-2a7862d26a60
[92mINFO [0m:      Sent reply
01/23/2025 11:47:55:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:48:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:48:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 739c2c23-c57a-4f7c-b927-07c8683be3e7
01/23/2025 11:48:27:INFO:Received: train message 739c2c23-c57a-4f7c-b927-07c8683be3e7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:01:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:07:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:07:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 37f40d96-9672-4a43-9129-fef9628d4ce7
01/23/2025 12:07:36:INFO:Received: evaluate message 37f40d96-9672-4a43-9129-fef9628d4ce7
[92mINFO [0m:      Sent reply
01/23/2025 12:11:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:12:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:12:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8b59c95b-d532-4e25-a536-bea5f50ce5ea
01/23/2025 12:12:07:INFO:Received: train message 8b59c95b-d532-4e25-a536-bea5f50ce5ea
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:24:43:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:31:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:31:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 007cf1eb-33ae-4554-ae09-2edc2e50c5a6
01/23/2025 12:31:09:INFO:Received: evaluate message 007cf1eb-33ae-4554-ae09-2edc2e50c5a6
[92mINFO [0m:      Sent reply
01/23/2025 12:35:14:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:35:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:35:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message be004d91-af86-48ed-9b14-bd258ee0aea7
01/23/2025 12:35:36:INFO:Received: train message be004d91-af86-48ed-9b14-bd258ee0aea7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:48:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:54:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:54:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 761c6e4c-646d-4182-8220-164108d6196e
01/23/2025 12:54:50:INFO:Received: evaluate message 761c6e4c-646d-4182-8220-164108d6196e
[92mINFO [0m:      Sent reply
01/23/2025 12:58:55:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:59:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:59:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a0534496-ab8c-4b35-94fc-16ca7225f750
01/23/2025 12:59:14:INFO:Received: train message a0534496-ab8c-4b35-94fc-16ca7225f750
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:12:11:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:18:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:18:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7aa47cc6-a41f-4fda-ab33-f046c3ba9918
01/23/2025 13:18:39:INFO:Received: evaluate message 7aa47cc6-a41f-4fda-ab33-f046c3ba9918
[92mINFO [0m:      Sent reply
01/23/2025 13:22:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:22:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:22:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 40b298f4-7e42-4ac7-bf6b-bc1a9795d76a
01/23/2025 13:22:54:INFO:Received: train message 40b298f4-7e42-4ac7-bf6b-bc1a9795d76a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:35:40:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:42:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:42:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5e07b353-1d71-49cb-91d7-7a8b861811f3
01/23/2025 13:42:09:INFO:Received: evaluate message 5e07b353-1d71-49cb-91d7-7a8b861811f3
[92mINFO [0m:      Sent reply
01/23/2025 13:46:10:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:46:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:46:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b6c39d88-2aae-4732-9405-805d669d03a6
01/23/2025 13:46:44:INFO:Received: train message b6c39d88-2aae-4732-9405-805d669d03a6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:59:16:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:05:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:05:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 749ce9dd-5913-476f-b60f-fddacc932052
01/23/2025 14:05:39:INFO:Received: evaluate message 749ce9dd-5913-476f-b60f-fddacc932052
[92mINFO [0m:      Sent reply
01/23/2025 14:09:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:09:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:09:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5a3cd08d-9609-44fa-abb6-3a87f5283f5b
01/23/2025 14:09:57:INFO:Received: train message 5a3cd08d-9609-44fa-abb6-3a87f5283f5b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:22:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:29:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:29:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eb1746ac-a7a8-4850-b7cf-be39d1866a56
01/23/2025 14:29:24:INFO:Received: evaluate message eb1746ac-a7a8-4850-b7cf-be39d1866a56
[92mINFO [0m:      Sent reply
01/23/2025 14:33:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:33:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:33:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c3affed2-9fd5-4cc3-b0f6-f03f8dcde7af
01/23/2025 14:33:46:INFO:Received: train message c3affed2-9fd5-4cc3-b0f6-f03f8dcde7af
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:46:39:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:52:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:52:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cdfe3530-7881-4567-afa0-8cddd3747db0
01/23/2025 14:52:59:INFO:Received: evaluate message cdfe3530-7881-4567-afa0-8cddd3747db0
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 10, target_epsilon: 10, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.43

{'loss': [137.23898267745972], 'accuracy': [0.3383004430124849], 'auc': [0.5877396505618926]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.54

{'loss': [137.23898267745972, 134.08246445655823], 'accuracy': [0.3383004430124849, 0.3407168747482884], 'auc': [0.5877396505618926, 0.620088227094536]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.03

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.16

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.46

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.02

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.16

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.02

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.59
[92mINFO [0m:      Sent reply
01/23/2025 14:56:47:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:57:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:57:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7b89ab28-c37d-4150-9200-67133c7e5362
01/23/2025 14:57:23:INFO:Received: train message 7b89ab28-c37d-4150-9200-67133c7e5362
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:10:01:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:16:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:16:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d06aba39-aad3-42f1-920d-c07c58f812cf
01/23/2025 15:16:39:INFO:Received: evaluate message d06aba39-aad3-42f1-920d-c07c58f812cf
[92mINFO [0m:      Sent reply
01/23/2025 15:20:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:21:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:21:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cd27ba6a-1330-4848-8328-62b3edfad361
01/23/2025 15:21:07:INFO:Received: train message cd27ba6a-1330-4848-8328-62b3edfad361
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:33:54:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:40:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:40:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message df26e3e0-f4e4-48d5-980d-13bd546b10b8
01/23/2025 15:40:24:INFO:Received: evaluate message df26e3e0-f4e4-48d5-980d-13bd546b10b8
[92mINFO [0m:      Sent reply
01/23/2025 15:44:24:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:44:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:44:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e9b2c728-6bf0-484c-ba3a-aab09e4dfe12
01/23/2025 15:44:57:INFO:Received: train message e9b2c728-6bf0-484c-ba3a-aab09e4dfe12
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:57:33:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:03:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:03:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2cf38761-3179-44e7-a66f-f93edbbb59c6
01/23/2025 16:03:49:INFO:Received: evaluate message 2cf38761-3179-44e7-a66f-f93edbbb59c6
[92mINFO [0m:      Sent reply
01/23/2025 16:07:49:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:08:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:08:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c7e753ea-2bc1-46fe-9cab-127215ddd636
01/23/2025 16:08:14:INFO:Received: train message c7e753ea-2bc1-46fe-9cab-127215ddd636
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:20:52:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:27:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:27:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6b074683-9673-4442-bf9a-bbce60098de2
01/23/2025 16:27:17:INFO:Received: evaluate message 6b074683-9673-4442-bf9a-bbce60098de2
[92mINFO [0m:      Sent reply
01/23/2025 16:31:22:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:31:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:31:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c8c0081a-7774-487b-aee1-674f6ce6d1b8
01/23/2025 16:31:51:INFO:Received: train message c8c0081a-7774-487b-aee1-674f6ce6d1b8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:44:44:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:51:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:51:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c27d11ac-6617-49e5-b93c-77bea60b44ee
01/23/2025 16:51:03:INFO:Received: evaluate message c27d11ac-6617-49e5-b93c-77bea60b44ee
[92mINFO [0m:      Sent reply
01/23/2025 16:55:00:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:55:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:55:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a9ea9353-0b36-4692-9c32-5fe8101eab9b
01/23/2025 16:55:20:INFO:Received: train message a9ea9353-0b36-4692-9c32-5fe8101eab9b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:08:04:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:14:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:14:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f877a862-f226-4a78-a847-db1eb71bb57b
01/23/2025 17:14:32:INFO:Received: evaluate message f877a862-f226-4a78-a847-db1eb71bb57b

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.30

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.25

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.16

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.38

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.23

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06778716463595628
Epsilon = 10.00 and Loss = 0.14
[92mINFO [0m:      Sent reply
01/23/2025 17:18:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:19:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:19:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 27accf5a-cf83-467a-b879-43fca8d33f3f
01/23/2025 17:19:05:INFO:Received: train message 27accf5a-cf83-467a-b879-43fca8d33f3f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:31:55:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:38:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:38:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5858c908-4128-4889-9845-729c86ed881a
01/23/2025 17:38:01:INFO:Received: evaluate message 5858c908-4128-4889-9845-729c86ed881a
[92mINFO [0m:      Sent reply
01/23/2025 17:41:31:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:42:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:42:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3564a8f6-fbd4-49a4-b4cf-17a84b1ffbbf
01/23/2025 17:42:44:INFO:Received: train message 3564a8f6-fbd4-49a4-b4cf-17a84b1ffbbf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:55:25:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:01:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:01:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f0e68b8a-9031-4201-83a8-e3082200c817
01/23/2025 18:01:43:INFO:Received: evaluate message f0e68b8a-9031-4201-83a8-e3082200c817
[92mINFO [0m:      Sent reply
01/23/2025 18:05:48:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:06:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:06:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b469bae2-cd3d-4d80-af65-d1f6a414b234
01/23/2025 18:06:28:INFO:Received: train message b469bae2-cd3d-4d80-af65-d1f6a414b234
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:18:58:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:25:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:25:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0c028937-17ca-471e-8ee5-deee02316581
01/23/2025 18:25:13:INFO:Received: evaluate message 0c028937-17ca-471e-8ee5-deee02316581
[92mINFO [0m:      Sent reply
01/23/2025 18:28:34:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:29:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:29:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8705b026-37ac-4c78-8929-dc0f2a313e33
01/23/2025 18:29:51:INFO:Received: train message 8705b026-37ac-4c78-8929-dc0f2a313e33
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:42:28:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:48:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:48:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f08300e9-7753-42be-8df4-4523dadbb3ec
01/23/2025 18:48:57:INFO:Received: evaluate message f08300e9-7753-42be-8df4-4523dadbb3ec
[92mINFO [0m:      Sent reply
01/23/2025 18:52:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:53:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:53:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 04eb7191-51c7-42a0-8da0-df47ed3767c8
01/23/2025 18:53:06:INFO:Received: train message 04eb7191-51c7-42a0-8da0-df47ed3767c8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:05:45:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:12:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:12:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ac7dc0ad-08f2-4cd5-a8bb-06f450fa31d2
01/23/2025 19:12:20:INFO:Received: evaluate message ac7dc0ad-08f2-4cd5-a8bb-06f450fa31d2

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.06326802032689254
Epsilon = 10.00 and Loss = 0.09

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.05874887601782878
Epsilon = 10.00 and Loss = 0.32

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.05422973170876503
Epsilon = 10.00 and Loss = 0.44

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.04971058739970128
Epsilon = 10.00 and Loss = 0.31

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.04519144309063753
Epsilon = 10.00 and Loss = 0.79
[92mINFO [0m:      Sent reply
01/23/2025 19:16:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:17:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:17:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ee42e9e-f858-4d6a-a94a-02955811c85f
01/23/2025 19:17:01:INFO:Received: train message 8ee42e9e-f858-4d6a-a94a-02955811c85f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:29:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:36:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:36:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f41bf3b0-c1b7-41e2-acca-ed719f0c1471
01/23/2025 19:36:03:INFO:Received: evaluate message f41bf3b0-c1b7-41e2-acca-ed719f0c1471
[92mINFO [0m:      Sent reply
01/23/2025 19:40:07:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:40:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:40:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8a58610e-5e54-4803-a4e8-80ae6361b64d
01/23/2025 19:40:24:INFO:Received: train message 8a58610e-5e54-4803-a4e8-80ae6361b64d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:53:01:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:59:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:59:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ba529481-5b3b-4c8d-a509-42bdae8086c8
01/23/2025 19:59:29:INFO:Received: evaluate message ba529481-5b3b-4c8d-a509-42bdae8086c8
[92mINFO [0m:      Sent reply
01/23/2025 20:03:28:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:04:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:04:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1dddb225-7c78-40b4-888a-f1f97018b481
01/23/2025 20:04:01:INFO:Received: train message 1dddb225-7c78-40b4-888a-f1f97018b481
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:16:53:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:22:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:22:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b9a0c269-a99a-47b4-9c74-de5850c40f88
01/23/2025 20:22:58:INFO:Received: evaluate message b9a0c269-a99a-47b4-9c74-de5850c40f88
[92mINFO [0m:      Sent reply
01/23/2025 20:26:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:27:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:27:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1797d5ee-2302-4129-968f-92da7315f47d
01/23/2025 20:27:27:INFO:Received: train message 1797d5ee-2302-4129-968f-92da7315f47d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:40:21:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:46:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:46:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3839bef3-5454-4664-9535-410ca90edd6c
01/23/2025 20:46:42:INFO:Received: evaluate message 3839bef3-5454-4664-9535-410ca90edd6c

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.04067229878157377
Epsilon = 10.00 and Loss = 0.47

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.03615315447251002
Epsilon = 10.00 and Loss = 0.33

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.03163401016344627
Epsilon = 10.00 and Loss = 0.40

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.027114865854382515
Epsilon = 10.00 and Loss = 0.65
[92mINFO [0m:      Sent reply
01/23/2025 20:50:42:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:51:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:51:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fa759ec9-df8b-4df2-9e7e-f8d6a8c2f975
01/23/2025 20:51:14:INFO:Received: train message fa759ec9-df8b-4df2-9e7e-f8d6a8c2f975
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:03:46:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:10:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:10:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d9310085-9685-4b9e-8db9-b1445bb4ecfc
01/23/2025 21:10:31:INFO:Received: evaluate message d9310085-9685-4b9e-8db9-b1445bb4ecfc
[92mINFO [0m:      Sent reply
01/23/2025 21:14:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:15:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:15:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bae0f12d-30d9-4e67-a685-29a7746961f7
01/23/2025 21:15:12:INFO:Received: train message bae0f12d-30d9-4e67-a685-29a7746961f7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:27:53:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:34:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:34:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0c55f6d8-cd49-40c4-ab36-74315f21045c
01/23/2025 21:34:40:INFO:Received: evaluate message 0c55f6d8-cd49-40c4-ab36-74315f21045c
[92mINFO [0m:      Sent reply
01/23/2025 21:38:46:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:39:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:39:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f3ce3b64-0ce8-41d1-8fcb-6c8ea16b0328
01/23/2025 21:39:17:INFO:Received: train message f3ce3b64-0ce8-41d1-8fcb-6c8ea16b0328
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:53:30:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:05:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:05:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f25a0eb1-5d00-4737-a0ee-6ad264038350
01/23/2025 22:05:25:INFO:Received: evaluate message f25a0eb1-5d00-4737-a0ee-6ad264038350
[92mINFO [0m:      Sent reply
01/23/2025 22:10:09:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:10:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:10:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6c92c39c-bf02-4d78-9a86-52224709ca86
01/23/2025 22:10:25:INFO:Received: train message 6c92c39c-bf02-4d78-9a86-52224709ca86

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.022595721545318765
Epsilon = 10.00 and Loss = 0.39

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.018076577236255012
Epsilon = 10.00 and Loss = 0.02

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.013557432927191254
Epsilon = 10.00 and Loss = 0.21

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:25:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:37:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:37:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a1caac9a-4a7c-4061-8c68-36bd1dc0a28a
01/23/2025 22:37:57:INFO:Received: evaluate message a1caac9a-4a7c-4061-8c68-36bd1dc0a28a
[92mINFO [0m:      Sent reply
01/23/2025 22:42:50:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:43:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:43:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e3ec2e82-1c1a-472e-ba95-b3028b91fd55
01/23/2025 22:43:17:INFO:Received: train message e3ec2e82-1c1a-472e-ba95-b3028b91fd55
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 23:00:44:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:15:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:15:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bcac6691-b78e-455b-95bd-cf33aabe2362
01/23/2025 23:15:14:INFO:Received: evaluate message bcac6691-b78e-455b-95bd-cf33aabe2362
[92mINFO [0m:      Sent reply
01/23/2025 23:19:57:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:20:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:20:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 18167960-c4d7-4074-9cfc-2d24ebdacb09
01/23/2025 23:20:34:INFO:Received: train message 18167960-c4d7-4074-9cfc-2d24ebdacb09
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 23:36:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:48:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:48:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9d1d421b-c0c4-4697-a679-8826c22f87ad
01/23/2025 23:48:55:INFO:Received: evaluate message 9d1d421b-c0c4-4697-a679-8826c22f87ad
[92mINFO [0m:      Sent reply
01/23/2025 23:53:46:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:53:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:53:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 490b94d3-c79d-448c-8946-fd3f73e61e16
01/23/2025 23:53:49:INFO:Received: reconnect message 490b94d3-c79d-448c-8946-fd3f73e61e16
01/23/2025 23:53:49:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/23/2025 23:53:49:INFO:Disconnect and shut down
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.009038288618127503
Epsilon = 10.00 and Loss = 0.03

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.004519144309063751
Epsilon = 10.00 and Loss = 0.15

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808]}

Base Noise Multiplier Received:  0.68389892578125
Data Scaling Factor: 8.008536199810306 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0549100860953331, 4.424698829650879, 0.09816860407590866, 0.07609306275844574, 0.21945856511592865, 0.11280928552150726, 0.19942307472229004, 0.23741166293621063]
Noise Multiplier after list and tensor:  0.6778716463595629
Noise Multiplier after Epsilon Scaling:  0.06778716463595628
Noise Multiplier after Convergence: 0.0
Epsilon = 10.00 and Loss = 0.48

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529, 116.09026396274567], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993, 0.5372533225936368], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808, 0.731616827854312]}



Final client history:
{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529, 116.09026396274567], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993, 0.5372533225936368], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808, 0.731616827854312]}

