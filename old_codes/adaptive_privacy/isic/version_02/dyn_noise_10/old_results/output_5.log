nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/23/2025 11:19:41:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/23/2025 11:19:41:DEBUG:ChannelConnectivity.IDLE
01/23/2025 11:19:41:DEBUG:ChannelConnectivity.CONNECTING
01/23/2025 11:19:41:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/23/2025 11:24:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:24:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1d653d05-1511-4353-963e-f21785147a55
01/23/2025 11:24:38:INFO:Received: train message 1d653d05-1511-4353-963e-f21785147a55
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:28:30:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:43:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:43:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9017070f-36f8-4b16-b377-23b90da42186
01/23/2025 11:43:56:INFO:Received: evaluate message 9017070f-36f8-4b16-b377-23b90da42186
[92mINFO [0m:      Sent reply
01/23/2025 11:47:59:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 11:48:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 11:48:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0854c4a2-225b-4be7-8d8e-39691a1f5c97
01/23/2025 11:48:15:INFO:Received: train message 0854c4a2-225b-4be7-8d8e-39691a1f5c97
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 11:51:47:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:07:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:07:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d5ec766a-ea80-43dc-be70-fd3cc87d8b5b
01/23/2025 12:07:36:INFO:Received: evaluate message d5ec766a-ea80-43dc-be70-fd3cc87d8b5b
[92mINFO [0m:      Sent reply
01/23/2025 12:11:37:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:12:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:12:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7da5a516-887a-4d72-b81d-45c3b4800372
01/23/2025 12:12:00:INFO:Received: train message 7da5a516-887a-4d72-b81d-45c3b4800372
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:16:05:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:31:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:31:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ed85f95e-e480-4b74-9127-5ebde74e5694
01/23/2025 12:31:01:INFO:Received: evaluate message ed85f95e-e480-4b74-9127-5ebde74e5694
[92mINFO [0m:      Sent reply
01/23/2025 12:34:39:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:35:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:35:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 526043b6-c442-46d2-a9d1-8acc27da7281
01/23/2025 12:35:42:INFO:Received: train message 526043b6-c442-46d2-a9d1-8acc27da7281
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 12:39:50:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:54:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:54:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f578722d-b0a8-42bb-a0ea-bf18da55923f
01/23/2025 12:54:51:INFO:Received: evaluate message f578722d-b0a8-42bb-a0ea-bf18da55923f
[92mINFO [0m:      Sent reply
01/23/2025 12:58:55:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 12:59:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 12:59:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c52514c1-c3f8-4eac-9200-c9104ab228ed
01/23/2025 12:59:18:INFO:Received: train message c52514c1-c3f8-4eac-9200-c9104ab228ed
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:03:30:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:18:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:18:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1ec40dad-d4e2-43f2-855e-10929f39b8e2
01/23/2025 13:18:43:INFO:Received: evaluate message 1ec40dad-d4e2-43f2-855e-10929f39b8e2
[92mINFO [0m:      Sent reply
01/23/2025 13:22:41:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:23:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:23:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dedf6a8e-b855-41be-b525-407b125df612
01/23/2025 13:23:12:INFO:Received: train message dedf6a8e-b855-41be-b525-407b125df612
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:27:18:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:42:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:42:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 73057ec4-9cef-40a9-80ad-58f51923ae83
01/23/2025 13:42:15:INFO:Received: evaluate message 73057ec4-9cef-40a9-80ad-58f51923ae83
[92mINFO [0m:      Sent reply
01/23/2025 13:46:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 13:46:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 13:46:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 917af020-4195-4539-b502-0cd4d0c412d0
01/23/2025 13:46:40:INFO:Received: train message 917af020-4195-4539-b502-0cd4d0c412d0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 13:50:48:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:05:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:05:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 336dff9e-7dfc-45a2-b6de-e3dffbf39f9e
01/23/2025 14:05:29:INFO:Received: evaluate message 336dff9e-7dfc-45a2-b6de-e3dffbf39f9e
[92mINFO [0m:      Sent reply
01/23/2025 14:08:39:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:10:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:10:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 805275d4-7bc4-4e38-b43a-a57bca795be3
01/23/2025 14:10:09:INFO:Received: train message 805275d4-7bc4-4e38-b43a-a57bca795be3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:14:23:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:29:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:29:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 88fab5e4-11da-4f58-9026-8213283738b5
01/23/2025 14:29:28:INFO:Received: evaluate message 88fab5e4-11da-4f58-9026-8213283738b5
[92mINFO [0m:      Sent reply
01/23/2025 14:33:29:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:33:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:33:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message beb9cf02-885e-4c16-9ca2-4bb4b2316a3a
01/23/2025 14:33:56:INFO:Received: train message beb9cf02-885e-4c16-9ca2-4bb4b2316a3a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 14:37:53:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:53:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:53:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9dfdb89e-b1cc-4cf1-b689-f4f6e910c94f
01/23/2025 14:53:13:INFO:Received: evaluate message 9dfdb89e-b1cc-4cf1-b689-f4f6e910c94f
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_02/dyn_noise_10']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 25331, num_classes: 8

Privacy Params:
 epsilon: 10, target_epsilon: 10, target_delta: 1e-05

Device: cuda:0

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 2.10

{'loss': [137.23898267745972], 'accuracy': [0.3383004430124849], 'auc': [0.5877396505618926]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 1.84

{'loss': [137.23898267745972, 134.08246445655823], 'accuracy': [0.3383004430124849, 0.3407168747482884], 'auc': [0.5877396505618926, 0.620088227094536]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 1.49

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 1.36

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 2.22

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 1.63

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 1.29

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 1.79

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 2.84
[92mINFO [0m:      Sent reply
01/23/2025 14:57:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 14:57:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 14:57:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 337eb1ce-2fae-47bb-9d05-af002e0241c6
01/23/2025 14:57:38:INFO:Received: train message 337eb1ce-2fae-47bb-9d05-af002e0241c6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:01:55:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:16:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:16:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message afcc0fa1-6f97-427d-87ec-82fdbff80727
01/23/2025 15:16:41:INFO:Received: evaluate message afcc0fa1-6f97-427d-87ec-82fdbff80727
[92mINFO [0m:      Sent reply
01/23/2025 15:20:46:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:21:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:21:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 10ceefa0-bc08-4e0c-80c7-372c437b232b
01/23/2025 15:21:12:INFO:Received: train message 10ceefa0-bc08-4e0c-80c7-372c437b232b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:25:18:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:40:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:40:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6c149a58-00c8-423e-9e2f-8e058361aad3
01/23/2025 15:40:30:INFO:Received: evaluate message 6c149a58-00c8-423e-9e2f-8e058361aad3
[92mINFO [0m:      Sent reply
01/23/2025 15:44:28:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 15:44:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 15:44:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f2c1ebd3-9788-40a6-81d3-e2aff169b816
01/23/2025 15:44:57:INFO:Received: train message f2c1ebd3-9788-40a6-81d3-e2aff169b816
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 15:48:38:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:03:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:03:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d5a60b09-c6b1-4d56-8ebf-d75a09d5b9e8
01/23/2025 16:03:40:INFO:Received: evaluate message d5a60b09-c6b1-4d56-8ebf-d75a09d5b9e8
[92mINFO [0m:      Sent reply
01/23/2025 16:07:29:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:08:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:08:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fce518ec-aeec-4ebf-a308-b2ecaae2dbf7
01/23/2025 16:08:19:INFO:Received: train message fce518ec-aeec-4ebf-a308-b2ecaae2dbf7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:12:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:27:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:27:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5d3611d4-ad89-4386-b23e-c7f2201c7048
01/23/2025 16:27:08:INFO:Received: evaluate message 5d3611d4-ad89-4386-b23e-c7f2201c7048
[92mINFO [0m:      Sent reply
01/23/2025 16:31:15:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:31:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:31:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e90d02c7-e762-42b4-bd29-517775c867dd
01/23/2025 16:31:40:INFO:Received: train message e90d02c7-e762-42b4-bd29-517775c867dd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:35:41:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:50:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:50:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 019168b0-9d97-4689-940f-68a9ff622929
01/23/2025 16:50:46:INFO:Received: evaluate message 019168b0-9d97-4689-940f-68a9ff622929
[92mINFO [0m:      Sent reply
01/23/2025 16:53:35:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 16:55:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 16:55:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fc38fd43-aeaf-48df-ae63-527a08859d3b
01/23/2025 16:55:20:INFO:Received: train message fc38fd43-aeaf-48df-ae63-527a08859d3b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 16:59:20:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:14:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:14:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3c262780-97b4-423a-995d-c702efa5d7c8
01/23/2025 17:14:33:INFO:Received: evaluate message 3c262780-97b4-423a-995d-c702efa5d7c8

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 1.95

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 1.19

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 2.17

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 2.07

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 2.23

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.25870027989149097
Epsilon = 10.00 and Loss = 2.03
[92mINFO [0m:      Sent reply
01/23/2025 17:18:36:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:19:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:19:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8f1e8d37-5131-4d31-a262-3d4eefbe7915
01/23/2025 17:19:03:INFO:Received: train message 8f1e8d37-5131-4d31-a262-3d4eefbe7915
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:23:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:38:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:38:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 00ccdb59-e2b7-4916-9306-1b3789d0a355
01/23/2025 17:38:18:INFO:Received: evaluate message 00ccdb59-e2b7-4916-9306-1b3789d0a355
[92mINFO [0m:      Sent reply
01/23/2025 17:42:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 17:42:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 17:42:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 78898d78-e672-47af-90f0-cf2c29183315
01/23/2025 17:42:30:INFO:Received: train message 78898d78-e672-47af-90f0-cf2c29183315
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 17:46:20:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:01:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:01:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d14adcb1-bfee-45a0-a497-66522d941549
01/23/2025 18:01:49:INFO:Received: evaluate message d14adcb1-bfee-45a0-a497-66522d941549
[92mINFO [0m:      Sent reply
01/23/2025 18:05:57:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:06:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:06:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4e7c41d4-812c-46dd-a589-8b9ba3445e40
01/23/2025 18:06:22:INFO:Received: train message 4e7c41d4-812c-46dd-a589-8b9ba3445e40
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:10:07:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:25:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:25:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message edb0831a-de6f-40d4-8632-cd2706bb6410
01/23/2025 18:25:30:INFO:Received: evaluate message edb0831a-de6f-40d4-8632-cd2706bb6410
[92mINFO [0m:      Sent reply
01/23/2025 18:29:27:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:29:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:29:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3ec2c869-f343-454d-a5ed-7dd5cc3b0c6c
01/23/2025 18:29:38:INFO:Received: train message 3ec2c869-f343-454d-a5ed-7dd5cc3b0c6c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:32:15:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:48:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:48:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7d5c1aa2-77e8-41fa-9f1d-1d2c371ed963
01/23/2025 18:48:49:INFO:Received: evaluate message 7d5c1aa2-77e8-41fa-9f1d-1d2c371ed963
[92mINFO [0m:      Sent reply
01/23/2025 18:52:51:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 18:53:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 18:53:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 68e8714c-a321-4f62-b936-c24e62b53d5a
01/23/2025 18:53:19:INFO:Received: train message 68e8714c-a321-4f62-b936-c24e62b53d5a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 18:57:24:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:12:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:12:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2a5ba167-e355-4ed9-9368-4d617ba2c084
01/23/2025 19:12:26:INFO:Received: evaluate message 2a5ba167-e355-4ed9-9368-4d617ba2c084

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.2414535945653916
Epsilon = 10.00 and Loss = 2.15

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.22420690923929218
Epsilon = 10.00 and Loss = 1.48

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.2069602239131928
Epsilon = 10.00 and Loss = 1.95

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.1897135385870934
Epsilon = 10.00 and Loss = 2.38

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.172466853260994
Epsilon = 10.00 and Loss = 2.19
[92mINFO [0m:      Sent reply
01/23/2025 19:16:31:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:16:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:16:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ccea6d34-6a46-423f-805e-5ed162e091c7
01/23/2025 19:16:43:INFO:Received: train message ccea6d34-6a46-423f-805e-5ed162e091c7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:19:51:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:36:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:36:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bfb371b7-85de-424b-8adc-94b4c3748314
01/23/2025 19:36:01:INFO:Received: evaluate message bfb371b7-85de-424b-8adc-94b4c3748314
[92mINFO [0m:      Sent reply
01/23/2025 19:40:06:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:40:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:40:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3fd57c27-4443-4042-b016-4a7c4d34a061
01/23/2025 19:40:25:INFO:Received: train message 3fd57c27-4443-4042-b016-4a7c4d34a061
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 19:44:03:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 19:59:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 19:59:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 03ccea91-6c5a-4b3d-834f-9575905ece25
01/23/2025 19:59:20:INFO:Received: evaluate message 03ccea91-6c5a-4b3d-834f-9575905ece25
[92mINFO [0m:      Sent reply
01/23/2025 20:02:54:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:03:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:03:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b6a9e7e8-c072-488f-9dc8-7b443959bfd5
01/23/2025 20:03:51:INFO:Received: train message b6a9e7e8-c072-488f-9dc8-7b443959bfd5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:07:26:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:23:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:23:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a90fbc17-4e5b-438b-8e05-3ff44ba29b07
01/23/2025 20:23:18:INFO:Received: evaluate message a90fbc17-4e5b-438b-8e05-3ff44ba29b07
[92mINFO [0m:      Sent reply
01/23/2025 20:27:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:27:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:27:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fc82483d-25a2-47e4-86aa-e2c732378e99
01/23/2025 20:27:31:INFO:Received: train message fc82483d-25a2-47e4-86aa-e2c732378e99
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:31:22:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:46:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:46:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d4914725-19c2-4d5e-a5f2-715f97ad48e7
01/23/2025 20:46:49:INFO:Received: evaluate message d4914725-19c2-4d5e-a5f2-715f97ad48e7

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.15522016793489457
Epsilon = 10.00 and Loss = 2.43

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.1379734826087952
Epsilon = 10.00 and Loss = 1.80

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.1207267972826958
Epsilon = 10.00 and Loss = 1.94

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.1034801119565964
Epsilon = 10.00 and Loss = 1.67
[92mINFO [0m:      Sent reply
01/23/2025 20:50:46:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 20:51:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 20:51:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 491f4923-3de7-4ba0-b1ba-fdf380e8ceb0
01/23/2025 20:51:16:INFO:Received: train message 491f4923-3de7-4ba0-b1ba-fdf380e8ceb0
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 20:55:02:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:10:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:10:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 65c3101b-6e09-4743-88dc-099b2d75c833
01/23/2025 21:10:20:INFO:Received: evaluate message 65c3101b-6e09-4743-88dc-099b2d75c833
[92mINFO [0m:      Sent reply
01/23/2025 21:14:23:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:14:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:14:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6e1876f4-e055-4f48-beda-d8e465d41b7b
01/23/2025 21:14:56:INFO:Received: train message 6e1876f4-e055-4f48-beda-d8e465d41b7b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:18:13:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:34:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:34:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0ac8394e-fdf5-42d7-8db9-b547e73d2e66
01/23/2025 21:34:39:INFO:Received: evaluate message 0ac8394e-fdf5-42d7-8db9-b547e73d2e66
[92mINFO [0m:      Sent reply
01/23/2025 21:38:56:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 21:39:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 21:39:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 979fc44c-6222-410e-99c9-7e35e5dd59e6
01/23/2025 21:39:24:INFO:Received: train message 979fc44c-6222-410e-99c9-7e35e5dd59e6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 21:43:38:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:05:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:05:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8bc8ac56-0066-4e10-b115-3a838bf8ed08
01/23/2025 22:05:15:INFO:Received: evaluate message 8bc8ac56-0066-4e10-b115-3a838bf8ed08
[92mINFO [0m:      Sent reply
01/23/2025 22:10:10:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:10:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:10:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ecdbfdda-3a4a-4996-8683-e679565a8015
01/23/2025 22:10:38:INFO:Received: train message ecdbfdda-3a4a-4996-8683-e679565a8015

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.086233426630497
Epsilon = 10.00 and Loss = 2.13

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.06898674130439761
Epsilon = 10.00 and Loss = 1.96

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.051740055978298186
Epsilon = 10.00 and Loss = 1.25

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:15:18:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:37:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:37:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b89fe234-e9bd-4ca9-b505-f70d8dd07d59
01/23/2025 22:37:50:INFO:Received: evaluate message b89fe234-e9bd-4ca9-b505-f70d8dd07d59
[92mINFO [0m:      Sent reply
01/23/2025 22:42:50:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 22:43:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 22:43:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6edb9f89-5b95-4ab1-8e45-f43d06bb2860
01/23/2025 22:43:07:INFO:Received: train message 6edb9f89-5b95-4ab1-8e45-f43d06bb2860
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 22:47:09:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:15:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:15:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 71aa592b-65df-4a60-b346-46a532a983b5
01/23/2025 23:15:21:INFO:Received: evaluate message 71aa592b-65df-4a60-b346-46a532a983b5
[92mINFO [0m:      Sent reply
01/23/2025 23:20:09:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:20:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:20:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5f902e97-5aa8-4b22-b52b-81f0d6d855e3
01/23/2025 23:20:23:INFO:Received: train message 5f902e97-5aa8-4b22-b52b-81f0d6d855e3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/23/2025 23:24:12:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:48:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:48:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9ae82920-3d6a-410b-9e17-1c57bfca868a
01/23/2025 23:48:58:INFO:Received: evaluate message 9ae82920-3d6a-410b-9e17-1c57bfca868a
[92mINFO [0m:      Sent reply
01/23/2025 23:53:49:INFO:Sent reply
[92mINFO [0m:      
01/23/2025 23:53:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/23/2025 23:53:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 70fa46fc-d58c-4e96-a0d6-5cbda1611567
01/23/2025 23:53:49:INFO:Received: reconnect message 70fa46fc-d58c-4e96-a0d6-5cbda1611567
01/23/2025 23:53:49:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/23/2025 23:53:49:INFO:Disconnect and shut down
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.03449337065219879
Epsilon = 10.00 and Loss = 2.46

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.017246685326099395
Epsilon = 10.00 and Loss = 1.65

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808]}

Base Noise Multiplier Received:  0.53558349609375
Data Scaling Factor: 38.67328244274809 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [4.417308330535889, 10.170501708984375, 0.32718363404273987, 0.9647272229194641, 3.6468591690063477, 0.5324124693870544, 0.29478219151496887, 0.3422476649284363]
Noise Multiplier after list and tensor:  2.5870027989149094
Noise Multiplier after Epsilon Scaling:  0.25870027989149097
Noise Multiplier after Convergence: 0.0
Epsilon = 10.00 and Loss = 2.07

{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529, 116.09026396274567], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993, 0.5372533225936368], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808, 0.731616827854312]}



Final client history:
{'loss': [137.23898267745972, 134.08246445655823, 136.50129997730255, 138.68937921524048, 139.77128911018372, 139.64172565937042, 137.6421983242035, 135.82592260837555, 132.65138185024261, 130.51821053028107, 128.63891410827637, 127.28007924556732, 125.9071455001831, 124.84411978721619, 123.52748727798462, 122.66383385658264, 121.6530350446701, 121.15643334388733, 120.60817646980286, 119.69176495075226, 119.29281044006348, 118.69239163398743, 118.04494619369507, 117.50876820087433, 117.10123109817505, 117.10221004486084, 116.7288248538971, 116.15087568759918, 116.01985085010529, 116.09026396274567], 'accuracy': [0.3383004430124849, 0.3407168747482884, 0.342327829238824, 0.34353604510672575, 0.3495771244462344, 0.3568264196536448, 0.37535239629480466, 0.39428111155859846, 0.41522351993556184, 0.4309303262182843, 0.44703987112364074, 0.4587192911800242, 0.4671768022553363, 0.47362062021747886, 0.4816753926701571, 0.4877164720096657, 0.49295207410390657, 0.4973821989528796, 0.5026178010471204, 0.509061619009263, 0.5118807893677003, 0.5159081755940395, 0.5195328231977446, 0.522754732178816, 0.5247684252919855, 0.5255739025372533, 0.5300040273862263, 0.5308095046314941, 0.5344341522351993, 0.5372533225936368], 'auc': [0.5877396505618926, 0.620088227094536, 0.634044103019688, 0.6416926486242707, 0.647897844942319, 0.6532735014964851, 0.6585939920688253, 0.6636922400708751, 0.6692355450490037, 0.6739765691828631, 0.6787644661574044, 0.6830271542360524, 0.6871049544939689, 0.6907218374514706, 0.694736098121962, 0.6981423781026579, 0.7015472651555668, 0.7043517911808491, 0.7071334455651364, 0.7101973643929125, 0.7126418327786991, 0.7152616297876564, 0.7178179842200959, 0.7203159641961545, 0.7226743140632264, 0.7244231714217761, 0.7264684107252344, 0.7286912139793894, 0.730389215145808, 0.731616827854312]}

