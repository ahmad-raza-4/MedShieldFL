nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/30/2025 04:45:04:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/30/2025 04:45:04:DEBUG:ChannelConnectivity.IDLE
01/30/2025 04:45:04:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/30/2025 04:45:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:45:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dc8c3fa3-daaf-45e4-a148-83c38006f17b
01/30/2025 04:45:34:INFO:Received: train message dc8c3fa3-daaf-45e4-a148-83c38006f17b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:17:09:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:17:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:17:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7d861d1a-6736-4988-b5f5-1e842ed2bf90
01/30/2025 05:17:49:INFO:Received: evaluate message 7d861d1a-6736-4988-b5f5-1e842ed2bf90
[92mINFO [0m:      Sent reply
01/30/2025 05:22:24:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:22:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:22:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6fbde4e7-e069-4339-9e65-c1f8327d4c11
01/30/2025 05:22:44:INFO:Received: train message 6fbde4e7-e069-4339-9e65-c1f8327d4c11
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 06:04:05:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:04:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:04:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fd90bafc-8cea-4b71-aa01-907a73d51681
01/30/2025 06:04:59:INFO:Received: evaluate message fd90bafc-8cea-4b71-aa01-907a73d51681
[92mINFO [0m:      Sent reply
01/30/2025 06:09:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:10:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:10:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f3fc5a96-1faf-4dce-b7dc-9732aa04176d
01/30/2025 06:10:34:INFO:Received: train message f3fc5a96-1faf-4dce-b7dc-9732aa04176d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 06:50:01:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:50:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:50:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 82ebc1eb-f904-432b-8235-8c48ab9c9816
01/30/2025 06:50:29:INFO:Received: evaluate message 82ebc1eb-f904-432b-8235-8c48ab9c9816
[92mINFO [0m:      Sent reply
01/30/2025 06:55:14:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:56:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:56:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9002c997-b8ce-4ef5-804c-3672b23af464
01/30/2025 06:56:12:INFO:Received: train message 9002c997-b8ce-4ef5-804c-3672b23af464
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 07:28:25:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:29:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:29:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 16434eda-9e36-458b-99ed-c6aca64aa367
01/30/2025 07:29:09:INFO:Received: evaluate message 16434eda-9e36-458b-99ed-c6aca64aa367
[92mINFO [0m:      Sent reply
01/30/2025 07:34:06:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:34:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:34:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a8f71be2-b894-4075-81f6-2e0a34dad6fa
01/30/2025 07:34:44:INFO:Received: train message a8f71be2-b894-4075-81f6-2e0a34dad6fa
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 08:17:39:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 08:18:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 08:18:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c96dcf0b-d050-40fb-aeb1-d08487972106
01/30/2025 08:18:12:INFO:Received: evaluate message c96dcf0b-d050-40fb-aeb1-d08487972106
[92mINFO [0m:      Sent reply
01/30/2025 08:22:16:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 08:22:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 08:22:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 759df228-4d3f-4a64-b0ab-f4e1b748c099
01/30/2025 08:22:16:INFO:Received: reconnect message 759df228-4d3f-4a64-b0ab-f4e1b748c099
01/30/2025 08:22:16:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 08:22:16:INFO:Disconnect and shut down
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.04254982993006706, 0.06371893733739853, 0.04102909192442894, 0.013642445206642151, 0.019247861579060555, 0.004490716382861137, 0.005730428267270327, 0.011894719675183296]
Noise Multiplier after list and tensor:  0.025288003787864
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644], 'accuracy': [0.6741844542891663], 'auc': [0.9136487085523237]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.04254982993006706, 0.06371893733739853, 0.04102909192442894, 0.013642445206642151, 0.019247861579060555, 0.004490716382861137, 0.005730428267270327, 0.011894719675183296]
Noise Multiplier after list and tensor:  0.025288003787864
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512], 'accuracy': [0.6741844542891663, 0.6677406363270237], 'auc': [0.9136487085523237, 0.9122972121312665]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.04254982993006706, 0.06371893733739853, 0.04102909192442894, 0.013642445206642151, 0.019247861579060555, 0.004490716382861137, 0.005730428267270327, 0.011894719675183296]
Noise Multiplier after list and tensor:  0.025288003787864
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.04254982993006706, 0.06371893733739853, 0.04102909192442894, 0.013642445206642151, 0.019247861579060555, 0.004490716382861137, 0.005730428267270327, 0.011894719675183296]
Noise Multiplier after list and tensor:  0.025288003787864
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4122161865234375
Data Scaling Factor: 0.5339570898532021 where Client Data Size: 9930
Noise Multiplier after Fisher Scaling:  [0.04254982993006706, 0.06371893733739853, 0.04102909192442894, 0.013642445206642151, 0.019247861579060555, 0.004490716382861137, 0.005730428267270327, 0.011894719675183296]
Noise Multiplier after list and tensor:  0.025288003787864
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 9930, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765, 1.4033728663449896], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438, 0.6524365686669351], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285, 0.9103161857948601]}



Final client history:
{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765, 1.4033728663449896], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438, 0.6524365686669351], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285, 0.9103161857948601]}

