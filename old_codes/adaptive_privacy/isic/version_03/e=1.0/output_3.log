nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 07:13:08:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 07:13:08:DEBUG:ChannelConnectivity.IDLE
01/29/2025 07:13:08:DEBUG:ChannelConnectivity.CONNECTING
01/29/2025 07:13:08:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 07:18:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:18:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b7de3490-5e15-4f82-a583-d6cfbb874ce5
01/29/2025 07:18:30:INFO:Received: train message b7de3490-5e15-4f82-a583-d6cfbb874ce5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 07:34:05:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:46:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:46:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 25a4b942-becd-48c1-a7be-091aa60d74dc
01/29/2025 07:46:44:INFO:Received: evaluate message 25a4b942-becd-48c1-a7be-091aa60d74dc
[92mINFO [0m:      Sent reply
01/29/2025 07:50:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:51:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:51:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a36f67cd-bc67-4e70-9f07-9857651cb5eb
01/29/2025 07:51:27:INFO:Received: train message a36f67cd-bc67-4e70-9f07-9857651cb5eb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:04:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:18:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:18:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 76917cf6-81fb-4de0-90ed-500e3605b64e
01/29/2025 08:18:25:INFO:Received: evaluate message 76917cf6-81fb-4de0-90ed-500e3605b64e
[92mINFO [0m:      Sent reply
01/29/2025 08:22:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:23:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:23:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 66ea26c8-8598-42be-9bcd-e59eef2ce5b7
01/29/2025 08:23:20:INFO:Received: train message 66ea26c8-8598-42be-9bcd-e59eef2ce5b7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:35:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:47:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:47:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d79352c1-449c-4ff7-9083-e588d139d466
01/29/2025 08:47:04:INFO:Received: evaluate message d79352c1-449c-4ff7-9083-e588d139d466
[92mINFO [0m:      Sent reply
01/29/2025 08:50:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:51:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:51:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7aab70c8-0af7-474e-9b7e-ea592bc8fb5c
01/29/2025 08:51:47:INFO:Received: train message 7aab70c8-0af7-474e-9b7e-ea592bc8fb5c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:06:15:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:18:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:18:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1aba4f37-e0ca-4d58-b267-c628521b44f4
01/29/2025 09:18:56:INFO:Received: evaluate message 1aba4f37-e0ca-4d58-b267-c628521b44f4
[92mINFO [0m:      Sent reply
01/29/2025 09:25:05:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:25:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:25:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6a1e08a3-3ca8-4d3e-a382-65e11bd336af
01/29/2025 09:25:59:INFO:Received: train message 6a1e08a3-3ca8-4d3e-a382-65e11bd336af
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:41:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:58:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:58:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message da68f4bc-5e50-474b-be42-3b8ebf4e1daf
01/29/2025 09:58:20:INFO:Received: evaluate message da68f4bc-5e50-474b-be42-3b8ebf4e1daf
[92mINFO [0m:      Sent reply
01/29/2025 10:03:02:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:04:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:04:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 424399bf-1d21-4843-97c1-9d0d4d33fe93
01/29/2025 10:04:08:INFO:Received: train message 424399bf-1d21-4843-97c1-9d0d4d33fe93
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 10:16:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:26:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:26:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e0daa1b8-0b87-42b7-af51-40b67fa520a7
01/29/2025 10:26:52:INFO:Received: evaluate message e0daa1b8-0b87-42b7-af51-40b67fa520a7
[92mINFO [0m:      Sent reply
01/29/2025 10:31:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:31:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:31:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c085993b-3612-4e62-9c41-9e03721467a5
01/29/2025 10:31:44:INFO:Received: train message c085993b-3612-4e62-9c41-9e03721467a5
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657], 'accuracy': [0.517921868707209], 'auc': [0.7610659482721037]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953], 'accuracy': [0.517921868707209, 0.5819573097060008], 'auc': [0.7610659482721037, 0.8125282828523395]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 10:44:28:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:55:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:55:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ac9787e4-e832-4355-bacb-35f8a2f14cf4
01/29/2025 10:55:28:INFO:Received: evaluate message ac9787e4-e832-4355-bacb-35f8a2f14cf4
[92mINFO [0m:      Sent reply
01/29/2025 10:59:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:00:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:00:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 43e13775-d0b4-464f-a7d8-075aa1cf487b
01/29/2025 11:00:18:INFO:Received: train message 43e13775-d0b4-464f-a7d8-075aa1cf487b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:13:27:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:26:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:26:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0881fdbf-4e0a-4088-9813-6d0a483b2731
01/29/2025 11:26:35:INFO:Received: evaluate message 0881fdbf-4e0a-4088-9813-6d0a483b2731
[92mINFO [0m:      Sent reply
01/29/2025 11:31:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:33:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:33:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 42708046-f4d6-4d12-9c62-36daf1e5896f
01/29/2025 11:33:19:INFO:Received: train message 42708046-f4d6-4d12-9c62-36daf1e5896f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:56:15:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:13:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:13:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c33c992d-84d8-469a-b214-69af5927c26a
01/29/2025 12:13:24:INFO:Received: evaluate message c33c992d-84d8-469a-b214-69af5927c26a
[92mINFO [0m:      Sent reply
01/29/2025 12:18:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:19:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:19:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 95b3fcc4-b4dc-4730-8df0-7acda3b3ed52
01/29/2025 12:19:07:INFO:Received: train message 95b3fcc4-b4dc-4730-8df0-7acda3b3ed52
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:35:28:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:50:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:50:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 44692ec7-d5fb-4bee-ab8c-f393e5654462
01/29/2025 12:50:18:INFO:Received: evaluate message 44692ec7-d5fb-4bee-ab8c-f393e5654462
[92mINFO [0m:      Sent reply
01/29/2025 12:55:11:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:55:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:55:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e7de7b89-14d4-483c-8781-7de94d9177f6
01/29/2025 12:55:43:INFO:Received: train message e7de7b89-14d4-483c-8781-7de94d9177f6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:11:38:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:31:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:31:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 45711b39-c97d-4a87-802d-85fb14b14a0e
01/29/2025 13:31:43:INFO:Received: evaluate message 45711b39-c97d-4a87-802d-85fb14b14a0e
[92mINFO [0m:      Sent reply
01/29/2025 13:36:17:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:37:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:37:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4e384ce2-7e2e-49bc-97c2-7aef71c3eddb
01/29/2025 13:37:27:INFO:Received: train message 4e384ce2-7e2e-49bc-97c2-7aef71c3eddb
[0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:54:33:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:08:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:08:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 589aed43-a26d-417e-a3bb-25da334324e4
01/29/2025 14:08:05:INFO:Received: evaluate message 589aed43-a26d-417e-a3bb-25da334324e4
[92mINFO [0m:      Sent reply
01/29/2025 14:13:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:14:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:14:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5f04c7b7-56ce-46d2-b6fd-5180cebd979c
01/29/2025 14:14:20:INFO:Received: train message 5f04c7b7-56ce-46d2-b6fd-5180cebd979c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:30:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:52:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:52:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2342cdea-c175-4c8e-a4ca-9aadd6f09d7f
01/29/2025 14:52:03:INFO:Received: evaluate message 2342cdea-c175-4c8e-a4ca-9aadd6f09d7f
[92mINFO [0m:      Sent reply
01/29/2025 14:56:42:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:57:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:57:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c8c75a74-a7de-4c02-aa1c-6da3cdd548d4
01/29/2025 14:57:38:INFO:Received: train message c8c75a74-a7de-4c02-aa1c-6da3cdd548d4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:16:29:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:30:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:30:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ddd7586e-4834-4e34-8d2e-4df99db4efa8
01/29/2025 15:30:46:INFO:Received: evaluate message ddd7586e-4834-4e34-8d2e-4df99db4efa8
[92mINFO [0m:      Sent reply
01/29/2025 15:35:30:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:36:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:36:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f7b1d0cd-7379-4d05-b9ac-969049a6dccf
01/29/2025 15:36:25:INFO:Received: train message f7b1d0cd-7379-4d05-b9ac-969049a6dccf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:54:10:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:08:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:08:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 756be73c-71db-4055-b113-8801d965ef3c
01/29/2025 16:08:41:INFO:Received: evaluate message 756be73c-71db-4055-b113-8801d965ef3c
[92mINFO [0m:      Sent reply
01/29/2025 16:14:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:14:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:14:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 550a6927-e54a-4e06-832a-035a9f5dd857
01/29/2025 16:14:55:INFO:Received: train message 550a6927-e54a-4e06-832a-035a9f5dd857
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:35:28:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:48:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:48:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f02952ef-424c-471a-812a-d5a0f73c6609
01/29/2025 16:48:46:INFO:Received: evaluate message f02952ef-424c-471a-812a-d5a0f73c6609
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 16:55:07:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:56:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:56:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message db62fbe3-7adf-4f72-87ca-75052ffd031a
01/29/2025 16:56:50:INFO:Received: train message db62fbe3-7adf-4f72-87ca-75052ffd031a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:12:50:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:31:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:31:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6e3ee8ed-a6c4-4c59-9a96-01aa3b7d641f
01/29/2025 17:31:14:INFO:Received: evaluate message 6e3ee8ed-a6c4-4c59-9a96-01aa3b7d641f
[92mINFO [0m:      Sent reply
01/29/2025 17:35:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:36:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:36:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 53ed6e1c-e7dc-4889-882a-aba83b3942db
01/29/2025 17:36:29:INFO:Received: train message 53ed6e1c-e7dc-4889-882a-aba83b3942db
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:53:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:12:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:12:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 963604ba-2400-43e1-a46e-664033202f3c
01/29/2025 18:12:53:INFO:Received: evaluate message 963604ba-2400-43e1-a46e-664033202f3c
[92mINFO [0m:      Sent reply
01/29/2025 18:17:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:18:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:18:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4c0a6d4a-60cd-437f-b5e5-aad11affe65e
01/29/2025 18:18:38:INFO:Received: train message 4c0a6d4a-60cd-437f-b5e5-aad11affe65e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:33:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:51:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:51:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4ca18575-2e14-4337-bd75-c332a1cb7416
01/29/2025 18:51:53:INFO:Received: evaluate message 4ca18575-2e14-4337-bd75-c332a1cb7416
[92mINFO [0m:      Sent reply
01/29/2025 18:56:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:57:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:57:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cecb61ec-1af0-4ce5-bd81-95a680482b25
01/29/2025 18:57:08:INFO:Received: train message cecb61ec-1af0-4ce5-bd81-95a680482b25
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:15:15:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:30:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:30:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8898492f-89fe-489b-9867-e0a16eb946eb
01/29/2025 19:30:18:INFO:Received: evaluate message 8898492f-89fe-489b-9867-e0a16eb946eb

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 19:34:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:35:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:35:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 56df54ea-cc5e-4380-86a6-a9d0a6a1f7a6
01/29/2025 19:35:20:INFO:Received: train message 56df54ea-cc5e-4380-86a6-a9d0a6a1f7a6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:52:03:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:09:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:09:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e98d5608-ea3a-4911-b975-f0cc0d587460
01/29/2025 20:09:30:INFO:Received: evaluate message e98d5608-ea3a-4911-b975-f0cc0d587460
[92mINFO [0m:      Sent reply
01/29/2025 20:13:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:14:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:14:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message df25ccda-b2d5-4587-b547-acf2e2ed3309
01/29/2025 20:14:40:INFO:Received: train message df25ccda-b2d5-4587-b547-acf2e2ed3309
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:32:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 62f69635-b25c-4c10-ba63-8f85dcf0340d
01/29/2025 20:48:16:INFO:Received: evaluate message 62f69635-b25c-4c10-ba63-8f85dcf0340d
[92mINFO [0m:      Sent reply
01/29/2025 20:53:17:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:53:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:53:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cef65e93-6411-4f4b-afc4-c35cfc782179
01/29/2025 20:53:44:INFO:Received: train message cef65e93-6411-4f4b-afc4-c35cfc782179
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:10:02:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:28:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:28:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 31e3d471-a0b8-4ba5-89c9-d970567a39e5
01/29/2025 21:28:50:INFO:Received: evaluate message 31e3d471-a0b8-4ba5-89c9-d970567a39e5
[92mINFO [0m:      Sent reply
01/29/2025 21:34:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:35:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:35:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a175ede1-c119-495c-9505-551776ee8151
01/29/2025 21:35:26:INFO:Received: train message a175ede1-c119-495c-9505-551776ee8151

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:50:07:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:11:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:11:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 072aa138-b20e-46d4-9ed0-f67bd48fee4e
01/29/2025 22:11:16:INFO:Received: evaluate message 072aa138-b20e-46d4-9ed0-f67bd48fee4e
[92mINFO [0m:      Sent reply
01/29/2025 22:16:42:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:17:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:17:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0e58ec8b-b5b7-4558-ac4f-47b35b5f5699
01/29/2025 22:17:24:INFO:Received: train message 0e58ec8b-b5b7-4558-ac4f-47b35b5f5699
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:31:15:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:54:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:54:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 185e95dd-03ec-45bd-adc6-733a6b5aaa55
01/29/2025 22:54:10:INFO:Received: evaluate message 185e95dd-03ec-45bd-adc6-733a6b5aaa55
[92mINFO [0m:      Sent reply
01/29/2025 22:59:03:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:59:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:59:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e2637538-341a-4ff5-9b23-bd70c50a8e73
01/29/2025 22:59:29:INFO:Received: train message e2637538-341a-4ff5-9b23-bd70c50a8e73
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:15:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:39:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:39:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6ebdcc14-a5a5-43fc-a98a-e81ffc40c3ba
01/29/2025 23:39:16:INFO:Received: evaluate message 6ebdcc14-a5a5-43fc-a98a-e81ffc40c3ba
[92mINFO [0m:      Sent reply
01/29/2025 23:43:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:45:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:45:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 61f4ae4d-8c50-404c-b4f8-f416dda295b9
01/29/2025 23:45:04:INFO:Received: train message 61f4ae4d-8c50-404c-b4f8-f416dda295b9
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:57:51:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:21:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:21:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d1e38165-d79b-460a-9eb4-0fb3d36b5639
01/30/2025 00:21:09:INFO:Received: evaluate message d1e38165-d79b-460a-9eb4-0fb3d36b5639
[92mINFO [0m:      Sent reply
01/30/2025 00:26:38:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:27:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:27:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ce3ca652-de7f-44af-ab82-d0d39fe33107
01/30/2025 00:27:08:INFO:Received: train message ce3ca652-de7f-44af-ab82-d0d39fe33107
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:41:14:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:08:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:08:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c0af69d7-d508-478b-b1ca-46dcc155f19e
01/30/2025 01:08:07:INFO:Received: evaluate message c0af69d7-d508-478b-b1ca-46dcc155f19e
[92mINFO [0m:      Sent reply
01/30/2025 01:13:20:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:13:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:13:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1521c9eb-f1d7-4934-9c65-1f10110c911b
01/30/2025 01:13:37:INFO:Received: train message 1521c9eb-f1d7-4934-9c65-1f10110c911b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:28:34:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:55:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:55:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8b2971b3-728e-4075-b8e4-208ac36e1d3a
01/30/2025 01:55:18:INFO:Received: evaluate message 8b2971b3-728e-4075-b8e4-208ac36e1d3a
[92mINFO [0m:      Sent reply
01/30/2025 01:59:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:00:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:00:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8f05017e-7f7f-416d-9fba-603f7ebbb7d7
01/30/2025 02:00:12:INFO:Received: train message 8f05017e-7f7f-416d-9fba-603f7ebbb7d7
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  2.83203125
Data Scaling Factor: 0.14470075818680433 where Client Data Size: 2691
Noise Multiplier after Fisher Scaling:  [0.07223828136920929, 0.17778372764587402, 0.024411702528595924, 0.0053547704592347145, 0.060408540070056915, 0.012052689678966999, 0.0136244622990489, 0.010913537815213203]
Noise Multiplier after list and tensor:  0.047098463983274996
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:15:26:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:37:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:37:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 940f84f4-08aa-462a-98ca-6baffa6e6952
01/30/2025 02:37:33:INFO:Received: evaluate message 940f84f4-08aa-462a-98ca-6baffa6e6952
[92mINFO [0m:      Sent reply
01/30/2025 02:42:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:42:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:42:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message d0fc1869-6d56-42f7-8c8d-f1a500abed2c
01/30/2025 02:42:27:INFO:Received: reconnect message d0fc1869-6d56-42f7-8c8d-f1a500abed2c
01/30/2025 02:42:27:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 02:42:27:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 2691, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047, 1.3677138108857638], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804, 0.6331051147805075], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232, 0.8855101957814356]}



Final client history:
{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047, 1.3677138108857638], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804, 0.6331051147805075], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232, 0.8855101957814356]}

