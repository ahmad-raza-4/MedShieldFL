nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 07:13:31:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 07:13:31:DEBUG:ChannelConnectivity.IDLE
01/29/2025 07:13:31:DEBUG:ChannelConnectivity.CONNECTING
01/29/2025 07:13:31:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 07:18:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:18:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 40799817-1408-44f3-9aa8-0de864f9ffe9
01/29/2025 07:18:31:INFO:Received: train message 40799817-1408-44f3-9aa8-0de864f9ffe9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 07:35:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:46:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:46:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cac2a789-1443-4535-b7e2-1e889b29c9ec
01/29/2025 07:46:48:INFO:Received: evaluate message cac2a789-1443-4535-b7e2-1e889b29c9ec
[92mINFO [0m:      Sent reply
01/29/2025 07:50:57:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:51:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:51:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d8a43d1a-9745-4a93-aef8-e54904161ae8
01/29/2025 07:51:32:INFO:Received: train message d8a43d1a-9745-4a93-aef8-e54904161ae8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:06:11:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:18:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:18:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e6c353be-b454-4074-a711-a7f9717d6988
01/29/2025 08:18:36:INFO:Received: evaluate message e6c353be-b454-4074-a711-a7f9717d6988
[92mINFO [0m:      Sent reply
01/29/2025 08:22:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:23:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:23:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b3b80642-259f-47ed-b9b3-f310992f9d33
01/29/2025 08:23:20:INFO:Received: train message b3b80642-259f-47ed-b9b3-f310992f9d33
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:36:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:47:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:47:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 86937469-296f-4bab-aa58-8b3051995dfb
01/29/2025 08:47:15:INFO:Received: evaluate message 86937469-296f-4bab-aa58-8b3051995dfb
[92mINFO [0m:      Sent reply
01/29/2025 08:51:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:51:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:51:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b7e3f0df-65e3-403d-86fd-7ef1cbc1fee9
01/29/2025 08:51:35:INFO:Received: train message b7e3f0df-65e3-403d-86fd-7ef1cbc1fee9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:07:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:19:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:19:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 298d74ed-a25c-44d7-8276-f2be1241049e
01/29/2025 09:19:08:INFO:Received: evaluate message 298d74ed-a25c-44d7-8276-f2be1241049e
[92mINFO [0m:      Sent reply
01/29/2025 09:25:38:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:26:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:26:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c0389a5f-9151-4a71-b107-e5b2e32e7e68
01/29/2025 09:26:29:INFO:Received: train message c0389a5f-9151-4a71-b107-e5b2e32e7e68
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:42:31:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:58:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:58:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b2e85d84-4c26-4f70-a823-31e6744bb22c
01/29/2025 09:58:33:INFO:Received: evaluate message b2e85d84-4c26-4f70-a823-31e6744bb22c
[92mINFO [0m:      Sent reply
01/29/2025 10:03:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:03:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:03:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7249748a-2001-4234-bb98-0918419ea2f5
01/29/2025 10:03:47:INFO:Received: train message 7249748a-2001-4234-bb98-0918419ea2f5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 10:17:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:26:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:26:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 33033f9b-49b1-437c-8219-6105a5bafdb3
01/29/2025 10:26:53:INFO:Received: evaluate message 33033f9b-49b1-437c-8219-6105a5bafdb3
[92mINFO [0m:      Sent reply
01/29/2025 10:31:10:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:31:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:31:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fb5f31f7-9ad4-43b2-ad93-9ed2b81e0f9b
01/29/2025 10:31:38:INFO:Received: train message fb5f31f7-9ad4-43b2-ad93-9ed2b81e0f9b
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657], 'accuracy': [0.517921868707209], 'auc': [0.7610659482721037]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953], 'accuracy': [0.517921868707209, 0.5819573097060008], 'auc': [0.7610659482721037, 0.8125282828523395]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 10:45:27:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:55:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:55:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2827d512-4880-4f2e-849a-52d4486e4af0
01/29/2025 10:55:30:INFO:Received: evaluate message 2827d512-4880-4f2e-849a-52d4486e4af0
[92mINFO [0m:      Sent reply
01/29/2025 10:59:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:00:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:00:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6d6807e9-24e7-4150-b02e-eddc488468e6
01/29/2025 11:00:06:INFO:Received: train message 6d6807e9-24e7-4150-b02e-eddc488468e6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:14:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:26:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:26:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a741f052-63f5-4c36-b469-31abf31016e0
01/29/2025 11:26:45:INFO:Received: evaluate message a741f052-63f5-4c36-b469-31abf31016e0
[92mINFO [0m:      Sent reply
01/29/2025 11:32:05:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:32:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:32:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 450cc31b-9d15-4d34-b9a4-a6f48ee62da6
01/29/2025 11:32:40:INFO:Received: train message 450cc31b-9d15-4d34-b9a4-a6f48ee62da6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:57:50:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:13:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:13:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e3924c41-8c7a-4c71-b9f8-6ed32185a95b
01/29/2025 12:13:46:INFO:Received: evaluate message e3924c41-8c7a-4c71-b9f8-6ed32185a95b
[92mINFO [0m:      Sent reply
01/29/2025 12:18:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:19:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:19:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 125c83e6-3734-4665-89c3-5fadaa170499
01/29/2025 12:19:22:INFO:Received: train message 125c83e6-3734-4665-89c3-5fadaa170499
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:37:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:50:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:50:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 281ee25b-57fe-479d-9b02-dfcd12d8e825
01/29/2025 12:50:22:INFO:Received: evaluate message 281ee25b-57fe-479d-9b02-dfcd12d8e825
[92mINFO [0m:      Sent reply
01/29/2025 12:55:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:55:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:55:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9eeb88f0-a3f5-4bd7-ae8d-23585070bc3b
01/29/2025 12:55:35:INFO:Received: train message 9eeb88f0-a3f5-4bd7-ae8d-23585070bc3b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:13:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:31:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:31:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 866f1f36-8e5d-45c1-ba28-f7931cbcdec0
01/29/2025 13:31:35:INFO:Received: evaluate message 866f1f36-8e5d-45c1-ba28-f7931cbcdec0
[92mINFO [0m:      Sent reply
01/29/2025 13:35:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:37:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:37:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f23b09a8-49f4-45f5-a9c8-fb912ac433e6
01/29/2025 13:37:37:INFO:Received: train message f23b09a8-49f4-45f5-a9c8-fb912ac433e6
[0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:56:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:08:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:08:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eeff0723-d0a5-4e86-9e26-1ae2ba1c6b14
01/29/2025 14:08:00:INFO:Received: evaluate message eeff0723-d0a5-4e86-9e26-1ae2ba1c6b14
[92mINFO [0m:      Sent reply
01/29/2025 14:13:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:14:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:14:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 85847f70-e7be-4fbc-8474-c8ddc41a17f6
01/29/2025 14:14:21:INFO:Received: train message 85847f70-e7be-4fbc-8474-c8ddc41a17f6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:32:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:52:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:52:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b7b0c340-0471-44d2-a171-81f2e57193e5
01/29/2025 14:52:01:INFO:Received: evaluate message b7b0c340-0471-44d2-a171-81f2e57193e5
[92mINFO [0m:      Sent reply
01/29/2025 14:56:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:57:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:57:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 076d57a7-535c-4a53-8cb9-105428b122a7
01/29/2025 14:57:38:INFO:Received: train message 076d57a7-535c-4a53-8cb9-105428b122a7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:18:10:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:31:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:31:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4a6b5d6a-ee7a-452f-be26-8243aabe7600
01/29/2025 15:31:04:INFO:Received: evaluate message 4a6b5d6a-ee7a-452f-be26-8243aabe7600
[92mINFO [0m:      Sent reply
01/29/2025 15:35:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:36:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:36:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7dbcc17e-2b1e-484b-ba77-16d6c4c0ec28
01/29/2025 15:36:31:INFO:Received: train message 7dbcc17e-2b1e-484b-ba77-16d6c4c0ec28
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:55:45:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:08:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:08:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2b50ae5b-0c26-4473-84ac-2a11c4bda90a
01/29/2025 16:08:28:INFO:Received: evaluate message 2b50ae5b-0c26-4473-84ac-2a11c4bda90a
[92mINFO [0m:      Sent reply
01/29/2025 16:13:34:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3ab3ea6c-5cdb-4369-98cf-bdd41ba99cb7
01/29/2025 16:15:21:INFO:Received: train message 3ab3ea6c-5cdb-4369-98cf-bdd41ba99cb7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:37:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:48:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:48:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1190f93e-afb2-4b78-854e-fc621f1f632d
01/29/2025 16:48:56:INFO:Received: evaluate message 1190f93e-afb2-4b78-854e-fc621f1f632d
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 16:55:50:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:56:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:56:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ad4cad13-1aac-4eeb-9b0b-4057fb9db247
01/29/2025 16:56:45:INFO:Received: train message ad4cad13-1aac-4eeb-9b0b-4057fb9db247
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:14:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:30:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:30:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message db0f4b4e-d937-4718-9947-f1af0630e313
01/29/2025 17:30:58:INFO:Received: evaluate message db0f4b4e-d937-4718-9947-f1af0630e313
[92mINFO [0m:      Sent reply
01/29/2025 17:35:27:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:36:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:36:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1f67ffe1-9b20-4347-ade4-78f73454dd40
01/29/2025 17:36:19:INFO:Received: train message 1f67ffe1-9b20-4347-ade4-78f73454dd40
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:55:31:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:12:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:12:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 131e1d68-af7b-42a7-98fa-06846af97fa3
01/29/2025 18:12:33:INFO:Received: evaluate message 131e1d68-af7b-42a7-98fa-06846af97fa3
[92mINFO [0m:      Sent reply
01/29/2025 18:16:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:18:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:18:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a3b37c78-7060-43dd-b60f-64382882deae
01/29/2025 18:18:17:INFO:Received: train message a3b37c78-7060-43dd-b60f-64382882deae
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:35:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:51:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:51:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1237f92e-5144-40e1-b648-0dcf3447ca0f
01/29/2025 18:51:53:INFO:Received: evaluate message 1237f92e-5144-40e1-b648-0dcf3447ca0f
[92mINFO [0m:      Sent reply
01/29/2025 18:56:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:56:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:56:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c535985c-a3a2-4c31-8a24-72b033ab3e06
01/29/2025 18:56:55:INFO:Received: train message c535985c-a3a2-4c31-8a24-72b033ab3e06
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:16:44:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:30:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:30:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7322a3e3-c70a-45f5-babf-dcba7c796b71
01/29/2025 19:30:20:INFO:Received: evaluate message 7322a3e3-c70a-45f5-babf-dcba7c796b71

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 19:34:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:35:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:35:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f2c6430a-7cc6-437a-82aa-b7b11a17f3ce
01/29/2025 19:35:23:INFO:Received: train message f2c6430a-7cc6-437a-82aa-b7b11a17f3ce
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:53:38:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:09:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:09:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f0aee66c-e827-4e25-95e4-142668770358
01/29/2025 20:09:31:INFO:Received: evaluate message f0aee66c-e827-4e25-95e4-142668770358
[92mINFO [0m:      Sent reply
01/29/2025 20:13:50:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:15:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:15:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 19a227d2-ccb4-45a2-9775-c09a7ba66291
01/29/2025 20:15:04:INFO:Received: train message 19a227d2-ccb4-45a2-9775-c09a7ba66291
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:34:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 95753333-6a37-4643-a099-0afef2787ce7
01/29/2025 20:48:38:INFO:Received: evaluate message 95753333-6a37-4643-a099-0afef2787ce7
[92mINFO [0m:      Sent reply
01/29/2025 20:53:31:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:54:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:54:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message da7e24d5-334c-4ac5-933d-4cabe7d26c09
01/29/2025 20:54:00:INFO:Received: train message da7e24d5-334c-4ac5-933d-4cabe7d26c09
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:11:39:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:28:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:28:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3dc9439f-8267-4f9d-a916-b7078173f670
01/29/2025 21:28:51:INFO:Received: evaluate message 3dc9439f-8267-4f9d-a916-b7078173f670
[92mINFO [0m:      Sent reply
01/29/2025 21:34:01:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:35:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:35:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4f3e2294-a7fa-4d09-adb5-6ac9f156f698
01/29/2025 21:35:33:INFO:Received: train message 4f3e2294-a7fa-4d09-adb5-6ac9f156f698

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:51:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:11:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:11:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eb39c17a-42d3-4137-bb72-cc2b5aa8442a
01/29/2025 22:11:13:INFO:Received: evaluate message eb39c17a-42d3-4137-bb72-cc2b5aa8442a
[92mINFO [0m:      Sent reply
01/29/2025 22:16:39:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:17:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:17:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ff782f74-c632-4cab-9a67-2e32e69ed226
01/29/2025 22:17:12:INFO:Received: train message ff782f74-c632-4cab-9a67-2e32e69ed226
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:32:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:53:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:53:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4ad17b83-7f9a-45ad-bd76-38148bc7b69b
01/29/2025 22:53:54:INFO:Received: evaluate message 4ad17b83-7f9a-45ad-bd76-38148bc7b69b
[92mINFO [0m:      Sent reply
01/29/2025 22:58:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:59:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:59:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9c58310e-55fa-4064-a5ad-5ebaa95665d3
01/29/2025 22:59:38:INFO:Received: train message 9c58310e-55fa-4064-a5ad-5ebaa95665d3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:17:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:38:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:38:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ab1f1443-ba8a-4012-a879-2f1de4d98350
01/29/2025 23:38:44:INFO:Received: evaluate message ab1f1443-ba8a-4012-a879-2f1de4d98350
[92mINFO [0m:      Sent reply
01/29/2025 23:42:35:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:45:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:45:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 914cb7eb-6f08-41a6-8597-040ca961fd49
01/29/2025 23:45:10:INFO:Received: train message 914cb7eb-6f08-41a6-8597-040ca961fd49
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:01:16:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:20:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:20:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9b6864b7-ef5b-4dc8-8ef7-cab8ed4260d8
01/30/2025 00:20:55:INFO:Received: evaluate message 9b6864b7-ef5b-4dc8-8ef7-cab8ed4260d8
[92mINFO [0m:      Sent reply
01/30/2025 00:26:26:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:27:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:27:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e5f9b883-e3a3-425d-8ac9-5c5cc97bde19
01/30/2025 00:27:04:INFO:Received: train message e5f9b883-e3a3-425d-8ac9-5c5cc97bde19
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:42:20:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:07:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:07:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0d9311ad-fcc6-48c1-814e-f5f151e7eee6
01/30/2025 01:07:48:INFO:Received: evaluate message 0d9311ad-fcc6-48c1-814e-f5f151e7eee6
[92mINFO [0m:      Sent reply
01/30/2025 01:13:07:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:13:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:13:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 31db9e11-3f55-4952-9fa7-daab05d04a4a
01/30/2025 01:13:50:INFO:Received: train message 31db9e11-3f55-4952-9fa7-daab05d04a4a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:30:16:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:55:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:55:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 34908c6c-b9d6-4cd2-aa42-46f40cf28950
01/30/2025 01:55:29:INFO:Received: evaluate message 34908c6c-b9d6-4cd2-aa42-46f40cf28950
[92mINFO [0m:      Sent reply
01/30/2025 01:59:46:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:00:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:00:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eef5d376-1d58-4e3b-a7a6-dce9e4182757
01/30/2025 02:00:18:INFO:Received: train message eef5d376-1d58-4e3b-a7a6-dce9e4182757
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  3.046875
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.022970974445343018, 0.34723636507987976, 0.005748302675783634, 0.007394855841994286, 0.01770319975912571, 0.0069572050124406815, 0.008682903833687305, 0.018611740320920944]
Noise Multiplier after list and tensor:  0.05441319337114692
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:17:50:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:37:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:37:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0d70f44d-a499-477b-90f4-3e8182a06b21
01/30/2025 02:37:34:INFO:Received: evaluate message 0d70f44d-a499-477b-90f4-3e8182a06b21
[92mINFO [0m:      Sent reply
01/30/2025 02:42:27:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:42:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:42:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message bd973666-2609-49ca-b42a-8aa93ed58aff
01/30/2025 02:42:27:INFO:Received: reconnect message bd973666-2609-49ca-b42a-8aa93ed58aff
01/30/2025 02:42:27:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 02:42:27:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047, 1.3677138108857638], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804, 0.6331051147805075], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232, 0.8855101957814356]}



Final client history:
{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047, 1.3677138108857638], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804, 0.6331051147805075], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232, 0.8855101957814356]}

