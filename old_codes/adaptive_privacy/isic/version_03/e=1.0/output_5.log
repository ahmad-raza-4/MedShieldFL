nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/29/2025 07:10:08:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/29/2025 07:10:08:DEBUG:ChannelConnectivity.IDLE
01/29/2025 07:10:08:DEBUG:ChannelConnectivity.CONNECTING
01/29/2025 07:10:08:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/29/2025 07:18:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:18:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 899dc38a-9c55-42f5-8f9f-75d910564309
01/29/2025 07:18:30:INFO:Received: train message 899dc38a-9c55-42f5-8f9f-75d910564309
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 07:25:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:46:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:46:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6ad26ee1-f8f3-4535-9114-4a5ef5c3c4d1
01/29/2025 07:46:51:INFO:Received: evaluate message 6ad26ee1-f8f3-4535-9114-4a5ef5c3c4d1
[92mINFO [0m:      Sent reply
01/29/2025 07:50:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:51:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:51:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d0a87aac-f0ac-48cd-a9c7-91e3913e9b10
01/29/2025 07:51:15:INFO:Received: train message d0a87aac-f0ac-48cd-a9c7-91e3913e9b10
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 07:55:14:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:18:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:18:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1215bcb2-4faa-4bf9-8266-e66d2e3ffb30
01/29/2025 08:18:36:INFO:Received: evaluate message 1215bcb2-4faa-4bf9-8266-e66d2e3ffb30
[92mINFO [0m:      Sent reply
01/29/2025 08:22:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:23:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:23:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 706dac89-90f9-4da7-9536-8653838c7594
01/29/2025 08:23:29:INFO:Received: train message 706dac89-90f9-4da7-9536-8653838c7594
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:27:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:46:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:46:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 29f87dbb-9825-42f9-a184-f0cc5f35dc38
01/29/2025 08:46:57:INFO:Received: evaluate message 29f87dbb-9825-42f9-a184-f0cc5f35dc38
[92mINFO [0m:      Sent reply
01/29/2025 08:50:35:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:51:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:51:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a31c9c84-0c00-44c2-a3df-27e1fdb09626
01/29/2025 08:51:55:INFO:Received: train message a31c9c84-0c00-44c2-a3df-27e1fdb09626
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:56:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:19:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:19:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 98d747bd-81e2-4cdb-9adf-1bf68f951449
01/29/2025 09:19:07:INFO:Received: evaluate message 98d747bd-81e2-4cdb-9adf-1bf68f951449
[92mINFO [0m:      Sent reply
01/29/2025 09:25:33:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:26:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:26:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0fb6e307-5aee-4e2b-844c-20327186a87f
01/29/2025 09:26:10:INFO:Received: train message 0fb6e307-5aee-4e2b-844c-20327186a87f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:30:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:58:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:58:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0b210604-79e9-4b55-8e32-45b145f4a20e
01/29/2025 09:58:27:INFO:Received: evaluate message 0b210604-79e9-4b55-8e32-45b145f4a20e
[92mINFO [0m:      Sent reply
01/29/2025 10:03:05:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:03:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:03:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 63407314-3c9d-4b29-82a1-343a0acdf1e4
01/29/2025 10:03:57:INFO:Received: train message 63407314-3c9d-4b29-82a1-343a0acdf1e4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 10:08:08:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:26:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:26:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a9b312b4-be7d-492b-a712-5dc4861bf363
01/29/2025 10:26:47:INFO:Received: evaluate message a9b312b4-be7d-492b-a712-5dc4861bf363
[92mINFO [0m:      Sent reply
01/29/2025 10:31:03:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:31:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:31:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0022618c-6446-48ca-be99-ec8420b4582b
01/29/2025 10:31:38:INFO:Received: train message 0022618c-6446-48ca-be99-ec8420b4582b
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/e=1.0']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 1.0, target_epsilon: 1.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657], 'accuracy': [0.517921868707209], 'auc': [0.7610659482721037]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953], 'accuracy': [0.517921868707209, 0.5819573097060008], 'auc': [0.7610659482721037, 0.8125282828523395]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  /home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 10:35:44:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 10:55:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 10:55:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2d65738a-feaf-4ece-bce7-3642e2b32a2b
01/29/2025 10:55:18:INFO:Received: evaluate message 2d65738a-feaf-4ece-bce7-3642e2b32a2b
[92mINFO [0m:      Sent reply
01/29/2025 10:59:42:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:00:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:00:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7661e9ad-f320-43e5-ada8-69e3b58c43bb
01/29/2025 11:00:16:INFO:Received: train message 7661e9ad-f320-43e5-ada8-69e3b58c43bb
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:07:03:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:26:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:26:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4d3de576-f1d2-4a78-9a1b-67b3e04c727f
01/29/2025 11:26:36:INFO:Received: evaluate message 4d3de576-f1d2-4a78-9a1b-67b3e04c727f
[92mINFO [0m:      Sent reply
01/29/2025 11:32:12:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 11:33:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 11:33:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 800622a9-f18a-43f8-99cb-5d10ca1b1fb8
01/29/2025 11:33:19:INFO:Received: train message 800622a9-f18a-43f8-99cb-5d10ca1b1fb8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 11:39:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:13:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:13:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 94b10a85-944f-45a5-ac90-d3205798379e
01/29/2025 12:13:35:INFO:Received: evaluate message 94b10a85-944f-45a5-ac90-d3205798379e
[92mINFO [0m:      Sent reply
01/29/2025 12:18:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:19:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:19:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c5cee73c-d44f-44fa-a63c-b622baa4a23a
01/29/2025 12:19:05:INFO:Received: train message c5cee73c-d44f-44fa-a63c-b622baa4a23a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 12:23:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:50:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:50:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6ea29fc0-8a09-4d04-b48b-69f503257720
01/29/2025 12:50:14:INFO:Received: evaluate message 6ea29fc0-8a09-4d04-b48b-69f503257720
[92mINFO [0m:      Sent reply
01/29/2025 12:55:08:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 12:55:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 12:55:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eac20fbb-f1f9-4127-ab43-3343f28ad278
01/29/2025 12:55:52:INFO:Received: train message eac20fbb-f1f9-4127-ab43-3343f28ad278
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:00:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:31:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:31:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 13892d50-0b0b-430c-a6ca-628189cb86d6
01/29/2025 13:31:37:INFO:Received: evaluate message 13892d50-0b0b-430c-a6ca-628189cb86d6
[92mINFO [0m:      Sent reply
01/29/2025 13:36:35:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 13:37:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 13:37:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6b395511-a8ba-4b90-9865-9614b12403a0
01/29/2025 13:37:28:INFO:Received: train message 6b395511-a8ba-4b90-9865-9614b12403a0
[0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 13:43:33:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:07:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:07:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 633546d1-f9e9-44b9-b5a3-5fb85015fecd
01/29/2025 14:07:59:INFO:Received: evaluate message 633546d1-f9e9-44b9-b5a3-5fb85015fecd
[92mINFO [0m:      Sent reply
01/29/2025 14:14:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:14:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:14:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 29ff15eb-bdb1-40f1-82c2-57715d6f5b90
01/29/2025 14:14:30:INFO:Received: train message 29ff15eb-bdb1-40f1-82c2-57715d6f5b90
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 14:19:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:51:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:51:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aaf1e159-713d-443d-bcef-94f59598a31f
01/29/2025 14:51:29:INFO:Received: evaluate message aaf1e159-713d-443d-bcef-94f59598a31f
[92mINFO [0m:      Sent reply
01/29/2025 14:56:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 14:57:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 14:57:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f5ae0d6f-4d4e-4d08-9dc3-7216ec018294
01/29/2025 14:57:26:INFO:Received: train message f5ae0d6f-4d4e-4d08-9dc3-7216ec018294
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:02:56:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:30:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:30:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 42da0272-d862-4fb1-b8d9-4aa12d912d1c
01/29/2025 15:30:38:INFO:Received: evaluate message 42da0272-d862-4fb1-b8d9-4aa12d912d1c
[92mINFO [0m:      Sent reply
01/29/2025 15:35:16:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 15:36:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 15:36:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c04a863f-ec00-473b-a65d-8d09ec10db5a
01/29/2025 15:36:43:INFO:Received: train message c04a863f-ec00-473b-a65d-8d09ec10db5a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 15:41:49:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:08:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:08:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cc9a754c-621b-41d2-9733-4e6bb54b02e9
01/29/2025 16:08:26:INFO:Received: evaluate message cc9a754c-621b-41d2-9733-4e6bb54b02e9
[92mINFO [0m:      Sent reply
01/29/2025 16:14:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:15:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:15:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f7bf4af7-1cea-4357-876c-b465201e0b47
01/29/2025 16:15:27:INFO:Received: train message f7bf4af7-1cea-4357-876c-b465201e0b47
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 16:21:57:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:48:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:48:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d5b49454-4912-4f4d-a279-eb913311eaa0
01/29/2025 16:48:34:INFO:Received: evaluate message d5b49454-4912-4f4d-a279-eb913311eaa0
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 16:53:17:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 16:56:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 16:56:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bae36244-742e-4d77-852c-5bba3db04085
01/29/2025 16:56:45:INFO:Received: train message bae36244-742e-4d77-852c-5bba3db04085
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:01:44:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:31:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:31:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8cc312bb-8065-496d-8dc3-d2f171dbc958
01/29/2025 17:31:11:INFO:Received: evaluate message 8cc312bb-8065-496d-8dc3-d2f171dbc958
[92mINFO [0m:      Sent reply
01/29/2025 17:35:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 17:36:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 17:36:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 451fb862-0f39-44b7-9ccf-b3f9103bdb9b
01/29/2025 17:36:26:INFO:Received: train message 451fb862-0f39-44b7-9ccf-b3f9103bdb9b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 17:41:11:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:12:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:12:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5cc8cc7d-bfca-4daa-951e-60df0dfdc5b7
01/29/2025 18:12:46:INFO:Received: evaluate message 5cc8cc7d-bfca-4daa-951e-60df0dfdc5b7
[92mINFO [0m:      Sent reply
01/29/2025 18:17:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:18:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:18:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c27c275d-bb0c-4ef0-9648-0947c4e13f75
01/29/2025 18:18:28:INFO:Received: train message c27c275d-bb0c-4ef0-9648-0947c4e13f75
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 18:23:40:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:51:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:51:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 26871546-7942-4d8d-8d77-638149c6a680
01/29/2025 18:51:54:INFO:Received: evaluate message 26871546-7942-4d8d-8d77-638149c6a680
[92mINFO [0m:      Sent reply
01/29/2025 18:56:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 18:57:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 18:57:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7446c849-50e4-40f7-b96f-aa1d4c4f3bb6
01/29/2025 18:57:08:INFO:Received: train message 7446c849-50e4-40f7-b96f-aa1d4c4f3bb6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:02:17:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:30:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:30:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dbd97b85-f0e2-4a57-9c9d-161e2c65c82e
01/29/2025 19:30:02:INFO:Received: evaluate message dbd97b85-f0e2-4a57-9c9d-161e2c65c82e

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 19:33:27:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 19:35:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 19:35:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b85d36b6-8a9b-452c-b655-db003efc138d
01/29/2025 19:35:16:INFO:Received: train message b85d36b6-8a9b-452c-b655-db003efc138d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 19:41:05:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:09:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:09:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 84bc897e-e204-46f9-9996-ff4dc0c32467
01/29/2025 20:09:45:INFO:Received: evaluate message 84bc897e-e204-46f9-9996-ff4dc0c32467
[92mINFO [0m:      Sent reply
01/29/2025 20:14:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:15:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:15:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message aef9af87-9b1e-45f0-892e-e03de8e3a0ec
01/29/2025 20:15:06:INFO:Received: train message aef9af87-9b1e-45f0-892e-e03de8e3a0ec
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:19:50:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:48:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:48:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 84a934e6-6ed8-4214-a58e-a5abaec8bfb8
01/29/2025 20:48:30:INFO:Received: evaluate message 84a934e6-6ed8-4214-a58e-a5abaec8bfb8
[92mINFO [0m:      Sent reply
01/29/2025 20:52:26:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 20:54:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 20:54:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 494f17b0-d829-4762-b55f-f006251bcafc
01/29/2025 20:54:12:INFO:Received: train message 494f17b0-d829-4762-b55f-f006251bcafc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 20:58:11:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:29:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:29:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 600b4bdf-95a6-4f83-8aa0-15c82fa7ead0
01/29/2025 21:29:10:INFO:Received: evaluate message 600b4bdf-95a6-4f83-8aa0-15c82fa7ead0
[92mINFO [0m:      Sent reply
01/29/2025 21:34:42:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 21:35:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 21:35:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 74281533-1201-4f2f-b5a6-a46b247e2ffd
01/29/2025 21:35:33:INFO:Received: train message 74281533-1201-4f2f-b5a6-a46b247e2ffd

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926]}

Step 2a: Compute base noise multiplier
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 21:40:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:11:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:11:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 001bb949-fd73-4b78-be2c-e70cfb5a66b1
01/29/2025 22:11:19:INFO:Received: evaluate message 001bb949-fd73-4b78-be2c-e70cfb5a66b1
[92mINFO [0m:      Sent reply
01/29/2025 22:15:43:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:17:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:17:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fa28a3fd-269a-41a1-b64e-e909737e05bf
01/29/2025 22:17:14:INFO:Received: train message fa28a3fd-269a-41a1-b64e-e909737e05bf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 22:21:11:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:53:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:53:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6b9dc9a5-c0ad-46ab-b06e-854de083b91b
01/29/2025 22:53:58:INFO:Received: evaluate message 6b9dc9a5-c0ad-46ab-b06e-854de083b91b
[92mINFO [0m:      Sent reply
01/29/2025 22:58:44:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 22:59:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 22:59:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eea34bdb-be80-4e4b-ac8a-a32550cfe2a2
01/29/2025 22:59:37:INFO:Received: train message eea34bdb-be80-4e4b-ac8a-a32550cfe2a2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:04:11:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:38:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:38:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bded19de-405e-4dde-8445-feabf92ed4bc
01/29/2025 23:38:56:INFO:Received: evaluate message bded19de-405e-4dde-8445-feabf92ed4bc
[92mINFO [0m:      Sent reply
01/29/2025 23:44:50:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 23:45:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 23:45:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f005e21b-37eb-44ab-8101-e86373cfc785
01/29/2025 23:45:41:INFO:Received: train message f005e21b-37eb-44ab-8101-e86373cfc785
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 23:50:52:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:20:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:20:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8e770d70-d038-4ef9-af7b-abf7543ae3c6
01/30/2025 00:20:39:INFO:Received: evaluate message 8e770d70-d038-4ef9-af7b-abf7543ae3c6
[92mINFO [0m:      Sent reply
01/30/2025 00:24:34:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 00:27:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 00:27:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 47bbe373-87f8-4cad-bb69-bd8141a26e7b
01/30/2025 00:27:02:INFO:Received: train message 47bbe373-87f8-4cad-bb69-bd8141a26e7b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 00:30:22:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:07:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:07:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b9981a59-0e28-4f3d-abd8-da395ed3c6b2
01/30/2025 01:07:51:INFO:Received: evaluate message b9981a59-0e28-4f3d-abd8-da395ed3c6b2
[92mINFO [0m:      Sent reply
01/30/2025 01:11:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:13:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:13:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bf21a22f-6bb2-483d-b635-3f6159a23e9e
01/30/2025 01:13:56:INFO:Received: train message bf21a22f-6bb2-483d-b635-3f6159a23e9e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 01:17:46:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 01:55:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 01:55:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f728dee7-540e-46ea-a3b2-570c4a02bcdc
01/30/2025 01:55:14:INFO:Received: evaluate message f728dee7-540e-46ea-a3b2-570c4a02bcdc
[92mINFO [0m:      Sent reply
01/30/2025 01:58:51:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:00:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:00:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 834dd25b-91ed-42b9-a813-65b27c1e245d
01/30/2025 02:00:04:INFO:Received: train message 834dd25b-91ed-42b9-a813-65b27c1e245d
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  1.572265625
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.012485931627452374, 0.022433647885918617, 0.000621320097707212, 0.002084630774334073, 0.01057477854192257, 0.0022567473351955414, 0.00109660136513412, 0.0005243646446615458]
Noise Multiplier after list and tensor:  0.0065097527840407565
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 02:03:34:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:37:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:37:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 09557a72-91e1-4479-afd8-7fb8afe3f97a
01/30/2025 02:37:22:INFO:Received: evaluate message 09557a72-91e1-4479-afd8-7fb8afe3f97a
[92mINFO [0m:      Sent reply
01/30/2025 02:41:32:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 02:42:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 02:42:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message cbeaa243-e6c6-4703-a5d6-7485c85d417a
01/30/2025 02:42:27:INFO:Received: reconnect message cbeaa243-e6c6-4703-a5d6-7485c85d417a
01/30/2025 02:42:27:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 02:42:27:INFO:Disconnect and shut down
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047, 1.3677138108857638], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804, 0.6331051147805075], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232, 0.8855101957814356]}



Final client history:
{'loss': [1.6011118934464657, 1.4168153402548953, 1.373100141772233, 1.3257163697512362, 1.3472420066487054, 1.336053108549675, 1.3530915468583125, 1.3881158032726484, 1.3297041001408223, 1.3433489405335892, 1.3397422970613548, 1.327443077350114, 1.3340952949470155, 1.326012600466843, 1.3376552316626857, 1.3216666301027498, 1.330597258723171, 1.3323779528149664, 1.3456170717123004, 1.3247061288505613, 1.315439360664105, 1.3122858208751562, 1.325458560314478, 1.324055685038726, 1.3005003080798214, 1.3216871776457904, 1.3183046643214509, 1.3208395782805622, 1.3373274654330047, 1.3677138108857638], 'accuracy': [0.517921868707209, 0.5819573097060008, 0.5932339911397503, 0.6041079339508659, 0.6089407974224729, 0.612565445026178, 0.616995569875151, 0.6117599677809102, 0.6274667740636327, 0.6270640354409988, 0.6314941602899719, 0.635118807893677, 0.635118807893677, 0.63954893274265, 0.6427708417237213, 0.6407571486105518, 0.6423681031010874, 0.6371325010068466, 0.6347160692710431, 0.6375352396294804, 0.6375352396294804, 0.643979057591623, 0.6451872734595248, 0.6463954893274265, 0.6467982279500604, 0.6403544099879178, 0.6419653644784535, 0.6407571486105518, 0.6375352396294804, 0.6331051147805075], 'auc': [0.7610659482721037, 0.8125282828523395, 0.8356868932087593, 0.8494549895244254, 0.8544909432987458, 0.8580318952141566, 0.8611662296523082, 0.863505981713706, 0.8684735839465461, 0.869692124484233, 0.8724331412688939, 0.8753511181517042, 0.875049363925323, 0.8780695008552948, 0.8766051387029271, 0.8792974983424561, 0.8788688797283504, 0.8805941185858581, 0.8793369839365954, 0.8812260189988629, 0.880845415027288, 0.8826837031868663, 0.8829162536768926, 0.8828113795678458, 0.8854459054131241, 0.8859143320911311, 0.8873866247484289, 0.8882960397149686, 0.8878015174350232, 0.8855101957814356]}

