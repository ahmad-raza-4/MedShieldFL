nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/28/2025 21:55:10:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/28/2025 21:55:10:DEBUG:ChannelConnectivity.IDLE
01/28/2025 21:55:10:DEBUG:ChannelConnectivity.CONNECTING
01/28/2025 21:55:10:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/28/2025 22:01:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:01:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a5b5b5d1-c2a7-448a-8480-12b767fec30f
01/28/2025 22:01:36:INFO:Received: train message a5b5b5d1-c2a7-448a-8480-12b767fec30f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 22:05:26:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:21:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:21:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d58e468c-c503-4827-8ca8-82e7168e12a8
01/28/2025 22:21:48:INFO:Received: evaluate message d58e468c-c503-4827-8ca8-82e7168e12a8
[92mINFO [0m:      Sent reply
01/28/2025 22:25:43:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:26:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:26:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f0df1c33-02a6-4ba9-9cb2-f3e850131dbd
01/28/2025 22:26:24:INFO:Received: train message f0df1c33-02a6-4ba9-9cb2-f3e850131dbd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 22:30:27:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:48:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:48:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c93bb0fc-2e98-44f6-b286-3843024d473f
01/28/2025 22:48:19:INFO:Received: evaluate message c93bb0fc-2e98-44f6-b286-3843024d473f
[92mINFO [0m:      Sent reply
01/28/2025 22:52:12:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:52:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:52:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 378d3cea-f2ea-4cf4-bf45-de7344e8a113
01/28/2025 22:52:55:INFO:Received: train message 378d3cea-f2ea-4cf4-bf45-de7344e8a113
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 22:56:56:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:13:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:13:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 41a3095c-6fb5-4958-874a-dc724301c094
01/28/2025 23:13:22:INFO:Received: evaluate message 41a3095c-6fb5-4958-874a-dc724301c094
[92mINFO [0m:      Sent reply
01/28/2025 23:17:25:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:17:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:17:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6dbefd78-5abe-4146-9218-92af3891fc08
01/28/2025 23:17:57:INFO:Received: train message 6dbefd78-5abe-4146-9218-92af3891fc08
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 23:22:06:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:38:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:38:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1a7d94d0-5f26-4c21-86f2-00744c65d866
01/28/2025 23:38:26:INFO:Received: evaluate message 1a7d94d0-5f26-4c21-86f2-00744c65d866
[92mINFO [0m:      Sent reply
01/28/2025 23:41:46:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:43:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:43:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6fdb8da6-7d16-4d52-bb4d-fd3ad4ca4fc1
01/28/2025 23:43:05:INFO:Received: train message 6fdb8da6-7d16-4d52-bb4d-fd3ad4ca4fc1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 23:47:09:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:04:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:04:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6f101137-ae24-4044-a600-31dfd30443fd
01/29/2025 00:04:48:INFO:Received: evaluate message 6f101137-ae24-4044-a600-31dfd30443fd
[92mINFO [0m:      Sent reply
01/29/2025 00:08:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:09:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:09:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 018c6097-2ace-4ba8-9364-5fc49667da92
01/29/2025 00:09:34:INFO:Received: train message 018c6097-2ace-4ba8-9364-5fc49667da92
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 00:13:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:31:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:31:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b67e5939-4fff-4fb3-a4dc-c0c47623fc5b
01/29/2025 00:31:46:INFO:Received: evaluate message b67e5939-4fff-4fb3-a4dc-c0c47623fc5b
[92mINFO [0m:      Sent reply
01/29/2025 00:35:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:36:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:36:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e8c3746b-e93a-48df-a9da-6d5453b3aac6
01/29/2025 00:36:37:INFO:Received: train message e8c3746b-e93a-48df-a9da-6d5453b3aac6
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007], 'accuracy': [0.5823600483286347], 'auc': [0.844982871338986]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152], 'accuracy': [0.5823600483286347, 0.6182037857430528], 'auc': [0.844982871338986, 0.8747687441438666]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 00:40:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:58:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:58:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7f3f7ccd-7cb6-4dd5-ab3e-6bed53a63fd2
01/29/2025 00:58:21:INFO:Received: evaluate message 7f3f7ccd-7cb6-4dd5-ab3e-6bed53a63fd2
[92mINFO [0m:      Sent reply
01/29/2025 01:02:18:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:03:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:03:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e80aff34-b76c-4d30-aa8b-3d541706ef39
01/29/2025 01:03:03:INFO:Received: train message e80aff34-b76c-4d30-aa8b-3d541706ef39
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 01:07:33:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:28:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:28:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8ad484d2-33ad-4823-b1f8-14f29349490d
01/29/2025 01:28:53:INFO:Received: evaluate message 8ad484d2-33ad-4823-b1f8-14f29349490d
[92mINFO [0m:      Sent reply
01/29/2025 01:32:54:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:33:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:33:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a1520bf4-0207-40f7-8d96-00383c2e40b6
01/29/2025 01:33:34:INFO:Received: train message a1520bf4-0207-40f7-8d96-00383c2e40b6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 01:37:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:00:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:00:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 13db7cc9-55d1-4e7a-ab76-bc8d0ec7f3e9
01/29/2025 02:00:49:INFO:Received: evaluate message 13db7cc9-55d1-4e7a-ab76-bc8d0ec7f3e9
[92mINFO [0m:      Sent reply
01/29/2025 02:04:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:05:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:05:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e3abc0ef-f874-4ab0-b25c-7e68eff0a365
01/29/2025 02:05:28:INFO:Received: train message e3abc0ef-f874-4ab0-b25c-7e68eff0a365
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 02:09:36:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:29:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:29:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d5ab80fb-186e-44af-a4de-1cea68dc8be7
01/29/2025 02:29:34:INFO:Received: evaluate message d5ab80fb-186e-44af-a4de-1cea68dc8be7
[92mINFO [0m:      Sent reply
01/29/2025 02:33:28:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:34:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:34:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ed13b3ad-ee42-4f0f-856a-fc6012709104
01/29/2025 02:34:04:INFO:Received: train message ed13b3ad-ee42-4f0f-856a-fc6012709104
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 02:38:13:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:01:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:01:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bc826572-2771-4660-a903-467c25e42b27
01/29/2025 03:01:29:INFO:Received: evaluate message bc826572-2771-4660-a903-467c25e42b27
[92mINFO [0m:      Sent reply
01/29/2025 03:05:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:05:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:05:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0ced0eca-58c9-469b-a8fb-e36f7e1a1f6b
01/29/2025 03:05:54:INFO:Received: train message 0ced0eca-58c9-469b-a8fb-e36f7e1a1f6b
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 03:09:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:26:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:26:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a0fbbc37-ae35-4811-8b07-e14b7fcd38ea
01/29/2025 03:26:57:INFO:Received: evaluate message a0fbbc37-ae35-4811-8b07-e14b7fcd38ea
[92mINFO [0m:      Sent reply
01/29/2025 03:30:47:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:31:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:31:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1cc23a90-effd-4651-ac20-19201f849151
01/29/2025 03:31:15:INFO:Received: train message 1cc23a90-effd-4651-ac20-19201f849151
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 03:35:01:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:55:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:55:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dd50bd96-0768-40ca-a7b4-9fd77a3332c7
01/29/2025 03:55:19:INFO:Received: evaluate message dd50bd96-0768-40ca-a7b4-9fd77a3332c7
[92mINFO [0m:      Sent reply
01/29/2025 03:59:21:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:59:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:59:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 778609cb-eec5-4597-9f50-a03b4ad0643b
01/29/2025 03:59:52:INFO:Received: train message 778609cb-eec5-4597-9f50-a03b4ad0643b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 04:04:01:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:20:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:20:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d3ba2d36-1853-4579-b44f-a0f9360f0175
01/29/2025 04:20:21:INFO:Received: evaluate message d3ba2d36-1853-4579-b44f-a0f9360f0175
[92mINFO [0m:      Sent reply
01/29/2025 04:24:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:24:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:24:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2e774f32-849e-4700-81c9-c387d9eade20
01/29/2025 04:24:53:INFO:Received: train message 2e774f32-849e-4700-81c9-c387d9eade20
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 04:28:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:47:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:47:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 81227efe-5579-40c2-b743-72bf91b17d91
01/29/2025 04:47:41:INFO:Received: evaluate message 81227efe-5579-40c2-b743-72bf91b17d91
[92mINFO [0m:      Sent reply
01/29/2025 04:51:37:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:52:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:52:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 86642b1d-6972-429b-b0d8-384fd17a022c
01/29/2025 04:52:08:INFO:Received: train message 86642b1d-6972-429b-b0d8-384fd17a022c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 04:56:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:13:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:13:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 71476382-adc1-4e54-b778-a832ed6c55c5
01/29/2025 05:13:13:INFO:Received: evaluate message 71476382-adc1-4e54-b778-a832ed6c55c5
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 05:17:03:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:17:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:17:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fe1633bf-e3ce-4831-b41b-0b8c7c1aa1d5
01/29/2025 05:17:28:INFO:Received: train message fe1633bf-e3ce-4831-b41b-0b8c7c1aa1d5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 05:21:35:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:40:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:40:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8ef5eb2e-38ec-41c2-8d8a-e0e191a9cb5c
01/29/2025 05:40:25:INFO:Received: evaluate message 8ef5eb2e-38ec-41c2-8d8a-e0e191a9cb5c
[92mINFO [0m:      Sent reply
01/29/2025 05:44:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:44:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:44:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d88b4bfc-9a6f-4f78-a5d3-4fa9a722d6ba
01/29/2025 05:44:50:INFO:Received: train message d88b4bfc-9a6f-4f78-a5d3-4fa9a722d6ba
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 05:48:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 06:10:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 06:10:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a0954ffa-f246-4bc7-9a70-46a5c0f18244
01/29/2025 06:10:12:INFO:Received: evaluate message a0954ffa-f246-4bc7-9a70-46a5c0f18244
[92mINFO [0m:      Sent reply
01/29/2025 06:15:46:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 06:16:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 06:16:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 25c6aaf9-11ad-4d46-be52-2e216ce9276a
01/29/2025 06:16:04:INFO:Received: train message 25c6aaf9-11ad-4d46-be52-2e216ce9276a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 06:21:33:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:01:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:01:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 923e982f-0de8-4ba2-87c1-8c8d64fc37f9
01/29/2025 07:01:59:INFO:Received: evaluate message 923e982f-0de8-4ba2-87c1-8c8d64fc37f9
[92mINFO [0m:      Sent reply
01/29/2025 07:07:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:08:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:08:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ba85256e-c6bf-440b-9203-1276e849c516
01/29/2025 07:08:23:INFO:Received: train message ba85256e-c6bf-440b-9203-1276e849c516
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 07:14:08:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:57:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:57:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ed3612c4-2427-48a6-80a5-aa3319d9e47a
01/29/2025 07:57:23:INFO:Received: evaluate message ed3612c4-2427-48a6-80a5-aa3319d9e47a

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826, 1.3427717304978695], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846, 0.6512283527990335], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434, 0.9055308040441616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
[92mINFO [0m:      Sent reply
01/29/2025 08:03:31:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:03:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:03:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8ab6d1c1-acb7-48fe-a33c-8b848022699c
01/29/2025 08:03:58:INFO:Received: train message 8ab6d1c1-acb7-48fe-a33c-8b848022699c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:09:36:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:53:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:53:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 73b3f20f-06e9-4558-ba18-e9fab999cbd0
01/29/2025 08:53:51:INFO:Received: evaluate message 73b3f20f-06e9-4558-ba18-e9fab999cbd0
[92mINFO [0m:      Sent reply
01/29/2025 09:00:35:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:01:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:01:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6786a804-e271-41bc-8ce3-431845a12f38
01/29/2025 09:01:47:INFO:Received: train message 6786a804-e271-41bc-8ce3-431845a12f38
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:07:42:INFO:Sent reply
01/29/2025 09:26:16:DEBUG:ChannelConnectivity.IDLE
01/29/2025 09:26:17:DEBUG:gRPC channel closed
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826, 1.3427717304978695, 1.3111424360887878], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846, 0.6512283527990335, 0.6556584776480064], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434, 0.9055308040441616, 0.90569082690568]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826, 1.3427717304978695, 1.3111424360887878, 1.3010476263741837], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846, 0.6512283527990335, 0.6556584776480064, 0.6536447845348369], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434, 0.9055308040441616, 0.90569082690568, 0.9057821824917216]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002898256527259946, 0.005200020503252745, 0.00014673771511297673, 0.00048026995500549674, 0.002456895774230361, 0.0005316861206665635, 0.00025161559460684657, 0.00012140608305344358]
Noise Multiplier after list and tensor:  0.0015108610341485473
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Traceback (most recent call last):
  File "client_5.py", line 380, in <module>
    fl.client.start_client(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 291, in start_client
    _start_client_internal(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 453, in _start_client_internal
    message = receive()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/grpc_client/connection.py", line 138, in receive
    proto = next(server_message_iterator)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/grpc/_channel.py", line 543, in __next__
    return self._next()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/grpc/_channel.py", line 969, in _next
    raise self
grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:
	status = StatusCode.UNAVAILABLE
	details = "ping timeout"
	debug_error_string = "UNKNOWN:Error received from peer ipv4:127.0.0.1:8052 {created_time:"2025-01-29T09:26:16.973282067-08:00", grpc_status:14, grpc_message:"ping timeout"}"
>
nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/30/2025 04:36:34:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/30/2025 04:36:34:DEBUG:ChannelConnectivity.IDLE
01/30/2025 04:36:34:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/30/2025 04:45:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:45:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8fe6157c-6f63-4373-8ee8-b6578a93cb23
01/30/2025 04:45:38:INFO:Received: train message 8fe6157c-6f63-4373-8ee8-b6578a93cb23
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 04:50:13:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:17:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:17:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3edaf72c-c834-406f-8c9d-e4c2ab3006f8
01/30/2025 05:17:47:INFO:Received: evaluate message 3edaf72c-c834-406f-8c9d-e4c2ab3006f8
[92mINFO [0m:      Sent reply
01/30/2025 05:22:19:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:22:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:22:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d821d5ad-f6e9-43ab-bd00-925372262ec7
01/30/2025 05:22:50:INFO:Received: train message d821d5ad-f6e9-43ab-bd00-925372262ec7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:27:20:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:04:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:04:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message efd41f55-f64e-479b-9571-6ace0ba1bcd9
01/30/2025 06:04:53:INFO:Received: evaluate message efd41f55-f64e-479b-9571-6ace0ba1bcd9
[92mINFO [0m:      Sent reply
01/30/2025 06:09:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:10:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:10:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d258cc98-069a-41fc-9978-e3c975bbc504
01/30/2025 06:10:27:INFO:Received: train message d258cc98-069a-41fc-9978-e3c975bbc504
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 06:16:39:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:50:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:50:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message db0f1704-4232-42cb-a8d9-aea3f1f0f3bd
01/30/2025 06:50:46:INFO:Received: evaluate message db0f1704-4232-42cb-a8d9-aea3f1f0f3bd
[92mINFO [0m:      Sent reply
01/30/2025 06:55:40:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:56:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:56:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message fca8a7b8-d0e1-40a2-a32c-ef15baf16d06
01/30/2025 06:56:16:INFO:Received: train message fca8a7b8-d0e1-40a2-a32c-ef15baf16d06
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 07:00:57:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:29:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:29:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5540db6-9b72-4783-8325-fb1236638d29
01/30/2025 07:29:01:INFO:Received: evaluate message b5540db6-9b72-4783-8325-fb1236638d29
[92mINFO [0m:      Sent reply
01/30/2025 07:33:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:34:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:34:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e1e6c415-1ea3-4080-a23f-43c5b3bd96bf
01/30/2025 07:34:50:INFO:Received: train message e1e6c415-1ea3-4080-a23f-43c5b3bd96bf
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 07:39:16:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 08:18:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 08:18:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b7672d8f-d4c3-4d33-b203-46a141f20373
01/30/2025 08:18:01:INFO:Received: evaluate message b7672d8f-d4c3-4d33-b203-46a141f20373
[92mINFO [0m:      Sent reply
01/30/2025 08:21:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 08:22:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 08:22:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 12a8656a-4f6b-46ca-a9ab-8bbad91f6204
01/30/2025 08:22:16:INFO:Received: reconnect message 12a8656a-4f6b-46ca-a9ab-8bbad91f6204
01/30/2025 08:22:17:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 08:22:17:INFO:Disconnect and shut down
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002891905140131712, 0.005248260218650103, 0.0001428904797649011, 0.00048751963186077774, 0.0024544422049075365, 0.0005293713184073567, 0.00026577201788313687, 0.00011791951692430303]
Noise Multiplier after list and tensor:  0.0015172600660662283
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644], 'accuracy': [0.6741844542891663], 'auc': [0.9136487085523237]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002891905140131712, 0.005248260218650103, 0.0001428904797649011, 0.00048751963186077774, 0.0024544422049075365, 0.0005293713184073567, 0.00026577201788313687, 0.00011791951692430303]
Noise Multiplier after list and tensor:  0.0015172600660662283
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512], 'accuracy': [0.6741844542891663, 0.6677406363270237], 'auc': [0.9136487085523237, 0.9122972121312665]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002891905140131712, 0.005248260218650103, 0.0001428904797649011, 0.00048751963186077774, 0.0024544422049075365, 0.0005293713184073567, 0.00026577201788313687, 0.00011791951692430303]
Noise Multiplier after list and tensor:  0.0015172600660662283
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002891905140131712, 0.005248260218650103, 0.0001428904797649011, 0.00048751963186077774, 0.0024544422049075365, 0.0005293713184073567, 0.00026577201788313687, 0.00011791951692430303]
Noise Multiplier after list and tensor:  0.0015172600660662283
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.365447998046875
Data Scaling Factor: 0.035220734527074256 where Client Data Size: 655
Noise Multiplier after Fisher Scaling:  [0.002891905140131712, 0.005248260218650103, 0.0001428904797649011, 0.00048751963186077774, 0.0024544422049075365, 0.0005293713184073567, 0.00026577201788313687, 0.00011791951692430303]
Noise Multiplier after list and tensor:  0.0015172600660662283
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 655, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765, 1.4033728663449896], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438, 0.6524365686669351], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285, 0.9103161857948601]}



Final client history:
{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765, 1.4033728663449896], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438, 0.6524365686669351], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285, 0.9103161857948601]}

