nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/28/2025 21:58:15:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/28/2025 21:58:15:DEBUG:ChannelConnectivity.IDLE
01/28/2025 21:58:15:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/28/2025 22:01:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:01:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message db8c65cf-5c20-4d48-bc4d-14c3f37a175a
01/28/2025 22:01:36:INFO:Received: train message db8c65cf-5c20-4d48-bc4d-14c3f37a175a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 22:13:54:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:21:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:21:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b2dfc49b-f6f4-4ef2-8f7c-5318b816607a
01/28/2025 22:21:58:INFO:Received: evaluate message b2dfc49b-f6f4-4ef2-8f7c-5318b816607a
[92mINFO [0m:      Sent reply
01/28/2025 22:25:54:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:26:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:26:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 30e85b6d-a357-44fc-9bd7-2bf7adf5b51c
01/28/2025 22:26:27:INFO:Received: train message 30e85b6d-a357-44fc-9bd7-2bf7adf5b51c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 22:38:55:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:48:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:48:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 62584aa2-7746-428c-8ced-9ae3d07fe85c
01/28/2025 22:48:31:INFO:Received: evaluate message 62584aa2-7746-428c-8ced-9ae3d07fe85c
[92mINFO [0m:      Sent reply
01/28/2025 22:52:24:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 22:52:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 22:52:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ca388911-be56-4687-8d61-35c042bb007a
01/28/2025 22:52:39:INFO:Received: train message ca388911-be56-4687-8d61-35c042bb007a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 23:05:06:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:13:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:13:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c97802c1-5ec3-4646-a6c7-3dbd57ea92a8
01/28/2025 23:13:15:INFO:Received: evaluate message c97802c1-5ec3-4646-a6c7-3dbd57ea92a8
[92mINFO [0m:      Sent reply
01/28/2025 23:17:04:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:17:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:17:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7d6cdf98-5de8-4041-af55-f835fbde7ae7
01/28/2025 23:17:45:INFO:Received: train message 7d6cdf98-5de8-4041-af55-f835fbde7ae7
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 23:30:35:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:38:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:38:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d84d447c-a96a-44e0-9640-c1f4490821e9
01/28/2025 23:38:40:INFO:Received: evaluate message d84d447c-a96a-44e0-9640-c1f4490821e9
[92mINFO [0m:      Sent reply
01/28/2025 23:42:35:INFO:Sent reply
[92mINFO [0m:      
01/28/2025 23:43:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/28/2025 23:43:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b78697e7-49b0-42ff-9fe0-b04cd52a5ccc
01/28/2025 23:43:02:INFO:Received: train message b78697e7-49b0-42ff-9fe0-b04cd52a5ccc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/28/2025 23:56:06:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:04:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:04:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1f4947de-9fd1-4d3a-a117-67125932b3d0
01/29/2025 00:04:59:INFO:Received: evaluate message 1f4947de-9fd1-4d3a-a117-67125932b3d0
[92mINFO [0m:      Sent reply
01/29/2025 00:08:59:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:09:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:09:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3575520e-cb85-43d0-89b0-f38f4d0c1ec9
01/29/2025 00:09:35:INFO:Received: train message 3575520e-cb85-43d0-89b0-f38f4d0c1ec9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 00:22:58:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:31:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:31:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2c9bfe66-d142-4ac6-9710-f03047c8d025
01/29/2025 00:31:58:INFO:Received: evaluate message 2c9bfe66-d142-4ac6-9710-f03047c8d025
[92mINFO [0m:      Sent reply
01/29/2025 00:36:00:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:36:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:36:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5094c953-91dd-4eea-aea3-1be2354b7bf6
01/29/2025 00:36:30:INFO:Received: train message 5094c953-91dd-4eea-aea3-1be2354b7bf6
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007], 'accuracy': [0.5823600483286347], 'auc': [0.844982871338986]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152], 'accuracy': [0.5823600483286347, 0.6182037857430528], 'auc': [0.844982871338986, 0.8747687441438666]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 00:49:22:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 00:58:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 00:58:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3ddd9337-3548-4d72-a69c-150958f905b7
01/29/2025 00:58:26:INFO:Received: evaluate message 3ddd9337-3548-4d72-a69c-150958f905b7
[92mINFO [0m:      Sent reply
01/29/2025 01:02:20:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:02:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:02:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e5126924-72f7-45dd-8c28-9c60512a3d34
01/29/2025 01:02:34:INFO:Received: train message e5126924-72f7-45dd-8c28-9c60512a3d34
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 01:16:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:28:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:28:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6b7a3bc1-10c2-40ac-b2be-fb703536a2dd
01/29/2025 01:28:54:INFO:Received: evaluate message 6b7a3bc1-10c2-40ac-b2be-fb703536a2dd
[92mINFO [0m:      Sent reply
01/29/2025 01:32:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 01:33:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 01:33:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d8b8d264-7848-4668-8bad-7b81d7665670
01/29/2025 01:33:23:INFO:Received: train message d8b8d264-7848-4668-8bad-7b81d7665670
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 01:47:53:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:00:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:00:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d0be347d-b8dd-4677-9bc5-d20d760e35a5
01/29/2025 02:00:52:INFO:Received: evaluate message d0be347d-b8dd-4677-9bc5-d20d760e35a5
[92mINFO [0m:      Sent reply
01/29/2025 02:04:50:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:05:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:05:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e4598f58-0ddc-4599-92ca-85c72f4db5b8
01/29/2025 02:05:33:INFO:Received: train message e4598f58-0ddc-4599-92ca-85c72f4db5b8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 02:19:41:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:29:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:29:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f854653f-7311-4408-aac1-4eb983b66f00
01/29/2025 02:29:47:INFO:Received: evaluate message f854653f-7311-4408-aac1-4eb983b66f00
[92mINFO [0m:      Sent reply
01/29/2025 02:33:38:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 02:34:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 02:34:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 92260b1f-1ec6-4bb1-89c3-3f19e60ca93c
01/29/2025 02:34:16:INFO:Received: train message 92260b1f-1ec6-4bb1-89c3-3f19e60ca93c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 02:47:42:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:01:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:01:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f287ef58-42ce-42aa-9279-94d14827b528
01/29/2025 03:01:15:INFO:Received: evaluate message f287ef58-42ce-42aa-9279-94d14827b528
[92mINFO [0m:      Sent reply
01/29/2025 03:04:23:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:05:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:05:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5a204a02-6588-47c6-98f6-096885b6a627
01/29/2025 03:05:41:INFO:Received: train message 5a204a02-6588-47c6-98f6-096885b6a627
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 03:18:23:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:26:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:26:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 76506e33-f90f-45bd-82d8-bc8e1d0e493d
01/29/2025 03:26:59:INFO:Received: evaluate message 76506e33-f90f-45bd-82d8-bc8e1d0e493d
[92mINFO [0m:      Sent reply
01/29/2025 03:30:48:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:31:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:31:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c20a880c-ed80-43dd-a6f8-fa7bbe66824b
01/29/2025 03:31:20:INFO:Received: train message c20a880c-ed80-43dd-a6f8-fa7bbe66824b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 03:44:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:55:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:55:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 69ad3eb8-4f47-4b90-bef5-5debc628cc3c
01/29/2025 03:55:25:INFO:Received: evaluate message 69ad3eb8-4f47-4b90-bef5-5debc628cc3c
[92mINFO [0m:      Sent reply
01/29/2025 03:59:25:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 03:59:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 03:59:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b2ec2740-dd84-49bb-a076-4d776c36d73d
01/29/2025 03:59:58:INFO:Received: train message b2ec2740-dd84-49bb-a076-4d776c36d73d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 04:12:34:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:20:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:20:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f1c0dc4c-1d18-4713-b581-f5ea1236b6fe
01/29/2025 04:20:24:INFO:Received: evaluate message f1c0dc4c-1d18-4713-b581-f5ea1236b6fe
[92mINFO [0m:      Sent reply
01/29/2025 04:24:21:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:24:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:24:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c85f4fe9-1271-4fd6-96cd-ab17931c375d
01/29/2025 04:24:42:INFO:Received: train message c85f4fe9-1271-4fd6-96cd-ab17931c375d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 04:37:28:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:47:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:47:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0e31786e-3950-4d8e-82f3-cb60491960ed
01/29/2025 04:47:38:INFO:Received: evaluate message 0e31786e-3950-4d8e-82f3-cb60491960ed
[92mINFO [0m:      Sent reply
01/29/2025 04:51:34:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 04:52:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 04:52:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ff34253e-73b2-4b76-b349-a4d9e5367908
01/29/2025 04:52:08:INFO:Received: train message ff34253e-73b2-4b76-b349-a4d9e5367908
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 05:04:43:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:13:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:13:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3739c4ef-56e6-4631-bb85-b3f114d13689
01/29/2025 05:13:07:INFO:Received: evaluate message 3739c4ef-56e6-4631-bb85-b3f114d13689
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 05:16:55:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:17:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:17:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3dc91900-31d1-44cf-a979-c745248ab5b6
01/29/2025 05:17:30:INFO:Received: train message 3dc91900-31d1-44cf-a979-c745248ab5b6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 05:30:24:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:40:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:40:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f3710307-f10f-4946-b5cc-8777a3a3fd2a
01/29/2025 05:40:26:INFO:Received: evaluate message f3710307-f10f-4946-b5cc-8777a3a3fd2a
[92mINFO [0m:      Sent reply
01/29/2025 05:44:19:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 05:44:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 05:44:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 618de6b8-f7cd-441e-8e9c-c617a41ed49f
01/29/2025 05:44:34:INFO:Received: train message 618de6b8-f7cd-441e-8e9c-c617a41ed49f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 05:57:04:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 06:10:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 06:10:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 912b41e3-0156-4c43-b9e4-d393e6470078
01/29/2025 06:10:05:INFO:Received: evaluate message 912b41e3-0156-4c43-b9e4-d393e6470078
[92mINFO [0m:      Sent reply
01/29/2025 06:15:34:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 06:16:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 06:16:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message da6f6725-b26c-4bde-90e6-9002598de433
01/29/2025 06:16:19:INFO:Received: train message da6f6725-b26c-4bde-90e6-9002598de433
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 06:36:16:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:02:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:02:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f2adb1f4-c222-4590-ae06-c6b8d3a37ad3
01/29/2025 07:02:09:INFO:Received: evaluate message f2adb1f4-c222-4590-ae06-c6b8d3a37ad3
[92mINFO [0m:      Sent reply
01/29/2025 07:07:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:08:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:08:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4600f2ff-39bb-4eee-9606-3b9b7e066a25
01/29/2025 07:08:29:INFO:Received: train message 4600f2ff-39bb-4eee-9606-3b9b7e066a25
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 07:30:39:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 07:57:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 07:57:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e4b8c302-2933-4a47-9dc4-ea29ee8ff6e6
01/29/2025 07:57:20:INFO:Received: evaluate message e4b8c302-2933-4a47-9dc4-ea29ee8ff6e6

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826, 1.3427717304978695], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846, 0.6512283527990335], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434, 0.9055308040441616]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/29/2025 08:03:33:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:04:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:04:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 29cc1fb1-9611-48cf-858d-cab0a8e17b62
01/29/2025 08:04:09:INFO:Received: train message 29cc1fb1-9611-48cf-858d-cab0a8e17b62
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 08:24:52:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 08:53:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 08:53:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0a065092-e17f-4762-bc48-dc5c17dec62f
01/29/2025 08:53:53:INFO:Received: evaluate message 0a065092-e17f-4762-bc48-dc5c17dec62f
[92mINFO [0m:      Sent reply
01/29/2025 09:00:51:INFO:Sent reply
[92mINFO [0m:      
01/29/2025 09:01:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/29/2025 09:01:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message abd96579-d586-4042-92a6-0bcb1f676bbe
01/29/2025 09:01:35:INFO:Received: train message abd96579-d586-4042-92a6-0bcb1f676bbe
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/29/2025 09:24:31:INFO:Sent reply
01/29/2025 09:25:37:DEBUG:ChannelConnectivity.IDLE
01/29/2025 09:25:37:DEBUG:gRPC channel closed

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826, 1.3427717304978695, 1.3111424360887878], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846, 0.6512283527990335, 0.6556584776480064], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434, 0.9055308040441616, 0.90569082690568]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3608992811753007, 1.300519117606152, 1.323021809324064, 1.293262200469794, 1.3604948223957456, 1.2889031325191247, 1.3200314006544245, 1.2770398001108159, 1.2908978383722585, 1.2427704440601592, 1.276568870653897, 1.3120317486952353, 1.332838527603203, 1.3191274831096251, 1.2966736374341141, 1.3040719327070134, 1.2811004871857343, 1.2987692734189826, 1.3427717304978695, 1.3111424360887878, 1.3010476263741837], 'accuracy': [0.5823600483286347, 0.6182037857430528, 0.6258558195730971, 0.6335078534031413, 0.6190092629883206, 0.6435763189689891, 0.6367297623842126, 0.6431735803463552, 0.6447845348368909, 0.6524365686669351, 0.6512283527990335, 0.6492146596858639, 0.6467982279500604, 0.6528393072895691, 0.6552557390253725, 0.6548530004027386, 0.6616995569875151, 0.6637132501006846, 0.6512283527990335, 0.6556584776480064, 0.6536447845348369], 'auc': [0.844982871338986, 0.8747687441438666, 0.8822868957877615, 0.8884132531178145, 0.8880229386231708, 0.8933845703584757, 0.8954235444531713, 0.8978449247552104, 0.8982861041298316, 0.9014050885944129, 0.9009762165631289, 0.9017726276536048, 0.9018897649235087, 0.9031000303438206, 0.9038280374187236, 0.904878804297358, 0.90543062207902, 0.9070129518690434, 0.9055308040441616, 0.90569082690568, 0.9057821824917216]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031638245563954115, 0.0471484512090683, 0.0008216062560677528, 0.0009917294373735785, 0.0024220412597060204, 0.0009445092873647809, 0.001171750482171774, 0.002414756454527378]
Noise Multiplier after list and tensor:  0.007384833617834374
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Traceback (most recent call last):
  File "client_2.py", line 380, in <module>
    fl.client.start_client(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 291, in start_client
    _start_client_internal(
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/app.py", line 453, in _start_client_internal
    message = receive()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/flwr/client/grpc_client/connection.py", line 138, in receive
    proto = next(server_message_iterator)
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/grpc/_channel.py", line 543, in __next__
    return self._next()
  File "/home/dgxuser16/.local/lib/python3.8/site-packages/grpc/_channel.py", line 969, in _next
    raise self
grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:
	status = StatusCode.UNAVAILABLE
	details = "ping timeout"
	debug_error_string = "UNKNOWN:Error received from peer ipv4:127.0.0.1:8052 {created_time:"2025-01-29T09:25:37.003502732-08:00", grpc_status:14, grpc_message:"ping timeout"}"
>
nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/30/2025 04:39:47:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/30/2025 04:39:47:DEBUG:ChannelConnectivity.IDLE
01/30/2025 04:39:47:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/30/2025 04:45:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 04:45:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2a6106c2-1a10-4e3f-a361-4a004ae81c2d
01/30/2025 04:45:41:INFO:Received: train message 2a6106c2-1a10-4e3f-a361-4a004ae81c2d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:02:48:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:17:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:17:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 24e00713-989b-4dcc-a672-79773ca19c1f
01/30/2025 05:17:33:INFO:Received: evaluate message 24e00713-989b-4dcc-a672-79773ca19c1f
[92mINFO [0m:      Sent reply
01/30/2025 05:21:57:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 05:23:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 05:23:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b7409a15-7b66-4efe-abd4-1e046a5154b1
01/30/2025 05:23:02:INFO:Received: train message b7409a15-7b66-4efe-abd4-1e046a5154b1
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 05:41:01:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:04:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:04:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a5c55860-de44-4974-899f-4ce1ad847221
01/30/2025 06:04:53:INFO:Received: evaluate message a5c55860-de44-4974-899f-4ce1ad847221
[92mINFO [0m:      Sent reply
01/30/2025 06:09:41:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:10:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:10:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3f4238f0-39cd-481c-ab15-439e5a312d76
01/30/2025 06:10:30:INFO:Received: train message 3f4238f0-39cd-481c-ab15-439e5a312d76
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 06:29:43:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:50:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:50:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 02bf2ce4-8cac-4a22-9ec2-9fab53a5838d
01/30/2025 06:50:43:INFO:Received: evaluate message 02bf2ce4-8cac-4a22-9ec2-9fab53a5838d
[92mINFO [0m:      Sent reply
01/30/2025 06:55:07:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 06:56:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 06:56:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6addb697-6edf-4403-87a6-f6f826722af2
01/30/2025 06:56:08:INFO:Received: train message 6addb697-6edf-4403-87a6-f6f826722af2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 07:10:40:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:29:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:29:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ef82912c-7cda-48d5-b72b-d95c55f09bac
01/30/2025 07:29:09:INFO:Received: evaluate message ef82912c-7cda-48d5-b72b-d95c55f09bac
[92mINFO [0m:      Sent reply
01/30/2025 07:32:40:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 07:34:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 07:34:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3b85bcf6-b23d-40c2-ac95-2660978c669e
01/30/2025 07:34:25:INFO:Received: train message 3b85bcf6-b23d-40c2-ac95-2660978c669e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 07:48:55:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 08:18:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 08:18:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 73ff9cbd-74c7-4a12-9be0-b18e04c40546
01/30/2025 08:18:15:INFO:Received: evaluate message 73ff9cbd-74c7-4a12-9be0-b18e04c40546
[92mINFO [0m:      Sent reply
01/30/2025 08:22:11:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 08:22:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 08:22:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message b15a036a-baf0-4b62-aff2-defd49a4bbc8
01/30/2025 08:22:16:INFO:Received: reconnect message b15a036a-baf0-4b62-aff2-defd49a4bbc8
01/30/2025 08:22:16:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/30/2025 08:22:16:INFO:Disconnect and shut down
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/adaptive_privacy/isic/version_03']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.003103615017607808, 0.04723050445318222, 0.000779548950958997, 0.0010140143567696214, 0.00241271173581481, 0.0009477138519287109, 0.0011859863298013806, 0.002531498670578003]
Noise Multiplier after list and tensor:  0.007400699170830194
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644], 'accuracy': [0.6741844542891663], 'auc': [0.9136487085523237]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.003103615017607808, 0.04723050445318222, 0.000779548950958997, 0.0010140143567696214, 0.00241271173581481, 0.0009477138519287109, 0.0011859863298013806, 0.002531498670578003]
Noise Multiplier after list and tensor:  0.007400699170830194
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512], 'accuracy': [0.6741844542891663, 0.6677406363270237], 'auc': [0.9136487085523237, 0.9122972121312665]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.003103615017607808, 0.04723050445318222, 0.000779548950958997, 0.0010140143567696214, 0.00241271173581481, 0.0009477138519287109, 0.0011859863298013806, 0.002531498670578003]
Noise Multiplier after list and tensor:  0.007400699170830194
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.003103615017607808, 0.04723050445318222, 0.000779548950958997, 0.0010140143567696214, 0.00241271173581481, 0.0009477138519287109, 0.0011859863298013806, 0.002531498670578003]
Noise Multiplier after list and tensor:  0.007400699170830194
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.003103615017607808, 0.04723050445318222, 0.000779548950958997, 0.0010140143567696214, 0.00241271173581481, 0.0009477138519287109, 0.0011859863298013806, 0.002531498670578003]
Noise Multiplier after list and tensor:  0.007400699170830194
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765, 1.4033728663449896], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438, 0.6524365686669351], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285, 0.9103161857948601]}



Final client history:
{'loss': [1.3041349914925644, 1.3200830717841512, 1.3802238330695114, 1.3488902609481765, 1.4033728663449896], 'accuracy': [0.6741844542891663, 0.6677406363270237, 0.6580749093838099, 0.6584776480064438, 0.6524365686669351], 'auc': [0.9136487085523237, 0.9122972121312665, 0.911279071094018, 0.9130985899073285, 0.9103161857948601]}

