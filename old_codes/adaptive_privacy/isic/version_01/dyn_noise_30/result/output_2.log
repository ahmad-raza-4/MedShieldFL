nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_30/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/18/2025 11:04:10:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/18/2025 11:04:10:DEBUG:ChannelConnectivity.IDLE
01/18/2025 11:04:10:DEBUG:ChannelConnectivity.CONNECTING
01/18/2025 11:04:10:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/18/2025 11:09:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:09:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2288f8c3-c033-42c3-b4a9-18d0ba4ea2c5
01/18/2025 11:09:10:INFO:Received: train message 2288f8c3-c033-42c3-b4a9-18d0ba4ea2c5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:23:03:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:37:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:37:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 867572ac-403c-4c4a-9259-a8ddc6b00198
01/18/2025 11:37:09:INFO:Received: evaluate message 867572ac-403c-4c4a-9259-a8ddc6b00198
[92mINFO [0m:      Sent reply
01/18/2025 11:41:23:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:41:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:41:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a3b69bb7-1ebb-4c45-ac3e-a990ed4caeee
01/18/2025 11:41:50:INFO:Received: train message a3b69bb7-1ebb-4c45-ac3e-a990ed4caeee
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:55:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:05:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:05:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b53307ad-5235-4509-9fe4-1cf1344b0fd0
01/18/2025 12:05:58:INFO:Received: evaluate message b53307ad-5235-4509-9fe4-1cf1344b0fd0
[92mINFO [0m:      Sent reply
01/18/2025 12:10:07:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:10:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:10:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 859f817b-7674-4c10-a24c-d1bea0e62e95
01/18/2025 12:10:30:INFO:Received: train message 859f817b-7674-4c10-a24c-d1bea0e62e95
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:24:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:33:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:33:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ca7f229a-fb1b-4496-b693-67672b1ad2f9
01/18/2025 12:33:12:INFO:Received: evaluate message ca7f229a-fb1b-4496-b693-67672b1ad2f9
[92mINFO [0m:      Sent reply
01/18/2025 12:37:40:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:38:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:38:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ff74b3a0-fb05-472b-b334-cb7d448e0c3d
01/18/2025 12:38:29:INFO:Received: train message ff74b3a0-fb05-472b-b334-cb7d448e0c3d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:53:26:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:02:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:02:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d88a06aa-8426-48eb-b50f-0c29957233b4
01/18/2025 13:02:11:INFO:Received: evaluate message d88a06aa-8426-48eb-b50f-0c29957233b4
[92mINFO [0m:      Sent reply
01/18/2025 13:06:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:07:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:07:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7c1f2a1f-cb63-4812-8c3a-2a221092572c
01/18/2025 13:07:26:INFO:Received: train message 7c1f2a1f-cb63-4812-8c3a-2a221092572c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:22:53:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:31:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:31:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a0e352a5-6bc9-41c0-b4db-9e7f037a3345
01/18/2025 13:31:25:INFO:Received: evaluate message a0e352a5-6bc9-41c0-b4db-9e7f037a3345
[92mINFO [0m:      Sent reply
01/18/2025 13:36:16:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:36:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:36:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2f1509fb-a83b-4f9e-a986-b4e150812b8b
01/18/2025 13:36:41:INFO:Received: train message 2f1509fb-a83b-4f9e-a986-b4e150812b8b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:50:40:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:59:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:59:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 06dda1e9-18f5-468f-a2b5-bd0e0712062e
01/18/2025 13:59:18:INFO:Received: evaluate message 06dda1e9-18f5-468f-a2b5-bd0e0712062e
[92mINFO [0m:      Sent reply
01/18/2025 14:03:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:04:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:04:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e6c2d5ff-3447-4cdb-9a02-99118b392280
01/18/2025 14:04:35:INFO:Received: train message e6c2d5ff-3447-4cdb-9a02-99118b392280
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:17:40:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:28:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:28:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d55c528a-7da2-44e2-960b-5fa48209b4ca
01/18/2025 14:28:17:INFO:Received: evaluate message d55c528a-7da2-44e2-960b-5fa48209b4ca
[92mINFO [0m:      Sent reply
01/18/2025 14:33:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:33:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:33:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 05d049ee-51f3-4e52-92cb-10cf4fb50620
01/18/2025 14:33:40:INFO:Received: train message 05d049ee-51f3-4e52-92cb-10cf4fb50620
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:46:32:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:57:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:57:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3af04e4f-6fb7-4dfe-a973-ef880c24439c
01/18/2025 14:57:15:INFO:Received: evaluate message 3af04e4f-6fb7-4dfe-a973-ef880c24439c
[92mINFO [0m:      Sent reply
01/18/2025 15:02:02:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:02:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:02:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b1e09a55-5768-4d02-8f05-d8bfaf845cce
01/18/2025 15:02:52:INFO:Received: train message b1e09a55-5768-4d02-8f05-d8bfaf845cce
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:15:51:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:28:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:28:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message cecac868-c8db-41f2-8719-59695f9dcc7b
01/18/2025 15:28:10:INFO:Received: evaluate message cecac868-c8db-41f2-8719-59695f9dcc7b
[92mINFO [0m:      Sent reply
01/18/2025 15:32:53:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:33:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:33:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0c6a27f7-b1a3-404a-a8f5-251b1169ee03
01/18/2025 15:33:36:INFO:Received: train message 0c6a27f7-b1a3-404a-a8f5-251b1169ee03
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:46:57:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:02:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:02:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c318fd55-c9e1-435a-878e-727e5f32759c
01/18/2025 16:02:24:INFO:Received: evaluate message c318fd55-c9e1-435a-878e-727e5f32759c
[92mINFO [0m:      Sent reply
01/18/2025 16:06:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:07:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:07:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 11afd113-7df8-4a7f-9568-55825784ae25
01/18/2025 16:07:07:INFO:Received: train message 11afd113-7df8-4a7f-9568-55825784ae25
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:20:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:34:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:34:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 50523c52-faa1-463e-a053-16dad6b8c105
01/18/2025 16:34:25:INFO:Received: evaluate message 50523c52-faa1-463e-a053-16dad6b8c105
[92mINFO [0m:      Sent reply
01/18/2025 16:38:37:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:39:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:39:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cc85e0d1-2b9f-40c6-84f5-ff325962ac0f
01/18/2025 16:39:10:INFO:Received: train message cc85e0d1-2b9f-40c6-84f5-ff325962ac0f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:54:03:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:04:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:04:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e7926df7-0956-4283-ad6e-e45a6c88e89c
01/18/2025 17:04:56:INFO:Received: evaluate message e7926df7-0956-4283-ad6e-e45a6c88e89c
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_30', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise_30']
BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535], 'accuracy': [0.3419250906161901], 'auc': [0.5460484530250033]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187], 'accuracy': [0.3419250906161901, 0.3403141361256545], 'auc': [0.5460484530250033, 0.5873079809432222]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00
[92mINFO [0m:      Sent reply
01/18/2025 17:09:15:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:09:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:09:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 77c4a97e-7558-4654-915c-ed8dfb5cfbab
01/18/2025 17:09:57:INFO:Received: train message 77c4a97e-7558-4654-915c-ed8dfb5cfbab
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:24:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:34:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:34:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f9e05950-73e7-4d66-843d-56b4e322a1f1
01/18/2025 17:34:23:INFO:Received: evaluate message f9e05950-73e7-4d66-843d-56b4e322a1f1
[92mINFO [0m:      Sent reply
01/18/2025 17:38:36:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:39:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:39:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6033ca10-5563-4e39-813f-25c16799e705
01/18/2025 17:39:37:INFO:Received: train message 6033ca10-5563-4e39-813f-25c16799e705
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:52:39:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:02:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:02:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 75000bf3-f341-414e-a875-b9f665e6fb0b
01/18/2025 18:02:51:INFO:Received: evaluate message 75000bf3-f341-414e-a875-b9f665e6fb0b
[92mINFO [0m:      Sent reply
01/18/2025 18:06:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:08:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:08:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 168381a9-b5fd-42b3-8999-ef436f25ca2a
01/18/2025 18:08:04:INFO:Received: train message 168381a9-b5fd-42b3-8999-ef436f25ca2a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:21:08:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:30:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:30:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 38703878-f0d3-4b29-9605-3244a89c80f9
01/18/2025 18:30:08:INFO:Received: evaluate message 38703878-f0d3-4b29-9605-3244a89c80f9
[92mINFO [0m:      Sent reply
01/18/2025 18:34:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:35:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:35:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 83c4b8a1-dbd1-4e04-9375-61e67af062b3
01/18/2025 18:35:17:INFO:Received: train message 83c4b8a1-dbd1-4e04-9375-61e67af062b3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:48:20:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:57:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:57:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6ee88914-ada7-428c-9fec-1b503e91defe
01/18/2025 18:57:24:INFO:Received: evaluate message 6ee88914-ada7-428c-9fec-1b503e91defe
[92mINFO [0m:      Sent reply
01/18/2025 19:01:29:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:02:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:02:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ec69d598-c7d0-449e-b97f-bea7a6a84a4b
01/18/2025 19:02:34:INFO:Received: train message ec69d598-c7d0-449e-b97f-bea7a6a84a4b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:15:43:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:25:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:25:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7df2f536-51bd-49b1-a79f-63989d190838
01/18/2025 19:25:20:INFO:Received: evaluate message 7df2f536-51bd-49b1-a79f-63989d190838
[92mINFO [0m:      Sent reply
01/18/2025 19:29:21:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:30:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:30:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d70fcc7a-e9dd-4468-aa57-6080c16f3374
01/18/2025 19:30:54:INFO:Received: train message d70fcc7a-e9dd-4468-aa57-6080c16f3374
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:44:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:53:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:53:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2423e42b-05d2-42ce-9ebb-57185bd67254
01/18/2025 19:53:20:INFO:Received: evaluate message 2423e42b-05d2-42ce-9ebb-57185bd67254
[92mINFO [0m:      Sent reply
01/18/2025 19:57:29:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:58:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:58:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ed10d024-a068-4578-92e1-7cd41e33969a
01/18/2025 19:58:59:INFO:Received: train message ed10d024-a068-4578-92e1-7cd41e33969a

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.2966459090821445
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.27686951514333485
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.25709312120452527
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.23731672726571562
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031]}

BaseNM 0.41748046875/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:12:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:21:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:21:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 575fa531-d95f-49bd-905f-ffa233daa7dc
01/18/2025 20:21:14:INFO:Received: evaluate message 575fa531-d95f-49bd-905f-ffa233daa7dc
[92mINFO [0m:      Sent reply
01/18/2025 20:25:41:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:26:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:26:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 53cdfcfc-907b-4ef6-abe7-f26480c6d677
01/18/2025 20:26:28:INFO:Received: train message 53cdfcfc-907b-4ef6-abe7-f26480c6d677
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:39:08:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:45:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:45:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1158e4eb-b57b-4166-a0a2-f95241f5e9a5
01/18/2025 20:45:34:INFO:Received: evaluate message 1158e4eb-b57b-4166-a0a2-f95241f5e9a5
[92mINFO [0m:      Sent reply
01/18/2025 20:49:26:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:49:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:49:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8148fd38-8fe5-4040-b4d8-de8d57022b05
01/18/2025 20:49:59:INFO:Received: train message 8148fd38-8fe5-4040-b4d8-de8d57022b05
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 21:02:52:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:09:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:09:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f3a9aacc-bcac-4993-8d0b-fe2a46b68cbb
01/18/2025 21:09:15:INFO:Received: evaluate message f3a9aacc-bcac-4993-8d0b-fe2a46b68cbb
[92mINFO [0m:      Sent reply
01/18/2025 21:13:23:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:14:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:14:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 49fe1367-be5f-4f92-8a20-2ee038554bf4
01/18/2025 21:14:01:INFO:Received: train message 49fe1367-be5f-4f92-8a20-2ee038554bf4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 21:26:48:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:33:16:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:33:16:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3b0dedf0-bb6b-48b2-be97-ead399eecd24
01/18/2025 21:33:16:INFO:Received: evaluate message 3b0dedf0-bb6b-48b2-be97-ead399eecd24
[92mINFO [0m:      Sent reply
01/18/2025 21:37:22:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:37:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:37:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 33901ed1-8303-44bd-98f8-38f5fb363076
01/18/2025 21:37:56:INFO:Received: train message 33901ed1-8303-44bd-98f8-38f5fb363076
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 21:50:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 21:57:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 21:57:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9ff118e7-5784-478d-8e43-2319284175cd
01/18/2025 21:57:05:INFO:Received: evaluate message 9ff118e7-5784-478d-8e43-2319284175cd
[92mINFO [0m:      Sent reply
01/18/2025 22:01:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:01:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:01:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 92fd66d2-4c95-478f-af0e-8357d46bc266
01/18/2025 22:01:34:INFO:Received: train message 92fd66d2-4c95-478f-af0e-8357d46bc266
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 22:14:29:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:20:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:20:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4c1b3276-e4fe-4970-a681-abba4bbdf9c8
01/18/2025 22:20:47:INFO:Received: evaluate message 4c1b3276-e4fe-4970-a681-abba4bbdf9c8

noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.21754033332690598
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.19776393938809636
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.1779875454492867
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.15821115151047707
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.13843475757166743
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.11865836363285781
Epsilon = 30.00
[92mINFO [0m:      Sent reply
01/18/2025 22:24:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:25:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:25:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f9bd8930-631d-40dc-b57c-2ad438fe50cd
01/18/2025 22:25:31:INFO:Received: train message f9bd8930-631d-40dc-b57c-2ad438fe50cd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 22:38:14:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:44:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:44:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 33ed7deb-a818-4bec-a611-6fead52af488
01/18/2025 22:44:45:INFO:Received: evaluate message 33ed7deb-a818-4bec-a611-6fead52af488
[92mINFO [0m:      Sent reply
01/18/2025 22:48:48:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 22:49:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 22:49:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7691c465-c1b5-4a16-b916-c69195d86c7f
01/18/2025 22:49:20:INFO:Received: train message 7691c465-c1b5-4a16-b916-c69195d86c7f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 23:02:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:08:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:08:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 90bbcaa8-588b-444d-a053-611380001e6d
01/18/2025 23:08:38:INFO:Received: evaluate message 90bbcaa8-588b-444d-a053-611380001e6d
[92mINFO [0m:      Sent reply
01/18/2025 23:12:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:13:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:13:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 264e2c20-e82c-407d-9d6c-985c31be82f9
01/18/2025 23:13:18:INFO:Received: train message 264e2c20-e82c-407d-9d6c-985c31be82f9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 23:26:07:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:32:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:32:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 913a1461-4f4a-42a2-903f-55e85e8845bf
01/18/2025 23:32:34:INFO:Received: evaluate message 913a1461-4f4a-42a2-903f-55e85e8845bf
[92mINFO [0m:      Sent reply
01/18/2025 23:36:31:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:37:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:37:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 01e25e6b-db7c-4e49-923f-181f7e83a20e
01/18/2025 23:37:15:INFO:Received: train message 01e25e6b-db7c-4e49-923f-181f7e83a20e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 23:49:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 23:56:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 23:56:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message de38a8d6-31e0-4372-af04-77111745689c
01/18/2025 23:56:09:INFO:Received: evaluate message de38a8d6-31e0-4372-af04-77111745689c

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.09888196969404818
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.07910557575523855
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.059329181816428884
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.03955278787761926
Epsilon = 30.00
[92mINFO [0m:      Sent reply
01/19/2025 00:00:01:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:00:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:00:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2cba08a4-d72c-43ee-ae08-24b00cd18c5b
01/19/2025 00:00:36:INFO:Received: train message 2cba08a4-d72c-43ee-ae08-24b00cd18c5b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/19/2025 00:13:12:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:19:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:19:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 17499d12-1ac5-4443-9a52-a0945fc0a24d
01/19/2025 00:19:48:INFO:Received: evaluate message 17499d12-1ac5-4443-9a52-a0945fc0a24d
[92mINFO [0m:      Sent reply
01/19/2025 00:23:58:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:24:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:24:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 33b369e5-5371-4686-a5a4-a7c2f56fd72e
01/19/2025 00:24:30:INFO:Received: train message 33b369e5-5371-4686-a5a4-a7c2f56fd72e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/19/2025 00:37:17:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:43:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:43:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 36901b76-e996-45a8-90d1-6c7e1b42e096
01/19/2025 00:43:33:INFO:Received: evaluate message 36901b76-e996-45a8-90d1-6c7e1b42e096
[92mINFO [0m:      Sent reply
01/19/2025 00:47:25:INFO:Sent reply
[92mINFO [0m:      
01/19/2025 00:47:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/19/2025 00:47:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message bafab488-b09c-446c-ab79-e3fc384e3ccb
01/19/2025 00:47:39:INFO:Received: reconnect message bafab488-b09c-446c-ab79-e3fc384e3ccb
01/19/2025 00:47:39:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/19/2025 00:47:39:INFO:Disconnect and shut down

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966, 117.42482489347458], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925, 0.5296012887635925], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297, 0.7193555349179758]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.01977639393880963
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966, 117.42482489347458, 117.25908535718918], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925, 0.5296012887635925, 0.5328231977446637], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297, 0.7193555349179758, 0.7212846690688237]}

BaseNM 0.41748046875
noise multiplier 0.2966459090821445
Noise multiplier before  adjustment: 0.2966459090821445
Noise multiplier before convergence adjustment: 0.2966459090821445
Updated noise multiplier after convergence adjustment: 0.0
Epsilon = 30.00

{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966, 117.42482489347458, 117.25908535718918, 117.31337916851044], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925, 0.5296012887635925, 0.5328231977446637, 0.5332259363672976], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297, 0.7193555349179758, 0.7212846690688237, 0.7229776848061777]}



Final client history:
{'loss': [141.89181208610535, 135.49430978298187, 137.1008837223053, 139.42722713947296, 140.38146257400513, 140.1488618850708, 138.49389910697937, 136.6689555644989, 133.6193369626999, 131.62890231609344, 129.86334824562073, 128.8125473856926, 127.34060329198837, 126.31609010696411, 125.00594735145569, 124.22317373752594, 123.08636915683746, 122.69723302125931, 122.06729191541672, 121.11292892694473, 120.60332131385803, 120.17026662826538, 119.4384194612503, 119.05765295028687, 118.52940011024475, 118.54788601398468, 118.15519630908966, 117.42482489347458, 117.25908535718918, 117.31337916851044], 'accuracy': [0.3419250906161901, 0.3403141361256545, 0.3415223519935562, 0.34313330648409185, 0.34756343133306483, 0.35481272654047524, 0.3697140555779299, 0.38260169150221507, 0.4015304067660089, 0.4196536447845348, 0.43616592831252515, 0.4478453483689086, 0.4571083366894885, 0.46677406363270235, 0.4724124043495771, 0.47724526782118404, 0.4828836085380588, 0.48892468787756743, 0.49496576721707614, 0.5042287555376561, 0.5066451872734595, 0.5110753121224325, 0.5155054369714056, 0.5219492549335482, 0.5275875956504229, 0.5279903342730567, 0.5296012887635925, 0.5296012887635925, 0.5328231977446637, 0.5332259363672976], 'auc': [0.5460484530250033, 0.5873079809432222, 0.6123466175339257, 0.6255987195877803, 0.635134593630355, 0.6420977321827006, 0.6482744795069026, 0.6537858380055177, 0.6591302272665509, 0.6637053251581886, 0.6684711322983466, 0.6722962584776735, 0.6760643712103915, 0.679713381286971, 0.6834217381657681, 0.6869806688082736, 0.6904838307350867, 0.6931174504447031, 0.6958782406082653, 0.6990661484489127, 0.701726967048038, 0.7042518908730465, 0.7070529084088146, 0.7095426281970368, 0.7122546946207959, 0.7141621364853833, 0.7165603094050297, 0.7193555349179758, 0.7212846690688237, 0.7229776848061777]}

