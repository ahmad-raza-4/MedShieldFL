nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/18/2025 06:21:48:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/18/2025 06:21:48:DEBUG:ChannelConnectivity.IDLE
01/18/2025 06:21:48:DEBUG:ChannelConnectivity.CONNECTING
01/18/2025 06:21:48:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/18/2025 06:25:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:25:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c54163a9-5b8d-4fb5-ad36-aeec10d97eff
01/18/2025 06:25:17:INFO:Received: train message c54163a9-5b8d-4fb5-ad36-aeec10d97eff
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 06:34:32:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:44:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:44:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 161dd49f-100d-4638-afb1-7e7155236b84
01/18/2025 06:44:36:INFO:Received: evaluate message 161dd49f-100d-4638-afb1-7e7155236b84
[92mINFO [0m:      Sent reply
01/18/2025 06:48:44:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:49:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:49:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f7b8af32-d3f4-4281-9801-dc4472f9439c
01/18/2025 06:49:11:INFO:Received: train message f7b8af32-d3f4-4281-9801-dc4472f9439c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 06:58:25:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:08:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:08:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c71ebb2f-3099-45af-aec9-805cd856cf01
01/18/2025 07:08:22:INFO:Received: evaluate message c71ebb2f-3099-45af-aec9-805cd856cf01
[92mINFO [0m:      Sent reply
01/18/2025 07:12:20:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:12:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:12:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5f7cacae-4d9b-4d8a-a3a1-7b06f15cee9d
01/18/2025 07:12:53:INFO:Received: train message 5f7cacae-4d9b-4d8a-a3a1-7b06f15cee9d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:21:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:32:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:32:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a7bf6b0d-e609-4525-9630-371e1ffd2ce2
01/18/2025 07:32:15:INFO:Received: evaluate message a7bf6b0d-e609-4525-9630-371e1ffd2ce2
[92mINFO [0m:      Sent reply
01/18/2025 07:36:14:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:36:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:36:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 05085b37-8283-4f3e-be12-86b250593d39
01/18/2025 07:36:27:INFO:Received: train message 05085b37-8283-4f3e-be12-86b250593d39
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:44:19:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:55:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:55:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f72c8ebc-49e7-4316-8679-cd4e1bda31db
01/18/2025 07:55:33:INFO:Received: evaluate message f72c8ebc-49e7-4316-8679-cd4e1bda31db
[92mINFO [0m:      Sent reply
01/18/2025 07:59:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:00:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:00:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e0d42f29-8060-4054-b240-8ef233ee9089
01/18/2025 08:00:05:INFO:Received: train message e0d42f29-8060-4054-b240-8ef233ee9089
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:08:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:19:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:19:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fbfdba25-e218-45f4-8261-70c6e40ff7d3
01/18/2025 08:19:51:INFO:Received: evaluate message fbfdba25-e218-45f4-8261-70c6e40ff7d3
[92mINFO [0m:      Sent reply
01/18/2025 08:24:04:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:24:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:24:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message af0dea58-9281-4dde-be55-6d830af3a7ed
01/18/2025 08:24:32:INFO:Received: train message af0dea58-9281-4dde-be55-6d830af3a7ed
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:33:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:49:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:49:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 21295c16-0609-4da4-9a59-d3693d67c51a
01/18/2025 08:49:26:INFO:Received: evaluate message 21295c16-0609-4da4-9a59-d3693d67c51a
[92mINFO [0m:      Sent reply
01/18/2025 08:53:32:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:53:52:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:53:52:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5279c89d-8579-415a-b763-72e2d1521b01
01/18/2025 08:53:52:INFO:Received: train message 5279c89d-8579-415a-b763-72e2d1521b01
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:03:48:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:19:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:19:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dbe2da73-493d-43e9-99eb-e701a101d1cb
01/18/2025 09:19:39:INFO:Received: evaluate message dbe2da73-493d-43e9-99eb-e701a101d1cb
[92mINFO [0m:      Sent reply
01/18/2025 09:23:40:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:24:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:24:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 67633705-6363-4f41-9476-846dfe92dd47
01/18/2025 09:24:39:INFO:Received: train message 67633705-6363-4f41-9476-846dfe92dd47
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:35:30:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:49:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:49:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0f23f794-0212-499d-a09e-fd8c2401a34c
01/18/2025 09:49:51:INFO:Received: evaluate message 0f23f794-0212-499d-a09e-fd8c2401a34c
[92mINFO [0m:      Sent reply
01/18/2025 09:53:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:54:18:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:54:18:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9be98ef9-64a1-46b7-8faf-3430bc949ac6
01/18/2025 09:54:18:INFO:Received: train message 9be98ef9-64a1-46b7-8faf-3430bc949ac6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:04:19:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:19:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:19:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1ca7673e-21c8-4b97-a8ce-ae4c8c4e10cd
01/18/2025 10:19:04:INFO:Received: evaluate message 1ca7673e-21c8-4b97-a8ce-ae4c8c4e10cd
[92mINFO [0m:      Sent reply
01/18/2025 10:23:15:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:24:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:24:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9445e40d-29ce-4a34-883c-33b73c0368a2
01/18/2025 10:24:22:INFO:Received: train message 9445e40d-29ce-4a34-883c-33b73c0368a2
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:35:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:48:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:48:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 434db8b8-46ce-4921-bfbb-c02da0978c5f
01/18/2025 10:48:41:INFO:Received: evaluate message 434db8b8-46ce-4921-bfbb-c02da0978c5f
[92mINFO [0m:      Sent reply
01/18/2025 10:53:17:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:53:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:53:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 514dc343-2750-4987-ad22-4ef999e2ed0d
01/18/2025 10:53:35:INFO:Received: train message 514dc343-2750-4987-ad22-4ef999e2ed0d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:03:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:15:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:15:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6e274ac3-a7ae-49e0-91cb-9c63d950c335
01/18/2025 11:15:39:INFO:Received: evaluate message 6e274ac3-a7ae-49e0-91cb-9c63d950c335
[92mINFO [0m:      Sent reply
01/18/2025 11:19:46:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:20:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:20:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 24be7834-b943-4400-955e-4f079db6ed34
01/18/2025 11:20:44:INFO:Received: train message 24be7834-b943-4400-955e-4f079db6ed34
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:29:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:42:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:42:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0984dce0-af3f-416a-babb-ffe9e3dc4379
01/18/2025 11:42:43:INFO:Received: evaluate message 0984dce0-af3f-416a-babb-ffe9e3dc4379
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise']
BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275], 'accuracy': [0.3419250906161901], 'auc': [0.5457303512912005]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585], 'accuracy': [0.3419250906161901, 0.3407168747482884], 'auc': [0.5457303512912005, 0.5868275242355229]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 11:46:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:47:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:47:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0d01a744-8d9e-4de8-a6e2-fa8625ac3487
01/18/2025 11:47:48:INFO:Received: train message 0d01a744-8d9e-4de8-a6e2-fa8625ac3487
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:56:44:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:10:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:10:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message df42f369-0a6e-4aa7-9d70-c54675216b1f
01/18/2025 12:10:37:INFO:Received: evaluate message df42f369-0a6e-4aa7-9d70-c54675216b1f
[92mINFO [0m:      Sent reply
01/18/2025 12:15:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:16:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:16:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 004b7540-e5d6-405e-853c-d63e23486037
01/18/2025 12:16:23:INFO:Received: train message 004b7540-e5d6-405e-853c-d63e23486037
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:25:49:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:39:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:39:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c2df92aa-3794-445d-9dda-2e1c9dd30c23
01/18/2025 12:39:34:INFO:Received: evaluate message c2df92aa-3794-445d-9dda-2e1c9dd30c23
[92mINFO [0m:      Sent reply
01/18/2025 12:45:02:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:45:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:45:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c75a04b9-0ec5-4513-9058-ca1d7fbbbe86
01/18/2025 12:45:35:INFO:Received: train message c75a04b9-0ec5-4513-9058-ca1d7fbbbe86
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:55:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:10:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:10:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fee0aa8b-7ad2-4cea-b8fb-ccc60a747eba
01/18/2025 13:10:22:INFO:Received: evaluate message fee0aa8b-7ad2-4cea-b8fb-ccc60a747eba
[92mINFO [0m:      Sent reply
01/18/2025 13:15:53:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:16:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:16:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dd5c84fb-82ab-473e-b938-c54af7a6c666
01/18/2025 13:16:30:INFO:Received: train message dd5c84fb-82ab-473e-b938-c54af7a6c666
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:25:52:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:43:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:43:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c71b673d-f6a2-4bb5-a4be-111061335e4f
01/18/2025 13:43:50:INFO:Received: evaluate message c71b673d-f6a2-4bb5-a4be-111061335e4f
[92mINFO [0m:      Sent reply
01/18/2025 13:48:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:49:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:49:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1f17473c-f4c5-4036-a820-aa68500333f3
01/18/2025 13:49:19:INFO:Received: train message 1f17473c-f4c5-4036-a820-aa68500333f3
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:58:12:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:17:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:17:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 031e9439-9000-4594-bbea-8a81cc7ae08f
01/18/2025 14:17:40:INFO:Received: evaluate message 031e9439-9000-4594-bbea-8a81cc7ae08f
[92mINFO [0m:      Sent reply
01/18/2025 14:21:42:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:22:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:22:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5ac289af-286b-47e7-b894-b2f3a7d9865f
01/18/2025 14:22:19:INFO:Received: train message 5ac289af-286b-47e7-b894-b2f3a7d9865f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:31:34:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:48:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:48:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5b209ab3-9652-4e90-bd7a-6e38f5e7ad31
01/18/2025 14:48:42:INFO:Received: evaluate message 5b209ab3-9652-4e90-bd7a-6e38f5e7ad31
[92mINFO [0m:      Sent reply
01/18/2025 14:52:43:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:53:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:53:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0906cc21-9183-47b6-abf9-e53e368e7e89
01/18/2025 14:53:31:INFO:Received: train message 0906cc21-9183-47b6-abf9-e53e368e7e89

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5391866732388735
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.5032408950229486
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.4672951168070237
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.4313493385910988
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088]}

BaseNM 0.72113037109375
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:02:53:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:17:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:17:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fb74dcc5-83c9-4795-bf9e-714d1535fc5e
01/18/2025 15:17:08:INFO:Received: evaluate message fb74dcc5-83c9-4795-bf9e-714d1535fc5e
[92mINFO [0m:      Sent reply
01/18/2025 15:21:06:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:22:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:22:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e4a0a7f6-eed7-479f-b6ac-d8408a44b656
01/18/2025 15:22:09:INFO:Received: train message e4a0a7f6-eed7-479f-b6ac-d8408a44b656
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:31:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:45:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:45:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2c28877a-ddd4-4e05-8dd0-8b01a44e41ef
01/18/2025 15:45:33:INFO:Received: evaluate message 2c28877a-ddd4-4e05-8dd0-8b01a44e41ef
[92mINFO [0m:      Sent reply
01/18/2025 15:49:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:50:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:50:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 08ccd34e-c0ee-4ee5-bdd1-64b794b657c9
01/18/2025 15:50:45:INFO:Received: train message 08ccd34e-c0ee-4ee5-bdd1-64b794b657c9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:00:22:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:13:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:13:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a728a6a1-8356-4eae-8fbe-2e3a2e9c9f9f
01/18/2025 16:13:44:INFO:Received: evaluate message a728a6a1-8356-4eae-8fbe-2e3a2e9c9f9f
[92mINFO [0m:      Sent reply
01/18/2025 16:18:07:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:19:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:19:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a778960b-1b42-4c51-9cd8-0af5257a8249
01/18/2025 16:19:32:INFO:Received: train message a778960b-1b42-4c51-9cd8-0af5257a8249
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:29:03:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:41:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:41:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2eebb8f4-97ea-4798-9946-73743a6060a8
01/18/2025 16:41:58:INFO:Received: evaluate message 2eebb8f4-97ea-4798-9946-73743a6060a8
[92mINFO [0m:      Sent reply
01/18/2025 16:46:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:47:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:47:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0d1dd351-df38-4dea-8b67-09727757c372
01/18/2025 16:47:12:INFO:Received: train message 0d1dd351-df38-4dea-8b67-09727757c372
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:56:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:09:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:09:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d7a87955-671e-48ba-b54c-ae7146d6cbbf
01/18/2025 17:09:10:INFO:Received: evaluate message d7a87955-671e-48ba-b54c-ae7146d6cbbf
[92mINFO [0m:      Sent reply
01/18/2025 17:13:37:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:14:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:14:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ebf78f1c-64db-410f-b730-dd5a967f538d
01/18/2025 17:14:01:INFO:Received: train message ebf78f1c-64db-410f-b730-dd5a967f538d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:23:42:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:36:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:36:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2a68ca4f-ef48-410d-930b-527599683ca5
01/18/2025 17:36:24:INFO:Received: evaluate message 2a68ca4f-ef48-410d-930b-527599683ca5
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.39540356037517393
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.359457782159249
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.3235120039433241
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.2875662257273992
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.2516204475114743
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.2156746692955494
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 17:41:11:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:41:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:41:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1218bf0c-a9c4-4be6-81ac-1b28b0d3779c
01/18/2025 17:41:37:INFO:Received: train message 1218bf0c-a9c4-4be6-81ac-1b28b0d3779c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:50:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:04:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:04:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a27f7533-6f77-4c82-83d8-7b592bbd8279
01/18/2025 18:04:40:INFO:Received: evaluate message a27f7533-6f77-4c82-83d8-7b592bbd8279
[92mINFO [0m:      Sent reply
01/18/2025 18:09:27:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:09:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:09:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2c3ea963-0685-4230-862d-004a0c4e6b07
01/18/2025 18:09:54:INFO:Received: train message 2c3ea963-0685-4230-862d-004a0c4e6b07
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:19:16:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:32:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:32:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 084b6120-7f69-4f85-a493-fa1e7bd1f158
01/18/2025 18:32:49:INFO:Received: evaluate message 084b6120-7f69-4f85-a493-fa1e7bd1f158
[92mINFO [0m:      Sent reply
01/18/2025 18:37:40:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:37:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:37:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message aae82ea1-ff48-4272-8e93-e3005ef54b92
01/18/2025 18:37:58:INFO:Received: train message aae82ea1-ff48-4272-8e93-e3005ef54b92
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:47:11:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:01:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:01:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f27477b3-a5bd-49eb-bd7c-a78f33505594
01/18/2025 19:01:28:INFO:Received: evaluate message f27477b3-a5bd-49eb-bd7c-a78f33505594
[92mINFO [0m:      Sent reply
01/18/2025 19:06:03:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:06:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:06:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 37ed5611-f878-4ff0-a39c-efd9436c2968
01/18/2025 19:06:33:INFO:Received: train message 37ed5611-f878-4ff0-a39c-efd9436c2968
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:15:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:30:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:30:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 54dbc71a-379b-4a05-83a7-14e99e585a48
01/18/2025 19:30:24:INFO:Received: evaluate message 54dbc71a-379b-4a05-83a7-14e99e585a48

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.1797288910796245
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.14378311286369963
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.10783733464777467
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.07189155643184979
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 19:35:32:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:35:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:35:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message aa9a9103-cba2-4ab4-84ed-a9ce569dbe22
01/18/2025 19:35:49:INFO:Received: train message aa9a9103-cba2-4ab4-84ed-a9ce569dbe22
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:45:13:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:00:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:00:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2314b94a-79dc-4ea4-9157-081612c92b35
01/18/2025 20:00:21:INFO:Received: evaluate message 2314b94a-79dc-4ea4-9157-081612c92b35
[92mINFO [0m:      Sent reply
01/18/2025 20:05:51:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:06:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:06:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0cdb1e4d-47a7-4972-8438-c0956e4c7cf9
01/18/2025 20:06:27:INFO:Received: train message 0cdb1e4d-47a7-4972-8438-c0956e4c7cf9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:15:56:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:28:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:28:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b5ff8e57-42c1-4f7d-8bfc-1533feb6b619
01/18/2025 20:28:56:INFO:Received: evaluate message b5ff8e57-42c1-4f7d-8bfc-1533feb6b619
[92mINFO [0m:      Sent reply
01/18/2025 20:32:58:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:32:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:32:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 0f62a0dd-da56-427b-b46d-159bb5f5ff12
01/18/2025 20:32:58:INFO:Received: reconnect message 0f62a0dd-da56-427b-b46d-159bb5f5ff12
01/18/2025 20:32:58:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/18/2025 20:32:58:INFO:Disconnect and shut down

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.035945778215924894
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554]}

BaseNM 0.72113037109375
noise multiplier 0.5391866732388735
Noise multiplier before  adjustment: 0.5391866732388735
Noise multiplier before convergence adjustment: 0.5391866732388735
Updated noise multiplier after convergence adjustment: 0.0
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864, 118.1268835067749], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585, 0.5316149818767619], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554, 0.7213411777674161]}



Final client history:
{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864, 118.1268835067749], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585, 0.5316149818767619], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554, 0.7213411777674161]}

