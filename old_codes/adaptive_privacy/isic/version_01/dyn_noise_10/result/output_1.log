nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/dgxuser16/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/18/2025 06:24:55:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/18/2025 06:24:55:DEBUG:ChannelConnectivity.IDLE
01/18/2025 06:24:55:DEBUG:ChannelConnectivity.CONNECTING
01/18/2025 06:24:55:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/18/2025 06:25:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:25:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 224f23c9-fe5a-465b-a6db-91583a07fca9
01/18/2025 06:25:04:INFO:Received: train message 224f23c9-fe5a-465b-a6db-91583a07fca9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 06:44:03:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:44:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:44:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 08936d71-9472-4251-bdac-77d6224a0e5c
01/18/2025 06:44:39:INFO:Received: evaluate message 08936d71-9472-4251-bdac-77d6224a0e5c
[92mINFO [0m:      Sent reply
01/18/2025 06:48:44:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 06:49:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 06:49:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message e66ac507-3893-4015-877f-87145c9233f6
01/18/2025 06:49:04:INFO:Received: train message e66ac507-3893-4015-877f-87145c9233f6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:07:55:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:08:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:08:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 80372368-21d5-4373-b2d8-693c97e7a578
01/18/2025 07:08:32:INFO:Received: evaluate message 80372368-21d5-4373-b2d8-693c97e7a578
[92mINFO [0m:      Sent reply
01/18/2025 07:12:34:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:13:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:13:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4425177a-faeb-405e-9e26-1ab20abf2bb6
01/18/2025 07:13:06:INFO:Received: train message 4425177a-faeb-405e-9e26-1ab20abf2bb6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:31:40:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:32:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:32:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4954e406-e56d-42e1-ac64-76538ad8c25c
01/18/2025 07:32:09:INFO:Received: evaluate message 4954e406-e56d-42e1-ac64-76538ad8c25c
[92mINFO [0m:      Sent reply
01/18/2025 07:36:08:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:36:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:36:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 59149504-4ae8-4d5b-9432-c040afcd5863
01/18/2025 07:36:41:INFO:Received: train message 59149504-4ae8-4d5b-9432-c040afcd5863
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 07:55:16:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 07:55:42:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 07:55:42:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3eddbb6c-e58f-4911-ad6a-47331aa8deff
01/18/2025 07:55:42:INFO:Received: evaluate message 3eddbb6c-e58f-4911-ad6a-47331aa8deff
[92mINFO [0m:      Sent reply
01/18/2025 07:59:41:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:00:19:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:00:19:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ab0325f7-39f4-4602-adf8-2783fab6d7c5
01/18/2025 08:00:19:INFO:Received: train message ab0325f7-39f4-4602-adf8-2783fab6d7c5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:19:14:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:19:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:19:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5b7d71dc-a946-4cf1-9e8c-7cee8ffa8caf
01/18/2025 08:19:35:INFO:Received: evaluate message 5b7d71dc-a946-4cf1-9e8c-7cee8ffa8caf
[92mINFO [0m:      Sent reply
01/18/2025 08:23:45:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:24:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:24:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ff6ec678-3a04-4b9c-85aa-90c39483f204
01/18/2025 08:24:24:INFO:Received: train message ff6ec678-3a04-4b9c-85aa-90c39483f204
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 08:48:46:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:49:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:49:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b6ccdc5b-d88b-4e9e-b241-45f21d6e5254
01/18/2025 08:49:23:INFO:Received: evaluate message b6ccdc5b-d88b-4e9e-b241-45f21d6e5254
[92mINFO [0m:      Sent reply
01/18/2025 08:53:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 08:54:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 08:54:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8e019677-7c19-45eb-a69c-d34287a495dc
01/18/2025 08:54:07:INFO:Received: train message 8e019677-7c19-45eb-a69c-d34287a495dc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:19:12:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:19:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:19:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3d4637b1-bea2-47f8-ad7f-46bf5fe429ac
01/18/2025 09:19:51:INFO:Received: evaluate message 3d4637b1-bea2-47f8-ad7f-46bf5fe429ac
[92mINFO [0m:      Sent reply
01/18/2025 09:24:02:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:24:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:24:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 874c946d-1445-434f-8c3b-cfb8e9615b69
01/18/2025 09:24:31:INFO:Received: train message 874c946d-1445-434f-8c3b-cfb8e9615b69
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 09:49:10:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:49:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:49:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 573c10f9-a03f-40f2-8c89-6fe2cbd80a84
01/18/2025 09:49:45:INFO:Received: evaluate message 573c10f9-a03f-40f2-8c89-6fe2cbd80a84
[92mINFO [0m:      Sent reply
01/18/2025 09:53:51:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 09:54:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 09:54:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a94df301-9a03-42fd-8f18-d9f21a2b4717
01/18/2025 09:54:27:INFO:Received: train message a94df301-9a03-42fd-8f18-d9f21a2b4717
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:18:36:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:19:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:19:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 37369b59-1677-41c8-9bd7-e9a4ebb553a4
01/18/2025 10:19:08:INFO:Received: evaluate message 37369b59-1677-41c8-9bd7-e9a4ebb553a4
[92mINFO [0m:      Sent reply
01/18/2025 10:23:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:24:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:24:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 59bbf4e0-6a3c-4a3f-99a2-df4f465cb9ca
01/18/2025 10:24:11:INFO:Received: train message 59bbf4e0-6a3c-4a3f-99a2-df4f465cb9ca
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 10:48:01:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:48:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:48:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e9c0d889-24df-4285-88e9-b32935f836c8
01/18/2025 10:48:31:INFO:Received: evaluate message e9c0d889-24df-4285-88e9-b32935f836c8
[92mINFO [0m:      Sent reply
01/18/2025 10:53:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 10:53:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 10:53:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b05fd6cc-7328-4b2b-953d-df8552c252d8
01/18/2025 10:53:57:INFO:Received: train message b05fd6cc-7328-4b2b-953d-df8552c252d8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:15:00:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:15:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:15:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 083c8452-67a7-4872-a6df-7f2025fc723a
01/18/2025 11:15:35:INFO:Received: evaluate message 083c8452-67a7-4872-a6df-7f2025fc723a
[92mINFO [0m:      Sent reply
01/18/2025 11:20:09:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:20:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:20:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 08cf621e-a8cf-471f-8eee-2f5760b1c024
01/18/2025 11:20:31:INFO:Received: train message 08cf621e-a8cf-471f-8eee-2f5760b1c024
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 11:42:02:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:42:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:42:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ef6d4635-e58f-4879-b54e-0704c22c8689
01/18/2025 11:42:39:INFO:Received: evaluate message ef6d4635-e58f-4879-b54e-0704c22c8689
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/dyn_noise']
BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275], 'accuracy': [0.3419250906161901], 'auc': [0.5457303512912005]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585], 'accuracy': [0.3419250906161901, 0.3407168747482884], 'auc': [0.5457303512912005, 0.5868275242355229]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 11:47:31:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 11:48:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 11:48:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f8577d49-6bc4-46dc-80ff-29d6170f7ccc
01/18/2025 11:48:04:INFO:Received: train message f8577d49-6bc4-46dc-80ff-29d6170f7ccc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:10:00:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:10:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:10:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3538bfe4-c3a4-4f06-a92e-3bc14775dfae
01/18/2025 12:10:37:INFO:Received: evaluate message 3538bfe4-c3a4-4f06-a92e-3bc14775dfae
[92mINFO [0m:      Sent reply
01/18/2025 12:15:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:16:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:16:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6c7b44b1-65de-4895-ae42-2804d0a8a35e
01/18/2025 12:16:38:INFO:Received: train message 6c7b44b1-65de-4895-ae42-2804d0a8a35e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 12:38:37:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:39:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:39:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bf9a8f12-671d-425b-909a-6918455036d1
01/18/2025 12:39:39:INFO:Received: evaluate message bf9a8f12-671d-425b-909a-6918455036d1
[92mINFO [0m:      Sent reply
01/18/2025 12:44:55:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 12:45:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 12:45:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9848f240-68d3-4afa-a7a3-fd3b67642de6
01/18/2025 12:45:34:INFO:Received: train message 9848f240-68d3-4afa-a7a3-fd3b67642de6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:09:38:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:10:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:10:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c9fdb6a6-cbf4-43c7-aa3b-2b2bcfe1b5d2
01/18/2025 13:10:24:INFO:Received: evaluate message c9fdb6a6-cbf4-43c7-aa3b-2b2bcfe1b5d2
[92mINFO [0m:      Sent reply
01/18/2025 13:15:55:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:16:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:16:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bccf2a8b-e63e-457f-b7d7-98b95c9b9e7b
01/18/2025 13:16:23:INFO:Received: train message bccf2a8b-e63e-457f-b7d7-98b95c9b9e7b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 13:43:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:44:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:44:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 50611642-7a10-4887-8abe-985a948925ea
01/18/2025 13:44:00:INFO:Received: evaluate message 50611642-7a10-4887-8abe-985a948925ea
[92mINFO [0m:      Sent reply
01/18/2025 13:48:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 13:49:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 13:49:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 223673ee-af7b-4d3b-a9f4-1ca0efd425fa
01/18/2025 13:49:40:INFO:Received: train message 223673ee-af7b-4d3b-a9f4-1ca0efd425fa
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:17:07:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:17:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:17:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 20afa1b5-cd36-4376-a61c-dfec6bf7b79d
01/18/2025 14:17:47:INFO:Received: evaluate message 20afa1b5-cd36-4376-a61c-dfec6bf7b79d
[92mINFO [0m:      Sent reply
01/18/2025 14:21:53:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:22:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:22:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b39094c8-fbdb-4e5a-979a-61c8a251bcf8
01/18/2025 14:22:24:INFO:Received: train message b39094c8-fbdb-4e5a-979a-61c8a251bcf8
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 14:48:13:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:48:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:48:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 82d1cc1b-089e-4b9a-b48a-58ce6bb63e18
01/18/2025 14:48:48:INFO:Received: evaluate message 82d1cc1b-089e-4b9a-b48a-58ce6bb63e18
[92mINFO [0m:      Sent reply
01/18/2025 14:52:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 14:53:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 14:53:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 868c7498-c5ff-46fe-892c-deeb096dbef7
01/18/2025 14:53:31:INFO:Received: train message 868c7498-c5ff-46fe-892c-deeb096dbef7

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.5323553457856178
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.49686498939991
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.4613746330142021
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.42588427662849426
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088]}

BaseNM 0.72113037109375
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:16:42:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:17:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:17:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 788bf5d5-e28c-4574-9e42-c50839815f72
01/18/2025 15:17:21:INFO:Received: evaluate message 788bf5d5-e28c-4574-9e42-c50839815f72
[92mINFO [0m:      Sent reply
01/18/2025 15:21:35:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:22:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:22:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 83783f7e-958f-4ca1-bcc1-72799f601d7a
01/18/2025 15:22:00:INFO:Received: train message 83783f7e-958f-4ca1-bcc1-72799f601d7a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 15:44:57:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:45:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:45:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5ce916c6-2bc4-4309-b442-97ae8959dc5e
01/18/2025 15:45:33:INFO:Received: evaluate message 5ce916c6-2bc4-4309-b442-97ae8959dc5e
[92mINFO [0m:      Sent reply
01/18/2025 15:50:01:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 15:50:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 15:50:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8e1bc5b4-871d-48f4-ac16-4cff01b78652
01/18/2025 15:50:20:INFO:Received: train message 8e1bc5b4-871d-48f4-ac16-4cff01b78652
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:13:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:14:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:14:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1c1cf100-66a2-4ce8-95b6-ff2efb9aeadb
01/18/2025 16:14:01:INFO:Received: evaluate message 1c1cf100-66a2-4ce8-95b6-ff2efb9aeadb
[92mINFO [0m:      Sent reply
01/18/2025 16:18:48:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:19:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:19:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bcfab077-181b-4f76-bd59-f66aabf3a49d
01/18/2025 16:19:31:INFO:Received: train message bcfab077-181b-4f76-bd59-f66aabf3a49d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 16:41:19:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:42:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:42:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dfe290d7-60eb-4455-a97d-4a21166a1b0a
01/18/2025 16:42:02:INFO:Received: evaluate message dfe290d7-60eb-4455-a97d-4a21166a1b0a
[92mINFO [0m:      Sent reply
01/18/2025 16:46:31:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 16:46:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 16:46:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 32385a81-a9e2-4d65-b5fd-b2c33b430941
01/18/2025 16:46:48:INFO:Received: train message 32385a81-a9e2-4d65-b5fd-b2c33b430941
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:08:23:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:09:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:09:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f4c7d458-3fab-48bf-9e86-f50f8f1d3e66
01/18/2025 17:09:10:INFO:Received: evaluate message f4c7d458-3fab-48bf-9e86-f50f8f1d3e66
[92mINFO [0m:      Sent reply
01/18/2025 17:13:33:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:13:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:13:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f4f70646-f679-423d-a9e6-ca61b781831f
01/18/2025 17:13:58:INFO:Received: train message f4f70646-f679-423d-a9e6-ca61b781831f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 17:35:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:36:24:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:36:24:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 179a8354-0ff2-41b9-a2c3-a96b09307092
01/18/2025 17:36:24:INFO:Received: evaluate message 179a8354-0ff2-41b9-a2c3-a96b09307092
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.39039392024278646
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.3549035638570786
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.3194132074713707
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.28392285108566284
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.248432494699955
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.21294213831424713
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 17:40:44:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 17:41:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 17:41:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8792917c-cd29-423f-bc45-51bdf5b85eed
01/18/2025 17:41:47:INFO:Received: train message 8792917c-cd29-423f-bc45-51bdf5b85eed
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:03:59:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:04:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:04:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9ad6b27b-5ff7-4e3c-9516-ed513c4106df
01/18/2025 18:04:26:INFO:Received: evaluate message 9ad6b27b-5ff7-4e3c-9516-ed513c4106df
[92mINFO [0m:      Sent reply
01/18/2025 18:08:42:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:10:03:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:10:03:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8e0ba21a-df8f-427f-9618-937cffec0fd5
01/18/2025 18:10:03:INFO:Received: train message 8e0ba21a-df8f-427f-9618-937cffec0fd5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 18:32:05:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:32:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:32:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f96b0a83-6979-4d02-831a-13f1bca1634c
01/18/2025 18:32:36:INFO:Received: evaluate message f96b0a83-6979-4d02-831a-13f1bca1634c
[92mINFO [0m:      Sent reply
01/18/2025 18:37:12:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 18:38:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 18:38:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 2e9e21f7-cf17-4acd-9859-bcea404bedee
01/18/2025 18:38:13:INFO:Received: train message 2e9e21f7-cf17-4acd-9859-bcea404bedee
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:00:41:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:01:20:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:01:20:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9de92a3e-4f36-4154-8dba-4b0a22ff1e20
01/18/2025 19:01:20:INFO:Received: evaluate message 9de92a3e-4f36-4154-8dba-4b0a22ff1e20
[92mINFO [0m:      Sent reply
01/18/2025 19:05:47:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:06:31:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:06:31:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f78741ec-7686-4d86-a7b5-4031a506439b
01/18/2025 19:06:31:INFO:Received: train message f78741ec-7686-4d86-a7b5-4031a506439b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:29:42:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:30:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:30:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aecb3cdf-3907-4da0-a830-f3e5bdc978f1
01/18/2025 19:30:01:INFO:Received: evaluate message aecb3cdf-3907-4da0-a830-f3e5bdc978f1

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.1774517819285393
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.14196142554283145
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.10647106915712354
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.0709807127714157
Epsilon = 10.00
[92mINFO [0m:      Sent reply
01/18/2025 19:33:49:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 19:36:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 19:36:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cdfbd190-77bb-4b63-86c2-eca204c75db5
01/18/2025 19:36:07:INFO:Received: train message cdfbd190-77bb-4b63-86c2-eca204c75db5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 19:59:40:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:00:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:00:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message fe8e4286-b78b-477a-8798-03595b3ffb6f
01/18/2025 20:00:11:INFO:Received: evaluate message fe8e4286-b78b-477a-8798-03595b3ffb6f
[92mINFO [0m:      Sent reply
01/18/2025 20:04:50:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:06:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:06:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1270a617-8972-4430-aa2f-2be1525b1d21
01/18/2025 20:06:30:INFO:Received: train message 1270a617-8972-4430-aa2f-2be1525b1d21
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
[92mINFO [0m:      Sent reply
01/18/2025 20:28:18:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:28:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:28:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8884edfb-158a-4e00-ab50-095ccf3141cd
01/18/2025 20:28:55:INFO:Received: evaluate message 8884edfb-158a-4e00-ab50-095ccf3141cd
[92mINFO [0m:      Sent reply
01/18/2025 20:32:54:INFO:Sent reply
[92mINFO [0m:      
01/18/2025 20:32:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/18/2025 20:32:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 93377977-242f-4896-9702-7cbf898026a5
01/18/2025 20:32:58:INFO:Received: reconnect message 93377977-242f-4896-9702-7cbf898026a5
01/18/2025 20:32:58:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/18/2025 20:32:58:INFO:Disconnect and shut down

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.03549035638570785
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554]}

BaseNM 0.72113037109375
noise multiplier 0.5323553457856178
Noise multiplier before  adjustment: 0.5323553457856178
Noise multiplier before convergence adjustment: 0.5323553457856178
Updated noise multiplier after convergence adjustment: 0.0
Epsilon = 10.00

{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864, 118.1268835067749], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585, 0.5316149818767619], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554, 0.7213411777674161]}



Final client history:
{'loss': [142.0397173166275, 135.7128220796585, 137.69252002239227, 140.31297075748444, 141.28822207450867, 141.20222914218903, 139.4740377664566, 137.77971923351288, 134.7427213191986, 132.70206606388092, 130.76300370693207, 129.79644256830215, 128.3980985879898, 127.30027866363525, 126.08611905574799, 125.23086887598038, 124.0175793170929, 123.64478379487991, 122.994096159935, 122.10566818714142, 121.58415073156357, 121.08059453964233, 120.31857693195343, 119.80531972646713, 119.2253770828247, 119.3676108121872, 118.90310478210449, 118.28096359968185, 118.09677052497864, 118.1268835067749], 'accuracy': [0.3419250906161901, 0.3407168747482884, 0.3415223519935562, 0.3427305678614579, 0.3463552154651631, 0.3527990334273057, 0.3685058397100282, 0.3801852597664116, 0.3987112364075715, 0.41602899718082964, 0.4309303262182843, 0.44381796214256947, 0.45469190495368506, 0.4611357229158276, 0.4695932339911397, 0.47482883608538057, 0.4804671768022553, 0.48489730165122835, 0.490938380990737, 0.500201369311317, 0.5042287555376561, 0.5078534031413613, 0.5126862666129682, 0.5175191300845751, 0.5251711639146194, 0.5263793797825211, 0.5287958115183246, 0.5300040273862263, 0.5291985501409585, 0.5316149818767619], 'auc': [0.5457303512912005, 0.5868275242355229, 0.611496999858683, 0.6245150073549827, 0.6339346539116171, 0.6408841264307137, 0.6471534828579546, 0.652593677692908, 0.6579652193582111, 0.6622118898248956, 0.667093855408547, 0.6709550886376612, 0.6746503766122796, 0.6784762303360287, 0.6820273886953274, 0.6855880295369721, 0.6890737498108701, 0.6916916512951088, 0.6944654448637728, 0.6974421753567641, 0.7000943363892467, 0.7026796399356435, 0.7055120219440829, 0.7080410999861098, 0.7108557706338847, 0.7125606842499568, 0.7151584172986201, 0.7177875020086697, 0.7197591731515554, 0.7213411777674161]}

