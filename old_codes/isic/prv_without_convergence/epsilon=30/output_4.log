nohup: ignoring input
Traceback (most recent call last):
  File "client_4.py", line 16, in <module>
    from flamby.datasets.fed_isic2019 import FedIsic2019
ModuleNotFoundError: No module named 'flamby'
nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adaptive_dp/prv/epsilon=30/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/30/2025 12:33:05:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/30/2025 12:33:05:DEBUG:ChannelConnectivity.IDLE
01/30/2025 12:33:05:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/30/2025 12:40:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 12:40:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a93cfb07-5abf-4185-bb12-5cd04a8c2322
01/30/2025 12:40:07:INFO:Received: train message a93cfb07-5abf-4185-bb12-5cd04a8c2322
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 12:50:04:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 13:06:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 13:06:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 80005637-2487-4b23-b3ec-54557827daf2
01/30/2025 13:06:48:INFO:Received: evaluate message 80005637-2487-4b23-b3ec-54557827daf2
[92mINFO [0m:      Sent reply
01/30/2025 13:10:48:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 13:11:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 13:11:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3bc5fef5-1a7e-4b3d-adf8-a9c7cd29fe0c
01/30/2025 13:11:08:INFO:Received: train message 3bc5fef5-1a7e-4b3d-adf8-a9c7cd29fe0c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 13:21:08:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 13:40:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 13:40:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e1fb8167-242b-4b86-92da-4e0730ac44a3
01/30/2025 13:40:45:INFO:Received: evaluate message e1fb8167-242b-4b86-92da-4e0730ac44a3
[92mINFO [0m:      Sent reply
01/30/2025 13:44:56:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 13:45:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 13:45:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a84f8cca-cff5-487c-af96-7e9dc52e8526
01/30/2025 13:45:40:INFO:Received: train message a84f8cca-cff5-487c-af96-7e9dc52e8526
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 13:55:04:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 14:09:57:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 14:09:57:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4d5eb2a2-9ef4-4091-bf9d-152ffb18d49a
01/30/2025 14:09:57:INFO:Received: evaluate message 4d5eb2a2-9ef4-4091-bf9d-152ffb18d49a
[92mINFO [0m:      Sent reply
01/30/2025 14:14:04:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 14:14:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 14:14:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a0e7354c-d14e-4c4c-bfa2-628c3e5a5334
01/30/2025 14:14:13:INFO:Received: train message a0e7354c-d14e-4c4c-bfa2-628c3e5a5334
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 14:23:40:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 14:44:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 14:44:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 39f787e1-83ec-4993-a64c-90f7e46b2e04
01/30/2025 14:44:38:INFO:Received: evaluate message 39f787e1-83ec-4993-a64c-90f7e46b2e04
[92mINFO [0m:      Sent reply
01/30/2025 14:48:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 14:49:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 14:49:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message df002170-1fa1-4503-b7d4-e2c417a7c9bd
01/30/2025 14:49:05:INFO:Received: train message df002170-1fa1-4503-b7d4-e2c417a7c9bd
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 14:58:16:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 15:16:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 15:16:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c2e03f5a-0127-4e7a-8d84-aee10b534eff
01/30/2025 15:16:23:INFO:Received: evaluate message c2e03f5a-0127-4e7a-8d84-aee10b534eff
[92mINFO [0m:      Sent reply
01/30/2025 15:20:43:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 15:21:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 15:21:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ba0e2928-312c-4dbe-903c-e539b7469b5a
01/30/2025 15:21:01:INFO:Received: train message ba0e2928-312c-4dbe-903c-e539b7469b5a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 15:30:49:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 15:46:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 15:46:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ba5fec42-defc-49ae-976f-5a98bf2fcefb
01/30/2025 15:46:37:INFO:Received: evaluate message ba5fec42-defc-49ae-976f-5a98bf2fcefb
[92mINFO [0m:      Sent reply
01/30/2025 15:50:54:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 15:51:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 15:51:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a338bc85-8e96-419d-9218-3a4e14d15e2e
01/30/2025 15:51:22:INFO:Received: train message a338bc85-8e96-419d-9218-3a4e14d15e2e
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adaptive_dp/prv/epsilon=30', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adaptive_dp/prv/epsilon=30']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671], 'accuracy': [0.5730970600080548], 'auc': [0.8431309076755067]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373], 'accuracy': [0.5730970600080548, 0.6198147402335884], 'auc': [0.8431309076755067, 0.8753916239274806]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 16:01:26:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 16:21:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 16:21:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message eaa1fa2f-1397-4821-a7f7-99f980b70ff8
01/30/2025 16:21:08:INFO:Received: evaluate message eaa1fa2f-1397-4821-a7f7-99f980b70ff8
[92mINFO [0m:      Sent reply
01/30/2025 16:25:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 16:25:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 16:25:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message dcd31b1f-9f6f-43d9-80fe-b63c080da69b
01/30/2025 16:25:36:INFO:Received: train message dcd31b1f-9f6f-43d9-80fe-b63c080da69b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 16:35:45:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 16:49:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 16:49:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5cdef52c-bcb9-4914-8ef5-46e8337524cd
01/30/2025 16:49:15:INFO:Received: evaluate message 5cdef52c-bcb9-4914-8ef5-46e8337524cd
[92mINFO [0m:      Sent reply
01/30/2025 16:53:39:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 16:54:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 16:54:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 77db6e8e-bd18-441a-b31b-cb082e6f6661
01/30/2025 16:54:10:INFO:Received: train message 77db6e8e-bd18-441a-b31b-cb082e6f6661
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 17:04:07:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 17:26:04:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 17:26:04:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e302ce9b-13ff-4288-88c5-77f43b80139b
01/30/2025 17:26:04:INFO:Received: evaluate message e302ce9b-13ff-4288-88c5-77f43b80139b
[92mINFO [0m:      Sent reply
01/30/2025 17:30:13:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 17:30:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 17:30:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message af4546c8-c453-4fe8-90df-de76cb429a22
01/30/2025 17:30:35:INFO:Received: train message af4546c8-c453-4fe8-90df-de76cb429a22
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 17:40:17:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 17:54:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 17:54:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b6832879-b439-47fb-bdc7-c4cbe6e9ef54
01/30/2025 17:54:55:INFO:Received: evaluate message b6832879-b439-47fb-bdc7-c4cbe6e9ef54
[92mINFO [0m:      Sent reply
01/30/2025 17:59:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 17:59:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 17:59:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4004eca8-80cf-4773-a8ff-823705549e8e
01/30/2025 17:59:45:INFO:Received: train message 4004eca8-80cf-4773-a8ff-823705549e8e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 18:09:45:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 18:27:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 18:27:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6f38144c-f9ec-48aa-b935-4fa6c7897990
01/30/2025 18:27:55:INFO:Received: evaluate message 6f38144c-f9ec-48aa-b935-4fa6c7897990
[92mINFO [0m:      Sent reply
01/30/2025 18:31:59:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 18:32:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 18:32:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 43a49d9a-0f03-43e0-b2bc-faa747dbcbdd
01/30/2025 18:32:29:INFO:Received: train message 43a49d9a-0f03-43e0-b2bc-faa747dbcbdd
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 18:42:13:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 18:56:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 18:56:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0f66a4a4-eaaf-4a0d-8305-ba6ac12dd94c
01/30/2025 18:56:39:INFO:Received: evaluate message 0f66a4a4-eaaf-4a0d-8305-ba6ac12dd94c
[92mINFO [0m:      Sent reply
01/30/2025 19:00:54:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 19:01:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 19:01:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message bda2c595-1fc9-4876-8e67-eefb0df068b9
01/30/2025 19:01:30:INFO:Received: train message bda2c595-1fc9-4876-8e67-eefb0df068b9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 19:11:33:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 19:30:02:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 19:30:02:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bb32330b-150f-48dd-ac61-ea67ae6f22ed
01/30/2025 19:30:02:INFO:Received: evaluate message bb32330b-150f-48dd-ac61-ea67ae6f22ed
[92mINFO [0m:      Sent reply
01/30/2025 19:34:09:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 19:34:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 19:34:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7a4ddfb5-b982-4b9f-98bd-d118d3f75d1b
01/30/2025 19:34:54:INFO:Received: train message 7a4ddfb5-b982-4b9f-98bd-d118d3f75d1b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 19:44:21:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 19:58:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 19:58:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8ebccb19-6233-4d6d-976a-a84ff36bd83d
01/30/2025 19:58:34:INFO:Received: evaluate message 8ebccb19-6233-4d6d-976a-a84ff36bd83d
[92mINFO [0m:      Sent reply
01/30/2025 20:02:48:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 20:03:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 20:03:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 9e228050-c95c-4f1a-a0f0-4b7f6cb20525
01/30/2025 20:03:23:INFO:Received: train message 9e228050-c95c-4f1a-a0f0-4b7f6cb20525
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 20:13:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 20:32:05:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 20:32:05:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e17d6e1e-548b-49dd-bacb-fd3d2a3e0191
01/30/2025 20:32:05:INFO:Received: evaluate message e17d6e1e-548b-49dd-bacb-fd3d2a3e0191
[92mINFO [0m:      Sent reply
01/30/2025 20:36:15:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 20:36:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 20:36:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ec8032bb-f1cd-46bf-9dbe-563cf0a282f5
01/30/2025 20:36:45:INFO:Received: train message ec8032bb-f1cd-46bf-9dbe-563cf0a282f5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 20:46:44:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 21:00:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 21:00:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 48229153-0e92-4d6e-ba12-7b60f555b9dd
01/30/2025 21:00:59:INFO:Received: evaluate message 48229153-0e92-4d6e-ba12-7b60f555b9dd
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 21:05:06:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 21:05:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 21:05:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 8c58701c-f419-4aae-9618-147f1204415a
01/30/2025 21:05:50:INFO:Received: train message 8c58701c-f419-4aae-9618-147f1204415a
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 21:16:06:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 21:32:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 21:32:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 58f4f58b-f95a-4e11-b148-0225f0394021
01/30/2025 21:32:40:INFO:Received: evaluate message 58f4f58b-f95a-4e11-b148-0225f0394021
[92mINFO [0m:      Sent reply
01/30/2025 21:36:57:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 21:37:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 21:37:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6747ab5f-526b-4639-8fb1-6f2c6b70da6b
01/30/2025 21:37:21:INFO:Received: train message 6747ab5f-526b-4639-8fb1-6f2c6b70da6b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 21:48:06:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 22:07:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 22:07:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 58a99e7e-9d23-4f99-beba-5f9646fb153b
01/30/2025 22:07:10:INFO:Received: evaluate message 58a99e7e-9d23-4f99-beba-5f9646fb153b
[92mINFO [0m:      Sent reply
01/30/2025 22:11:45:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 22:12:26:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 22:12:26:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 778f102b-5860-4749-90eb-f2190016b3dc
01/30/2025 22:12:26:INFO:Received: train message 778f102b-5860-4749-90eb-f2190016b3dc
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 22:24:06:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 22:47:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 22:47:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ff76dfcf-58c3-42f4-9e96-e6a3ea59f694
01/30/2025 22:47:21:INFO:Received: evaluate message ff76dfcf-58c3-42f4-9e96-e6a3ea59f694
[92mINFO [0m:      Sent reply
01/30/2025 22:51:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 22:52:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 22:52:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7b8f13d2-1492-429c-b1ac-76c5cec1c163
01/30/2025 22:52:41:INFO:Received: train message 7b8f13d2-1492-429c-b1ac-76c5cec1c163
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 23:03:09:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 23:19:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 23:19:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 8ffb39e6-f36f-4291-9fbf-e0c9e4183e65
01/30/2025 23:19:38:INFO:Received: evaluate message 8ffb39e6-f36f-4291-9fbf-e0c9e4183e65

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
[92mINFO [0m:      Sent reply
01/30/2025 23:24:24:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 23:25:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 23:25:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 67734942-3815-4c3c-b2b6-c21f35718a64
01/30/2025 23:25:27:INFO:Received: train message 67734942-3815-4c3c-b2b6-c21f35718a64
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 23:36:30:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 23:56:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 23:56:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0bef8e11-96bd-40a9-b6d7-ea6886346094
01/30/2025 23:56:43:INFO:Received: evaluate message 0bef8e11-96bd-40a9-b6d7-ea6886346094
[92mINFO [0m:      Sent reply
01/31/2025 00:01:26:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 00:02:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 00:02:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message eff3ee22-574e-4048-a9d9-b722086e71a6
01/31/2025 00:02:11:INFO:Received: train message eff3ee22-574e-4048-a9d9-b722086e71a6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 00:16:35:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 00:39:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 00:39:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message da542e18-0c96-46e0-877e-149a0aa7a736
01/31/2025 00:39:09:INFO:Received: evaluate message da542e18-0c96-46e0-877e-149a0aa7a736
[92mINFO [0m:      Sent reply
01/31/2025 00:43:36:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 00:44:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 00:44:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5cd8f1d5-f017-41f0-b298-4a0a26b577e4
01/31/2025 00:44:25:INFO:Received: train message 5cd8f1d5-f017-41f0-b298-4a0a26b577e4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 00:55:20:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 01:13:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 01:13:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 69091e22-b90e-4228-b63f-fddf0807da7b
01/31/2025 01:13:08:INFO:Received: evaluate message 69091e22-b90e-4228-b63f-fddf0807da7b
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 01:17:42:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 01:18:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 01:18:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message de05b26a-2034-4868-9604-573d3ea55151
01/31/2025 01:18:14:INFO:Received: train message de05b26a-2034-4868-9604-573d3ea55151
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 01:27:30:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 01:43:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 01:43:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ac438639-a3ac-40cb-8c52-1e5e77da2636
01/31/2025 01:43:00:INFO:Received: evaluate message ac438639-a3ac-40cb-8c52-1e5e77da2636
[92mINFO [0m:      Sent reply
01/31/2025 01:46:51:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 01:47:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 01:47:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 122f2ff7-4877-4733-84f9-9b39a9cc4801
01/31/2025 01:47:21:INFO:Received: train message 122f2ff7-4877-4733-84f9-9b39a9cc4801
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 01:56:22:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 02:12:22:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 02:12:22:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7fa630bc-657a-4992-8898-34ed492ebdd4
01/31/2025 02:12:22:INFO:Received: evaluate message 7fa630bc-657a-4992-8898-34ed492ebdd4
[92mINFO [0m:      Sent reply
01/31/2025 02:16:15:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 02:16:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 02:16:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6f61560c-5ad1-4312-bc79-b9e5ba9ce624
01/31/2025 02:16:34:INFO:Received: train message 6f61560c-5ad1-4312-bc79-b9e5ba9ce624
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 02:25:13:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 02:38:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 02:38:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 27b69a87-4ab1-4c2d-8a91-51a5d71eef3a
01/31/2025 02:38:11:INFO:Received: evaluate message 27b69a87-4ab1-4c2d-8a91-51a5d71eef3a

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 02:42:29:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 02:42:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 02:42:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c1291ebb-45a5-4e55-979c-8d1f2dfe21ed
01/31/2025 02:42:48:INFO:Received: train message c1291ebb-45a5-4e55-979c-8d1f2dfe21ed
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 02:51:44:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:06:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:06:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5fd0885b-bfd1-4fe3-be44-03a0a2451bdb
01/31/2025 03:06:32:INFO:Received: evaluate message 5fd0885b-bfd1-4fe3-be44-03a0a2451bdb
[92mINFO [0m:      Sent reply
01/31/2025 03:10:23:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:10:38:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:10:38:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3183491f-93a1-4582-bb94-9f8ccafc69ce
01/31/2025 03:10:38:INFO:Received: train message 3183491f-93a1-4582-bb94-9f8ccafc69ce
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 03:18:57:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:33:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:33:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message b7619548-bff3-4a99-8b9f-245fae7b3a6f
01/31/2025 03:33:17:INFO:Received: evaluate message b7619548-bff3-4a99-8b9f-245fae7b3a6f
[92mINFO [0m:      Sent reply
01/31/2025 03:37:16:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:37:40:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:37:40:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ce811016-d7b1-4923-8508-4e84525bfac4
01/31/2025 03:37:40:INFO:Received: train message ce811016-d7b1-4923-8508-4e84525bfac4
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 03:46:48:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:59:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:59:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2682d8dc-c035-4149-80f0-86610c005f2d
01/31/2025 03:59:39:INFO:Received: evaluate message 2682d8dc-c035-4149-80f0-86610c005f2d

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338, 1.3332027150074224], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742, 0.6544502617801047], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267, 0.9093262993126497]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 04:03:46:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 04:04:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 04:04:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 61d8e34a-abe3-4b2d-a105-4d8d7c73f503
01/31/2025 04:04:29:INFO:Received: train message 61d8e34a-abe3-4b2d-a105-4d8d7c73f503
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 04:13:39:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 04:28:28:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 04:28:28:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 2be465a5-60fb-4eb3-bb14-c36b44e44db2
01/31/2025 04:28:28:INFO:Received: evaluate message 2be465a5-60fb-4eb3-bb14-c36b44e44db2
[92mINFO [0m:      Sent reply
01/31/2025 04:32:43:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 04:32:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 04:32:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 12fc8a67-3983-484b-8049-2ff1d408ca73
01/31/2025 04:32:45:INFO:Received: reconnect message 12fc8a67-3983-484b-8049-2ff1d408ca73
01/31/2025 04:32:45:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/31/2025 04:32:45:INFO:Disconnect and shut down

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338, 1.3332027150074224, 1.3677619359352107], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742, 0.6544502617801047, 0.6480064438179621], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267, 0.9093262993126497, 0.9082844410647312]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.3968048095703125
Data Scaling Factor: 0.09716620960369952 where Client Data Size: 1807
Noise Multiplier after Fisher Scaling:  [0.005038570147007704, 0.012607920914888382, 0.004269896075129509, 0.00267989793792367, 0.007123640272766352, 0.001315119443461299, 0.0004480550123844296, 0.002910723676905036]
Noise Multiplier after list and tensor:  0.004549227935058298
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 1807, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338, 1.3332027150074224, 1.3677619359352107, 1.339125461219253], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742, 0.6544502617801047, 0.6480064438179621, 0.662505034232783], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267, 0.9093262993126497, 0.9082844410647312, 0.909541772155786]}



Final client history:
{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338, 1.3332027150074224, 1.3677619359352107, 1.339125461219253], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742, 0.6544502617801047, 0.6480064438179621, 0.662505034232783], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267, 0.9093262993126497, 0.9082844410647312, 0.909541772155786]}

