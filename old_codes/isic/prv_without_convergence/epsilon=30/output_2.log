nohup: ignoring input
Traceback (most recent call last):
  File "client_2.py", line 16, in <module>
    from flamby.datasets.fed_isic2019 import FedIsic2019
ModuleNotFoundError: No module named 'flamby'
nohup: ignoring input
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(mpl.__version__) >= "3.0":
/home/dgxuser16/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/dgxuser16/.local/lib/python3.8/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.
  mpl_cm.register_cmap(_name, _cmap)
/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adaptive_dp/prv/epsilon=30/flamby/datasets/fed_isic2019/dataset.py:222: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  albumentations.Flip(p=0.5),
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_holes` is deprecated. Use num_holes_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_height` is deprecated. Use hole_height_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/pydantic/main.py:214: DeprecationWarning: `max_width` is deprecated. Use hole_width_range instead.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
/home/dgxuser16/.local/lib/python3.8/site-packages/albumentations/core/validation.py:45: DeprecationWarning: always_apply is deprecated. Use `p=1` if you want to always apply the transform. self.p will be set to 1.
  original_init(self, **validated_kwargs)
01/30/2025 12:34:57:DEBUG:Opened insecure gRPC connection (no certificates were passed)
01/30/2025 12:34:57:DEBUG:ChannelConnectivity.IDLE
01/30/2025 12:34:57:DEBUG:ChannelConnectivity.READY
[92mINFO [0m:      
01/30/2025 12:40:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 12:40:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message d923ed7d-1250-47d6-9a8f-b945315f087d
01/30/2025 12:40:07:INFO:Received: train message d923ed7d-1250-47d6-9a8f-b945315f087d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 12:54:37:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 13:06:48:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 13:06:48:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 30479ebb-ed0f-4ab6-a1c7-727cd2ab3b39
01/30/2025 13:06:48:INFO:Received: evaluate message 30479ebb-ed0f-4ab6-a1c7-727cd2ab3b39
[92mINFO [0m:      Sent reply
01/30/2025 13:10:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 13:11:23:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 13:11:23:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message a6f88b80-2647-4913-96f6-67d5df599956
01/30/2025 13:11:23:INFO:Received: train message a6f88b80-2647-4913-96f6-67d5df599956
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 13:25:45:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 13:40:34:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 13:40:34:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a51de214-0ea6-43a2-9bab-e0b4947d9902
01/30/2025 13:40:34:INFO:Received: evaluate message a51de214-0ea6-43a2-9bab-e0b4947d9902
[92mINFO [0m:      Sent reply
01/30/2025 13:44:34:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 13:45:37:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 13:45:37:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b6b2f48e-3e8b-4fee-90cf-f38a35b715a5
01/30/2025 13:45:37:INFO:Received: train message b6b2f48e-3e8b-4fee-90cf-f38a35b715a5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 13:59:22:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 14:09:53:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 14:09:53:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 64098f35-495a-4341-b746-c1fcc1453e6a
01/30/2025 14:09:53:INFO:Received: evaluate message 64098f35-495a-4341-b746-c1fcc1453e6a
[92mINFO [0m:      Sent reply
01/30/2025 14:14:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 14:14:36:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 14:14:36:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 150e0002-69b6-48f6-96dc-343c5496857e
01/30/2025 14:14:36:INFO:Received: train message 150e0002-69b6-48f6-96dc-343c5496857e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 14:29:24:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 14:44:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 14:44:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message e36a780c-80b0-4982-b031-1b5cafd33fd5
01/30/2025 14:44:21:INFO:Received: evaluate message e36a780c-80b0-4982-b031-1b5cafd33fd5
[92mINFO [0m:      Sent reply
01/30/2025 14:48:26:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 14:49:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 14:49:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ed83bf83-36fd-4de2-8d34-4ed0951867d5
01/30/2025 14:49:13:INFO:Received: train message ed83bf83-36fd-4de2-8d34-4ed0951867d5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 15:03:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 15:16:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 15:16:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 047508e8-ff3b-4b71-bf4b-85dc85189403
01/30/2025 15:16:33:INFO:Received: evaluate message 047508e8-ff3b-4b71-bf4b-85dc85189403
[92mINFO [0m:      Sent reply
01/30/2025 15:20:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 15:21:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 15:21:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 06de5816-bbcc-41db-aba3-350a5620f6f9
01/30/2025 15:21:15:INFO:Received: train message 06de5816-bbcc-41db-aba3-350a5620f6f9
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 15:35:50:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 15:46:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 15:46:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message f0276713-e992-43e5-8c78-6ea0beb5db78
01/30/2025 15:46:25:INFO:Received: evaluate message f0276713-e992-43e5-8c78-6ea0beb5db78
[92mINFO [0m:      Sent reply
01/30/2025 15:50:35:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 15:51:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 15:51:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ac662d9b-e2ea-4ede-b851-aebc708bd55b
01/30/2025 15:51:10:INFO:Received: train message ac662d9b-e2ea-4ede-b851-aebc708bd55b
['/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adaptive_dp/prv/epsilon=30', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python38.zip', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/lib-dynload', '/home/dgxuser16/.local/lib/python3.8/site-packages', '/home/dgxuser16/anaconda3/envs/ihpc/lib/python3.8/site-packages', '/raid/home/dgxuser16/NTL/mccarthy/ahmad/github/adaptive_privacy_fl/isic/adaptive_dp/prv/epsilon=30']
Params:
 batch_size: 32, local_epochs: 3, full_dataset_size: 18597, num_classes: 8

Privacy Params:
 epsilon: 30.0, target_epsilon: 30.0, target_delta: 1e-05

Device: cuda:0

Step 1: Client Initialized
Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671], 'accuracy': [0.5730970600080548], 'auc': [0.8431309076755067]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373], 'accuracy': [0.5730970600080548, 0.6198147402335884], 'auc': [0.8431309076755067, 0.8753916239274806]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 16:05:51:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 16:21:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 16:21:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message bdbcaff8-8978-48fc-bc00-37e8192f7baa
01/30/2025 16:21:06:INFO:Received: evaluate message bdbcaff8-8978-48fc-bc00-37e8192f7baa
[92mINFO [0m:      Sent reply
01/30/2025 16:25:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 16:25:21:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 16:25:21:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 6af7b34e-2af1-4fc0-8efe-608993446b60
01/30/2025 16:25:21:INFO:Received: train message 6af7b34e-2af1-4fc0-8efe-608993446b60
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 16:39:31:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 16:49:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 16:49:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a816fccf-5f69-4cfa-8286-4f1d1b616683
01/30/2025 16:49:11:INFO:Received: evaluate message a816fccf-5f69-4cfa-8286-4f1d1b616683
[92mINFO [0m:      Sent reply
01/30/2025 16:53:22:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 16:54:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 16:54:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1b38ffb9-803a-46d1-accc-cadc89ba2267
01/30/2025 16:54:14:INFO:Received: train message 1b38ffb9-803a-46d1-accc-cadc89ba2267
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 17:09:01:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 17:26:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 17:26:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message a23319f1-c779-41b4-956a-278a83ecaf0a
01/30/2025 17:26:00:INFO:Received: evaluate message a23319f1-c779-41b4-956a-278a83ecaf0a
[92mINFO [0m:      Sent reply
01/30/2025 17:30:13:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 17:30:43:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 17:30:43:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ed68eced-0de4-4aad-8943-6caa9849959b
01/30/2025 17:30:43:INFO:Received: train message ed68eced-0de4-4aad-8943-6caa9849959b
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 17:44:28:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 17:54:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 17:54:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message ba5e6ee3-ac9b-427f-b794-4b1ef8100c6f
01/30/2025 17:54:55:INFO:Received: evaluate message ba5e6ee3-ac9b-427f-b794-4b1ef8100c6f
[92mINFO [0m:      Sent reply
01/30/2025 17:59:03:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 17:59:39:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 17:59:39:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 1232926e-bc1c-4164-a850-980d342bd5d5
01/30/2025 17:59:39:INFO:Received: train message 1232926e-bc1c-4164-a850-980d342bd5d5
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 18:14:17:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 18:27:55:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 18:27:55:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 178ed034-84bd-456e-a8ca-61d1e2640470
01/30/2025 18:27:55:INFO:Received: evaluate message 178ed034-84bd-456e-a8ca-61d1e2640470
[92mINFO [0m:      Sent reply
01/30/2025 18:31:57:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 18:32:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 18:32:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 84a1218b-572a-4c10-b150-8264608e6155
01/30/2025 18:32:29:INFO:Received: train message 84a1218b-572a-4c10-b150-8264608e6155
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 18:46:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 18:56:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 18:56:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 22bfa8e0-640c-4aa2-9d5c-32ce289ef8ba
01/30/2025 18:56:09:INFO:Received: evaluate message 22bfa8e0-640c-4aa2-9d5c-32ce289ef8ba
[92mINFO [0m:      Sent reply
01/30/2025 19:00:02:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 19:01:30:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 19:01:30:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5e66b111-9dcf-4e8f-ba80-b6c1b06cb87c
01/30/2025 19:01:30:INFO:Received: train message 5e66b111-9dcf-4e8f-ba80-b6c1b06cb87c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 19:16:22:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 19:30:12:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 19:30:12:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message dff1953c-f32c-4bd8-a4e4-c5874166fa59
01/30/2025 19:30:12:INFO:Received: evaluate message dff1953c-f32c-4bd8-a4e4-c5874166fa59
[92mINFO [0m:      Sent reply
01/30/2025 19:34:19:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 19:34:54:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 19:34:54:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message daf8c471-3f0e-4cdf-8ec0-1d2fdc100c5f
01/30/2025 19:34:54:INFO:Received: train message daf8c471-3f0e-4cdf-8ec0-1d2fdc100c5f
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 19:48:48:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 19:58:25:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 19:58:25:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 7c77b919-e461-4076-b6e5-085ebd7234ed
01/30/2025 19:58:25:INFO:Received: evaluate message 7c77b919-e461-4076-b6e5-085ebd7234ed
[92mINFO [0m:      Sent reply
01/30/2025 20:02:39:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 20:03:06:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 20:03:06:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 162bd9d5-5f55-4a25-9931-98faed64b19e
01/30/2025 20:03:06:INFO:Received: train message 162bd9d5-5f55-4a25-9931-98faed64b19e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 20:17:57:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 20:32:00:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 20:32:00:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message aef2e7b8-a606-41b9-9677-c71345ef84a9
01/30/2025 20:32:00:INFO:Received: evaluate message aef2e7b8-a606-41b9-9677-c71345ef84a9
[92mINFO [0m:      Sent reply
01/30/2025 20:35:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 20:36:51:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 20:36:51:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ed5043a0-88de-427c-99cb-e62b6446fa19
01/30/2025 20:36:51:INFO:Received: train message ed5043a0-88de-427c-99cb-e62b6446fa19
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 20:51:07:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 21:00:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 21:00:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 00c7bd37-2ee9-4a1a-aaf8-65b097a04619
01/30/2025 21:00:56:INFO:Received: evaluate message 00c7bd37-2ee9-4a1a-aaf8-65b097a04619

Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/30/2025 21:05:00:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 21:05:41:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 21:05:41:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 18e3f14e-2eef-4dfb-b377-6216abe4e126
01/30/2025 21:05:41:INFO:Received: train message 18e3f14e-2eef-4dfb-b377-6216abe4e126
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 21:20:27:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 21:32:35:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 21:32:35:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 0fad0145-abc0-47d8-a494-a055c65e7bff
01/30/2025 21:32:35:INFO:Received: evaluate message 0fad0145-abc0-47d8-a494-a055c65e7bff
[92mINFO [0m:      Sent reply
01/30/2025 21:36:58:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 21:37:10:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 21:37:10:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cc4882d6-09fd-42a7-ac01-11b83d10c02c
01/30/2025 21:37:10:INFO:Received: train message cc4882d6-09fd-42a7-ac01-11b83d10c02c
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 21:53:13:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 22:07:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 22:07:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c009e386-2dba-493e-ab4e-5e1282fae625
01/30/2025 22:07:14:INFO:Received: evaluate message c009e386-2dba-493e-ab4e-5e1282fae625
[92mINFO [0m:      Sent reply
01/30/2025 22:11:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 22:12:32:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 22:12:32:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 5751f080-ea9d-48d1-b6ce-b60dcc3903ba
01/30/2025 22:12:32:INFO:Received: train message 5751f080-ea9d-48d1-b6ce-b60dcc3903ba
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 22:30:18:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 22:47:09:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 22:47:09:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message c1f0d7d0-b968-495e-8d5f-2be6d11d2225
01/30/2025 22:47:09:INFO:Received: evaluate message c1f0d7d0-b968-495e-8d5f-2be6d11d2225
[92mINFO [0m:      Sent reply
01/30/2025 22:51:23:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 22:52:44:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 22:52:44:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message cc44540d-8b09-4e70-abca-56c789c78931
01/30/2025 22:52:44:INFO:Received: train message cc44540d-8b09-4e70-abca-56c789c78931
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 23:07:44:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 23:19:59:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 23:19:59:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 98904606-e0b1-45ee-acd2-c04aa496e124
01/30/2025 23:19:59:INFO:Received: evaluate message 98904606-e0b1-45ee-acd2-c04aa496e124

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
[92mINFO [0m:      Sent reply
01/30/2025 23:24:47:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 23:25:29:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 23:25:29:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 0c1f4d8a-9650-4d6d-a2bb-dfe0a7fdc422
01/30/2025 23:25:29:INFO:Received: train message 0c1f4d8a-9650-4d6d-a2bb-dfe0a7fdc422
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/30/2025 23:41:45:INFO:Sent reply
[92mINFO [0m:      
01/30/2025 23:56:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/30/2025 23:56:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 609e456c-7a22-4db4-b977-d03a029dfc4f
01/30/2025 23:56:33:INFO:Received: evaluate message 609e456c-7a22-4db4-b977-d03a029dfc4f
[92mINFO [0m:      Sent reply
01/31/2025 00:01:26:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 00:01:49:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 00:01:49:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 538de51d-f559-4e52-8a8a-c1b57a15284e
01/31/2025 00:01:49:INFO:Received: train message 538de51d-f559-4e52-8a8a-c1b57a15284e
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 00:22:12:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 00:38:58:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 00:38:58:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 4e4b9e31-a600-4d97-971c-71df0ce59d20
01/31/2025 00:38:58:INFO:Received: evaluate message 4e4b9e31-a600-4d97-971c-71df0ce59d20
[92mINFO [0m:      Sent reply
01/31/2025 00:43:28:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 00:44:17:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 00:44:17:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message c4e4dc39-0077-4664-ae92-425bf297b973
01/31/2025 00:44:17:INFO:Received: train message c4e4dc39-0077-4664-ae92-425bf297b973
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 00:59:29:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 01:13:14:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 01:13:14:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 1029e640-1f06-40da-9367-f50ee68cbaa2
01/31/2025 01:13:14:INFO:Received: evaluate message 1029e640-1f06-40da-9367-f50ee68cbaa2
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 01:17:42:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 01:18:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 01:18:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message b0537c5b-0230-4c26-a5db-d312e190b1f6
01/31/2025 01:18:08:INFO:Received: train message b0537c5b-0230-4c26-a5db-d312e190b1f6
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 01:31:39:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 01:43:01:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 01:43:01:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 9569907a-9b8a-4a43-b342-2920e340b87f
01/31/2025 01:43:01:INFO:Received: evaluate message 9569907a-9b8a-4a43-b342-2920e340b87f
[92mINFO [0m:      Sent reply
01/31/2025 01:46:53:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 01:47:11:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 01:47:11:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 3ffc4203-d25f-4240-8e5a-e3e14f0b8640
01/31/2025 01:47:11:INFO:Received: train message 3ffc4203-d25f-4240-8e5a-e3e14f0b8640
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 02:00:05:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 02:12:07:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 02:12:07:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 966d0eee-37ad-4616-874f-2f84122a29ee
01/31/2025 02:12:07:INFO:Received: evaluate message 966d0eee-37ad-4616-874f-2f84122a29ee
[92mINFO [0m:      Sent reply
01/31/2025 02:16:07:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 02:16:46:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 02:16:46:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 4541ccb6-a915-49c7-acd2-7f062efee495
01/31/2025 02:16:46:INFO:Received: train message 4541ccb6-a915-49c7-acd2-7f062efee495
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 02:29:33:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 02:38:15:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 02:38:15:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 3458e6cc-b7df-492c-9fdf-c2d1b1ee332f
01/31/2025 02:38:15:INFO:Received: evaluate message 3458e6cc-b7df-492c-9fdf-c2d1b1ee332f

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 02:42:34:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 02:43:08:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 02:43:08:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ab493c6d-5052-4ec2-b7b1-e2685553ce1d
01/31/2025 02:43:08:INFO:Received: train message ab493c6d-5052-4ec2-b7b1-e2685553ce1d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 02:56:12:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:06:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:06:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 6c3f341c-3ad1-4751-89fb-50c22a864547
01/31/2025 03:06:33:INFO:Received: evaluate message 6c3f341c-3ad1-4751-89fb-50c22a864547
[92mINFO [0m:      Sent reply
01/31/2025 03:10:25:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:10:56:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:10:56:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message ec72d39e-fec6-4ba6-8057-39eb2ad1940d
01/31/2025 03:10:56:INFO:Received: train message ec72d39e-fec6-4ba6-8057-39eb2ad1940d
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 03:24:12:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:33:13:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:33:13:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 756d1d77-3560-4706-943a-5143df7e331c
01/31/2025 03:33:13:INFO:Received: evaluate message 756d1d77-3560-4706-943a-5143df7e331c
[92mINFO [0m:      Sent reply
01/31/2025 03:37:13:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:37:47:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:37:47:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message f1ef9b87-b64e-430b-80c3-8e01f4f9e761
01/31/2025 03:37:47:INFO:Received: train message f1ef9b87-b64e-430b-80c3-8e01f4f9e761
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 03:51:18:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 03:59:50:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 03:59:50:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message 5f202c9c-6b8a-498b-85d3-ff8fffe44068
01/31/2025 03:59:50:INFO:Received: evaluate message 5f202c9c-6b8a-498b-85d3-ff8fffe44068

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338, 1.3332027150074224], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742, 0.6544502617801047], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267, 0.9093262993126497]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally
[92mINFO [0m:      Sent reply
01/31/2025 04:03:57:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 04:04:27:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 04:04:27:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: train message 7aebeea4-cfe4-45aa-8194-da4d92a63f36
01/31/2025 04:04:27:INFO:Received: train message 7aebeea4-cfe4-45aa-8194-da4d92a63f36
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/dgxuser16/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[92mINFO [0m:      Sent reply
01/31/2025 04:17:46:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 04:28:33:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 04:28:33:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: evaluate message d516da86-3b1e-4028-a774-e2ef20f0e913
01/31/2025 04:28:33:INFO:Received: evaluate message d516da86-3b1e-4028-a774-e2ef20f0e913
[92mINFO [0m:      Sent reply
01/31/2025 04:32:45:INFO:Sent reply
[92mINFO [0m:      
01/31/2025 04:32:45:INFO:
[92mINFO [0m:      [RUN 0, ROUND ]
01/31/2025 04:32:45:INFO:[RUN 0, ROUND ]
[92mINFO [0m:      Received: reconnect message 64ffb3d2-e451-47a8-83d5-c1bab42c13b1
01/31/2025 04:32:45:INFO:Received: reconnect message 64ffb3d2-e451-47a8-83d5-c1bab42c13b1
01/31/2025 04:32:45:DEBUG:gRPC channel closed
[92mINFO [0m:      Disconnect and shut down
01/31/2025 04:32:45:INFO:Disconnect and shut down

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338, 1.3332027150074224, 1.3677619359352107], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742, 0.6544502617801047, 0.6480064438179621], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267, 0.9093262993126497, 0.9082844410647312]}

Step 2a: Compute base noise multiplier
Step 2b: Update noise multiplier dynamically
Base Noise Multiplier Received:  0.4148101806640625
Data Scaling Factor: 0.1700811958918105 where Client Data Size: 3163
Noise Multiplier after Fisher Scaling:  [0.0031850594095885754, 0.047220539301633835, 0.0007901704520918429, 0.001000023097731173, 0.0024021491408348083, 0.0009426761534996331, 0.0011858722427859902, 0.002462478121742606]
Noise Multiplier after list and tensor:  0.007398620989988558
Step 2c: Re-initialize PrivacyEngine
Step 2d: Make model private
Step 2e: Perform local DP training
Training the model with the following parameters:
Epochs: 3, Trainloader Size: 3163, Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Step 2f: Log training details
Step 3: Evaluate the model locally

{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338, 1.3332027150074224, 1.3677619359352107, 1.339125461219253], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742, 0.6544502617801047, 0.6480064438179621, 0.662505034232783], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267, 0.9093262993126497, 0.9082844410647312, 0.909541772155786]}



Final client history:
{'loss': [1.4084583621710671, 1.2725621834667373, 1.3168729983219158, 1.2999527233529369, 1.3566342845253054, 1.2916058710252283, 1.2950427756163558, 1.2889692427120139, 1.2903667422873428, 1.2500551472351622, 1.2559858752602875, 1.3230857665722187, 1.3555577852483052, 1.3457826459739077, 1.3232904260455243, 1.3242473327916715, 1.313639383113437, 1.3134255881660943, 1.3392219109444958, 1.3088060808047335, 1.3166004419806885, 1.3150658368392187, 1.3448244878422864, 1.3660201151861666, 1.2957598722321921, 1.3263701412930298, 1.3108451827322338, 1.3332027150074224, 1.3677619359352107, 1.339125461219253], 'accuracy': [0.5730970600080548, 0.6198147402335884, 0.6254530809504631, 0.6318968989126057, 0.6194120016109544, 0.6411598872331856, 0.6383407168747482, 0.6391461941200162, 0.6459927507047926, 0.6552557390253725, 0.6548530004027386, 0.6459927507047926, 0.6419653644784535, 0.63954893274265, 0.6500201369311317, 0.6528393072895691, 0.657269432138542, 0.6604913411196134, 0.6459927507047926, 0.6524365686669351, 0.6536447845348369, 0.6592831252517116, 0.657269432138542, 0.6407571486105518, 0.6645187273459525, 0.657269432138542, 0.6564639548932742, 0.6544502617801047, 0.6480064438179621, 0.662505034232783], 'auc': [0.8431309076755067, 0.8753916239274806, 0.8817125205443745, 0.8885409477384878, 0.8892646095089933, 0.8932429062129719, 0.8958702539574019, 0.8970047184087203, 0.8980630585913656, 0.9009691131054357, 0.9018502466011953, 0.9015229232601887, 0.9009483925717396, 0.9029505918321159, 0.9028824403218962, 0.9043968804997253, 0.9044591957842897, 0.9061994477667014, 0.9053336265662957, 0.9052943786084139, 0.9053842388870433, 0.9071768501281737, 0.9066254137631117, 0.9052926947036205, 0.9083116935925082, 0.9073901757739193, 0.9088297216664267, 0.9093262993126497, 0.9082844410647312, 0.909541772155786]}

